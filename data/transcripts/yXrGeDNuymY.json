{
  "video": {
    "video_id": "yXrGeDNuymY",
    "title": "Loading pre-trained weights from OpenAI GPT-2",
    "duration": 3021.0,
    "index": 31
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 7.72
    },
    {
      "text": "hello everyone and welcome to this",
      "start": 5.2,
      "duration": 4.519
    },
    {
      "text": "lecture in the build large language",
      "start": 7.72,
      "duration": 5.48
    },
    {
      "text": "models from scratch Series today I am",
      "start": 9.719,
      "duration": 5.761
    },
    {
      "text": "very excited for this lecture because we",
      "start": 13.2,
      "duration": 4.44
    },
    {
      "text": "are going to look at how to load",
      "start": 15.48,
      "duration": 5.84
    },
    {
      "text": "pre-trained open AI weights into the GPT",
      "start": 17.64,
      "duration": 5.52
    },
    {
      "text": "model which we have",
      "start": 21.32,
      "duration": 5.279
    },
    {
      "text": "constructed so that the text generation",
      "start": 23.16,
      "duration": 4.84
    },
    {
      "text": "is",
      "start": 26.599,
      "duration": 3.84
    },
    {
      "text": "coherent this lecture will serve as a",
      "start": 28.0,
      "duration": 4.439
    },
    {
      "text": "culmination of all the hard work which",
      "start": 30.439,
      "duration": 4.761
    },
    {
      "text": "we have put in in the previous lecture",
      "start": 32.439,
      "duration": 5.521
    },
    {
      "text": "previous lectures U because we are going",
      "start": 35.2,
      "duration": 5.359
    },
    {
      "text": "to use the exact GPT architecture which",
      "start": 37.96,
      "duration": 5.079
    },
    {
      "text": "we have built ourselves using this",
      "start": 40.559,
      "duration": 4.441
    },
    {
      "text": "schematic which I'm showing you right",
      "start": 43.039,
      "duration": 4.601
    },
    {
      "text": "now and the GPT model class which we",
      "start": 45.0,
      "duration": 5.36
    },
    {
      "text": "have defined in this lecture series and",
      "start": 47.64,
      "duration": 5.239
    },
    {
      "text": "we are going to integrate this model",
      "start": 50.36,
      "duration": 6.519
    },
    {
      "text": "class with the gpt2 open AI weights",
      "start": 52.879,
      "duration": 7.32
    },
    {
      "text": "which they have publicly released",
      "start": 56.879,
      "duration": 5.401
    },
    {
      "text": "so let's get started with today's",
      "start": 60.199,
      "duration": 4.68
    },
    {
      "text": "lecture first let me cover what all we",
      "start": 62.28,
      "duration": 6.199
    },
    {
      "text": "have completed so far until now what we",
      "start": 64.879,
      "duration": 5.92
    },
    {
      "text": "saw in this pre-training series is that",
      "start": 68.479,
      "duration": 4.521
    },
    {
      "text": "we first saw how to define the losses",
      "start": 70.799,
      "duration": 5.241
    },
    {
      "text": "for a large language model how the cross",
      "start": 73.0,
      "duration": 5.4
    },
    {
      "text": "entropy loss comes into the picture to",
      "start": 76.04,
      "duration": 5.52
    },
    {
      "text": "find the loss between the",
      "start": 78.4,
      "duration": 6.16
    },
    {
      "text": "llm predicted output and the target",
      "start": 81.56,
      "duration": 5.32
    },
    {
      "text": "sentence then we looked at the llm",
      "start": 84.56,
      "duration": 4.76
    },
    {
      "text": "pre-training Loop itself and we",
      "start": 86.88,
      "duration": 4.52
    },
    {
      "text": "generated output text using our",
      "start": 89.32,
      "duration": 4.56
    },
    {
      "text": "pre-training Loop the text which was",
      "start": 91.4,
      "duration": 5.2
    },
    {
      "text": "generated from our Loop was not not very",
      "start": 93.88,
      "duration": 5.44
    },
    {
      "text": "coherent so let me just show you uh the",
      "start": 96.6,
      "duration": 4.4
    },
    {
      "text": "text which was generated using our",
      "start": 99.32,
      "duration": 3.56
    },
    {
      "text": "training so this was the training Loop",
      "start": 101.0,
      "duration": 4.36
    },
    {
      "text": "which we had and here you see that the",
      "start": 102.88,
      "duration": 5.04
    },
    {
      "text": "input was every effort moves you and the",
      "start": 105.36,
      "duration": 5.079
    },
    {
      "text": "output was not very coherent it was not",
      "start": 107.92,
      "duration": 5.6
    },
    {
      "text": "making too much sense and that is",
      "start": 110.439,
      "duration": 4.881
    },
    {
      "text": "understandable because we just we had",
      "start": 113.52,
      "duration": 4.68
    },
    {
      "text": "just trained this on one small book with",
      "start": 115.32,
      "duration": 4.88
    },
    {
      "text": "a which was a very small data set and we",
      "start": 118.2,
      "duration": 4.839
    },
    {
      "text": "had run it for 10ox later what we saw",
      "start": 120.2,
      "duration": 5.72
    },
    {
      "text": "was we integrated decoding strategies to",
      "start": 123.039,
      "duration": 5.72
    },
    {
      "text": "reduce overfitting the first strategy we",
      "start": 125.92,
      "duration": 5.48
    },
    {
      "text": "looked at was temperature scaling and",
      "start": 128.759,
      "duration": 4.521
    },
    {
      "text": "then the second strategy which we looked",
      "start": 131.4,
      "duration": 5.199
    },
    {
      "text": "at was topk sampling introduction of",
      "start": 133.28,
      "duration": 5.319
    },
    {
      "text": "these two strategies definitely reduced",
      "start": 136.599,
      "duration": 4.561
    },
    {
      "text": "overfitting a bit but even then the next",
      "start": 138.599,
      "duration": 4.36
    },
    {
      "text": "sentence which was generated as you can",
      "start": 141.16,
      "duration": 3.64
    },
    {
      "text": "see over here did not really make too",
      "start": 142.959,
      "duration": 4.64
    },
    {
      "text": "much sense and now we have come to",
      "start": 144.8,
      "duration": 4.439
    },
    {
      "text": "today's lecture where we are going to",
      "start": 147.599,
      "duration": 4.441
    },
    {
      "text": "load the pre-trained weights from open",
      "start": 149.239,
      "duration": 6.161
    },
    {
      "text": "AI in in an in in the hope that with the",
      "start": 152.04,
      "duration": 6.04
    },
    {
      "text": "pre-trained weights from openai the next",
      "start": 155.4,
      "duration": 4.68
    },
    {
      "text": "tokens which are generated or the next",
      "start": 158.08,
      "duration": 3.799
    },
    {
      "text": "Tok or the next sentences which are",
      "start": 160.08,
      "duration": 4.68
    },
    {
      "text": "generated they start making a lot more",
      "start": 161.879,
      "duration": 5.28
    },
    {
      "text": "sense we are going to spend a lot of",
      "start": 164.76,
      "duration": 4.36
    },
    {
      "text": "time in today's lecture in understanding",
      "start": 167.159,
      "duration": 3.561
    },
    {
      "text": "first of all how to download these",
      "start": 169.12,
      "duration": 4.399
    },
    {
      "text": "weights from open a second of all how to",
      "start": 170.72,
      "duration": 5.28
    },
    {
      "text": "integrate these weights with our code",
      "start": 173.519,
      "duration": 5.161
    },
    {
      "text": "which we have written already so let's",
      "start": 176.0,
      "duration": 5.48
    },
    {
      "text": "Dive Right into today to lecture first I",
      "start": 178.68,
      "duration": 6.08
    },
    {
      "text": "would like to thank open for releasing",
      "start": 181.48,
      "duration": 5.96
    },
    {
      "text": "these weights for gpt2 the training",
      "start": 184.76,
      "duration": 4.68
    },
    {
      "text": "itself might have taken around millions",
      "start": 187.44,
      "duration": 3.84
    },
    {
      "text": "of dollars because the training has been",
      "start": 189.44,
      "duration": 4.359
    },
    {
      "text": "done on a huge amount of data set and",
      "start": 191.28,
      "duration": 4.64
    },
    {
      "text": "these gpt2 weights are now publicly",
      "start": 193.799,
      "duration": 5.16
    },
    {
      "text": "available even on platforms such as",
      "start": 195.92,
      "duration": 6.16
    },
    {
      "text": "gagle so without access to this weights",
      "start": 198.959,
      "duration": 4.601
    },
    {
      "text": "today's lecture would not have been",
      "start": 202.08,
      "duration": 3.96
    },
    {
      "text": "possible and I would just like to take",
      "start": 203.56,
      "duration": 5.84
    },
    {
      "text": "this opportunity to thank all of the llm",
      "start": 206.04,
      "duration": 5.04
    },
    {
      "text": "company who are in the open source",
      "start": 209.4,
      "duration": 3.559
    },
    {
      "text": "domain especially and who are publicly",
      "start": 211.08,
      "duration": 4.12
    },
    {
      "text": "making available all the weights which",
      "start": 212.959,
      "duration": 4.48
    },
    {
      "text": "are used by these",
      "start": 215.2,
      "duration": 5.36
    },
    {
      "text": "models okay so I'm going to take you to",
      "start": 217.439,
      "duration": 5.241
    },
    {
      "text": "directly to code right now and let me",
      "start": 220.56,
      "duration": 3.959
    },
    {
      "text": "start explaining to you the different",
      "start": 222.68,
      "duration": 5.72
    },
    {
      "text": "steps in the code so previously for",
      "start": 224.519,
      "duration": 5.92
    },
    {
      "text": "educational purposes we trained a small",
      "start": 228.4,
      "duration": 4.44
    },
    {
      "text": "gpt2 model right which used a very",
      "start": 230.439,
      "duration": 5.64
    },
    {
      "text": "limited data set it was just a book this",
      "start": 232.84,
      "duration": 4.88
    },
    {
      "text": "approach allowed us to focus on the",
      "start": 236.079,
      "duration": 4.44
    },
    {
      "text": "fundamentals but we did not get coherent",
      "start": 237.72,
      "duration": 6.359
    },
    {
      "text": "text as the our prediction what we are",
      "start": 240.519,
      "duration": 5.681
    },
    {
      "text": "going to do is that open a has",
      "start": 244.079,
      "duration": 3.561
    },
    {
      "text": "fortunately shared their weights",
      "start": 246.2,
      "duration": 3.48
    },
    {
      "text": "publicly so we are going to load these",
      "start": 247.64,
      "duration": 5.12
    },
    {
      "text": "weights into our GPT model class and we",
      "start": 249.68,
      "duration": 5.479
    },
    {
      "text": "are going to use this model class itself",
      "start": 252.76,
      "duration": 4.84
    },
    {
      "text": "for the next text",
      "start": 255.159,
      "duration": 5.121
    },
    {
      "text": "Generation Um one thing to note before",
      "start": 257.6,
      "duration": 5.2
    },
    {
      "text": "we get started is that open originally",
      "start": 260.28,
      "duration": 5.4
    },
    {
      "text": "saved the gpt2 weights via tensor flow",
      "start": 262.8,
      "duration": 4.56
    },
    {
      "text": "whereas all of the coding which we have",
      "start": 265.68,
      "duration": 4.12
    },
    {
      "text": "done so far in this lecture series is",
      "start": 267.36,
      "duration": 5.52
    },
    {
      "text": "using pytorch so we will have to install",
      "start": 269.8,
      "duration": 5.48
    },
    {
      "text": "tensor flow and one more Library which",
      "start": 272.88,
      "duration": 4.4
    },
    {
      "text": "we are going to install is called tqdm",
      "start": 275.28,
      "duration": 4.759
    },
    {
      "text": "to track the download progress so here",
      "start": 277.28,
      "duration": 4.639
    },
    {
      "text": "you can see that in this line of code",
      "start": 280.039,
      "duration": 3.961
    },
    {
      "text": "I'm installing tensor flow and I'm also",
      "start": 281.919,
      "duration": 4.321
    },
    {
      "text": "installing the tqdm library the",
      "start": 284.0,
      "duration": 4.24
    },
    {
      "text": "tensorflow version should be greater",
      "start": 286.24,
      "duration": 5.16
    },
    {
      "text": "than or equal to 2.15 and the tqdm",
      "start": 288.24,
      "duration": 4.84
    },
    {
      "text": "library version should be greater than",
      "start": 291.4,
      "duration": 3.04
    },
    {
      "text": "or equal to",
      "start": 293.08,
      "duration": 3.92
    },
    {
      "text": "4.66 so you can install both of these",
      "start": 294.44,
      "duration": 4.88
    },
    {
      "text": "two libraries and then import them so",
      "start": 297.0,
      "duration": 4.36
    },
    {
      "text": "here I printed the tensorflow version",
      "start": 299.32,
      "duration": 4.68
    },
    {
      "text": "and the tqdm version on my",
      "start": 301.36,
      "duration": 5.36
    },
    {
      "text": "system once you install this these two",
      "start": 304.0,
      "duration": 4.96
    },
    {
      "text": "libraries you can you can pause the",
      "start": 306.72,
      "duration": 3.8
    },
    {
      "text": "video for the time being and then you",
      "start": 308.96,
      "duration": 4.239
    },
    {
      "text": "can continue with the rest of this video",
      "start": 310.52,
      "duration": 5.2
    },
    {
      "text": "now comes the very important step of",
      "start": 313.199,
      "duration": 7.361
    },
    {
      "text": "downloading the gpt2 weights and their",
      "start": 315.72,
      "duration": 7.08
    },
    {
      "text": "parameters this is a step which still",
      "start": 320.56,
      "duration": 4.6
    },
    {
      "text": "might be complex to so many students and",
      "start": 322.8,
      "duration": 4.36
    },
    {
      "text": "Engineers so I want to explain this in a",
      "start": 325.16,
      "duration": 4.28
    },
    {
      "text": "lot of detail first of all when you go",
      "start": 327.16,
      "duration": 4.92
    },
    {
      "text": "to platforms like kagle you will find",
      "start": 329.44,
      "duration": 5.039
    },
    {
      "text": "files to download so these are the seven",
      "start": 332.08,
      "duration": 5.0
    },
    {
      "text": "files um which contain all of the",
      "start": 334.479,
      "duration": 4.241
    },
    {
      "text": "information basically and you'll see",
      "start": 337.08,
      "duration": 3.52
    },
    {
      "text": "that the file size is about 500",
      "start": 338.72,
      "duration": 4.28
    },
    {
      "text": "megabytes but after you download this",
      "start": 340.6,
      "duration": 4.36
    },
    {
      "text": "there is a number of pre-processing",
      "start": 343.0,
      "duration": 4.24
    },
    {
      "text": "steps which need to be done before the",
      "start": 344.96,
      "duration": 5.76
    },
    {
      "text": "weights can be integrated with our GPT",
      "start": 347.24,
      "duration": 5.56
    },
    {
      "text": "architecture and I'm going to explain",
      "start": 350.72,
      "duration": 4.64
    },
    {
      "text": "those steps to you right now so let me",
      "start": 352.8,
      "duration": 4.679
    },
    {
      "text": "take you through to the vs code",
      "start": 355.36,
      "duration": 5.04
    },
    {
      "text": "interface where we are going to",
      "start": 357.479,
      "duration": 7.401
    },
    {
      "text": "look at this uh GPT download 3 function",
      "start": 360.4,
      "duration": 6.84
    },
    {
      "text": "and or this file this code file which",
      "start": 364.88,
      "duration": 3.64
    },
    {
      "text": "will help us",
      "start": 367.24,
      "duration": 4.28
    },
    {
      "text": "download um these seven files and not",
      "start": 368.52,
      "duration": 5.079
    },
    {
      "text": "just download we are going to extract",
      "start": 371.52,
      "duration": 4.48
    },
    {
      "text": "the parameters from these seven files",
      "start": 373.599,
      "duration": 4.04
    },
    {
      "text": "and then we are going to store them in a",
      "start": 376.0,
      "duration": 4.639
    },
    {
      "text": "very specific format all right so here",
      "start": 377.639,
      "duration": 7.641
    },
    {
      "text": "you can see the GPT download 3. py file",
      "start": 380.639,
      "duration": 6.241
    },
    {
      "text": "and first of all you'll see that when",
      "start": 385.28,
      "duration": 3.0
    },
    {
      "text": "you open this file it has three",
      "start": 386.88,
      "duration": 3.52
    },
    {
      "text": "functions it has the down load and load",
      "start": 388.28,
      "duration": 4.639
    },
    {
      "text": "gpt2 function which is the main function",
      "start": 390.4,
      "duration": 5.079
    },
    {
      "text": "which we are going to look at then this",
      "start": 392.919,
      "duration": 5.321
    },
    {
      "text": "function utilizes two helper functions",
      "start": 395.479,
      "duration": 5.041
    },
    {
      "text": "the first is called download file and",
      "start": 398.24,
      "duration": 4.76
    },
    {
      "text": "the second is load gpt2 params from the",
      "start": 400.52,
      "duration": 5.399
    },
    {
      "text": "TF checkpoint so as the file names",
      "start": 403.0,
      "duration": 4.84
    },
    {
      "text": "itself suggest what this main function",
      "start": 405.919,
      "duration": 3.56
    },
    {
      "text": "is going to do is that it's going to do",
      "start": 407.84,
      "duration": 4.28
    },
    {
      "text": "two things first it's going to download",
      "start": 409.479,
      "duration": 4.921
    },
    {
      "text": "the seven files these seven files which",
      "start": 412.12,
      "duration": 4.519
    },
    {
      "text": "I just showed you on kagle also it's",
      "start": 414.4,
      "duration": 3.919
    },
    {
      "text": "going to download those files and it's",
      "start": 416.639,
      "duration": 3.24
    },
    {
      "text": "going to save them on my Lo loal",
      "start": 418.319,
      "duration": 3.88
    },
    {
      "text": "computer that's what it's going to do as",
      "start": 419.879,
      "duration": 4.961
    },
    {
      "text": "the step number one and then in the",
      "start": 422.199,
      "duration": 4.68
    },
    {
      "text": "Second Step what we are going to do is",
      "start": 424.84,
      "duration": 3.639
    },
    {
      "text": "that we are going to take the downloaded",
      "start": 426.879,
      "duration": 3.76
    },
    {
      "text": "file and then we are going to call this",
      "start": 428.479,
      "duration": 7.521
    },
    {
      "text": "load gpt2 params from uh TF checkpoint",
      "start": 430.639,
      "duration": 7.96
    },
    {
      "text": "and uh what this function is going to do",
      "start": 436.0,
      "duration": 6.039
    },
    {
      "text": "is that this function is going to store",
      "start": 438.599,
      "duration": 5.761
    },
    {
      "text": "the parameters in the dictionary called",
      "start": 442.039,
      "duration": 5.041
    },
    {
      "text": "as params and this dictionary has a very",
      "start": 444.36,
      "duration": 4.48
    },
    {
      "text": "specific format which we are going to",
      "start": 447.08,
      "duration": 4.04
    },
    {
      "text": "see just in in a moment so just",
      "start": 448.84,
      "duration": 4.4
    },
    {
      "text": "downloading the data from kaggle is not",
      "start": 451.12,
      "duration": 4.04
    },
    {
      "text": "enough you need to understand the rest",
      "start": 453.24,
      "duration": 4.04
    },
    {
      "text": "of the code and the format in which this",
      "start": 455.16,
      "duration": 4.96
    },
    {
      "text": "params dictionary is returned so let's",
      "start": 457.28,
      "duration": 4.44
    },
    {
      "text": "start understanding this code",
      "start": 460.12,
      "duration": 3.799
    },
    {
      "text": "sequentially before that I just want to",
      "start": 461.72,
      "duration": 4.08
    },
    {
      "text": "take a moment and explain these",
      "start": 463.919,
      "duration": 4.12
    },
    {
      "text": "downloaded files so once you run this",
      "start": 465.8,
      "duration": 4.239
    },
    {
      "text": "code you will see that you'll get these",
      "start": 468.039,
      "duration": 3.801
    },
    {
      "text": "files which are downloaded on your local",
      "start": 470.039,
      "duration": 4.041
    },
    {
      "text": "machine without even running this code",
      "start": 471.84,
      "duration": 3.96
    },
    {
      "text": "you can even go to kaggle and download",
      "start": 474.08,
      "duration": 4.559
    },
    {
      "text": "these seven files now if a student has",
      "start": 475.8,
      "duration": 4.959
    },
    {
      "text": "not been through this llm lecture series",
      "start": 478.639,
      "duration": 4.161
    },
    {
      "text": "which we have developed they will not",
      "start": 480.759,
      "duration": 3.84
    },
    {
      "text": "understand these files and they might",
      "start": 482.8,
      "duration": 5.76
    },
    {
      "text": "seem a bit complicated these files but",
      "start": 484.599,
      "duration": 6.121
    },
    {
      "text": "for us for those students who have",
      "start": 488.56,
      "duration": 3.88
    },
    {
      "text": "followed this lecture series I want to",
      "start": 490.72,
      "duration": 3.56
    },
    {
      "text": "show you that now these files are very",
      "start": 492.44,
      "duration": 4.479
    },
    {
      "text": "easy to understand so I want to explain",
      "start": 494.28,
      "duration": 4.479
    },
    {
      "text": "each of these files sequentially the",
      "start": 496.919,
      "duration": 4.441
    },
    {
      "text": "first thing is checkpoint so if you go",
      "start": 498.759,
      "duration": 4.361
    },
    {
      "text": "to this file named checkpoint you will",
      "start": 501.36,
      "duration": 3.64
    },
    {
      "text": "see this thing called Model checkpoint",
      "start": 503.12,
      "duration": 4.519
    },
    {
      "text": "path what this essentially means is that",
      "start": 505.0,
      "duration": 5.639
    },
    {
      "text": "this is the path where all the current",
      "start": 507.639,
      "duration": 5.721
    },
    {
      "text": "parameters the weights of the gpt2 model",
      "start": 510.639,
      "duration": 5.76
    },
    {
      "text": "are stored so you'll see that model. CPT",
      "start": 513.36,
      "duration": 5.84
    },
    {
      "text": "we have three files named model. CPT the",
      "start": 516.399,
      "duration": 5.88
    },
    {
      "text": "most important file is model. cp. dat",
      "start": 519.2,
      "duration": 5.12
    },
    {
      "text": "this is where all the weights of the",
      "start": 522.279,
      "duration": 5.041
    },
    {
      "text": "gpt2 model are stored so this checkpoint",
      "start": 524.32,
      "duration": 5.079
    },
    {
      "text": "is just going to indicate the path at",
      "start": 527.32,
      "duration": 4.56
    },
    {
      "text": "which the model weights are stored the",
      "start": 529.399,
      "duration": 6.041
    },
    {
      "text": "second file is encoder do Json so let me",
      "start": 531.88,
      "duration": 5.44
    },
    {
      "text": "come to the top of this file this file",
      "start": 535.44,
      "duration": 3.639
    },
    {
      "text": "as a whole looks complex but it's",
      "start": 537.32,
      "duration": 4.56
    },
    {
      "text": "actually a vocabulary it's a vocabulary",
      "start": 539.079,
      "duration": 8.32
    },
    {
      "text": "of keys uh and token IDs so we have",
      "start": 541.88,
      "duration": 7.639
    },
    {
      "text": "tokens here and corresponding to every",
      "start": 547.399,
      "duration": 4.321
    },
    {
      "text": "token there is a token ID that's the",
      "start": 549.519,
      "duration": 3.88
    },
    {
      "text": "vocabulary which we are using and if you",
      "start": 551.72,
      "duration": 3.96
    },
    {
      "text": "remember the vocabulary size it's",
      "start": 553.399,
      "duration": 4.641
    },
    {
      "text": "50257 so it starts from zero and then",
      "start": 555.68,
      "duration": 4.839
    },
    {
      "text": "the end of text is 50256 so there are",
      "start": 558.04,
      "duration": 4.88
    },
    {
      "text": "50257 tokens in our",
      "start": 560.519,
      "duration": 5.56
    },
    {
      "text": "vocabulary then the second file is w.",
      "start": 562.92,
      "duration": 5.84
    },
    {
      "text": "bpe remember if you have followed this",
      "start": 566.079,
      "duration": 4.44
    },
    {
      "text": "lecture series we learned about the bite",
      "start": 568.76,
      "duration": 4.8
    },
    {
      "text": "pair encoder it's a subword tokenization",
      "start": 570.519,
      "duration": 5.081
    },
    {
      "text": "scheme and the way this encoder works is",
      "start": 573.56,
      "duration": 4.04
    },
    {
      "text": "that it looks for pairs of tokens which",
      "start": 575.6,
      "duration": 4.359
    },
    {
      "text": "are occurring the most frequently and",
      "start": 577.6,
      "duration": 4.04
    },
    {
      "text": "then it merges this pair and that",
      "start": 579.959,
      "duration": 3.44
    },
    {
      "text": "becomes a token in itself that's why",
      "start": 581.64,
      "duration": 4.4
    },
    {
      "text": "it's called subord tokenizer so this W",
      "start": 583.399,
      "duration": 6.521
    },
    {
      "text": "cap. BP gives a list of the tokens which",
      "start": 586.04,
      "duration": 5.28
    },
    {
      "text": "have been merged with the highest",
      "start": 589.92,
      "duration": 4.0
    },
    {
      "text": "probability at the top so all the tokens",
      "start": 591.32,
      "duration": 3.759
    },
    {
      "text": "which have been merged have been",
      "start": 593.92,
      "duration": 3.68
    },
    {
      "text": "mentioned in this list and the tokens",
      "start": 595.079,
      "duration": 4.44
    },
    {
      "text": "which have which come with the maximum",
      "start": 597.6,
      "duration": 4.08
    },
    {
      "text": "probability they at the top of this list",
      "start": 599.519,
      "duration": 3.961
    },
    {
      "text": "so you might be thinking what is this g",
      "start": 601.68,
      "duration": 4.56
    },
    {
      "text": "dot right this g dot is a special",
      "start": 603.48,
      "duration": 5.28
    },
    {
      "text": "convention uh which basically indicates",
      "start": 606.24,
      "duration": 4.48
    },
    {
      "text": "that one token has ended and a new token",
      "start": 608.76,
      "duration": 4.0
    },
    {
      "text": "has begin so you can think of it as a",
      "start": 610.72,
      "duration": 4.44
    },
    {
      "text": "space and which marks the beginning of a",
      "start": 612.76,
      "duration": 4.639
    },
    {
      "text": "new token so it's a special convention",
      "start": 615.16,
      "duration": 5.0
    },
    {
      "text": "used by open AI to indicate the end of",
      "start": 617.399,
      "duration": 5.12
    },
    {
      "text": "one token and the start of a new",
      "start": 620.16,
      "duration": 4.64
    },
    {
      "text": "token then the third file which I want",
      "start": 622.519,
      "duration": 5.201
    },
    {
      "text": "to show you is ham. Json this is the",
      "start": 624.8,
      "duration": 4.599
    },
    {
      "text": "file which essentially contains all the",
      "start": 627.72,
      "duration": 4.04
    },
    {
      "text": "the settings of our model the vocabulary",
      "start": 629.399,
      "duration": 4.241
    },
    {
      "text": "size which we are using is",
      "start": 631.76,
      "duration": 4.4
    },
    {
      "text": "50257 the context length which we have",
      "start": 633.64,
      "duration": 6.84
    },
    {
      "text": "is 1024 the embedding Dimension is 768",
      "start": 636.16,
      "duration": 5.679
    },
    {
      "text": "what is the embedding Dimension",
      "start": 640.48,
      "duration": 3.28
    },
    {
      "text": "essentially all the token IDs which you",
      "start": 641.839,
      "duration": 5.321
    },
    {
      "text": "see over here if you see any any token",
      "start": 643.76,
      "duration": 5.4
    },
    {
      "text": "every token has a token ID right and",
      "start": 647.16,
      "duration": 4.0
    },
    {
      "text": "every token ID will be converted",
      "start": 649.16,
      "duration": 5.119
    },
    {
      "text": "converted into a 768 dimensional Vector",
      "start": 651.16,
      "duration": 5.96
    },
    {
      "text": "space uh these vectors are not trained",
      "start": 654.279,
      "duration": 5.56
    },
    {
      "text": "and our training will involve parameters",
      "start": 657.12,
      "duration": 5.68
    },
    {
      "text": "corresponding to these as well then n",
      "start": 659.839,
      "duration": 5.881
    },
    {
      "text": "heads so n heads is basically the number",
      "start": 662.8,
      "duration": 6.36
    },
    {
      "text": "of attention heads present in each",
      "start": 665.72,
      "duration": 5.799
    },
    {
      "text": "Transformer block and N layer is",
      "start": 669.16,
      "duration": 5.2
    },
    {
      "text": "essentially the number of number of",
      "start": 671.519,
      "duration": 5.841
    },
    {
      "text": "Transformer blocks itself now remember",
      "start": 674.36,
      "duration": 6.279
    },
    {
      "text": "these are the these parameters the",
      "start": 677.36,
      "duration": 5.24
    },
    {
      "text": "setting values which I'm showing you",
      "start": 680.639,
      "duration": 6.681
    },
    {
      "text": "they are for the smallest uh gpt2 model",
      "start": 682.6,
      "duration": 7.84
    },
    {
      "text": "when open made the GP B2 weights public",
      "start": 687.32,
      "duration": 4.24
    },
    {
      "text": "they actually",
      "start": 690.44,
      "duration": 3.68
    },
    {
      "text": "made the weights of their larger models",
      "start": 691.56,
      "duration": 6.079
    },
    {
      "text": "also public so 124 million 355 million",
      "start": 694.12,
      "duration": 6.92
    },
    {
      "text": "774 million and 1558 million the weights",
      "start": 697.639,
      "duration": 5.76
    },
    {
      "text": "of all of these models were made public",
      "start": 701.04,
      "duration": 4.0
    },
    {
      "text": "but for the sake of Simplicity we are",
      "start": 703.399,
      "duration": 4.081
    },
    {
      "text": "going to look at 124 million model right",
      "start": 705.04,
      "duration": 4.88
    },
    {
      "text": "now which had 12 Transformer blocks as",
      "start": 707.48,
      "duration": 4.4
    },
    {
      "text": "you increase in the complexity of the",
      "start": 709.92,
      "duration": 3.919
    },
    {
      "text": "gpt2 model the number of Transformer",
      "start": 711.88,
      "duration": 5.079
    },
    {
      "text": "blocks also increase remember that the",
      "start": 713.839,
      "duration": 4.641
    },
    {
      "text": "same code which I'm going to show you",
      "start": 716.959,
      "duration": 3.201
    },
    {
      "text": "today can be run for these larger",
      "start": 718.48,
      "duration": 4.64
    },
    {
      "text": "architectures as well okay so I hope you",
      "start": 720.16,
      "duration": 4.6
    },
    {
      "text": "have understood the meanings of all of",
      "start": 723.12,
      "duration": 4.159
    },
    {
      "text": "these files now let us start going",
      "start": 724.76,
      "duration": 5.759
    },
    {
      "text": "through uh this download and load gpt2",
      "start": 727.279,
      "duration": 5.401
    },
    {
      "text": "code step by step and understand every",
      "start": 730.519,
      "duration": 3.241
    },
    {
      "text": "single",
      "start": 732.68,
      "duration": 3.76
    },
    {
      "text": "sentence so the first thing is that we",
      "start": 733.76,
      "duration": 5.56
    },
    {
      "text": "mention the URL so we'll make a call to",
      "start": 736.44,
      "duration": 5.48
    },
    {
      "text": "this API and we'll download these files",
      "start": 739.32,
      "duration": 4.16
    },
    {
      "text": "download the seven files which are",
      "start": 741.92,
      "duration": 3.719
    },
    {
      "text": "present on my local computer right now",
      "start": 743.48,
      "duration": 4.0
    },
    {
      "text": "so the download file function will be",
      "start": 745.639,
      "duration": 4.041
    },
    {
      "text": "called for this and I'm not going to go",
      "start": 747.48,
      "duration": 3.68
    },
    {
      "text": "through this in detail because this is",
      "start": 749.68,
      "duration": 3.279
    },
    {
      "text": "just downloading the file onto the local",
      "start": 751.16,
      "duration": 4.2
    },
    {
      "text": "machine the real interest which I have",
      "start": 752.959,
      "duration": 4.281
    },
    {
      "text": "is showing you what happens after the",
      "start": 755.36,
      "duration": 4.719
    },
    {
      "text": "file is downloaded so after the file is",
      "start": 757.24,
      "duration": 4.92
    },
    {
      "text": "downloaded first we have this PF",
      "start": 760.079,
      "duration": 5.401
    },
    {
      "text": "checkpoint path which is uh the latest",
      "start": 762.16,
      "duration": 5.679
    },
    {
      "text": "checkpoint directory where all the model",
      "start": 765.48,
      "duration": 4.28
    },
    {
      "text": "parameters are stored so what this",
      "start": 767.839,
      "duration": 4.68
    },
    {
      "text": "function will do is that tf. train so",
      "start": 769.76,
      "duration": 4.92
    },
    {
      "text": "tensorflow do tr. latest checkpoint",
      "start": 772.519,
      "duration": 4.161
    },
    {
      "text": "model directory so it will go to our",
      "start": 774.68,
      "duration": 3.399
    },
    {
      "text": "directory and it will look for this",
      "start": 776.68,
      "duration": 3.24
    },
    {
      "text": "checkpoint and it will look for this",
      "start": 778.079,
      "duration": 3.841
    },
    {
      "text": "model checkpoint path which is model.",
      "start": 779.92,
      "duration": 4.32
    },
    {
      "text": "CPT so it will know where the model",
      "start": 781.92,
      "duration": 3.84
    },
    {
      "text": "parameters are",
      "start": 784.24,
      "duration": 4.64
    },
    {
      "text": "stored awesome the next thing is that",
      "start": 785.76,
      "duration": 5.199
    },
    {
      "text": "settings we are going to maintain",
      "start": 788.88,
      "duration": 3.56
    },
    {
      "text": "another dictionary which is called as",
      "start": 790.959,
      "duration": 3.521
    },
    {
      "text": "settings and what the settings",
      "start": 792.44,
      "duration": 3.72
    },
    {
      "text": "dictionary is that it will exactly",
      "start": 794.48,
      "duration": 3.359
    },
    {
      "text": "contain the same thing what is present",
      "start": 796.16,
      "duration": 4.679
    },
    {
      "text": "in the ham. Json so it will contain the",
      "start": 797.839,
      "duration": 4.68
    },
    {
      "text": "vocabulary size it will contain the",
      "start": 800.839,
      "duration": 3.841
    },
    {
      "text": "context length the embedding Dimension",
      "start": 802.519,
      "duration": 3.961
    },
    {
      "text": "the number of attention heads and the",
      "start": 804.68,
      "duration": 5.32
    },
    {
      "text": "number of Transformer blocks",
      "start": 806.48,
      "duration": 6.64
    },
    {
      "text": "uh so let me go back to the code again",
      "start": 810.0,
      "duration": 5.32
    },
    {
      "text": "right so this is the settings dictionary",
      "start": 813.12,
      "duration": 4.76
    },
    {
      "text": "and then in this last line of the code",
      "start": 815.32,
      "duration": 3.959
    },
    {
      "text": "this is where all the magic is",
      "start": 817.88,
      "duration": 3.319
    },
    {
      "text": "essentially happening and you really",
      "start": 819.279,
      "duration": 4.0
    },
    {
      "text": "need to go into depth to understand what",
      "start": 821.199,
      "duration": 4.44
    },
    {
      "text": "happens in this one line of code so here",
      "start": 823.279,
      "duration": 4.401
    },
    {
      "text": "what we are doing is that uh we are",
      "start": 825.639,
      "duration": 4.681
    },
    {
      "text": "looking at the model path which is given",
      "start": 827.68,
      "duration": 5.399
    },
    {
      "text": "by this TF ckpt checkpoint answer flow",
      "start": 830.32,
      "duration": 5.6
    },
    {
      "text": "checkpoint path and we are going to take",
      "start": 833.079,
      "duration": 5.32
    },
    {
      "text": "the parameters from that path and we are",
      "start": 835.92,
      "duration": 5.56
    },
    {
      "text": "going to use the settings dictionary and",
      "start": 838.399,
      "duration": 4.8
    },
    {
      "text": "then the we'll return a params",
      "start": 841.48,
      "duration": 4.2
    },
    {
      "text": "dictionary and the code which does that",
      "start": 843.199,
      "duration": 4.961
    },
    {
      "text": "is this load gpt2 params from TF",
      "start": 845.68,
      "duration": 4.76
    },
    {
      "text": "checkpoint so from the TF checkpoint",
      "start": 848.16,
      "duration": 4.799
    },
    {
      "text": "parameters we are going to load the",
      "start": 850.44,
      "duration": 4.24
    },
    {
      "text": "parameters into a special dictionary",
      "start": 852.959,
      "duration": 3.0
    },
    {
      "text": "called as",
      "start": 854.68,
      "duration": 3.92
    },
    {
      "text": "params before explaining to you what is",
      "start": 855.959,
      "duration": 4.401
    },
    {
      "text": "happening in this code I first want to",
      "start": 858.6,
      "duration": 3.679
    },
    {
      "text": "show you what the params dictionary",
      "start": 860.36,
      "duration": 4.56
    },
    {
      "text": "looks like all right so here is how the",
      "start": 862.279,
      "duration": 5.401
    },
    {
      "text": "params dictionary actually looks like",
      "start": 864.92,
      "duration": 4.8
    },
    {
      "text": "the dictionary will have five Keys which",
      "start": 867.68,
      "duration": 3.839
    },
    {
      "text": "I'm going to show to you right now what",
      "start": 869.72,
      "duration": 3.479
    },
    {
      "text": "are these five keys and what do they",
      "start": 871.519,
      "duration": 4.401
    },
    {
      "text": "exactly mean the first key which this",
      "start": 873.199,
      "duration": 5.481
    },
    {
      "text": "dictionary is going to have is wte the",
      "start": 875.92,
      "duration": 6.599
    },
    {
      "text": "second key is WP the third key is blocks",
      "start": 878.68,
      "duration": 6.0
    },
    {
      "text": "the fourth key is the final",
      "start": 882.519,
      "duration": 4.481
    },
    {
      "text": "normalization scale and the fifth key is",
      "start": 884.68,
      "duration": 5.48
    },
    {
      "text": "final normalization shift to really",
      "start": 887.0,
      "duration": 4.959
    },
    {
      "text": "understand these five keys and why do we",
      "start": 890.16,
      "duration": 3.96
    },
    {
      "text": "need these five Keys you need to",
      "start": 891.959,
      "duration": 3.721
    },
    {
      "text": "understand the GPT",
      "start": 894.12,
      "duration": 3.839
    },
    {
      "text": "architecture um and we have spent a lot",
      "start": 895.68,
      "duration": 3.8
    },
    {
      "text": "of time on this I just just want to",
      "start": 897.959,
      "duration": 3.12
    },
    {
      "text": "summarize it quickly so that you can",
      "start": 899.48,
      "duration": 4.76
    },
    {
      "text": "relate to these keys so whenever a token",
      "start": 901.079,
      "duration": 4.921
    },
    {
      "text": "comes in whenever sentence comes in",
      "start": 904.24,
      "duration": 3.839
    },
    {
      "text": "rather we have to first convert the",
      "start": 906.0,
      "duration": 4.44
    },
    {
      "text": "input token IDs into token",
      "start": 908.079,
      "duration": 4.961
    },
    {
      "text": "embeddings so this will require weights",
      "start": 910.44,
      "duration": 4.24
    },
    {
      "text": "this will require parameters which need",
      "start": 913.04,
      "duration": 3.96
    },
    {
      "text": "to be trained then we will need to add",
      "start": 914.68,
      "duration": 4.36
    },
    {
      "text": "positional embeddings these are also",
      "start": 917.0,
      "duration": 4.24
    },
    {
      "text": "parameters which need to be trained the",
      "start": 919.04,
      "duration": 3.84
    },
    {
      "text": "result of token embeddings plus",
      "start": 921.24,
      "duration": 3.08
    },
    {
      "text": "positional embeddings is input",
      "start": 922.88,
      "duration": 3.72
    },
    {
      "text": "embeddings and these pass on to the",
      "start": 924.32,
      "duration": 4.84
    },
    {
      "text": "Transformer block so already you",
      "start": 926.6,
      "duration": 4.52
    },
    {
      "text": "understood the purposes of the first two",
      "start": 929.16,
      "duration": 3.799
    },
    {
      "text": "keys this is for the token embeddings",
      "start": 931.12,
      "duration": 4.68
    },
    {
      "text": "parameters the wp is for the positional",
      "start": 932.959,
      "duration": 5.601
    },
    {
      "text": "embedding parameters now after we get",
      "start": 935.8,
      "duration": 4.8
    },
    {
      "text": "the input embedding we have the Dropout",
      "start": 938.56,
      "duration": 4.12
    },
    {
      "text": "layer Dropout layer has no trainable",
      "start": 940.6,
      "duration": 4.28
    },
    {
      "text": "weights so we don't have any Keys",
      "start": 942.68,
      "duration": 4.36
    },
    {
      "text": "corresponding to that then we move into",
      "start": 944.88,
      "duration": 4.92
    },
    {
      "text": "this Transformer block the Transformer",
      "start": 947.04,
      "duration": 4.599
    },
    {
      "text": "block has several places where trainable",
      "start": 949.8,
      "duration": 4.039
    },
    {
      "text": "weights exist the first place is the",
      "start": 951.639,
      "duration": 4.12
    },
    {
      "text": "multi-head attention layer we have",
      "start": 953.839,
      "duration": 3.881
    },
    {
      "text": "queries keys and the values weight",
      "start": 955.759,
      "duration": 3.841
    },
    {
      "text": "matrices over here right",
      "start": 957.72,
      "duration": 4.96
    },
    {
      "text": "and these metries consists of parameters",
      "start": 959.6,
      "duration": 5.76
    },
    {
      "text": "which we don't know so this is one scope",
      "start": 962.68,
      "duration": 5.0
    },
    {
      "text": "for trainable parameters the second",
      "start": 965.36,
      "duration": 4.159
    },
    {
      "text": "scope for trainable parameters is the",
      "start": 967.68,
      "duration": 4.2
    },
    {
      "text": "speed forward neural network remember",
      "start": 969.519,
      "duration": 4.521
    },
    {
      "text": "this speed forward neural network has an",
      "start": 971.88,
      "duration": 4.84
    },
    {
      "text": "input so there's an input which comes in",
      "start": 974.04,
      "duration": 4.76
    },
    {
      "text": "then it passes through hidden layer the",
      "start": 976.72,
      "duration": 3.52
    },
    {
      "text": "the number of dimensions in the hidden",
      "start": 978.8,
      "duration": 3.959
    },
    {
      "text": "layer are four times the input and then",
      "start": 980.24,
      "duration": 4.279
    },
    {
      "text": "we have a final output layer which",
      "start": 982.759,
      "duration": 4.76
    },
    {
      "text": "projects it back to the same size so",
      "start": 984.519,
      "duration": 5.32
    },
    {
      "text": "this first layer",
      "start": 987.519,
      "duration": 4.201
    },
    {
      "text": "uh so this first layer of the neural",
      "start": 989.839,
      "duration": 4.601
    },
    {
      "text": "network is is the fully connected layer",
      "start": 991.72,
      "duration": 5.479
    },
    {
      "text": "so I'm going to name it as FC and the",
      "start": 994.44,
      "duration": 4.68
    },
    {
      "text": "second is the output projection layer so",
      "start": 997.199,
      "duration": 4.401
    },
    {
      "text": "I'm going to call it as P both of these",
      "start": 999.12,
      "duration": 4.159
    },
    {
      "text": "layers will have trainable weights so",
      "start": 1001.6,
      "duration": 3.32
    },
    {
      "text": "that's the second scope for trainable",
      "start": 1003.279,
      "duration": 3.92
    },
    {
      "text": "weights now if you look at the layer",
      "start": 1004.92,
      "duration": 3.76
    },
    {
      "text": "normalization one and layer",
      "start": 1007.199,
      "duration": 3.281
    },
    {
      "text": "normalization 2 let me highlight them",
      "start": 1008.68,
      "duration": 4.079
    },
    {
      "text": "with a different color layer",
      "start": 1010.48,
      "duration": 4.719
    },
    {
      "text": "normalization um let me choose purple",
      "start": 1012.759,
      "duration": 4.08
    },
    {
      "text": "color so if you look at layer",
      "start": 1015.199,
      "duration": 3.76
    },
    {
      "text": "normalization 2 and if you look at layer",
      "start": 1016.839,
      "duration": 4.56
    },
    {
      "text": "normalization one normally layer",
      "start": 1018.959,
      "duration": 4.44
    },
    {
      "text": "normalization does not have any weights",
      "start": 1021.399,
      "duration": 3.601
    },
    {
      "text": "because we just subtract the mean and",
      "start": 1023.399,
      "duration": 4.081
    },
    {
      "text": "divide by the square root of variance",
      "start": 1025.0,
      "duration": 4.52
    },
    {
      "text": "right but the way we have defined layer",
      "start": 1027.48,
      "duration": 4.4
    },
    {
      "text": "normalization is that after we do the",
      "start": 1029.52,
      "duration": 4.36
    },
    {
      "text": "scaling after we subtract the mean and",
      "start": 1031.88,
      "duration": 4.439
    },
    {
      "text": "divide by the square root of variance we",
      "start": 1033.88,
      "duration": 5.76
    },
    {
      "text": "also multiply with a trainable parameter",
      "start": 1036.319,
      "duration": 5.48
    },
    {
      "text": "called scale and we add a trainable",
      "start": 1039.64,
      "duration": 4.64
    },
    {
      "text": "parameter called shift it turns out that",
      "start": 1041.799,
      "duration": 4.361
    },
    {
      "text": "these two parameters actually make a big",
      "start": 1044.28,
      "duration": 5.24
    },
    {
      "text": "difference so that's why we have",
      "start": 1046.16,
      "duration": 5.639
    },
    {
      "text": "uh layer normalization 2 and layer",
      "start": 1049.52,
      "duration": 4.159
    },
    {
      "text": "normalization one also which come into",
      "start": 1051.799,
      "duration": 4.481
    },
    {
      "text": "the trainable weight parameters category",
      "start": 1053.679,
      "duration": 4.921
    },
    {
      "text": "and then we have the final output layer",
      "start": 1056.28,
      "duration": 4.08
    },
    {
      "text": "the final normalization layer which is",
      "start": 1058.6,
      "duration": 4.4
    },
    {
      "text": "another trainable weight Matrix category",
      "start": 1060.36,
      "duration": 4.84
    },
    {
      "text": "where wherever we have parameters to",
      "start": 1063.0,
      "duration": 5.0
    },
    {
      "text": "train those are Keys corresponding to",
      "start": 1065.2,
      "duration": 5.44
    },
    {
      "text": "this parameter dictionary so you already",
      "start": 1068.0,
      "duration": 4.32
    },
    {
      "text": "saw token embeddings and positional",
      "start": 1070.64,
      "duration": 5.32
    },
    {
      "text": "embeddings blocks corresponds to all the",
      "start": 1072.32,
      "duration": 5.56
    },
    {
      "text": "trainable weights and parameters in this",
      "start": 1075.96,
      "duration": 3.719
    },
    {
      "text": "Transformer block in this blue color",
      "start": 1077.88,
      "duration": 4.64
    },
    {
      "text": "Transformer block and then G and B",
      "start": 1079.679,
      "duration": 4.641
    },
    {
      "text": "correspond to the trainable weight",
      "start": 1082.52,
      "duration": 3.8
    },
    {
      "text": "parameters in this final layer Norm",
      "start": 1084.32,
      "duration": 4.04
    },
    {
      "text": "which is indicated as four right",
      "start": 1086.32,
      "duration": 5.44
    },
    {
      "text": "now so uh token embeddings right I hope",
      "start": 1088.36,
      "duration": 5.64
    },
    {
      "text": "you remember what what are token",
      "start": 1091.76,
      "duration": 5.68
    },
    {
      "text": "embeddings we have a vocabulary size of",
      "start": 1094.0,
      "duration": 7.08
    },
    {
      "text": "50257 right and corresponding to every",
      "start": 1097.44,
      "duration": 5.96
    },
    {
      "text": "token we have a vector whose Dimension",
      "start": 1101.08,
      "duration": 3.32
    },
    {
      "text": "is",
      "start": 1103.4,
      "duration": 4.56
    },
    {
      "text": "768 so the size of this is 50257 rows",
      "start": 1104.4,
      "duration": 5.48
    },
    {
      "text": "and 768 columns these are token",
      "start": 1107.96,
      "duration": 4.199
    },
    {
      "text": "embeddings now positional embeddings are",
      "start": 1109.88,
      "duration": 5.12
    },
    {
      "text": "governed by the context size in gpt2 the",
      "start": 1112.159,
      "duration": 4.4
    },
    {
      "text": "smallest model which we are considering",
      "start": 1115.0,
      "duration": 3.44
    },
    {
      "text": "the contact size is",
      "start": 1116.559,
      "duration": 6.201
    },
    {
      "text": "1024 because maximum we can look at 1024",
      "start": 1118.44,
      "duration": 6.52
    },
    {
      "text": "positions and then make the next token",
      "start": 1122.76,
      "duration": 4.56
    },
    {
      "text": "prediction the one 25th position does",
      "start": 1124.96,
      "duration": 5.44
    },
    {
      "text": "not matter to us so we only need for we",
      "start": 1127.32,
      "duration": 5.56
    },
    {
      "text": "only need embeddings corresponding to",
      "start": 1130.4,
      "duration": 4.24
    },
    {
      "text": "position number one position number two",
      "start": 1132.88,
      "duration": 3.36
    },
    {
      "text": "up till position number",
      "start": 1134.64,
      "duration": 4.2
    },
    {
      "text": "1024 and these embeddings each of these",
      "start": 1136.24,
      "duration": 4.76
    },
    {
      "text": "embeddings also has a dimension of",
      "start": 1138.84,
      "duration": 4.64
    },
    {
      "text": "768 because we need to add the token",
      "start": 1141.0,
      "duration": 4.159
    },
    {
      "text": "embeddings to the positional embeddings",
      "start": 1143.48,
      "duration": 3.319
    },
    {
      "text": "the vector dimension of both of these",
      "start": 1145.159,
      "duration": 4.161
    },
    {
      "text": "need to be exactly the same and that's",
      "start": 1146.799,
      "duration": 5.281
    },
    {
      "text": "there because it's 768 in both of these",
      "start": 1149.32,
      "duration": 7.479
    },
    {
      "text": "cases now so this WT key will have all",
      "start": 1152.08,
      "duration": 6.56
    },
    {
      "text": "these values corresponding to the Token",
      "start": 1156.799,
      "duration": 4.76
    },
    {
      "text": "embedding Matrix the wp key will have",
      "start": 1158.64,
      "duration": 4.48
    },
    {
      "text": "all the values corresponding to the",
      "start": 1161.559,
      "duration": 3.721
    },
    {
      "text": "positional embeddings Matrix now let's",
      "start": 1163.12,
      "duration": 4.2
    },
    {
      "text": "come to the blocks when you come to the",
      "start": 1165.28,
      "duration": 3.44
    },
    {
      "text": "blocks there are several things which",
      "start": 1167.32,
      "duration": 2.839
    },
    {
      "text": "which are happening in the block so let",
      "start": 1168.72,
      "duration": 5.079
    },
    {
      "text": "me just rub this part a bit so that",
      "start": 1170.159,
      "duration": 5.841
    },
    {
      "text": "things are a little more clearer over",
      "start": 1173.799,
      "duration": 4.961
    },
    {
      "text": "here great first we are going to look at",
      "start": 1176.0,
      "duration": 4.6
    },
    {
      "text": "the Mast multi-ad",
      "start": 1178.76,
      "duration": 5.12
    },
    {
      "text": "attention right this has the query this",
      "start": 1180.6,
      "duration": 5.48
    },
    {
      "text": "has the key and this as the",
      "start": 1183.88,
      "duration": 6.039
    },
    {
      "text": "value in GPT 2 when they release the",
      "start": 1186.08,
      "duration": 6.599
    },
    {
      "text": "weight they fuse this into one single",
      "start": 1189.919,
      "duration": 5.321
    },
    {
      "text": "big Matrix so we need all the weights",
      "start": 1192.679,
      "duration": 4.401
    },
    {
      "text": "corresponding to this large Matrix so",
      "start": 1195.24,
      "duration": 3.6
    },
    {
      "text": "that later we can split it into query",
      "start": 1197.08,
      "duration": 4.28
    },
    {
      "text": "key and value trainable Matrix so the",
      "start": 1198.84,
      "duration": 5.319
    },
    {
      "text": "way it is done in this blocks dictionary",
      "start": 1201.36,
      "duration": 4.16
    },
    {
      "text": "is that there will be a dictionary",
      "start": 1204.159,
      "duration": 2.88
    },
    {
      "text": "called or there will be a dictionary",
      "start": 1205.52,
      "duration": 3.48
    },
    {
      "text": "called parameter within that there will",
      "start": 1207.039,
      "duration": 4.601
    },
    {
      "text": "be a keys blocks this will link to",
      "start": 1209.0,
      "duration": 4.88
    },
    {
      "text": "another dictionary which has the keys",
      "start": 1211.64,
      "duration": 4.44
    },
    {
      "text": "Transformer this will link to another",
      "start": 1213.88,
      "duration": 5.08
    },
    {
      "text": "dictionary which is the keys H not why H",
      "start": 1216.08,
      "duration": 5.36
    },
    {
      "text": "not because there are 11 such 12 such",
      "start": 1218.96,
      "duration": 5.48
    },
    {
      "text": "Transformer blocks remember gpt2 has 12",
      "start": 1221.44,
      "duration": 4.599
    },
    {
      "text": "Transformer blocks so whatever I'm",
      "start": 1224.44,
      "duration": 2.96
    },
    {
      "text": "showed you right now that will be",
      "start": 1226.039,
      "duration": 3.201
    },
    {
      "text": "replicated 12 times",
      "start": 1227.4,
      "duration": 4.159
    },
    {
      "text": "right so there are 12 Transformer blocks",
      "start": 1229.24,
      "duration": 4.72
    },
    {
      "text": "the first one is h0 then there will be a",
      "start": 1231.559,
      "duration": 4.641
    },
    {
      "text": "key called ATN why because we are",
      "start": 1233.96,
      "duration": 5.079
    },
    {
      "text": "looking at the attention mechanism right",
      "start": 1236.2,
      "duration": 5.56
    },
    {
      "text": "and then and this attention mechanism",
      "start": 1239.039,
      "duration": 5.64
    },
    {
      "text": "also has the output projection so we",
      "start": 1241.76,
      "duration": 5.68
    },
    {
      "text": "need to specify in the attention",
      "start": 1244.679,
      "duration": 4.961
    },
    {
      "text": "mechanism what we are looking for in the",
      "start": 1247.44,
      "duration": 3.76
    },
    {
      "text": "attention mechanism currently we are",
      "start": 1249.64,
      "duration": 5.279
    },
    {
      "text": "looking for CN what is CN it is the",
      "start": 1251.2,
      "duration": 6.959
    },
    {
      "text": "fused Matrix of query key and value and",
      "start": 1254.919,
      "duration": 4.841
    },
    {
      "text": "within the this fuse Matrix we are",
      "start": 1258.159,
      "duration": 3.801
    },
    {
      "text": "looking at the weights So within this",
      "start": 1259.76,
      "duration": 3.64
    },
    {
      "text": "fuse Matrix we are looking at the",
      "start": 1261.96,
      "duration": 3.599
    },
    {
      "text": "weights so you see that's how we access",
      "start": 1263.4,
      "duration": 5.24
    },
    {
      "text": "the weight Matrix of the um attention",
      "start": 1265.559,
      "duration": 5.041
    },
    {
      "text": "layer so we look at the blocks key",
      "start": 1268.64,
      "duration": 4.279
    },
    {
      "text": "within that Transformers so let me show",
      "start": 1270.6,
      "duration": 4.0
    },
    {
      "text": "it to you here now we look at the blocks",
      "start": 1272.919,
      "duration": 3.36
    },
    {
      "text": "key within that we look at the",
      "start": 1274.6,
      "duration": 3.52
    },
    {
      "text": "Transformers within that we look at H",
      "start": 1276.279,
      "duration": 4.4
    },
    {
      "text": "not then attention then C attention",
      "start": 1278.12,
      "duration": 4.48
    },
    {
      "text": "which is the fused query key value and",
      "start": 1280.679,
      "duration": 3.24
    },
    {
      "text": "then we look at the",
      "start": 1282.6,
      "duration": 4.28
    },
    {
      "text": "weights similarly we do all this for the",
      "start": 1283.919,
      "duration": 6.081
    },
    {
      "text": "biases of the qu key quy and value this",
      "start": 1286.88,
      "duration": 6.039
    },
    {
      "text": "is only for one Transformer Block H we",
      "start": 1290.0,
      "duration": 5.0
    },
    {
      "text": "are going to repeat this for H1 H2 up",
      "start": 1292.919,
      "duration": 4.281
    },
    {
      "text": "till h11 since there are 12 such",
      "start": 1295.0,
      "duration": 4.52
    },
    {
      "text": "Transformer blocks so this is how you",
      "start": 1297.2,
      "duration": 4.56
    },
    {
      "text": "access the weights in the attention",
      "start": 1299.52,
      "duration": 4.0
    },
    {
      "text": "layers of the Transformer blocks you",
      "start": 1301.76,
      "duration": 3.08
    },
    {
      "text": "access this is how you access the",
      "start": 1303.52,
      "duration": 3.68
    },
    {
      "text": "weights and biases of the attention",
      "start": 1304.84,
      "duration": 4.76
    },
    {
      "text": "layer in the Transformer block uh and",
      "start": 1307.2,
      "duration": 4.24
    },
    {
      "text": "when I say attention layer I I mean the",
      "start": 1309.6,
      "duration": 4.12
    },
    {
      "text": "query key and the value weight",
      "start": 1311.44,
      "duration": 4.839
    },
    {
      "text": "matrices and the biases corresponding to",
      "start": 1313.72,
      "duration": 5.24
    },
    {
      "text": "each of these right so that's the first",
      "start": 1316.279,
      "duration": 4.721
    },
    {
      "text": "component which we looked at the second",
      "start": 1318.96,
      "duration": 3.959
    },
    {
      "text": "component is this feed forward neural",
      "start": 1321.0,
      "duration": 4.32
    },
    {
      "text": "network now here if you look closely",
      "start": 1322.919,
      "duration": 3.921
    },
    {
      "text": "there are two layers there's a fully",
      "start": 1325.32,
      "duration": 3.16
    },
    {
      "text": "connected layer and there is a",
      "start": 1326.84,
      "duration": 3.36
    },
    {
      "text": "projection layer so we'll have weights",
      "start": 1328.48,
      "duration": 4.04
    },
    {
      "text": "and biases corresponding to this first",
      "start": 1330.2,
      "duration": 5.079
    },
    {
      "text": "layer as well as the second layer and",
      "start": 1332.52,
      "duration": 4.8
    },
    {
      "text": "that is clearly mentioned here if you",
      "start": 1335.279,
      "duration": 3.88
    },
    {
      "text": "look at the fully connect feed forward",
      "start": 1337.32,
      "duration": 3.599
    },
    {
      "text": "neural network you have the Transformer",
      "start": 1339.159,
      "duration": 4.0
    },
    {
      "text": "Keys within that you have the H not",
      "start": 1340.919,
      "duration": 4.24
    },
    {
      "text": "which is the first Transformer block",
      "start": 1343.159,
      "duration": 4.081
    },
    {
      "text": "within that there is a key called MLP",
      "start": 1345.159,
      "duration": 3.801
    },
    {
      "text": "multi-layer perceptron because we are",
      "start": 1347.24,
      "duration": 3.52
    },
    {
      "text": "looking at that expansion contraction",
      "start": 1348.96,
      "duration": 3.8
    },
    {
      "text": "neural network currently we are",
      "start": 1350.76,
      "duration": 3.88
    },
    {
      "text": "accessing the fully connected layer and",
      "start": 1352.76,
      "duration": 3.56
    },
    {
      "text": "the weights of that layer fully",
      "start": 1354.64,
      "duration": 3.88
    },
    {
      "text": "connected layer the biases of that layer",
      "start": 1356.32,
      "duration": 3.92
    },
    {
      "text": "so what I'm highlighting right now",
      "start": 1358.52,
      "duration": 4.399
    },
    {
      "text": "corresponds to the first the fully",
      "start": 1360.24,
      "duration": 4.08
    },
    {
      "text": "connected layer and its weights and",
      "start": 1362.919,
      "duration": 3.801
    },
    {
      "text": "biases similarly to access the weights",
      "start": 1364.32,
      "duration": 4.239
    },
    {
      "text": "and biases of the projection layer we",
      "start": 1366.72,
      "duration": 5.28
    },
    {
      "text": "just have the key as ccore PJ and we",
      "start": 1368.559,
      "duration": 5.0
    },
    {
      "text": "access the weights and the",
      "start": 1372.0,
      "duration": 3.799
    },
    {
      "text": "biases so this is how we access the",
      "start": 1373.559,
      "duration": 4.0
    },
    {
      "text": "weights and the biases of the feed",
      "start": 1375.799,
      "duration": 3.321
    },
    {
      "text": "forward neural network work within the",
      "start": 1377.559,
      "duration": 4.961
    },
    {
      "text": "Transformer block okay and then usually",
      "start": 1379.12,
      "duration": 5.12
    },
    {
      "text": "at the end of the Transformer it's not",
      "start": 1382.52,
      "duration": 3.759
    },
    {
      "text": "shown here but we we actually have this",
      "start": 1384.24,
      "duration": 3.64
    },
    {
      "text": "output projection",
      "start": 1386.279,
      "duration": 3.681
    },
    {
      "text": "head this",
      "start": 1387.88,
      "duration": 5.44
    },
    {
      "text": "output this output",
      "start": 1389.96,
      "duration": 5.28
    },
    {
      "text": "projection and there are weights",
      "start": 1393.32,
      "duration": 4.4
    },
    {
      "text": "associated with that also so to access",
      "start": 1395.24,
      "duration": 5.84
    },
    {
      "text": "the C projection in the OR to access the",
      "start": 1397.72,
      "duration": 6.199
    },
    {
      "text": "output projection layer we just do",
      "start": 1401.08,
      "duration": 5.079
    },
    {
      "text": "Transformers then we go to hn the first",
      "start": 1403.919,
      "duration": 5.161
    },
    {
      "text": "block then go to ATN c pro which is the",
      "start": 1406.159,
      "duration": 4.601
    },
    {
      "text": "output projection layer and we get the",
      "start": 1409.08,
      "duration": 4.32
    },
    {
      "text": "weights and similarly we do this for H1",
      "start": 1410.76,
      "duration": 5.08
    },
    {
      "text": "H2 up to h11 so the 12 Transformer",
      "start": 1413.4,
      "duration": 4.279
    },
    {
      "text": "blocks so this is how you get the",
      "start": 1415.84,
      "duration": 4.199
    },
    {
      "text": "weights and the biases of the attention",
      "start": 1417.679,
      "duration": 4.681
    },
    {
      "text": "layers of the feed forward neural",
      "start": 1420.039,
      "duration": 5.201
    },
    {
      "text": "network and the output projection layer",
      "start": 1422.36,
      "duration": 5.04
    },
    {
      "text": "in the Transformer block but remember",
      "start": 1425.24,
      "duration": 3.96
    },
    {
      "text": "the Transformer block has two additional",
      "start": 1427.4,
      "duration": 3.56
    },
    {
      "text": "things it has the layer Norm one and",
      "start": 1429.2,
      "duration": 5.0
    },
    {
      "text": "layer Norm two and both of these have",
      "start": 1430.96,
      "duration": 5.68
    },
    {
      "text": "trainable scale and shift parameters",
      "start": 1434.2,
      "duration": 4.64
    },
    {
      "text": "right so we need to access those so the",
      "start": 1436.64,
      "duration": 3.6
    },
    {
      "text": "last thing which we are going to access",
      "start": 1438.84,
      "duration": 3.24
    },
    {
      "text": "in the Transformer block keys so these",
      "start": 1440.24,
      "duration": 4.28
    },
    {
      "text": "blocks Keys is that we are going to",
      "start": 1442.08,
      "duration": 6.04
    },
    {
      "text": "access Transformer slh not/ layer",
      "start": 1444.52,
      "duration": 5.96
    },
    {
      "text": "normalization one then the scaling which",
      "start": 1448.12,
      "duration": 4.799
    },
    {
      "text": "is denoted by G and the shifting which",
      "start": 1450.48,
      "duration": 4.559
    },
    {
      "text": "is denoted by B this is the scale",
      "start": 1452.919,
      "duration": 4.961
    },
    {
      "text": "parameter and B is the shift parameter",
      "start": 1455.039,
      "duration": 4.88
    },
    {
      "text": "similarly with respect to layer",
      "start": 1457.88,
      "duration": 5.2
    },
    {
      "text": "normalization 2 we will get the scale",
      "start": 1459.919,
      "duration": 5.281
    },
    {
      "text": "parameter denoted by G and we'll get the",
      "start": 1463.08,
      "duration": 5.479
    },
    {
      "text": "shift parameter denoted by B remember",
      "start": 1465.2,
      "duration": 5.079
    },
    {
      "text": "that all of these are sub dictionaries",
      "start": 1468.559,
      "duration": 4.36
    },
    {
      "text": "within the blocks dictionary and within",
      "start": 1470.279,
      "duration": 4.201
    },
    {
      "text": "the subd dictionaries ultimately we",
      "start": 1472.919,
      "duration": 3.48
    },
    {
      "text": "access the",
      "start": 1474.48,
      "duration": 4.96
    },
    {
      "text": "parameters so if you look at the blocks",
      "start": 1476.399,
      "duration": 5.16
    },
    {
      "text": "blocks Keys within the block Keys we are",
      "start": 1479.44,
      "duration": 4.28
    },
    {
      "text": "actually getting four things we are",
      "start": 1481.559,
      "duration": 4.12
    },
    {
      "text": "getting the attention layer weights",
      "start": 1483.72,
      "duration": 4.079
    },
    {
      "text": "query key value weights we are getting",
      "start": 1485.679,
      "duration": 4.281
    },
    {
      "text": "the feed forward neural network weights",
      "start": 1487.799,
      "duration": 3.681
    },
    {
      "text": "we are getting the output projection",
      "start": 1489.96,
      "duration": 2.959
    },
    {
      "text": "layer weights and we are getting the",
      "start": 1491.48,
      "duration": 3.36
    },
    {
      "text": "layer normalization weights so",
      "start": 1492.919,
      "duration": 3.721
    },
    {
      "text": "essentially with this we get all the",
      "start": 1494.84,
      "duration": 3.52
    },
    {
      "text": "trainable parameters within the trans",
      "start": 1496.64,
      "duration": 4.68
    },
    {
      "text": "Transformer block itself awesome but our",
      "start": 1498.36,
      "duration": 4.799
    },
    {
      "text": "task is not yet over because when we",
      "start": 1501.32,
      "duration": 3.92
    },
    {
      "text": "come outside of the Transformer block",
      "start": 1503.159,
      "duration": 4.4
    },
    {
      "text": "there is this final layer normalization",
      "start": 1505.24,
      "duration": 4.48
    },
    {
      "text": "right and actually let me Mark it with a",
      "start": 1507.559,
      "duration": 3.521
    },
    {
      "text": "different color there is this final",
      "start": 1509.72,
      "duration": 3.88
    },
    {
      "text": "layer normalization and similar to the",
      "start": 1511.08,
      "duration": 4.319
    },
    {
      "text": "layer normalizations earlier it will",
      "start": 1513.6,
      "duration": 4.36
    },
    {
      "text": "have the scale and the shift now",
      "start": 1515.399,
      "duration": 5.0
    },
    {
      "text": "remember that there are entirely",
      "start": 1517.96,
      "duration": 3.76
    },
    {
      "text": "different keys for the final",
      "start": 1520.399,
      "duration": 3.121
    },
    {
      "text": "normalization layer scale and that's the",
      "start": 1521.72,
      "duration": 4.079
    },
    {
      "text": "key named G and for the final",
      "start": 1523.52,
      "duration": 4.159
    },
    {
      "text": "normalization shift there is an entirely",
      "start": 1525.799,
      "duration": 3.961
    },
    {
      "text": "new key which is is called B so the",
      "start": 1527.679,
      "duration": 4.521
    },
    {
      "text": "params is the dictionary and then when",
      "start": 1529.76,
      "duration": 5.799
    },
    {
      "text": "you do params B params B you'll get the",
      "start": 1532.2,
      "duration": 6.839
    },
    {
      "text": "final Norm shift parameters and if you",
      "start": 1535.559,
      "duration": 7.881
    },
    {
      "text": "do params of G you'll get the final Norm",
      "start": 1539.039,
      "duration": 6.961
    },
    {
      "text": "scale parameters but to access let's say",
      "start": 1543.44,
      "duration": 4.479
    },
    {
      "text": "the attention head what you'll need to",
      "start": 1546.0,
      "duration": 3.76
    },
    {
      "text": "do is that I'm just showing a part of",
      "start": 1547.919,
      "duration": 3.961
    },
    {
      "text": "the code which lies ahead but I think it",
      "start": 1549.76,
      "duration": 4.96
    },
    {
      "text": "is important to get the attention head",
      "start": 1551.88,
      "duration": 4.88
    },
    {
      "text": "you'll do params blocks so you'll access",
      "start": 1554.72,
      "duration": 4.319
    },
    {
      "text": "the blocks Keys then you'll specify that",
      "start": 1556.76,
      "duration": 5.48
    },
    {
      "text": "block number 1 2 12 Etc then you will go",
      "start": 1559.039,
      "duration": 4.721
    },
    {
      "text": "to the ATN",
      "start": 1562.24,
      "duration": 5.039
    },
    {
      "text": "Keys what we mentioned over here ATN",
      "start": 1563.76,
      "duration": 6.399
    },
    {
      "text": "Keys then we'll go to the C ATN keys and",
      "start": 1567.279,
      "duration": 5.12
    },
    {
      "text": "then we'll go to the W key this is how",
      "start": 1570.159,
      "duration": 3.88
    },
    {
      "text": "we'll access the weights of the query",
      "start": 1572.399,
      "duration": 4.64
    },
    {
      "text": "key and the value uh matrices in the",
      "start": 1574.039,
      "duration": 6.0
    },
    {
      "text": "attention block attention layers so",
      "start": 1577.039,
      "duration": 4.64
    },
    {
      "text": "that's why we need all of these five",
      "start": 1580.039,
      "duration": 3.961
    },
    {
      "text": "Keys which are returned by the parameter",
      "start": 1581.679,
      "duration": 4.681
    },
    {
      "text": "dictionary so the whole goal",
      "start": 1584.0,
      "duration": 6.039
    },
    {
      "text": "of this GP load gpt2 params from the",
      "start": 1586.36,
      "duration": 6.0
    },
    {
      "text": "tensorflow checkpoint is to get the",
      "start": 1590.039,
      "duration": 5.64
    },
    {
      "text": "parameter values uh from this checkpoint",
      "start": 1592.36,
      "duration": 5.76
    },
    {
      "text": "and then convert the parameter values",
      "start": 1595.679,
      "duration": 4.72
    },
    {
      "text": "into this params dictionary so here you",
      "start": 1598.12,
      "duration": 3.919
    },
    {
      "text": "see we Define the params dictionary",
      "start": 1600.399,
      "duration": 3.921
    },
    {
      "text": "which is empty currently and we first",
      "start": 1602.039,
      "duration": 4.64
    },
    {
      "text": "only Define the blocks keys and then we",
      "start": 1604.32,
      "duration": 4.52
    },
    {
      "text": "fill the blocks keys with every",
      "start": 1606.679,
      "duration": 4.201
    },
    {
      "text": "attention layer the feed forward neural",
      "start": 1608.84,
      "duration": 4.36
    },
    {
      "text": "network the output projection head all",
      "start": 1610.88,
      "duration": 3.72
    },
    {
      "text": "of these are already present in the",
      "start": 1613.2,
      "duration": 3.599
    },
    {
      "text": "model checkpoint but we just need to put",
      "start": 1614.6,
      "duration": 4.319
    },
    {
      "text": "them in the appropriate values I'm not",
      "start": 1616.799,
      "duration": 4.24
    },
    {
      "text": "going to explain this part of the code",
      "start": 1618.919,
      "duration": 3.921
    },
    {
      "text": "but because the main learning lies in",
      "start": 1621.039,
      "duration": 5.481
    },
    {
      "text": "you understanding these five keys so",
      "start": 1622.84,
      "duration": 5.8
    },
    {
      "text": "this is how the blocks Keys is filled up",
      "start": 1626.52,
      "duration": 5.279
    },
    {
      "text": "and similarly all the other Keys wte",
      "start": 1628.64,
      "duration": 6.24
    },
    {
      "text": "WP uh G and B they are already present",
      "start": 1631.799,
      "duration": 6.721
    },
    {
      "text": "in this uh model checkpoint path so we",
      "start": 1634.88,
      "duration": 5.48
    },
    {
      "text": "just augment the params dictionary with",
      "start": 1638.52,
      "duration": 5.519
    },
    {
      "text": "all of those keys so when you finally",
      "start": 1640.36,
      "duration": 6.12
    },
    {
      "text": "execute the load gpt2 params from TF",
      "start": 1644.039,
      "duration": 5.201
    },
    {
      "text": "checkpoint which is mentioned over here",
      "start": 1646.48,
      "duration": 4.559
    },
    {
      "text": "the params dictionary will have those",
      "start": 1649.24,
      "duration": 3.4
    },
    {
      "text": "five Keys which I mentioned to you on",
      "start": 1651.039,
      "duration": 3.321
    },
    {
      "text": "the Whiteboard this is what is happening",
      "start": 1652.64,
      "duration": 4.399
    },
    {
      "text": "in this piece of code and then when you",
      "start": 1654.36,
      "duration": 4.28
    },
    {
      "text": "finish this function you return return",
      "start": 1657.039,
      "duration": 2.601
    },
    {
      "text": "two things you return return the",
      "start": 1658.64,
      "duration": 3.36
    },
    {
      "text": "settings dictionary which consists of",
      "start": 1659.64,
      "duration": 4.36
    },
    {
      "text": "the vocabulary size context length",
      "start": 1662.0,
      "duration": 4.159
    },
    {
      "text": "embeding Dimension number of attention",
      "start": 1664.0,
      "duration": 4.32
    },
    {
      "text": "heads and number of Transformer blocks",
      "start": 1666.159,
      "duration": 4.481
    },
    {
      "text": "and you also return the params which is",
      "start": 1668.32,
      "duration": 4.16
    },
    {
      "text": "the params dictionary consisting of the",
      "start": 1670.64,
      "duration": 3.68
    },
    {
      "text": "five Keys which I just showed to you on",
      "start": 1672.48,
      "duration": 3.039
    },
    {
      "text": "the",
      "start": 1674.32,
      "duration": 3.199
    },
    {
      "text": "Whiteboard I could have just skipped",
      "start": 1675.519,
      "duration": 3.801
    },
    {
      "text": "this part but then I really wanted to",
      "start": 1677.519,
      "duration": 3.801
    },
    {
      "text": "show you the nuts and bols of how the",
      "start": 1679.32,
      "duration": 4.04
    },
    {
      "text": "downloading is done if you want to do",
      "start": 1681.32,
      "duration": 3.839
    },
    {
      "text": "research in large language models it is",
      "start": 1683.36,
      "duration": 3.439
    },
    {
      "text": "very likely that you will need to",
      "start": 1685.159,
      "duration": 4.681
    },
    {
      "text": "download the pre-trained weights to do",
      "start": 1686.799,
      "duration": 5.641
    },
    {
      "text": "uh some testing or some training and for",
      "start": 1689.84,
      "duration": 4.4
    },
    {
      "text": "that you really need to understand the",
      "start": 1692.44,
      "duration": 4.2
    },
    {
      "text": "format in which gpt2 releases these",
      "start": 1694.24,
      "duration": 4.319
    },
    {
      "text": "weights if you don't understand the",
      "start": 1696.64,
      "duration": 4.32
    },
    {
      "text": "format and if you don't understand how",
      "start": 1698.559,
      "duration": 4.801
    },
    {
      "text": "to convert this format into this",
      "start": 1700.96,
      "duration": 4.04
    },
    {
      "text": "parameter dictionary it will be",
      "start": 1703.36,
      "duration": 4.0
    },
    {
      "text": "difficult to do novel research so I hope",
      "start": 1705.0,
      "duration": 4.64
    },
    {
      "text": "you have understood this this part uh",
      "start": 1707.36,
      "duration": 6.199
    },
    {
      "text": "now let me move back to the go uh to the",
      "start": 1709.64,
      "duration": 6.8
    },
    {
      "text": "Jupiter notebook and uh until now we",
      "start": 1713.559,
      "duration": 5.921
    },
    {
      "text": "have reached this stage where uh right",
      "start": 1716.44,
      "duration": 4.959
    },
    {
      "text": "up till here where we will now what",
      "start": 1719.48,
      "duration": 3.799
    },
    {
      "text": "we'll be doing is that from this GPT",
      "start": 1721.399,
      "duration": 2.961
    },
    {
      "text": "download",
      "start": 1723.279,
      "duration": 3.76
    },
    {
      "text": "3py this python file we'll import the",
      "start": 1724.36,
      "duration": 5.48
    },
    {
      "text": "download and load gpt2 function it this",
      "start": 1727.039,
      "duration": 4.681
    },
    {
      "text": "function the download and load gpt2",
      "start": 1729.84,
      "duration": 3.959
    },
    {
      "text": "function and then what we are going to",
      "start": 1731.72,
      "duration": 3.64
    },
    {
      "text": "do is that we are just going to run this",
      "start": 1733.799,
      "duration": 3.521
    },
    {
      "text": "function we have to pass two things we",
      "start": 1735.36,
      "duration": 4.0
    },
    {
      "text": "have to pass the model size because",
      "start": 1737.32,
      "duration": 4.56
    },
    {
      "text": "remember that the model size can be 355",
      "start": 1739.36,
      "duration": 5.4
    },
    {
      "text": "million 774 million and 1558 million",
      "start": 1741.88,
      "duration": 5.639
    },
    {
      "text": "also and I encourage you to experiment",
      "start": 1744.76,
      "duration": 4.919
    },
    {
      "text": "with this after today's lecture is over",
      "start": 1747.519,
      "duration": 3.961
    },
    {
      "text": "so we put in this model size and we",
      "start": 1749.679,
      "duration": 3.681
    },
    {
      "text": "specify the directory so I have",
      "start": 1751.48,
      "duration": 4.36
    },
    {
      "text": "specified the directory to be gpt2 so",
      "start": 1753.36,
      "duration": 4.96
    },
    {
      "text": "here you can see in the folder name gpt2",
      "start": 1755.84,
      "duration": 5.04
    },
    {
      "text": "all of my files have been stored and",
      "start": 1758.32,
      "duration": 4.28
    },
    {
      "text": "then what you can do is that you can run",
      "start": 1760.88,
      "duration": 3.279
    },
    {
      "text": "this piece of code so then settings",
      "start": 1762.6,
      "duration": 3.64
    },
    {
      "text": "dictionary as we saw we'll get the",
      "start": 1764.159,
      "duration": 3.64
    },
    {
      "text": "settings dictionary and we'll get the",
      "start": 1766.24,
      "duration": 3.279
    },
    {
      "text": "par dictionary when you run this piece",
      "start": 1767.799,
      "duration": 4.841
    },
    {
      "text": "of code now when you run this as I told",
      "start": 1769.519,
      "duration": 5.16
    },
    {
      "text": "you this total size of all of this is",
      "start": 1772.64,
      "duration": 4.519
    },
    {
      "text": "around 500 megabytes initially when I",
      "start": 1774.679,
      "duration": 4.281
    },
    {
      "text": "ran this code it took a very long time",
      "start": 1777.159,
      "duration": 4.76
    },
    {
      "text": "on my laptop because my laptop kept",
      "start": 1778.96,
      "duration": 5.719
    },
    {
      "text": "crashing it was not in a good internet",
      "start": 1781.919,
      "duration": 5.321
    },
    {
      "text": "area and then I moved to another place",
      "start": 1784.679,
      "duration": 4.041
    },
    {
      "text": "where the internet connectivity was a",
      "start": 1787.24,
      "duration": 3.0
    },
    {
      "text": "bit strong so here you can see I was",
      "start": 1788.72,
      "duration": 5.12
    },
    {
      "text": "getting speeds of 5 225 mb per second",
      "start": 1790.24,
      "duration": 5.4
    },
    {
      "text": "and then this entire loading took around",
      "start": 1793.84,
      "duration": 4.199
    },
    {
      "text": "5 to 10 minutes so I encourage you to SA",
      "start": 1795.64,
      "duration": 3.84
    },
    {
      "text": "sit in a place with a good internet",
      "start": 1798.039,
      "duration": 3.401
    },
    {
      "text": "connectivity and don't restart your",
      "start": 1799.48,
      "duration": 3.84
    },
    {
      "text": "session or close your laptop during this",
      "start": 1801.44,
      "duration": 4.119
    },
    {
      "text": "time because once this is loaded the",
      "start": 1803.32,
      "duration": 3.839
    },
    {
      "text": "rest of the code proceeds in a very",
      "start": 1805.559,
      "duration": 3.72
    },
    {
      "text": "smooth manner this is the most time",
      "start": 1807.159,
      "duration": 5.0
    },
    {
      "text": "consuming part of the code until this",
      "start": 1809.279,
      "duration": 5.081
    },
    {
      "text": "point now let's say this code is",
      "start": 1812.159,
      "duration": 4.24
    },
    {
      "text": "executed after it's executed since we",
      "start": 1814.36,
      "duration": 4.84
    },
    {
      "text": "have loaded this tqdm Library we'll see",
      "start": 1816.399,
      "duration": 4.4
    },
    {
      "text": "the progress which is happening so here",
      "start": 1819.2,
      "duration": 3.359
    },
    {
      "text": "you can see that I've have reached 100%",
      "start": 1820.799,
      "duration": 4.441
    },
    {
      "text": "in all of the different steps uh so",
      "start": 1822.559,
      "duration": 4.521
    },
    {
      "text": "after this code has been completed you",
      "start": 1825.24,
      "duration": 3.84
    },
    {
      "text": "can inspect things you can inspect the",
      "start": 1827.08,
      "duration": 4.24
    },
    {
      "text": "settings dictionary and you can inspect",
      "start": 1829.08,
      "duration": 4.56
    },
    {
      "text": "the parameter dictionary keys so if you",
      "start": 1831.32,
      "duration": 3.92
    },
    {
      "text": "print out the settings dictionary you'll",
      "start": 1833.64,
      "duration": 4.84
    },
    {
      "text": "see that it has keys like n vocab nctx n",
      "start": 1835.24,
      "duration": 6.12
    },
    {
      "text": "embed n head and N layer now as I",
      "start": 1838.48,
      "duration": 5.76
    },
    {
      "text": "mentioned this this is exactly the same",
      "start": 1841.36,
      "duration": 6.28
    },
    {
      "text": "as the ham. Json file here it's just",
      "start": 1844.24,
      "duration": 6.08
    },
    {
      "text": "being converted into a dictionary now uh",
      "start": 1847.64,
      "duration": 3.96
    },
    {
      "text": "and if you print the parameter",
      "start": 1850.32,
      "duration": 3.76
    },
    {
      "text": "dictionary Keys you will see blocks b g",
      "start": 1851.6,
      "duration": 6.88
    },
    {
      "text": "WP and wte we learned about this EXA ly",
      "start": 1854.08,
      "duration": 6.36
    },
    {
      "text": "on the Whiteboard where we saw that the",
      "start": 1858.48,
      "duration": 3.4
    },
    {
      "text": "parameter dictionary will have these",
      "start": 1860.44,
      "duration": 6.199
    },
    {
      "text": "five Keys wte WP blocks G and",
      "start": 1861.88,
      "duration": 7.399
    },
    {
      "text": "B awesome so I hope you have understood",
      "start": 1866.639,
      "duration": 4.721
    },
    {
      "text": "until this part where we have actually",
      "start": 1869.279,
      "duration": 6.64
    },
    {
      "text": "loaded uh the gpt2 architecture right",
      "start": 1871.36,
      "duration": 7.799
    },
    {
      "text": "now we have loaded all of the",
      "start": 1875.919,
      "duration": 5.521
    },
    {
      "text": "parameters into our laptop and the",
      "start": 1879.159,
      "duration": 4.441
    },
    {
      "text": "parameters seem to be loaded",
      "start": 1881.44,
      "duration": 4.88
    },
    {
      "text": "correctly what we can also do is that uh",
      "start": 1883.6,
      "duration": 5.079
    },
    {
      "text": "we could have printed the",
      "start": 1886.32,
      "duration": 4.92
    },
    {
      "text": "um parameter weight contents but that",
      "start": 1888.679,
      "duration": 4.201
    },
    {
      "text": "would take a lot of screen space hence",
      "start": 1891.24,
      "duration": 3.48
    },
    {
      "text": "we only printed the parameter dictionary",
      "start": 1892.88,
      "duration": 4.679
    },
    {
      "text": "keys not its values but we can go a step",
      "start": 1894.72,
      "duration": 5.24
    },
    {
      "text": "ahead and look at the params dictionary",
      "start": 1897.559,
      "duration": 6.08
    },
    {
      "text": "and print out the wte which is the key",
      "start": 1899.96,
      "duration": 5.319
    },
    {
      "text": "corresponding to the Token embedding",
      "start": 1903.639,
      "duration": 4.0
    },
    {
      "text": "vector and we saw that the dimension",
      "start": 1905.279,
      "duration": 5.76
    },
    {
      "text": "should be 50257 rows 768 columns let's",
      "start": 1907.639,
      "duration": 5.561
    },
    {
      "text": "just see if the dimensions make sense so",
      "start": 1911.039,
      "duration": 4.76
    },
    {
      "text": "if you if you access the params",
      "start": 1913.2,
      "duration": 5.199
    },
    {
      "text": "dictionary with the key wte you get this",
      "start": 1915.799,
      "duration": 5.561
    },
    {
      "text": "tensor whose shapee is 50257 and",
      "start": 1918.399,
      "duration": 5.4
    },
    {
      "text": "768 at least the dimensions seem to be",
      "start": 1921.36,
      "duration": 6.159
    },
    {
      "text": "making sense great so these values which",
      "start": 1923.799,
      "duration": 5.321
    },
    {
      "text": "you see on the screen right now they are",
      "start": 1927.519,
      "duration": 4.04
    },
    {
      "text": "optimized values which means that for",
      "start": 1929.12,
      "duration": 5.24
    },
    {
      "text": "every token the token embedding weight",
      "start": 1931.559,
      "duration": 5.36
    },
    {
      "text": "Dimension encodes some semantic meaning",
      "start": 1934.36,
      "duration": 4.48
    },
    {
      "text": "again we should be thankful to open a",
      "start": 1936.919,
      "duration": 3.64
    },
    {
      "text": "for releasing the weights publicly",
      "start": 1938.84,
      "duration": 3.199
    },
    {
      "text": "because they would have spent about a",
      "start": 1940.559,
      "duration": 4.96
    },
    {
      "text": "million dollars or even more for this",
      "start": 1942.039,
      "duration": 6.321
    },
    {
      "text": "pre-training awesome so as a I told you",
      "start": 1945.519,
      "duration": 4.76
    },
    {
      "text": "we could have also downloaded the 355",
      "start": 1948.36,
      "duration": 4.679
    },
    {
      "text": "million 774 million or 1.5 billion",
      "start": 1950.279,
      "duration": 4.841
    },
    {
      "text": "parameter which is this release which",
      "start": 1953.039,
      "duration": 4.401
    },
    {
      "text": "gpt2 had made and you can feel free to",
      "start": 1955.12,
      "duration": 4.679
    },
    {
      "text": "experiment with that but we have loaded",
      "start": 1957.44,
      "duration": 5.119
    },
    {
      "text": "the 124 million parameter now before",
      "start": 1959.799,
      "duration": 4.6
    },
    {
      "text": "moving forward one change which we'll",
      "start": 1962.559,
      "duration": 4.521
    },
    {
      "text": "need to do is until now when we use the",
      "start": 1964.399,
      "duration": 5.0
    },
    {
      "text": "GPT configuration in this lecture series",
      "start": 1967.08,
      "duration": 4.88
    },
    {
      "text": "we used a GPT we used this thing called",
      "start": 1969.399,
      "duration": 5.28
    },
    {
      "text": "GPT config 124 million and the",
      "start": 1971.96,
      "duration": 5.04
    },
    {
      "text": "configuration was almost exactly same as",
      "start": 1974.679,
      "duration": 5.12
    },
    {
      "text": "what's actually used in gpt2 except that",
      "start": 1977.0,
      "duration": 6.2
    },
    {
      "text": "we used a context size of 256 whereas",
      "start": 1979.799,
      "duration": 5.681
    },
    {
      "text": "the actual context size is 1024 so we'll",
      "start": 1983.2,
      "duration": 4.199
    },
    {
      "text": "need to change that so what we are going",
      "start": 1985.48,
      "duration": 3.6
    },
    {
      "text": "to do is that we are going to say that",
      "start": 1987.399,
      "duration": 4.0
    },
    {
      "text": "the new configuration is the same as our",
      "start": 1989.08,
      "duration": 4.959
    },
    {
      "text": "old configuration but we'll update the",
      "start": 1991.399,
      "duration": 5.481
    },
    {
      "text": "context length to be 10 to4 and the",
      "start": 1994.039,
      "duration": 3.961
    },
    {
      "text": "second thing which we are going to",
      "start": 1996.88,
      "duration": 3.639
    },
    {
      "text": "update is the query key value bias so",
      "start": 1998.0,
      "duration": 5.559
    },
    {
      "text": "when we trained the attention mechanism",
      "start": 2000.519,
      "duration": 6.04
    },
    {
      "text": "and when we run our own llm before we",
      "start": 2003.559,
      "duration": 4.96
    },
    {
      "text": "have put this query key value bias to",
      "start": 2006.559,
      "duration": 4.281
    },
    {
      "text": "false but in gpt2 this was actually put",
      "start": 2008.519,
      "duration": 4.561
    },
    {
      "text": "to true so we are also going to put this",
      "start": 2010.84,
      "duration": 4.76
    },
    {
      "text": "to True uh here I have added a small",
      "start": 2013.08,
      "duration": 5.64
    },
    {
      "text": "note that uh bias vectors are not",
      "start": 2015.6,
      "duration": 5.24
    },
    {
      "text": "commonly used in llms anymore because",
      "start": 2018.72,
      "duration": 3.48
    },
    {
      "text": "they don't improve the modeling",
      "start": 2020.84,
      "duration": 3.36
    },
    {
      "text": "performance and they are not that",
      "start": 2022.2,
      "duration": 4.16
    },
    {
      "text": "necessary however since we are working",
      "start": 2024.2,
      "duration": 3.719
    },
    {
      "text": "with pre-trained weights we need to",
      "start": 2026.36,
      "duration": 4.159
    },
    {
      "text": "match the settings for consistency and",
      "start": 2027.919,
      "duration": 4.201
    },
    {
      "text": "that's why what we are going to do is we",
      "start": 2030.519,
      "duration": 3.921
    },
    {
      "text": "are going to enable the query key value",
      "start": 2032.12,
      "duration": 4.32
    },
    {
      "text": "bias to be equal to true and we are",
      "start": 2034.44,
      "duration": 4.079
    },
    {
      "text": "going to use the context l to be",
      "start": 2036.44,
      "duration": 5.28
    },
    {
      "text": "1024 so then we uh create an instance of",
      "start": 2038.519,
      "duration": 5.481
    },
    {
      "text": "the GPT model class with this new",
      "start": 2041.72,
      "duration": 4.36
    },
    {
      "text": "configuration I just want to show you",
      "start": 2044.0,
      "duration": 4.639
    },
    {
      "text": "the GPT model class which we have so",
      "start": 2046.08,
      "duration": 4.24
    },
    {
      "text": "that it is on the",
      "start": 2048.639,
      "duration": 4.2
    },
    {
      "text": "screen in case you have you coming to",
      "start": 2050.32,
      "duration": 4.92
    },
    {
      "text": "this lecture for the first time we have",
      "start": 2052.839,
      "duration": 4.881
    },
    {
      "text": "developed a GPT model class which looks",
      "start": 2055.24,
      "duration": 5.48
    },
    {
      "text": "something like this yeah this is our GPT",
      "start": 2057.72,
      "duration": 7.119
    },
    {
      "text": "model class uh and now the main goal",
      "start": 2060.72,
      "duration": 6.159
    },
    {
      "text": "which we have is how are we going to",
      "start": 2064.839,
      "duration": 3.401
    },
    {
      "text": "integrate the",
      "start": 2066.879,
      "duration": 3.121
    },
    {
      "text": "weights which we have downloaded with",
      "start": 2068.24,
      "duration": 3.399
    },
    {
      "text": "the GPT model class which we have",
      "start": 2070.0,
      "duration": 4.599
    },
    {
      "text": "defined so let's learn about that a bit",
      "start": 2071.639,
      "duration": 4.641
    },
    {
      "text": "so there is a specific way in which we",
      "start": 2074.599,
      "duration": 3.841
    },
    {
      "text": "are going to do this integration so look",
      "start": 2076.28,
      "duration": 4.559
    },
    {
      "text": "at the GPT model class what we are doing",
      "start": 2078.44,
      "duration": 3.8
    },
    {
      "text": "currently is that we are just",
      "start": 2080.839,
      "duration": 2.961
    },
    {
      "text": "initializing the token embedding",
      "start": 2082.24,
      "duration": 3.28
    },
    {
      "text": "matrices the positional embedding",
      "start": 2083.8,
      "duration": 4.359
    },
    {
      "text": "matrices the Transformer blocks weights",
      "start": 2085.52,
      "duration": 4.159
    },
    {
      "text": "we are initializing them to random",
      "start": 2088.159,
      "duration": 5.081
    },
    {
      "text": "values but now our main goal is that the",
      "start": 2089.679,
      "duration": 5.641
    },
    {
      "text": "weights which we have downloaded from",
      "start": 2093.24,
      "duration": 4.839
    },
    {
      "text": "gpt2 and which are currently stored in",
      "start": 2095.32,
      "duration": 4.32
    },
    {
      "text": "this params dictionary which we have",
      "start": 2098.079,
      "duration": 4.0
    },
    {
      "text": "returned we need to somehow make sure",
      "start": 2099.64,
      "duration": 5.0
    },
    {
      "text": "that these weights are integrated with",
      "start": 2102.079,
      "duration": 4.481
    },
    {
      "text": "our GPT model class and instead of these",
      "start": 2104.64,
      "duration": 4.479
    },
    {
      "text": "random initializations using NM do nn.",
      "start": 2106.56,
      "duration": 5.039
    },
    {
      "text": "embedding we actually make the",
      "start": 2109.119,
      "duration": 4.601
    },
    {
      "text": "initializations uh from the downloaded",
      "start": 2111.599,
      "duration": 5.121
    },
    {
      "text": "gpt2 parameters so for that we first",
      "start": 2113.72,
      "duration": 5.879
    },
    {
      "text": "need to look at the Transformer block",
      "start": 2116.72,
      "duration": 4.24
    },
    {
      "text": "and I want to show you a couple of",
      "start": 2119.599,
      "duration": 4.801
    },
    {
      "text": "things in this uh Transformer block so I",
      "start": 2120.96,
      "duration": 5.639
    },
    {
      "text": "just control F here and searched for the",
      "start": 2124.4,
      "duration": 4.08
    },
    {
      "text": "Transformer block",
      "start": 2126.599,
      "duration": 4.281
    },
    {
      "text": "um yeah so here's the Transformer block",
      "start": 2128.48,
      "duration": 4.52
    },
    {
      "text": "okay what we are going to do in the code",
      "start": 2130.88,
      "duration": 4.04
    },
    {
      "text": "is that here you can see that there is a",
      "start": 2133.0,
      "duration": 4.16
    },
    {
      "text": "object called attention so that's a",
      "start": 2134.92,
      "duration": 4.56
    },
    {
      "text": "instance of the multi-ad attention class",
      "start": 2137.16,
      "duration": 3.679
    },
    {
      "text": "what we are going to do is that we are",
      "start": 2139.48,
      "duration": 2.84
    },
    {
      "text": "going to take this object and we are",
      "start": 2140.839,
      "duration": 3.921
    },
    {
      "text": "going to make sure that when you define",
      "start": 2142.32,
      "duration": 5.08
    },
    {
      "text": "this at object the query key and the",
      "start": 2144.76,
      "duration": 6.04
    },
    {
      "text": "value matrices are assigned to the query",
      "start": 2147.4,
      "duration": 5.0
    },
    {
      "text": "key and the value matrices which are",
      "start": 2150.8,
      "duration": 4.12
    },
    {
      "text": "obtained from the parameters",
      "start": 2152.4,
      "duration": 4.959
    },
    {
      "text": "dictionary uh from this dictionary over",
      "start": 2154.92,
      "duration": 5.679
    },
    {
      "text": "here this the attention layers from this",
      "start": 2157.359,
      "duration": 5.96
    },
    {
      "text": "dictionary similarly when we look at the",
      "start": 2160.599,
      "duration": 5.201
    },
    {
      "text": "feed forward neural network FF object we",
      "start": 2163.319,
      "duration": 4.121
    },
    {
      "text": "are going to make sure that this feed",
      "start": 2165.8,
      "duration": 4.279
    },
    {
      "text": "forward neural network receives values",
      "start": 2167.44,
      "duration": 4.6
    },
    {
      "text": "from this feed forward neural network",
      "start": 2170.079,
      "duration": 4.641
    },
    {
      "text": "weights dictionary which we have in the",
      "start": 2172.04,
      "duration": 3.84
    },
    {
      "text": "parameters",
      "start": 2174.72,
      "duration": 4.399
    },
    {
      "text": "dictionary so let me again take you back",
      "start": 2175.88,
      "duration": 5.719
    },
    {
      "text": "to the current code it's a bit down",
      "start": 2179.119,
      "duration": 4.681
    },
    {
      "text": "below but let me scroll down below so",
      "start": 2181.599,
      "duration": 5.641
    },
    {
      "text": "that um you understand what's really",
      "start": 2183.8,
      "duration": 5.799
    },
    {
      "text": "going on one awesome so now what we are",
      "start": 2187.24,
      "duration": 3.92
    },
    {
      "text": "going to do as I said is that we are",
      "start": 2189.599,
      "duration": 4.281
    },
    {
      "text": "going to link our GPT model class with",
      "start": 2191.16,
      "duration": 7.0
    },
    {
      "text": "the downloaded weights from open AI gpt2",
      "start": 2193.88,
      "duration": 6.0
    },
    {
      "text": "so the way we are going to do this is",
      "start": 2198.16,
      "duration": 4.199
    },
    {
      "text": "that first let's take a look at the",
      "start": 2199.88,
      "duration": 4.199
    },
    {
      "text": "attention block right let's take a look",
      "start": 2202.359,
      "duration": 3.401
    },
    {
      "text": "at the attention block and let's take a",
      "start": 2204.079,
      "duration": 3.441
    },
    {
      "text": "look at the queries the keys and the",
      "start": 2205.76,
      "duration": 4.28
    },
    {
      "text": "values so what we are doing here is that",
      "start": 2207.52,
      "duration": 5.28
    },
    {
      "text": "first let's access the queries keys and",
      "start": 2210.04,
      "duration": 5.88
    },
    {
      "text": "the values downloaded from open a gpt2",
      "start": 2212.8,
      "duration": 4.96
    },
    {
      "text": "and the way to access it as we have",
      "start": 2215.92,
      "duration": 3.84
    },
    {
      "text": "already seen is that you go to the",
      "start": 2217.76,
      "duration": 4.28
    },
    {
      "text": "params dictionary you go to the blocks",
      "start": 2219.76,
      "duration": 4.559
    },
    {
      "text": "Keys then you go to the Transformer sub",
      "start": 2222.04,
      "duration": 6.44
    },
    {
      "text": "Keys the hn the ATN the C ATN and the W",
      "start": 2224.319,
      "duration": 5.8
    },
    {
      "text": "this is exactly how we are accessing",
      "start": 2228.48,
      "duration": 3.599
    },
    {
      "text": "these weights but remember these weights",
      "start": 2230.119,
      "duration": 3.72
    },
    {
      "text": "are Fusion of queries keys and the",
      "start": 2232.079,
      "duration": 3.881
    },
    {
      "text": "values so we are going to split these",
      "start": 2233.839,
      "duration": 4.28
    },
    {
      "text": "along the columns and then we'll get the",
      "start": 2235.96,
      "duration": 4.0
    },
    {
      "text": "queries weight Matrix the keys weight",
      "start": 2238.119,
      "duration": 4.24
    },
    {
      "text": "Matrix and the values weight Matrix as I",
      "start": 2239.96,
      "duration": 4.72
    },
    {
      "text": "told you before we are going to get the",
      "start": 2242.359,
      "duration": 4.48
    },
    {
      "text": "at object remember I showed you in the",
      "start": 2244.68,
      "duration": 5.0
    },
    {
      "text": "Transformer block class the at object",
      "start": 2246.839,
      "duration": 4.48
    },
    {
      "text": "and then in that object I'm going to",
      "start": 2249.68,
      "duration": 4.88
    },
    {
      "text": "assign the queries the key and the value",
      "start": 2251.319,
      "duration": 7.76
    },
    {
      "text": "weight equal to the qore W the Kore W",
      "start": 2254.56,
      "duration": 7.2
    },
    {
      "text": "and the Vore W which has been obtained",
      "start": 2259.079,
      "duration": 5.0
    },
    {
      "text": "from open a gp22 that's it it's as",
      "start": 2261.76,
      "duration": 4.44
    },
    {
      "text": "simple as that this right here is the",
      "start": 2264.079,
      "duration": 3.201
    },
    {
      "text": "assignment",
      "start": 2266.2,
      "duration": 3.76
    },
    {
      "text": "step and the A and assign is the",
      "start": 2267.28,
      "duration": 4.88
    },
    {
      "text": "function which we have defined here what",
      "start": 2269.96,
      "duration": 4.0
    },
    {
      "text": "this assign does is that it takes left",
      "start": 2272.16,
      "duration": 4.4
    },
    {
      "text": "and right and uh it will first check",
      "start": 2273.96,
      "duration": 4.56
    },
    {
      "text": "whether these two values the shape is",
      "start": 2276.56,
      "duration": 4.48
    },
    {
      "text": "matching and if the shape is matching we",
      "start": 2278.52,
      "duration": 4.2
    },
    {
      "text": "just return the right values which means",
      "start": 2281.04,
      "duration": 3.279
    },
    {
      "text": "the left is just assigned the value",
      "start": 2282.72,
      "duration": 3.879
    },
    {
      "text": "equal to the right and then we return it",
      "start": 2284.319,
      "duration": 4.52
    },
    {
      "text": "if the shape does not match we it will",
      "start": 2286.599,
      "duration": 4.201
    },
    {
      "text": "give us an error and that means that we",
      "start": 2288.839,
      "duration": 5.0
    },
    {
      "text": "are not loading the gpt2 weights",
      "start": 2290.8,
      "duration": 4.6
    },
    {
      "text": "correctly and not assigning them",
      "start": 2293.839,
      "duration": 3.881
    },
    {
      "text": "correctly so this is the part where the",
      "start": 2295.4,
      "duration": 5.08
    },
    {
      "text": "trans uh where the attention block query",
      "start": 2297.72,
      "duration": 4.72
    },
    {
      "text": "key and the value weight matrices are",
      "start": 2300.48,
      "duration": 6.24
    },
    {
      "text": "updated similarly in this part the bias",
      "start": 2302.44,
      "duration": 6.24
    },
    {
      "text": "is updated so it's the same as the",
      "start": 2306.72,
      "duration": 4.48
    },
    {
      "text": "earlier part but then W is replaced with",
      "start": 2308.68,
      "duration": 5.36
    },
    {
      "text": "b um to update the bias",
      "start": 2311.2,
      "duration": 5.36
    },
    {
      "text": "terms now if you look at the Transformer",
      "start": 2314.04,
      "duration": 4.079
    },
    {
      "text": "block there are other things also there",
      "start": 2316.56,
      "duration": 4.12
    },
    {
      "text": "is this output projection layer which is",
      "start": 2318.119,
      "duration": 4.48
    },
    {
      "text": "accessible to trans through Transformer",
      "start": 2320.68,
      "duration": 4.96
    },
    {
      "text": "dh- at and-",
      "start": 2322.599,
      "duration": 5.641
    },
    {
      "text": "c-w so what we are going to do is that",
      "start": 2325.64,
      "duration": 4.88
    },
    {
      "text": "again we are going to access this output",
      "start": 2328.24,
      "duration": 4.119
    },
    {
      "text": "projection layer weights and we are",
      "start": 2330.52,
      "duration": 3.559
    },
    {
      "text": "going to assign these weights downloaded",
      "start": 2332.359,
      "duration": 5.24
    },
    {
      "text": "from open a to the at. output projection",
      "start": 2334.079,
      "duration": 5.28
    },
    {
      "text": "weight so we are going to look at the at",
      "start": 2337.599,
      "duration": 3.681
    },
    {
      "text": "object again and then we are going to",
      "start": 2339.359,
      "duration": 3.76
    },
    {
      "text": "assign output projection weights equal",
      "start": 2341.28,
      "duration": 3.799
    },
    {
      "text": "to what we have downloaded and this is",
      "start": 2343.119,
      "duration": 4.841
    },
    {
      "text": "the same for weights as well as biases",
      "start": 2345.079,
      "duration": 4.641
    },
    {
      "text": "right then we are going to look at the",
      "start": 2347.96,
      "duration": 3.96
    },
    {
      "text": "feed forward neural network what I'm",
      "start": 2349.72,
      "duration": 3.96
    },
    {
      "text": "highlighting on the screen right now is",
      "start": 2351.92,
      "duration": 3.36
    },
    {
      "text": "for the first layer which is the fully",
      "start": 2353.68,
      "duration": 3.8
    },
    {
      "text": "connected layer as I've shown over here",
      "start": 2355.28,
      "duration": 4.559
    },
    {
      "text": "the feed forward neural network has two",
      "start": 2357.48,
      "duration": 4.24
    },
    {
      "text": "layers the fully connected layer and the",
      "start": 2359.839,
      "duration": 3.76
    },
    {
      "text": "projection layer so in the fully",
      "start": 2361.72,
      "duration": 3.639
    },
    {
      "text": "connected layer what we are doing here",
      "start": 2363.599,
      "duration": 4.601
    },
    {
      "text": "is that we are accessing ing the weights",
      "start": 2365.359,
      "duration": 4.521
    },
    {
      "text": "and the biases of the fully connected",
      "start": 2368.2,
      "duration": 5.04
    },
    {
      "text": "layer from the gpt2 downloaded values",
      "start": 2369.88,
      "duration": 5.719
    },
    {
      "text": "and then we are assigning these weights",
      "start": 2373.24,
      "duration": 5.44
    },
    {
      "text": "and biases to the FF object which we saw",
      "start": 2375.599,
      "duration": 5.201
    },
    {
      "text": "in the Transformer block so that way the",
      "start": 2378.68,
      "duration": 4.08
    },
    {
      "text": "neural network the fully connected layer",
      "start": 2380.8,
      "duration": 4.6
    },
    {
      "text": "weights and biases are equal to the gpt2",
      "start": 2382.76,
      "duration": 5.16
    },
    {
      "text": "downloaded weights and biases now this",
      "start": 2385.4,
      "duration": 4.24
    },
    {
      "text": "same thing is done for the second layer",
      "start": 2387.92,
      "duration": 3.24
    },
    {
      "text": "which is the projection or the output",
      "start": 2389.64,
      "duration": 3.52
    },
    {
      "text": "layer of the multi-layer perceptron or",
      "start": 2391.16,
      "duration": 4.76
    },
    {
      "text": "the feed forward neural network and then",
      "start": 2393.16,
      "duration": 5.32
    },
    {
      "text": "finally we come to the last uh puzzle or",
      "start": 2395.92,
      "duration": 3.84
    },
    {
      "text": "the last building block of the",
      "start": 2398.48,
      "duration": 4.28
    },
    {
      "text": "Transformers rather and that is the",
      "start": 2399.76,
      "duration": 5.599
    },
    {
      "text": "layer normalization so there are two",
      "start": 2402.76,
      "duration": 3.92
    },
    {
      "text": "layer normalization the layer",
      "start": 2405.359,
      "duration": 2.681
    },
    {
      "text": "normalization one and Layer",
      "start": 2406.68,
      "duration": 4.04
    },
    {
      "text": "normalization Two and both have scale as",
      "start": 2408.04,
      "duration": 3.72
    },
    {
      "text": "well as",
      "start": 2410.72,
      "duration": 3.48
    },
    {
      "text": "shift right so this is what's Happening",
      "start": 2411.76,
      "duration": 4.599
    },
    {
      "text": "Here what I'm highlighting right now we",
      "start": 2414.2,
      "duration": 5.48
    },
    {
      "text": "are accessing the uh shift and the scale",
      "start": 2416.359,
      "duration": 6.24
    },
    {
      "text": "parameters from the gpt2 downloaded and",
      "start": 2419.68,
      "duration": 6.0
    },
    {
      "text": "then we're assigning those parameters to",
      "start": 2422.599,
      "duration": 5.52
    },
    {
      "text": "our GPT model class and what I'm",
      "start": 2425.68,
      "duration": 4.12
    },
    {
      "text": "highlighting on the screen right now is",
      "start": 2428.119,
      "duration": 3.761
    },
    {
      "text": "the similar process done for the second",
      "start": 2429.8,
      "duration": 4.48
    },
    {
      "text": "normalization layer which comes after",
      "start": 2431.88,
      "duration": 3.8
    },
    {
      "text": "the attention mechanism in the",
      "start": 2434.28,
      "duration": 5.28
    },
    {
      "text": "Transformer block okay now when we come",
      "start": 2435.68,
      "duration": 5.56
    },
    {
      "text": "out of the Transformer block you see",
      "start": 2439.56,
      "duration": 3.4
    },
    {
      "text": "there is another normalization layer",
      "start": 2441.24,
      "duration": 3.92
    },
    {
      "text": "right the final normalization layer and",
      "start": 2442.96,
      "duration": 4.28
    },
    {
      "text": "it just accessible through G and the B",
      "start": 2445.16,
      "duration": 4.32
    },
    {
      "text": "keys so what we are doing is that we are",
      "start": 2447.24,
      "duration": 4.76
    },
    {
      "text": "accessing the param G and the params B",
      "start": 2449.48,
      "duration": 5.24
    },
    {
      "text": "and then we are assigning our GPT model",
      "start": 2452.0,
      "duration": 4.839
    },
    {
      "text": "class scale and shift values to whatever",
      "start": 2454.72,
      "duration": 4.08
    },
    {
      "text": "is down loed from",
      "start": 2456.839,
      "duration": 5.121
    },
    {
      "text": "gpt2 now you must be thinking that okay",
      "start": 2458.8,
      "duration": 4.96
    },
    {
      "text": "there is if I look at the architecture",
      "start": 2461.96,
      "duration": 3.84
    },
    {
      "text": "closely there is this final layer there",
      "start": 2463.76,
      "duration": 3.8
    },
    {
      "text": "is this linear output",
      "start": 2465.8,
      "duration": 4.84
    },
    {
      "text": "layer and careful readers might remember",
      "start": 2467.56,
      "duration": 5.08
    },
    {
      "text": "that this linear output layer also is a",
      "start": 2470.64,
      "duration": 4.52
    },
    {
      "text": "neural network and where do we get the",
      "start": 2472.64,
      "duration": 4.479
    },
    {
      "text": "dimensions where do we get the weights",
      "start": 2475.16,
      "duration": 3.72
    },
    {
      "text": "of these we did not download this from",
      "start": 2477.119,
      "duration": 5.521
    },
    {
      "text": "GPT right so the way gpt2 was designed",
      "start": 2478.88,
      "duration": 5.719
    },
    {
      "text": "is that it uses this concept of weight",
      "start": 2482.64,
      "duration": 5.52
    },
    {
      "text": "tying which means that the token",
      "start": 2484.599,
      "duration": 6.921
    },
    {
      "text": "embedding weights are used for",
      "start": 2488.16,
      "duration": 6.0
    },
    {
      "text": "constructing this output head so the",
      "start": 2491.52,
      "duration": 4.48
    },
    {
      "text": "same token embedding weights are used",
      "start": 2494.16,
      "duration": 4.159
    },
    {
      "text": "for this output head layer so we don't",
      "start": 2496.0,
      "duration": 3.92
    },
    {
      "text": "have to Define any new weights for this",
      "start": 2498.319,
      "duration": 3.52
    },
    {
      "text": "layer we recycle the same weights what's",
      "start": 2499.92,
      "duration": 4.199
    },
    {
      "text": "used in the token embedding weight tying",
      "start": 2501.839,
      "duration": 4.641
    },
    {
      "text": "is not used these days too much but it",
      "start": 2504.119,
      "duration": 4.841
    },
    {
      "text": "was used in the gpt2 architecture and so",
      "start": 2506.48,
      "duration": 4.16
    },
    {
      "text": "we are also using the concept of weight",
      "start": 2508.96,
      "duration": 3.92
    },
    {
      "text": "tying that actually brings the total",
      "start": 2510.64,
      "duration": 4.84
    },
    {
      "text": "parameters from 162 million to 124",
      "start": 2512.88,
      "duration": 4.64
    },
    {
      "text": "million if you don't do weight time the",
      "start": 2515.48,
      "duration": 4.68
    },
    {
      "text": "number of parameters will be 164",
      "start": 2517.52,
      "duration": 5.64
    },
    {
      "text": "million awesome right so what we have",
      "start": 2520.16,
      "duration": 5.48
    },
    {
      "text": "done here is that until now this this",
      "start": 2523.16,
      "duration": 4.159
    },
    {
      "text": "piece of code over here load weights",
      "start": 2525.64,
      "duration": 4.439
    },
    {
      "text": "into GPT this code what it does is that",
      "start": 2527.319,
      "duration": 5.0
    },
    {
      "text": "it takes two values the first it takes",
      "start": 2530.079,
      "duration": 4.841
    },
    {
      "text": "the instance of the GPT model class and",
      "start": 2532.319,
      "duration": 4.161
    },
    {
      "text": "the second it takes the params",
      "start": 2534.92,
      "duration": 3.679
    },
    {
      "text": "dictionary so it takes this dictionary",
      "start": 2536.48,
      "duration": 3.68
    },
    {
      "text": "which essentially contains this",
      "start": 2538.599,
      "duration": 3.041
    },
    {
      "text": "dictionary which essentially contains",
      "start": 2540.16,
      "duration": 4.56
    },
    {
      "text": "all the weights of gpt2 and then it just",
      "start": 2541.64,
      "duration": 5.52
    },
    {
      "text": "assigns all of these weights into the GP",
      "start": 2544.72,
      "duration": 4.52
    },
    {
      "text": "G PT model that's what this load weights",
      "start": 2547.16,
      "duration": 4.76
    },
    {
      "text": "into GPT is doing and now if you see",
      "start": 2549.24,
      "duration": 4.24
    },
    {
      "text": "above we have already created an",
      "start": 2551.92,
      "duration": 4.24
    },
    {
      "text": "instance of the GPT model class and if",
      "start": 2553.48,
      "duration": 4.48
    },
    {
      "text": "you scroll even above we have already",
      "start": 2556.16,
      "duration": 4.679
    },
    {
      "text": "got the we have already got the params",
      "start": 2557.96,
      "duration": 4.92
    },
    {
      "text": "dictionary awesome right so now we just",
      "start": 2560.839,
      "duration": 4.48
    },
    {
      "text": "need to call this function and that's",
      "start": 2562.88,
      "duration": 3.8
    },
    {
      "text": "exactly what we are doing here we are",
      "start": 2565.319,
      "duration": 3.0
    },
    {
      "text": "calling this function load weights into",
      "start": 2566.68,
      "duration": 3.96
    },
    {
      "text": "GPT what this function does is that it",
      "start": 2568.319,
      "duration": 5.28
    },
    {
      "text": "takes the params values and then it uh",
      "start": 2570.64,
      "duration": 4.919
    },
    {
      "text": "which are downloaded from gpt2 and then",
      "start": 2573.599,
      "duration": 5.48
    },
    {
      "text": "it loads into our GPT model instance",
      "start": 2575.559,
      "duration": 5.681
    },
    {
      "text": "which means that our own GPT model which",
      "start": 2579.079,
      "duration": 4.641
    },
    {
      "text": "we have constructed from scratch is now",
      "start": 2581.24,
      "duration": 4.839
    },
    {
      "text": "fully ready it's fully functional to be",
      "start": 2583.72,
      "duration": 5.2
    },
    {
      "text": "tested so now let's go ahead and do the",
      "start": 2586.079,
      "duration": 4.361
    },
    {
      "text": "most exciting thing in this lecture",
      "start": 2588.92,
      "duration": 3.439
    },
    {
      "text": "which you all have been waiting for let",
      "start": 2590.44,
      "duration": 6.28
    },
    {
      "text": "us test our model which we used our own",
      "start": 2592.359,
      "duration": 6.921
    },
    {
      "text": "architecture with the gpt2 pre-train",
      "start": 2596.72,
      "duration": 4.92
    },
    {
      "text": "weights and let's see what the output is",
      "start": 2599.28,
      "duration": 5.0
    },
    {
      "text": "great so now let's go ahead and test our",
      "start": 2601.64,
      "duration": 5.36
    },
    {
      "text": "model uh so here you can see that here's",
      "start": 2604.28,
      "duration": 4.319
    },
    {
      "text": "the the generate function which",
      "start": 2607.0,
      "duration": 3.92
    },
    {
      "text": "basically generates new tokens and we",
      "start": 2608.599,
      "duration": 4.321
    },
    {
      "text": "are passing the model which we have",
      "start": 2610.92,
      "duration": 4.28
    },
    {
      "text": "defined and that this model now had has",
      "start": 2612.92,
      "duration": 4.32
    },
    {
      "text": "the weights which have been downloaded",
      "start": 2615.2,
      "duration": 5.119
    },
    {
      "text": "from gpt2 the input token IDs are every",
      "start": 2617.24,
      "duration": 5.28
    },
    {
      "text": "effort moves you and the maximum number",
      "start": 2620.319,
      "duration": 4.841
    },
    {
      "text": "of new tokens is 25 I have defined the",
      "start": 2622.52,
      "duration": 5.68
    },
    {
      "text": "temperature not too high 1.5 and the top",
      "start": 2625.16,
      "duration": 5.6
    },
    {
      "text": "K is 50 which means that 50 tokens will",
      "start": 2628.2,
      "duration": 4.639
    },
    {
      "text": "have the chance or the opportunity to be",
      "start": 2630.76,
      "duration": 5.88
    },
    {
      "text": "in the among the maximum new tokens or",
      "start": 2632.839,
      "duration": 7.081
    },
    {
      "text": "to be in the generated token so before",
      "start": 2636.64,
      "duration": 5.12
    },
    {
      "text": "we see the output for this let me show",
      "start": 2639.92,
      "duration": 3.199
    },
    {
      "text": "you our",
      "start": 2641.76,
      "duration": 4.68
    },
    {
      "text": "performance without using gpt2 weights",
      "start": 2643.119,
      "duration": 5.361
    },
    {
      "text": "and here you can see that if we do not",
      "start": 2646.44,
      "duration": 4.32
    },
    {
      "text": "use gpt2 weights we were getting",
      "start": 2648.48,
      "duration": 3.839
    },
    {
      "text": "something like this which did not make",
      "start": 2650.76,
      "duration": 3.96
    },
    {
      "text": "any sense at all and now what we are",
      "start": 2652.319,
      "duration": 4.841
    },
    {
      "text": "going to do is that we are going to run",
      "start": 2654.72,
      "duration": 4.599
    },
    {
      "text": "this right now so I'm going to run this",
      "start": 2657.16,
      "duration": 4.399
    },
    {
      "text": "live and just to show you that once you",
      "start": 2659.319,
      "duration": 5.28
    },
    {
      "text": "load the weights the running actually",
      "start": 2661.559,
      "duration": 4.681
    },
    {
      "text": "this code does not take too much time",
      "start": 2664.599,
      "duration": 2.96
    },
    {
      "text": "because we have already",
      "start": 2666.24,
      "duration": 3.2
    },
    {
      "text": "we already have pre-trained weights so",
      "start": 2667.559,
      "duration": 3.56
    },
    {
      "text": "here you can see with the star symbol",
      "start": 2669.44,
      "duration": 4.6
    },
    {
      "text": "that it's running right now and now it",
      "start": 2671.119,
      "duration": 5.281
    },
    {
      "text": "generated so every effort moves you",
      "start": 2674.04,
      "duration": 5.319
    },
    {
      "text": "toward finding an ideal new way to",
      "start": 2676.4,
      "duration": 4.84
    },
    {
      "text": "practice something what makes us want to",
      "start": 2679.359,
      "duration": 4.441
    },
    {
      "text": "be on top of that this is incredible",
      "start": 2681.24,
      "duration": 5.4
    },
    {
      "text": "right this output sentence is much more",
      "start": 2683.8,
      "duration": 4.559
    },
    {
      "text": "coherent than what we had obtained",
      "start": 2686.64,
      "duration": 4.56
    },
    {
      "text": "earlier so right now in this lecture we",
      "start": 2688.359,
      "duration": 5.48
    },
    {
      "text": "have built our own GPT from scratch and",
      "start": 2691.2,
      "duration": 5.6
    },
    {
      "text": "it seems to be working that's incredible",
      "start": 2693.839,
      "duration": 4.441
    },
    {
      "text": "all all the other students which have",
      "start": 2696.8,
      "duration": 3.319
    },
    {
      "text": "not been through this course or who have",
      "start": 2698.28,
      "duration": 4.24
    },
    {
      "text": "not seen these lectures would be just",
      "start": 2700.119,
      "duration": 5.161
    },
    {
      "text": "using chat GPT but now we have built our",
      "start": 2702.52,
      "duration": 4.839
    },
    {
      "text": "own GPT from scratch isn't that",
      "start": 2705.28,
      "duration": 4.52
    },
    {
      "text": "incredible it took us a long time to get",
      "start": 2707.359,
      "duration": 7.561
    },
    {
      "text": "here and the code length is also um the",
      "start": 2709.8,
      "duration": 8.24
    },
    {
      "text": "code length is also pretty large so you",
      "start": 2714.92,
      "duration": 5.199
    },
    {
      "text": "can scroll above and see how long the",
      "start": 2718.04,
      "duration": 4.319
    },
    {
      "text": "code has become but it's all worth it",
      "start": 2720.119,
      "duration": 4.081
    },
    {
      "text": "because we have learned how to build GPT",
      "start": 2722.359,
      "duration": 4.0
    },
    {
      "text": "from scratch now what you can do is you",
      "start": 2724.2,
      "duration": 4.56
    },
    {
      "text": "can go ahead and do your own research so",
      "start": 2726.359,
      "duration": 4.561
    },
    {
      "text": "for example if you want to change the",
      "start": 2728.76,
      "duration": 4.88
    },
    {
      "text": "temperature value to 10 I know that this",
      "start": 2730.92,
      "duration": 4.679
    },
    {
      "text": "is not good but I want to see the effect",
      "start": 2733.64,
      "duration": 3.8
    },
    {
      "text": "of a higher temperature value we cannot",
      "start": 2735.599,
      "duration": 4.361
    },
    {
      "text": "do this using chat GPT right but now",
      "start": 2737.44,
      "duration": 4.56
    },
    {
      "text": "since we have built our own GPT we can",
      "start": 2739.96,
      "duration": 4.119
    },
    {
      "text": "explore with so many things so here you",
      "start": 2742.0,
      "duration": 3.76
    },
    {
      "text": "see I've increased the temperature value",
      "start": 2744.079,
      "duration": 3.841
    },
    {
      "text": "to 10 and let's see the output text",
      "start": 2745.76,
      "duration": 4.04
    },
    {
      "text": "which is coming right now ideally it",
      "start": 2747.92,
      "duration": 3.56
    },
    {
      "text": "should be a bit random because",
      "start": 2749.8,
      "duration": 3.4
    },
    {
      "text": "increasing the temperature increases the",
      "start": 2751.48,
      "duration": 4.44
    },
    {
      "text": "entropy now see the output every effort",
      "start": 2753.2,
      "duration": 4.56
    },
    {
      "text": "moves you towards finding an ideal new",
      "start": 2755.92,
      "duration": 4.32
    },
    {
      "text": "set piece but only at times or for hours",
      "start": 2757.76,
      "duration": 5.28
    },
    {
      "text": "I was working on my first game called G",
      "start": 2760.24,
      "duration": 4.4
    },
    {
      "text": "so as expected increasing the",
      "start": 2763.04,
      "duration": 4.48
    },
    {
      "text": "temperature has given me random outputs",
      "start": 2764.64,
      "duration": 4.76
    },
    {
      "text": "but you see now this opens the door to",
      "start": 2767.52,
      "duration": 4.2
    },
    {
      "text": "so much more creativity hyperparameter",
      "start": 2769.4,
      "duration": 4.88
    },
    {
      "text": "tuning even you can do research such as",
      "start": 2771.72,
      "duration": 4.72
    },
    {
      "text": "small language models what if you want",
      "start": 2774.28,
      "duration": 3.4
    },
    {
      "text": "to change the",
      "start": 2776.44,
      "duration": 3.6
    },
    {
      "text": "architecture you can now easily go ahead",
      "start": 2777.68,
      "duration": 4.28
    },
    {
      "text": "and change the architecture right all",
      "start": 2780.04,
      "duration": 3.72
    },
    {
      "text": "you need to do is that you need to go to",
      "start": 2781.96,
      "duration": 4.399
    },
    {
      "text": "this GPT model uh once you have",
      "start": 2783.76,
      "duration": 3.88
    },
    {
      "text": "understood the the code once you have",
      "start": 2786.359,
      "duration": 2.96
    },
    {
      "text": "seen the previous lectures you need to",
      "start": 2787.64,
      "duration": 3.88
    },
    {
      "text": "go to the GPT model which we have",
      "start": 2789.319,
      "duration": 5.28
    },
    {
      "text": "defined so here's the GPT model right",
      "start": 2791.52,
      "duration": 4.76
    },
    {
      "text": "over here and then you can add or",
      "start": 2794.599,
      "duration": 3.681
    },
    {
      "text": "subtract a few layers what if you we",
      "start": 2796.28,
      "duration": 4.079
    },
    {
      "text": "don't need 12 Transformer blocks what if",
      "start": 2798.28,
      "duration": 3.799
    },
    {
      "text": "you want to test with a smaller language",
      "start": 2800.359,
      "duration": 4.0
    },
    {
      "text": "model all of these experiments now",
      "start": 2802.079,
      "duration": 4.52
    },
    {
      "text": "remain open to you so this lecture",
      "start": 2804.359,
      "duration": 4.121
    },
    {
      "text": "series is also the pathway for you to",
      "start": 2806.599,
      "duration": 3.841
    },
    {
      "text": "become a large language model researcher",
      "start": 2808.48,
      "duration": 4.24
    },
    {
      "text": "or a machine learning researcher because",
      "start": 2810.44,
      "duration": 3.919
    },
    {
      "text": "now you have something which works on",
      "start": 2812.72,
      "duration": 3.599
    },
    {
      "text": "your local computer and runs in a fast",
      "start": 2814.359,
      "duration": 4.041
    },
    {
      "text": "manner you can do iterations you can",
      "start": 2816.319,
      "duration": 4.121
    },
    {
      "text": "test so if you want to test the effect",
      "start": 2818.4,
      "duration": 3.8
    },
    {
      "text": "of top K if you want to test the effect",
      "start": 2820.44,
      "duration": 4.04
    },
    {
      "text": "of Maximum new tokens if you want to",
      "start": 2822.2,
      "duration": 3.96
    },
    {
      "text": "vary the number of attention heads the",
      "start": 2824.48,
      "duration": 3.56
    },
    {
      "text": "number of Transformer blocks the",
      "start": 2826.16,
      "duration": 3.64
    },
    {
      "text": "optimizer size you can even vary the",
      "start": 2828.04,
      "duration": 4.68
    },
    {
      "text": "optimizer step size so for example we",
      "start": 2829.8,
      "duration": 5.799
    },
    {
      "text": "have used Adam rate with the learning",
      "start": 2832.72,
      "duration": 6.92
    },
    {
      "text": "rate of 5 into 10us 4 weight DK of 0.1",
      "start": 2835.599,
      "duration": 6.96
    },
    {
      "text": "you can even change this and check the",
      "start": 2839.64,
      "duration": 5.16
    },
    {
      "text": "output all of this is now accessible to",
      "start": 2842.559,
      "duration": 4.361
    },
    {
      "text": "you so that's why I believe that we have",
      "start": 2844.8,
      "duration": 3.6
    },
    {
      "text": "achieved a significant Milestone",
      "start": 2846.92,
      "duration": 4.12
    },
    {
      "text": "completing this lecture if you have come",
      "start": 2848.4,
      "duration": 4.48
    },
    {
      "text": "to this lecture for the first time",
      "start": 2851.04,
      "duration": 3.4
    },
    {
      "text": "without watching the other videos in",
      "start": 2852.88,
      "duration": 5.28
    },
    {
      "text": "this lecture series uh it's amazing but",
      "start": 2854.44,
      "duration": 6.08
    },
    {
      "text": "now please go back and try to watch all",
      "start": 2858.16,
      "duration": 4.8
    },
    {
      "text": "the other videos to master your Concepts",
      "start": 2860.52,
      "duration": 5.52
    },
    {
      "text": "to become a very powerful llm engineer",
      "start": 2862.96,
      "duration": 6.2
    },
    {
      "text": "who is creating new Norms who is really",
      "start": 2866.04,
      "duration": 4.72
    },
    {
      "text": "doing Cutting Edge research you need to",
      "start": 2869.16,
      "duration": 3.28
    },
    {
      "text": "know the nuts and bolts of how the",
      "start": 2870.76,
      "duration": 4.24
    },
    {
      "text": "modeling process works and I hope that",
      "start": 2872.44,
      "duration": 5.159
    },
    {
      "text": "through this whiteboard approach",
      "start": 2875.0,
      "duration": 4.4
    },
    {
      "text": "and through this coding approach you are",
      "start": 2877.599,
      "duration": 4.72
    },
    {
      "text": "getting exposed to those nuts and bolts",
      "start": 2879.4,
      "duration": 4.84
    },
    {
      "text": "I've not seen any other content out",
      "start": 2882.319,
      "duration": 4.0
    },
    {
      "text": "there like this currently that's why",
      "start": 2884.24,
      "duration": 3.879
    },
    {
      "text": "it's very hard for researchers to even",
      "start": 2886.319,
      "duration": 4.201
    },
    {
      "text": "download or load publicly available",
      "start": 2888.119,
      "duration": 5.161
    },
    {
      "text": "weights so I'm trying my level best to",
      "start": 2890.52,
      "duration": 4.64
    },
    {
      "text": "teach you every single thing by not",
      "start": 2893.28,
      "duration": 3.92
    },
    {
      "text": "making short videos but by making longer",
      "start": 2895.16,
      "duration": 3.88
    },
    {
      "text": "format videos like this",
      "start": 2897.2,
      "duration": 4.2
    },
    {
      "text": "one and now we are confident that we",
      "start": 2899.04,
      "duration": 4.24
    },
    {
      "text": "have loaded the weights correctly why",
      "start": 2901.4,
      "duration": 3.6
    },
    {
      "text": "because the model is producing coherent",
      "start": 2903.28,
      "duration": 4.12
    },
    {
      "text": "text so again let me switch the",
      "start": 2905.0,
      "duration": 3.68
    },
    {
      "text": "temperature to",
      "start": 2907.4,
      "duration": 5.64
    },
    {
      "text": "0.1 um and let me run this again so now",
      "start": 2908.68,
      "duration": 6.08
    },
    {
      "text": "I'm running this and as you see earlier",
      "start": 2913.04,
      "duration": 3.68
    },
    {
      "text": "when the temperature was I think it was",
      "start": 2914.76,
      "duration": 3.88
    },
    {
      "text": "not 0.1 it was",
      "start": 2916.72,
      "duration": 4.8
    },
    {
      "text": "1.4 we need to run this again for 1.4",
      "start": 2918.64,
      "duration": 4.24
    },
    {
      "text": "but if you see for",
      "start": 2921.52,
      "duration": 5.68
    },
    {
      "text": "0.1 uh again the model is quite good but",
      "start": 2922.88,
      "duration": 6.08
    },
    {
      "text": "even I'll do for",
      "start": 2927.2,
      "duration": 4.04
    },
    {
      "text": "1.4 because we started with that",
      "start": 2928.96,
      "duration": 4.639
    },
    {
      "text": "condition earlier so we are confident",
      "start": 2931.24,
      "duration": 5.16
    },
    {
      "text": "that the model weights are uh loaded",
      "start": 2933.599,
      "duration": 4.2
    },
    {
      "text": "correctly",
      "start": 2936.4,
      "duration": 3.12
    },
    {
      "text": "because the model can produce coherent",
      "start": 2937.799,
      "duration": 3.841
    },
    {
      "text": "text at least the words make sense a",
      "start": 2939.52,
      "duration": 3.839
    },
    {
      "text": "tiny mistake in this process would cause",
      "start": 2941.64,
      "duration": 3.959
    },
    {
      "text": "the model to fail now in the next",
      "start": 2943.359,
      "duration": 3.72
    },
    {
      "text": "chapters what we are going to see is",
      "start": 2945.599,
      "duration": 2.76
    },
    {
      "text": "that now that we have mastered",
      "start": 2947.079,
      "duration": 2.681
    },
    {
      "text": "pre-training we are going to look at",
      "start": 2948.359,
      "duration": 4.521
    },
    {
      "text": "fine tuning so the llm subject does not",
      "start": 2949.76,
      "duration": 5.559
    },
    {
      "text": "end at pre pre-training after getting",
      "start": 2952.88,
      "duration": 3.84
    },
    {
      "text": "this model let's say if you want to",
      "start": 2955.319,
      "duration": 3.76
    },
    {
      "text": "build a text classifier let's say if you",
      "start": 2956.72,
      "duration": 4.32
    },
    {
      "text": "want to build an educational quiz app",
      "start": 2959.079,
      "duration": 4.28
    },
    {
      "text": "which is a very specific application how",
      "start": 2961.04,
      "duration": 4.279
    },
    {
      "text": "can you use the pre-train weights how",
      "start": 2963.359,
      "duration": 3.72
    },
    {
      "text": "can you fine tune these weights so that",
      "start": 2965.319,
      "duration": 3.48
    },
    {
      "text": "it's specific to the application which",
      "start": 2967.079,
      "duration": 4.161
    },
    {
      "text": "you are building we are going to learn",
      "start": 2968.799,
      "duration": 4.921
    },
    {
      "text": "about this in the next lecture so all",
      "start": 2971.24,
      "duration": 4.079
    },
    {
      "text": "the next lectures are going to be very",
      "start": 2973.72,
      "duration": 2.92
    },
    {
      "text": "interesting since they are going to be",
      "start": 2975.319,
      "duration": 3.361
    },
    {
      "text": "application oriented and for all of",
      "start": 2976.64,
      "duration": 4.12
    },
    {
      "text": "those we are going to use this GPT model",
      "start": 2978.68,
      "duration": 3.72
    },
    {
      "text": "which we ourselves have built not",
      "start": 2980.76,
      "duration": 4.24
    },
    {
      "text": "relying on any other GPT model and that",
      "start": 2982.4,
      "duration": 4.76
    },
    {
      "text": "gives us a lot of confidence as a large",
      "start": 2985.0,
      "duration": 4.28
    },
    {
      "text": "language model or a machine learning",
      "start": 2987.16,
      "duration": 4.399
    },
    {
      "text": "engineer so thanks everyone for this",
      "start": 2989.28,
      "duration": 4.2
    },
    {
      "text": "lecture this was a a bit of a long",
      "start": 2991.559,
      "duration": 4.28
    },
    {
      "text": "lecture and also a dense lecture but I",
      "start": 2993.48,
      "duration": 3.8
    },
    {
      "text": "hope you understood everything which I",
      "start": 2995.839,
      "duration": 5.161
    },
    {
      "text": "was trying to teach uh please uh try to",
      "start": 2997.28,
      "duration": 6.24
    },
    {
      "text": "reach out or comment if you have any",
      "start": 3001.0,
      "duration": 4.64
    },
    {
      "text": "doubts or any questions and I'll be",
      "start": 3003.52,
      "duration": 3.88
    },
    {
      "text": "happy to discuss this further and I'll",
      "start": 3005.64,
      "duration": 3.479
    },
    {
      "text": "also be happy to see what all research",
      "start": 3007.4,
      "duration": 3.959
    },
    {
      "text": "you have worked on by using this code",
      "start": 3009.119,
      "duration": 3.921
    },
    {
      "text": "file which I'll share with you thanks a",
      "start": 3011.359,
      "duration": 3.041
    },
    {
      "text": "lot everyone and I look forward to",
      "start": 3013.04,
      "duration": 5.279
    },
    {
      "text": "seeing you in the next lecture",
      "start": 3014.4,
      "duration": 3.919
    }
  ],
  "full_text": "[Music] hello everyone and welcome to this lecture in the build large language models from scratch Series today I am very excited for this lecture because we are going to look at how to load pre-trained open AI weights into the GPT model which we have constructed so that the text generation is coherent this lecture will serve as a culmination of all the hard work which we have put in in the previous lecture previous lectures U because we are going to use the exact GPT architecture which we have built ourselves using this schematic which I'm showing you right now and the GPT model class which we have defined in this lecture series and we are going to integrate this model class with the gpt2 open AI weights which they have publicly released so let's get started with today's lecture first let me cover what all we have completed so far until now what we saw in this pre-training series is that we first saw how to define the losses for a large language model how the cross entropy loss comes into the picture to find the loss between the llm predicted output and the target sentence then we looked at the llm pre-training Loop itself and we generated output text using our pre-training Loop the text which was generated from our Loop was not not very coherent so let me just show you uh the text which was generated using our training so this was the training Loop which we had and here you see that the input was every effort moves you and the output was not very coherent it was not making too much sense and that is understandable because we just we had just trained this on one small book with a which was a very small data set and we had run it for 10ox later what we saw was we integrated decoding strategies to reduce overfitting the first strategy we looked at was temperature scaling and then the second strategy which we looked at was topk sampling introduction of these two strategies definitely reduced overfitting a bit but even then the next sentence which was generated as you can see over here did not really make too much sense and now we have come to today's lecture where we are going to load the pre-trained weights from open AI in in an in in the hope that with the pre-trained weights from openai the next tokens which are generated or the next Tok or the next sentences which are generated they start making a lot more sense we are going to spend a lot of time in today's lecture in understanding first of all how to download these weights from open a second of all how to integrate these weights with our code which we have written already so let's Dive Right into today to lecture first I would like to thank open for releasing these weights for gpt2 the training itself might have taken around millions of dollars because the training has been done on a huge amount of data set and these gpt2 weights are now publicly available even on platforms such as gagle so without access to this weights today's lecture would not have been possible and I would just like to take this opportunity to thank all of the llm company who are in the open source domain especially and who are publicly making available all the weights which are used by these models okay so I'm going to take you to directly to code right now and let me start explaining to you the different steps in the code so previously for educational purposes we trained a small gpt2 model right which used a very limited data set it was just a book this approach allowed us to focus on the fundamentals but we did not get coherent text as the our prediction what we are going to do is that open a has fortunately shared their weights publicly so we are going to load these weights into our GPT model class and we are going to use this model class itself for the next text Generation Um one thing to note before we get started is that open originally saved the gpt2 weights via tensor flow whereas all of the coding which we have done so far in this lecture series is using pytorch so we will have to install tensor flow and one more Library which we are going to install is called tqdm to track the download progress so here you can see that in this line of code I'm installing tensor flow and I'm also installing the tqdm library the tensorflow version should be greater than or equal to 2.15 and the tqdm library version should be greater than or equal to 4.66 so you can install both of these two libraries and then import them so here I printed the tensorflow version and the tqdm version on my system once you install this these two libraries you can you can pause the video for the time being and then you can continue with the rest of this video now comes the very important step of downloading the gpt2 weights and their parameters this is a step which still might be complex to so many students and Engineers so I want to explain this in a lot of detail first of all when you go to platforms like kagle you will find files to download so these are the seven files um which contain all of the information basically and you'll see that the file size is about 500 megabytes but after you download this there is a number of pre-processing steps which need to be done before the weights can be integrated with our GPT architecture and I'm going to explain those steps to you right now so let me take you through to the vs code interface where we are going to look at this uh GPT download 3 function and or this file this code file which will help us download um these seven files and not just download we are going to extract the parameters from these seven files and then we are going to store them in a very specific format all right so here you can see the GPT download 3. py file and first of all you'll see that when you open this file it has three functions it has the down load and load gpt2 function which is the main function which we are going to look at then this function utilizes two helper functions the first is called download file and the second is load gpt2 params from the TF checkpoint so as the file names itself suggest what this main function is going to do is that it's going to do two things first it's going to download the seven files these seven files which I just showed you on kagle also it's going to download those files and it's going to save them on my Lo loal computer that's what it's going to do as the step number one and then in the Second Step what we are going to do is that we are going to take the downloaded file and then we are going to call this load gpt2 params from uh TF checkpoint and uh what this function is going to do is that this function is going to store the parameters in the dictionary called as params and this dictionary has a very specific format which we are going to see just in in a moment so just downloading the data from kaggle is not enough you need to understand the rest of the code and the format in which this params dictionary is returned so let's start understanding this code sequentially before that I just want to take a moment and explain these downloaded files so once you run this code you will see that you'll get these files which are downloaded on your local machine without even running this code you can even go to kaggle and download these seven files now if a student has not been through this llm lecture series which we have developed they will not understand these files and they might seem a bit complicated these files but for us for those students who have followed this lecture series I want to show you that now these files are very easy to understand so I want to explain each of these files sequentially the first thing is checkpoint so if you go to this file named checkpoint you will see this thing called Model checkpoint path what this essentially means is that this is the path where all the current parameters the weights of the gpt2 model are stored so you'll see that model. CPT we have three files named model. CPT the most important file is model. cp. dat this is where all the weights of the gpt2 model are stored so this checkpoint is just going to indicate the path at which the model weights are stored the second file is encoder do Json so let me come to the top of this file this file as a whole looks complex but it's actually a vocabulary it's a vocabulary of keys uh and token IDs so we have tokens here and corresponding to every token there is a token ID that's the vocabulary which we are using and if you remember the vocabulary size it's 50257 so it starts from zero and then the end of text is 50256 so there are 50257 tokens in our vocabulary then the second file is w. bpe remember if you have followed this lecture series we learned about the bite pair encoder it's a subword tokenization scheme and the way this encoder works is that it looks for pairs of tokens which are occurring the most frequently and then it merges this pair and that becomes a token in itself that's why it's called subord tokenizer so this W cap. BP gives a list of the tokens which have been merged with the highest probability at the top so all the tokens which have been merged have been mentioned in this list and the tokens which have which come with the maximum probability they at the top of this list so you might be thinking what is this g dot right this g dot is a special convention uh which basically indicates that one token has ended and a new token has begin so you can think of it as a space and which marks the beginning of a new token so it's a special convention used by open AI to indicate the end of one token and the start of a new token then the third file which I want to show you is ham. Json this is the file which essentially contains all the the settings of our model the vocabulary size which we are using is 50257 the context length which we have is 1024 the embedding Dimension is 768 what is the embedding Dimension essentially all the token IDs which you see over here if you see any any token every token has a token ID right and every token ID will be converted converted into a 768 dimensional Vector space uh these vectors are not trained and our training will involve parameters corresponding to these as well then n heads so n heads is basically the number of attention heads present in each Transformer block and N layer is essentially the number of number of Transformer blocks itself now remember these are the these parameters the setting values which I'm showing you they are for the smallest uh gpt2 model when open made the GP B2 weights public they actually made the weights of their larger models also public so 124 million 355 million 774 million and 1558 million the weights of all of these models were made public but for the sake of Simplicity we are going to look at 124 million model right now which had 12 Transformer blocks as you increase in the complexity of the gpt2 model the number of Transformer blocks also increase remember that the same code which I'm going to show you today can be run for these larger architectures as well okay so I hope you have understood the meanings of all of these files now let us start going through uh this download and load gpt2 code step by step and understand every single sentence so the first thing is that we mention the URL so we'll make a call to this API and we'll download these files download the seven files which are present on my local computer right now so the download file function will be called for this and I'm not going to go through this in detail because this is just downloading the file onto the local machine the real interest which I have is showing you what happens after the file is downloaded so after the file is downloaded first we have this PF checkpoint path which is uh the latest checkpoint directory where all the model parameters are stored so what this function will do is that tf. train so tensorflow do tr. latest checkpoint model directory so it will go to our directory and it will look for this checkpoint and it will look for this model checkpoint path which is model. CPT so it will know where the model parameters are stored awesome the next thing is that settings we are going to maintain another dictionary which is called as settings and what the settings dictionary is that it will exactly contain the same thing what is present in the ham. Json so it will contain the vocabulary size it will contain the context length the embedding Dimension the number of attention heads and the number of Transformer blocks uh so let me go back to the code again right so this is the settings dictionary and then in this last line of the code this is where all the magic is essentially happening and you really need to go into depth to understand what happens in this one line of code so here what we are doing is that uh we are looking at the model path which is given by this TF ckpt checkpoint answer flow checkpoint path and we are going to take the parameters from that path and we are going to use the settings dictionary and then the we'll return a params dictionary and the code which does that is this load gpt2 params from TF checkpoint so from the TF checkpoint parameters we are going to load the parameters into a special dictionary called as params before explaining to you what is happening in this code I first want to show you what the params dictionary looks like all right so here is how the params dictionary actually looks like the dictionary will have five Keys which I'm going to show to you right now what are these five keys and what do they exactly mean the first key which this dictionary is going to have is wte the second key is WP the third key is blocks the fourth key is the final normalization scale and the fifth key is final normalization shift to really understand these five keys and why do we need these five Keys you need to understand the GPT architecture um and we have spent a lot of time on this I just just want to summarize it quickly so that you can relate to these keys so whenever a token comes in whenever sentence comes in rather we have to first convert the input token IDs into token embeddings so this will require weights this will require parameters which need to be trained then we will need to add positional embeddings these are also parameters which need to be trained the result of token embeddings plus positional embeddings is input embeddings and these pass on to the Transformer block so already you understood the purposes of the first two keys this is for the token embeddings parameters the wp is for the positional embedding parameters now after we get the input embedding we have the Dropout layer Dropout layer has no trainable weights so we don't have any Keys corresponding to that then we move into this Transformer block the Transformer block has several places where trainable weights exist the first place is the multi-head attention layer we have queries keys and the values weight matrices over here right and these metries consists of parameters which we don't know so this is one scope for trainable parameters the second scope for trainable parameters is the speed forward neural network remember this speed forward neural network has an input so there's an input which comes in then it passes through hidden layer the the number of dimensions in the hidden layer are four times the input and then we have a final output layer which projects it back to the same size so this first layer uh so this first layer of the neural network is is the fully connected layer so I'm going to name it as FC and the second is the output projection layer so I'm going to call it as P both of these layers will have trainable weights so that's the second scope for trainable weights now if you look at the layer normalization one and layer normalization 2 let me highlight them with a different color layer normalization um let me choose purple color so if you look at layer normalization 2 and if you look at layer normalization one normally layer normalization does not have any weights because we just subtract the mean and divide by the square root of variance right but the way we have defined layer normalization is that after we do the scaling after we subtract the mean and divide by the square root of variance we also multiply with a trainable parameter called scale and we add a trainable parameter called shift it turns out that these two parameters actually make a big difference so that's why we have uh layer normalization 2 and layer normalization one also which come into the trainable weight parameters category and then we have the final output layer the final normalization layer which is another trainable weight Matrix category where wherever we have parameters to train those are Keys corresponding to this parameter dictionary so you already saw token embeddings and positional embeddings blocks corresponds to all the trainable weights and parameters in this Transformer block in this blue color Transformer block and then G and B correspond to the trainable weight parameters in this final layer Norm which is indicated as four right now so uh token embeddings right I hope you remember what what are token embeddings we have a vocabulary size of 50257 right and corresponding to every token we have a vector whose Dimension is 768 so the size of this is 50257 rows and 768 columns these are token embeddings now positional embeddings are governed by the context size in gpt2 the smallest model which we are considering the contact size is 1024 because maximum we can look at 1024 positions and then make the next token prediction the one 25th position does not matter to us so we only need for we only need embeddings corresponding to position number one position number two up till position number 1024 and these embeddings each of these embeddings also has a dimension of 768 because we need to add the token embeddings to the positional embeddings the vector dimension of both of these need to be exactly the same and that's there because it's 768 in both of these cases now so this WT key will have all these values corresponding to the Token embedding Matrix the wp key will have all the values corresponding to the positional embeddings Matrix now let's come to the blocks when you come to the blocks there are several things which which are happening in the block so let me just rub this part a bit so that things are a little more clearer over here great first we are going to look at the Mast multi-ad attention right this has the query this has the key and this as the value in GPT 2 when they release the weight they fuse this into one single big Matrix so we need all the weights corresponding to this large Matrix so that later we can split it into query key and value trainable Matrix so the way it is done in this blocks dictionary is that there will be a dictionary called or there will be a dictionary called parameter within that there will be a keys blocks this will link to another dictionary which has the keys Transformer this will link to another dictionary which is the keys H not why H not because there are 11 such 12 such Transformer blocks remember gpt2 has 12 Transformer blocks so whatever I'm showed you right now that will be replicated 12 times right so there are 12 Transformer blocks the first one is h0 then there will be a key called ATN why because we are looking at the attention mechanism right and then and this attention mechanism also has the output projection so we need to specify in the attention mechanism what we are looking for in the attention mechanism currently we are looking for CN what is CN it is the fused Matrix of query key and value and within the this fuse Matrix we are looking at the weights So within this fuse Matrix we are looking at the weights so you see that's how we access the weight Matrix of the um attention layer so we look at the blocks key within that Transformers so let me show it to you here now we look at the blocks key within that we look at the Transformers within that we look at H not then attention then C attention which is the fused query key value and then we look at the weights similarly we do all this for the biases of the qu key quy and value this is only for one Transformer Block H we are going to repeat this for H1 H2 up till h11 since there are 12 such Transformer blocks so this is how you access the weights in the attention layers of the Transformer blocks you access this is how you access the weights and biases of the attention layer in the Transformer block uh and when I say attention layer I I mean the query key and the value weight matrices and the biases corresponding to each of these right so that's the first component which we looked at the second component is this feed forward neural network now here if you look closely there are two layers there's a fully connected layer and there is a projection layer so we'll have weights and biases corresponding to this first layer as well as the second layer and that is clearly mentioned here if you look at the fully connect feed forward neural network you have the Transformer Keys within that you have the H not which is the first Transformer block within that there is a key called MLP multi-layer perceptron because we are looking at that expansion contraction neural network currently we are accessing the fully connected layer and the weights of that layer fully connected layer the biases of that layer so what I'm highlighting right now corresponds to the first the fully connected layer and its weights and biases similarly to access the weights and biases of the projection layer we just have the key as ccore PJ and we access the weights and the biases so this is how we access the weights and the biases of the feed forward neural network work within the Transformer block okay and then usually at the end of the Transformer it's not shown here but we we actually have this output projection head this output this output projection and there are weights associated with that also so to access the C projection in the OR to access the output projection layer we just do Transformers then we go to hn the first block then go to ATN c pro which is the output projection layer and we get the weights and similarly we do this for H1 H2 up to h11 so the 12 Transformer blocks so this is how you get the weights and the biases of the attention layers of the feed forward neural network and the output projection layer in the Transformer block but remember the Transformer block has two additional things it has the layer Norm one and layer Norm two and both of these have trainable scale and shift parameters right so we need to access those so the last thing which we are going to access in the Transformer block keys so these blocks Keys is that we are going to access Transformer slh not/ layer normalization one then the scaling which is denoted by G and the shifting which is denoted by B this is the scale parameter and B is the shift parameter similarly with respect to layer normalization 2 we will get the scale parameter denoted by G and we'll get the shift parameter denoted by B remember that all of these are sub dictionaries within the blocks dictionary and within the subd dictionaries ultimately we access the parameters so if you look at the blocks blocks Keys within the block Keys we are actually getting four things we are getting the attention layer weights query key value weights we are getting the feed forward neural network weights we are getting the output projection layer weights and we are getting the layer normalization weights so essentially with this we get all the trainable parameters within the trans Transformer block itself awesome but our task is not yet over because when we come outside of the Transformer block there is this final layer normalization right and actually let me Mark it with a different color there is this final layer normalization and similar to the layer normalizations earlier it will have the scale and the shift now remember that there are entirely different keys for the final normalization layer scale and that's the key named G and for the final normalization shift there is an entirely new key which is is called B so the params is the dictionary and then when you do params B params B you'll get the final Norm shift parameters and if you do params of G you'll get the final Norm scale parameters but to access let's say the attention head what you'll need to do is that I'm just showing a part of the code which lies ahead but I think it is important to get the attention head you'll do params blocks so you'll access the blocks Keys then you'll specify that block number 1 2 12 Etc then you will go to the ATN Keys what we mentioned over here ATN Keys then we'll go to the C ATN keys and then we'll go to the W key this is how we'll access the weights of the query key and the value uh matrices in the attention block attention layers so that's why we need all of these five Keys which are returned by the parameter dictionary so the whole goal of this GP load gpt2 params from the tensorflow checkpoint is to get the parameter values uh from this checkpoint and then convert the parameter values into this params dictionary so here you see we Define the params dictionary which is empty currently and we first only Define the blocks keys and then we fill the blocks keys with every attention layer the feed forward neural network the output projection head all of these are already present in the model checkpoint but we just need to put them in the appropriate values I'm not going to explain this part of the code but because the main learning lies in you understanding these five keys so this is how the blocks Keys is filled up and similarly all the other Keys wte WP uh G and B they are already present in this uh model checkpoint path so we just augment the params dictionary with all of those keys so when you finally execute the load gpt2 params from TF checkpoint which is mentioned over here the params dictionary will have those five Keys which I mentioned to you on the Whiteboard this is what is happening in this piece of code and then when you finish this function you return return two things you return return the settings dictionary which consists of the vocabulary size context length embeding Dimension number of attention heads and number of Transformer blocks and you also return the params which is the params dictionary consisting of the five Keys which I just showed to you on the Whiteboard I could have just skipped this part but then I really wanted to show you the nuts and bols of how the downloading is done if you want to do research in large language models it is very likely that you will need to download the pre-trained weights to do uh some testing or some training and for that you really need to understand the format in which gpt2 releases these weights if you don't understand the format and if you don't understand how to convert this format into this parameter dictionary it will be difficult to do novel research so I hope you have understood this this part uh now let me move back to the go uh to the Jupiter notebook and uh until now we have reached this stage where uh right up till here where we will now what we'll be doing is that from this GPT download 3py this python file we'll import the download and load gpt2 function it this function the download and load gpt2 function and then what we are going to do is that we are just going to run this function we have to pass two things we have to pass the model size because remember that the model size can be 355 million 774 million and 1558 million also and I encourage you to experiment with this after today's lecture is over so we put in this model size and we specify the directory so I have specified the directory to be gpt2 so here you can see in the folder name gpt2 all of my files have been stored and then what you can do is that you can run this piece of code so then settings dictionary as we saw we'll get the settings dictionary and we'll get the par dictionary when you run this piece of code now when you run this as I told you this total size of all of this is around 500 megabytes initially when I ran this code it took a very long time on my laptop because my laptop kept crashing it was not in a good internet area and then I moved to another place where the internet connectivity was a bit strong so here you can see I was getting speeds of 5 225 mb per second and then this entire loading took around 5 to 10 minutes so I encourage you to SA sit in a place with a good internet connectivity and don't restart your session or close your laptop during this time because once this is loaded the rest of the code proceeds in a very smooth manner this is the most time consuming part of the code until this point now let's say this code is executed after it's executed since we have loaded this tqdm Library we'll see the progress which is happening so here you can see that I've have reached 100% in all of the different steps uh so after this code has been completed you can inspect things you can inspect the settings dictionary and you can inspect the parameter dictionary keys so if you print out the settings dictionary you'll see that it has keys like n vocab nctx n embed n head and N layer now as I mentioned this this is exactly the same as the ham. Json file here it's just being converted into a dictionary now uh and if you print the parameter dictionary Keys you will see blocks b g WP and wte we learned about this EXA ly on the Whiteboard where we saw that the parameter dictionary will have these five Keys wte WP blocks G and B awesome so I hope you have understood until this part where we have actually loaded uh the gpt2 architecture right now we have loaded all of the parameters into our laptop and the parameters seem to be loaded correctly what we can also do is that uh we could have printed the um parameter weight contents but that would take a lot of screen space hence we only printed the parameter dictionary keys not its values but we can go a step ahead and look at the params dictionary and print out the wte which is the key corresponding to the Token embedding vector and we saw that the dimension should be 50257 rows 768 columns let's just see if the dimensions make sense so if you if you access the params dictionary with the key wte you get this tensor whose shapee is 50257 and 768 at least the dimensions seem to be making sense great so these values which you see on the screen right now they are optimized values which means that for every token the token embedding weight Dimension encodes some semantic meaning again we should be thankful to open a for releasing the weights publicly because they would have spent about a million dollars or even more for this pre-training awesome so as a I told you we could have also downloaded the 355 million 774 million or 1.5 billion parameter which is this release which gpt2 had made and you can feel free to experiment with that but we have loaded the 124 million parameter now before moving forward one change which we'll need to do is until now when we use the GPT configuration in this lecture series we used a GPT we used this thing called GPT config 124 million and the configuration was almost exactly same as what's actually used in gpt2 except that we used a context size of 256 whereas the actual context size is 1024 so we'll need to change that so what we are going to do is that we are going to say that the new configuration is the same as our old configuration but we'll update the context length to be 10 to4 and the second thing which we are going to update is the query key value bias so when we trained the attention mechanism and when we run our own llm before we have put this query key value bias to false but in gpt2 this was actually put to true so we are also going to put this to True uh here I have added a small note that uh bias vectors are not commonly used in llms anymore because they don't improve the modeling performance and they are not that necessary however since we are working with pre-trained weights we need to match the settings for consistency and that's why what we are going to do is we are going to enable the query key value bias to be equal to true and we are going to use the context l to be 1024 so then we uh create an instance of the GPT model class with this new configuration I just want to show you the GPT model class which we have so that it is on the screen in case you have you coming to this lecture for the first time we have developed a GPT model class which looks something like this yeah this is our GPT model class uh and now the main goal which we have is how are we going to integrate the weights which we have downloaded with the GPT model class which we have defined so let's learn about that a bit so there is a specific way in which we are going to do this integration so look at the GPT model class what we are doing currently is that we are just initializing the token embedding matrices the positional embedding matrices the Transformer blocks weights we are initializing them to random values but now our main goal is that the weights which we have downloaded from gpt2 and which are currently stored in this params dictionary which we have returned we need to somehow make sure that these weights are integrated with our GPT model class and instead of these random initializations using NM do nn. embedding we actually make the initializations uh from the downloaded gpt2 parameters so for that we first need to look at the Transformer block and I want to show you a couple of things in this uh Transformer block so I just control F here and searched for the Transformer block um yeah so here's the Transformer block okay what we are going to do in the code is that here you can see that there is a object called attention so that's a instance of the multi-ad attention class what we are going to do is that we are going to take this object and we are going to make sure that when you define this at object the query key and the value matrices are assigned to the query key and the value matrices which are obtained from the parameters dictionary uh from this dictionary over here this the attention layers from this dictionary similarly when we look at the feed forward neural network FF object we are going to make sure that this feed forward neural network receives values from this feed forward neural network weights dictionary which we have in the parameters dictionary so let me again take you back to the current code it's a bit down below but let me scroll down below so that um you understand what's really going on one awesome so now what we are going to do as I said is that we are going to link our GPT model class with the downloaded weights from open AI gpt2 so the way we are going to do this is that first let's take a look at the attention block right let's take a look at the attention block and let's take a look at the queries the keys and the values so what we are doing here is that first let's access the queries keys and the values downloaded from open a gpt2 and the way to access it as we have already seen is that you go to the params dictionary you go to the blocks Keys then you go to the Transformer sub Keys the hn the ATN the C ATN and the W this is exactly how we are accessing these weights but remember these weights are Fusion of queries keys and the values so we are going to split these along the columns and then we'll get the queries weight Matrix the keys weight Matrix and the values weight Matrix as I told you before we are going to get the at object remember I showed you in the Transformer block class the at object and then in that object I'm going to assign the queries the key and the value weight equal to the qore W the Kore W and the Vore W which has been obtained from open a gp22 that's it it's as simple as that this right here is the assignment step and the A and assign is the function which we have defined here what this assign does is that it takes left and right and uh it will first check whether these two values the shape is matching and if the shape is matching we just return the right values which means the left is just assigned the value equal to the right and then we return it if the shape does not match we it will give us an error and that means that we are not loading the gpt2 weights correctly and not assigning them correctly so this is the part where the trans uh where the attention block query key and the value weight matrices are updated similarly in this part the bias is updated so it's the same as the earlier part but then W is replaced with b um to update the bias terms now if you look at the Transformer block there are other things also there is this output projection layer which is accessible to trans through Transformer dh- at and- c-w so what we are going to do is that again we are going to access this output projection layer weights and we are going to assign these weights downloaded from open a to the at. output projection weight so we are going to look at the at object again and then we are going to assign output projection weights equal to what we have downloaded and this is the same for weights as well as biases right then we are going to look at the feed forward neural network what I'm highlighting on the screen right now is for the first layer which is the fully connected layer as I've shown over here the feed forward neural network has two layers the fully connected layer and the projection layer so in the fully connected layer what we are doing here is that we are accessing ing the weights and the biases of the fully connected layer from the gpt2 downloaded values and then we are assigning these weights and biases to the FF object which we saw in the Transformer block so that way the neural network the fully connected layer weights and biases are equal to the gpt2 downloaded weights and biases now this same thing is done for the second layer which is the projection or the output layer of the multi-layer perceptron or the feed forward neural network and then finally we come to the last uh puzzle or the last building block of the Transformers rather and that is the layer normalization so there are two layer normalization the layer normalization one and Layer normalization Two and both have scale as well as shift right so this is what's Happening Here what I'm highlighting right now we are accessing the uh shift and the scale parameters from the gpt2 downloaded and then we're assigning those parameters to our GPT model class and what I'm highlighting on the screen right now is the similar process done for the second normalization layer which comes after the attention mechanism in the Transformer block okay now when we come out of the Transformer block you see there is another normalization layer right the final normalization layer and it just accessible through G and the B keys so what we are doing is that we are accessing the param G and the params B and then we are assigning our GPT model class scale and shift values to whatever is down loed from gpt2 now you must be thinking that okay there is if I look at the architecture closely there is this final layer there is this linear output layer and careful readers might remember that this linear output layer also is a neural network and where do we get the dimensions where do we get the weights of these we did not download this from GPT right so the way gpt2 was designed is that it uses this concept of weight tying which means that the token embedding weights are used for constructing this output head so the same token embedding weights are used for this output head layer so we don't have to Define any new weights for this layer we recycle the same weights what's used in the token embedding weight tying is not used these days too much but it was used in the gpt2 architecture and so we are also using the concept of weight tying that actually brings the total parameters from 162 million to 124 million if you don't do weight time the number of parameters will be 164 million awesome right so what we have done here is that until now this this piece of code over here load weights into GPT this code what it does is that it takes two values the first it takes the instance of the GPT model class and the second it takes the params dictionary so it takes this dictionary which essentially contains this dictionary which essentially contains all the weights of gpt2 and then it just assigns all of these weights into the GP G PT model that's what this load weights into GPT is doing and now if you see above we have already created an instance of the GPT model class and if you scroll even above we have already got the we have already got the params dictionary awesome right so now we just need to call this function and that's exactly what we are doing here we are calling this function load weights into GPT what this function does is that it takes the params values and then it uh which are downloaded from gpt2 and then it loads into our GPT model instance which means that our own GPT model which we have constructed from scratch is now fully ready it's fully functional to be tested so now let's go ahead and do the most exciting thing in this lecture which you all have been waiting for let us test our model which we used our own architecture with the gpt2 pre-train weights and let's see what the output is great so now let's go ahead and test our model uh so here you can see that here's the the generate function which basically generates new tokens and we are passing the model which we have defined and that this model now had has the weights which have been downloaded from gpt2 the input token IDs are every effort moves you and the maximum number of new tokens is 25 I have defined the temperature not too high 1.5 and the top K is 50 which means that 50 tokens will have the chance or the opportunity to be in the among the maximum new tokens or to be in the generated token so before we see the output for this let me show you our performance without using gpt2 weights and here you can see that if we do not use gpt2 weights we were getting something like this which did not make any sense at all and now what we are going to do is that we are going to run this right now so I'm going to run this live and just to show you that once you load the weights the running actually this code does not take too much time because we have already we already have pre-trained weights so here you can see with the star symbol that it's running right now and now it generated so every effort moves you toward finding an ideal new way to practice something what makes us want to be on top of that this is incredible right this output sentence is much more coherent than what we had obtained earlier so right now in this lecture we have built our own GPT from scratch and it seems to be working that's incredible all all the other students which have not been through this course or who have not seen these lectures would be just using chat GPT but now we have built our own GPT from scratch isn't that incredible it took us a long time to get here and the code length is also um the code length is also pretty large so you can scroll above and see how long the code has become but it's all worth it because we have learned how to build GPT from scratch now what you can do is you can go ahead and do your own research so for example if you want to change the temperature value to 10 I know that this is not good but I want to see the effect of a higher temperature value we cannot do this using chat GPT right but now since we have built our own GPT we can explore with so many things so here you see I've increased the temperature value to 10 and let's see the output text which is coming right now ideally it should be a bit random because increasing the temperature increases the entropy now see the output every effort moves you towards finding an ideal new set piece but only at times or for hours I was working on my first game called G so as expected increasing the temperature has given me random outputs but you see now this opens the door to so much more creativity hyperparameter tuning even you can do research such as small language models what if you want to change the architecture you can now easily go ahead and change the architecture right all you need to do is that you need to go to this GPT model uh once you have understood the the code once you have seen the previous lectures you need to go to the GPT model which we have defined so here's the GPT model right over here and then you can add or subtract a few layers what if you we don't need 12 Transformer blocks what if you want to test with a smaller language model all of these experiments now remain open to you so this lecture series is also the pathway for you to become a large language model researcher or a machine learning researcher because now you have something which works on your local computer and runs in a fast manner you can do iterations you can test so if you want to test the effect of top K if you want to test the effect of Maximum new tokens if you want to vary the number of attention heads the number of Transformer blocks the optimizer size you can even vary the optimizer step size so for example we have used Adam rate with the learning rate of 5 into 10us 4 weight DK of 0.1 you can even change this and check the output all of this is now accessible to you so that's why I believe that we have achieved a significant Milestone completing this lecture if you have come to this lecture for the first time without watching the other videos in this lecture series uh it's amazing but now please go back and try to watch all the other videos to master your Concepts to become a very powerful llm engineer who is creating new Norms who is really doing Cutting Edge research you need to know the nuts and bolts of how the modeling process works and I hope that through this whiteboard approach and through this coding approach you are getting exposed to those nuts and bolts I've not seen any other content out there like this currently that's why it's very hard for researchers to even download or load publicly available weights so I'm trying my level best to teach you every single thing by not making short videos but by making longer format videos like this one and now we are confident that we have loaded the weights correctly why because the model is producing coherent text so again let me switch the temperature to 0.1 um and let me run this again so now I'm running this and as you see earlier when the temperature was I think it was not 0.1 it was 1.4 we need to run this again for 1.4 but if you see for 0.1 uh again the model is quite good but even I'll do for 1.4 because we started with that condition earlier so we are confident that the model weights are uh loaded correctly because the model can produce coherent text at least the words make sense a tiny mistake in this process would cause the model to fail now in the next chapters what we are going to see is that now that we have mastered pre-training we are going to look at fine tuning so the llm subject does not end at pre pre-training after getting this model let's say if you want to build a text classifier let's say if you want to build an educational quiz app which is a very specific application how can you use the pre-train weights how can you fine tune these weights so that it's specific to the application which you are building we are going to learn about this in the next lecture so all the next lectures are going to be very interesting since they are going to be application oriented and for all of those we are going to use this GPT model which we ourselves have built not relying on any other GPT model and that gives us a lot of confidence as a large language model or a machine learning engineer so thanks everyone for this lecture this was a a bit of a long lecture and also a dense lecture but I hope you understood everything which I was trying to teach uh please uh try to reach out or comment if you have any doubts or any questions and I'll be happy to discuss this further and I'll also be happy to see what all research you have worked on by using this code file which I'll share with you thanks a lot everyone and I look forward to seeing you in the next lecture"
}