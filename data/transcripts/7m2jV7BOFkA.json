{
  "video": {
    "video_id": "7m2jV7BOFkA",
    "title": "Evaluating fine-tuned LLM using Ollama",
    "duration": 3178.0,
    "index": 41
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 7.52
    },
    {
      "text": "hello everyone and welcome to this",
      "start": 5.2,
      "duration": 4.319
    },
    {
      "text": "lecture in the build large language",
      "start": 7.52,
      "duration": 5.159
    },
    {
      "text": "models from scratch Series today it's a",
      "start": 9.519,
      "duration": 5.04
    },
    {
      "text": "very interesting and exciting lecture",
      "start": 12.679,
      "duration": 3.68
    },
    {
      "text": "because we are going to complete the",
      "start": 14.559,
      "duration": 3.72
    },
    {
      "text": "instruction fine tuning project which we",
      "start": 16.359,
      "duration": 4.321
    },
    {
      "text": "have started in the previous lecture we",
      "start": 18.279,
      "duration": 5.481
    },
    {
      "text": "fine tuned the large language model on",
      "start": 20.68,
      "duration": 5.439
    },
    {
      "text": "this instruction input and output data",
      "start": 23.76,
      "duration": 5.759
    },
    {
      "text": "set and we proved that after the",
      "start": 26.119,
      "duration": 4.441
    },
    {
      "text": "fineable",
      "start": 29.519,
      "duration": 4.801
    },
    {
      "text": "the llm is much better at responding to",
      "start": 30.56,
      "duration": 6.08
    },
    {
      "text": "instructions so the previous lecture was",
      "start": 34.32,
      "duration": 4.04
    },
    {
      "text": "an awesome lecture and we optained a",
      "start": 36.64,
      "duration": 3.56
    },
    {
      "text": "great result at the end of it if you",
      "start": 38.36,
      "duration": 3.48
    },
    {
      "text": "have not seen the previous lecture I",
      "start": 40.2,
      "duration": 3.76
    },
    {
      "text": "would highly encourage you to go back",
      "start": 41.84,
      "duration": 5.039
    },
    {
      "text": "check that lecture again so let me just",
      "start": 43.96,
      "duration": 5.84
    },
    {
      "text": "show you in the FL map where we are in",
      "start": 46.879,
      "duration": 5.52
    },
    {
      "text": "the stage of the fine tuning process so",
      "start": 49.8,
      "duration": 5.68
    },
    {
      "text": "we have finished the stage number one so",
      "start": 52.399,
      "duration": 6.32
    },
    {
      "text": "let me change to purple ink we have",
      "start": 55.48,
      "duration": 4.84
    },
    {
      "text": "finished the stage number one which is",
      "start": 58.719,
      "duration": 3.68
    },
    {
      "text": "preparation of the data set and this",
      "start": 60.32,
      "duration": 4.479
    },
    {
      "text": "involved data set download batching the",
      "start": 62.399,
      "duration": 4.921
    },
    {
      "text": "data set and creating data loaders just",
      "start": 64.799,
      "duration": 5.081
    },
    {
      "text": "to revise this is the data set which we",
      "start": 67.32,
      "duration": 5.52
    },
    {
      "text": "have been using which consists of 1100",
      "start": 69.88,
      "duration": 5.599
    },
    {
      "text": "instruction input and output Pairs and",
      "start": 72.84,
      "duration": 4.88
    },
    {
      "text": "we are telling the llm that hey we know",
      "start": 75.479,
      "duration": 4.041
    },
    {
      "text": "that you are pre-trained very well but",
      "start": 77.72,
      "duration": 3.719
    },
    {
      "text": "you are not very good at responding to",
      "start": 79.52,
      "duration": 4.559
    },
    {
      "text": "instructions currently CCT currently and",
      "start": 81.439,
      "duration": 4.401
    },
    {
      "text": "I want to train you again so that you",
      "start": 84.079,
      "duration": 4.121
    },
    {
      "text": "are better at responding to instructions",
      "start": 85.84,
      "duration": 4.639
    },
    {
      "text": "here is the data which I have and I want",
      "start": 88.2,
      "duration": 4.199
    },
    {
      "text": "you to learn from this data so that you",
      "start": 90.479,
      "duration": 4.0
    },
    {
      "text": "get better at responding to",
      "start": 92.399,
      "duration": 4.76
    },
    {
      "text": "instructions so this is the data set we",
      "start": 94.479,
      "duration": 4.721
    },
    {
      "text": "need we spent a lot of time earlier on",
      "start": 97.159,
      "duration": 4.361
    },
    {
      "text": "batching the data set so we needed to",
      "start": 99.2,
      "duration": 5.04
    },
    {
      "text": "implement several steps so that within",
      "start": 101.52,
      "duration": 5.0
    },
    {
      "text": "each batch all of the input samples have",
      "start": 104.24,
      "duration": 6.08
    },
    {
      "text": "the same number of token IDs and after",
      "start": 106.52,
      "duration": 6.36
    },
    {
      "text": "the batching is done we created the",
      "start": 110.32,
      "duration": 4.92
    },
    {
      "text": "training we created the training the",
      "start": 112.88,
      "duration": 5.08
    },
    {
      "text": "testing and the validation data loaders",
      "start": 115.24,
      "duration": 5.76
    },
    {
      "text": "remember that we used 85 % of the data",
      "start": 117.96,
      "duration": 7.0
    },
    {
      "text": "for 85% of the data for training 10% of",
      "start": 121.0,
      "duration": 6.439
    },
    {
      "text": "the data for testing and 5% of the data",
      "start": 124.96,
      "duration": 5.159
    },
    {
      "text": "for validation and when I say data I",
      "start": 127.439,
      "duration": 4.641
    },
    {
      "text": "mean this instruction data file",
      "start": 130.119,
      "duration": 4.241
    },
    {
      "text": "instruction input and output",
      "start": 132.08,
      "duration": 5.32
    },
    {
      "text": "pairs right then we also completed stage",
      "start": 134.36,
      "duration": 5.28
    },
    {
      "text": "number two in stage number two we loaded",
      "start": 137.4,
      "duration": 4.199
    },
    {
      "text": "a pre-trained llm so we loaded the",
      "start": 139.64,
      "duration": 3.48
    },
    {
      "text": "weights which are publicly available",
      "start": 141.599,
      "duration": 4.121
    },
    {
      "text": "from gpt2 so that the model has a good",
      "start": 143.12,
      "duration": 5.04
    },
    {
      "text": "foundational base and then on top of it",
      "start": 145.72,
      "duration": 4.64
    },
    {
      "text": "in the previous lecture we find in tuned",
      "start": 148.16,
      "duration": 4.079
    },
    {
      "text": "the llm which means that we trained the",
      "start": 150.36,
      "duration": 5.68
    },
    {
      "text": "llm again um so that it learns based on",
      "start": 152.239,
      "duration": 6.801
    },
    {
      "text": "this instruction input output data",
      "start": 156.04,
      "duration": 5.6
    },
    {
      "text": "set and then we also saw the loss",
      "start": 159.04,
      "duration": 4.839
    },
    {
      "text": "function in the code so just let me take",
      "start": 161.64,
      "duration": 3.84
    },
    {
      "text": "you to code right now and show you the",
      "start": 163.879,
      "duration": 3.36
    },
    {
      "text": "loss function which we obtained so",
      "start": 165.48,
      "duration": 3.28
    },
    {
      "text": "here's the training and the validation",
      "start": 167.239,
      "duration": 3.161
    },
    {
      "text": "loss function which we obtained in the",
      "start": 168.76,
      "duration": 4.04
    },
    {
      "text": "previous lecture due to the memory and",
      "start": 170.4,
      "duration": 4.759
    },
    {
      "text": "the compute limitations on my CPU I just",
      "start": 172.8,
      "duration": 4.84
    },
    {
      "text": "ran it for one Epoch but as I mentioned",
      "start": 175.159,
      "duration": 4.761
    },
    {
      "text": "in the last lecture as well if you have",
      "start": 177.64,
      "duration": 4.28
    },
    {
      "text": "access to a GPU or if you have a",
      "start": 179.92,
      "duration": 4.52
    },
    {
      "text": "stronger PC such as a Macbook M3 for",
      "start": 181.92,
      "duration": 4.8
    },
    {
      "text": "example you can definitely try running",
      "start": 184.44,
      "duration": 5.24
    },
    {
      "text": "this code for two apox or even more we",
      "start": 186.72,
      "duration": 7.48
    },
    {
      "text": "are using a GPT gpt2 model with 355",
      "start": 189.68,
      "duration": 6.88
    },
    {
      "text": "million weights so it's a pretty huge",
      "start": 194.2,
      "duration": 5.399
    },
    {
      "text": "model and that's why a lot of llm",
      "start": 196.56,
      "duration": 5.039
    },
    {
      "text": "computations really depend on the",
      "start": 199.599,
      "duration": 4.2
    },
    {
      "text": "architecture of the CPU whether you're",
      "start": 201.599,
      "duration": 5.521
    },
    {
      "text": "using a GPU how fast is your computer",
      "start": 203.799,
      "duration": 6.401
    },
    {
      "text": "Etc anyways I I have shown you this code",
      "start": 207.12,
      "duration": 5.88
    },
    {
      "text": "on a computer with minimal configuration",
      "start": 210.2,
      "duration": 4.959
    },
    {
      "text": "so if I am able to run this on my PC I'm",
      "start": 213.0,
      "duration": 3.799
    },
    {
      "text": "pretty sure all of you would be able to",
      "start": 215.159,
      "duration": 4.561
    },
    {
      "text": "run up till here on your PC as well so",
      "start": 216.799,
      "duration": 4.561
    },
    {
      "text": "let's look at the response which we have",
      "start": 219.72,
      "duration": 4.879
    },
    {
      "text": "got we tested using one one test or one",
      "start": 221.36,
      "duration": 5.079
    },
    {
      "text": "validation sample and the validation",
      "start": 224.599,
      "duration": 4.72
    },
    {
      "text": "sample was that write a response that",
      "start": 226.439,
      "duration": 4.761
    },
    {
      "text": "appropriately completes the request and",
      "start": 229.319,
      "duration": 4.2
    },
    {
      "text": "the request was convert the active",
      "start": 231.2,
      "duration": 5.28
    },
    {
      "text": "sentence to passive convert the active",
      "start": 233.519,
      "duration": 5.241
    },
    {
      "text": "sentence to passive and the sentence was",
      "start": 236.48,
      "duration": 4.56
    },
    {
      "text": "the chef Cooks the me every day the",
      "start": 238.76,
      "duration": 4.119
    },
    {
      "text": "response given by our large language",
      "start": 241.04,
      "duration": 3.919
    },
    {
      "text": "model was the meal is prepared by the",
      "start": 242.879,
      "duration": 4.321
    },
    {
      "text": "chef every day now this was not the",
      "start": 244.959,
      "duration": 4.041
    },
    {
      "text": "correct answer right the correct answer",
      "start": 247.2,
      "duration": 4.959
    },
    {
      "text": "is the meal is cooked by the chef every",
      "start": 249.0,
      "duration": 6.159
    },
    {
      "text": "day whereas our llm used prepared and",
      "start": 252.159,
      "duration": 4.681
    },
    {
      "text": "that's probably because we did not train",
      "start": 255.159,
      "duration": 3.561
    },
    {
      "text": "it for multiple epochs due to the",
      "start": 256.84,
      "duration": 4.32
    },
    {
      "text": "compute limitations in fact I know for a",
      "start": 258.72,
      "duration": 4.16
    },
    {
      "text": "fact that if you run it for two epochs",
      "start": 261.16,
      "duration": 4.039
    },
    {
      "text": "instead of one this output changes to",
      "start": 262.88,
      "duration": 5.039
    },
    {
      "text": "the meal is cooked every day by the chef",
      "start": 265.199,
      "duration": 4.521
    },
    {
      "text": "even with one Epoch we can see that the",
      "start": 267.919,
      "duration": 4.041
    },
    {
      "text": "LM is doing much better instead of just",
      "start": 269.72,
      "duration": 5.0
    },
    {
      "text": "answering randomly before fine",
      "start": 271.96,
      "duration": 5.239
    },
    {
      "text": "tuning so this is the stage we are at",
      "start": 274.72,
      "duration": 4.36
    },
    {
      "text": "right now and now we have to move to the",
      "start": 277.199,
      "duration": 3.72
    },
    {
      "text": "third stage which is evaluating large",
      "start": 279.08,
      "duration": 4.08
    },
    {
      "text": "language models so you might be thinking",
      "start": 280.919,
      "duration": 4.361
    },
    {
      "text": "that we fine tuned the model and now LM",
      "start": 283.16,
      "duration": 4.24
    },
    {
      "text": "is responding to instructions that's",
      "start": 285.28,
      "duration": 5.44
    },
    {
      "text": "great right well the things do not stop",
      "start": 287.4,
      "duration": 6.079
    },
    {
      "text": "here because there is one big challenge",
      "start": 290.72,
      "duration": 5.0
    },
    {
      "text": "now you can see that this response the",
      "start": 293.479,
      "duration": 4.241
    },
    {
      "text": "meal is prepared by the every day by the",
      "start": 295.72,
      "duration": 4.319
    },
    {
      "text": "chef is not exactly correct because the",
      "start": 297.72,
      "duration": 4.039
    },
    {
      "text": "correct answer should be the meal is",
      "start": 300.039,
      "duration": 3.401
    },
    {
      "text": "cooked every day by the chef in fact",
      "start": 301.759,
      "duration": 6.201
    },
    {
      "text": "let's look at the correct answer um so",
      "start": 303.44,
      "duration": 7.84
    },
    {
      "text": "Chef so let me type Chef here and let's",
      "start": 307.96,
      "duration": 5.32
    },
    {
      "text": "go to",
      "start": 311.28,
      "duration": 5.8
    },
    {
      "text": "the actual",
      "start": 313.28,
      "duration": 3.8
    },
    {
      "text": "instruction yeah I think this was the",
      "start": 318.72,
      "duration": 4.24
    },
    {
      "text": "actual instruction which we had given",
      "start": 320.96,
      "duration": 3.88
    },
    {
      "text": "convert the active sentence to passive",
      "start": 322.96,
      "duration": 4.04
    },
    {
      "text": "the chef Cooks the meal every day and",
      "start": 324.84,
      "duration": 4.6
    },
    {
      "text": "this is the actual ground truth answer",
      "start": 327.0,
      "duration": 4.16
    },
    {
      "text": "the meal is scooped by the chef every",
      "start": 329.44,
      "duration": 4.08
    },
    {
      "text": "day whereas the llm which we have",
      "start": 331.16,
      "duration": 4.319
    },
    {
      "text": "trained or fine tuned answered the meal",
      "start": 333.52,
      "duration": 3.959
    },
    {
      "text": "is prepared every day by the chef so",
      "start": 335.479,
      "duration": 4.881
    },
    {
      "text": "it's clearly not correct right the real",
      "start": 337.479,
      "duration": 5.16
    },
    {
      "text": "question which we want to ask in the",
      "start": 340.36,
      "duration": 5.399
    },
    {
      "text": "evaluation stage of the llm is that how",
      "start": 342.639,
      "duration": 8.84
    },
    {
      "text": "do we know how correct are we so how to",
      "start": 345.759,
      "duration": 5.72
    },
    {
      "text": "measure how to measure the llm",
      "start": 352.4,
      "duration": 6.0
    },
    {
      "text": "performance it's not a straightforward",
      "start": 356.08,
      "duration": 3.88
    },
    {
      "text": "answer as a yes or no by binary",
      "start": 358.4,
      "duration": 3.72
    },
    {
      "text": "classification right we have to compare",
      "start": 359.96,
      "duration": 4.04
    },
    {
      "text": "between two sentences and we have to",
      "start": 362.12,
      "duration": 5.199
    },
    {
      "text": "compare and say that okay",
      "start": 364.0,
      "duration": 5.52
    },
    {
      "text": "ideally uh ideally it should have been",
      "start": 367.319,
      "duration": 4.041
    },
    {
      "text": "cooked but instead my llm is saying",
      "start": 369.52,
      "duration": 4.519
    },
    {
      "text": "prepared so I deduct some points for it",
      "start": 371.36,
      "duration": 5.04
    },
    {
      "text": "but how do we Define the scoring system",
      "start": 374.039,
      "duration": 5.0
    },
    {
      "text": "how do we compare between two sentences",
      "start": 376.4,
      "duration": 4.48
    },
    {
      "text": "and understand whether the llm is doing",
      "start": 379.039,
      "duration": 3.841
    },
    {
      "text": "a good job or the llm is not doing a",
      "start": 380.88,
      "duration": 4.0
    },
    {
      "text": "good job and that's why we have to",
      "start": 382.88,
      "duration": 4.08
    },
    {
      "text": "devote a whole separate lecture to",
      "start": 384.88,
      "duration": 4.48
    },
    {
      "text": "evaluating the large language model so",
      "start": 386.96,
      "duration": 4.4
    },
    {
      "text": "this itself consists of three stages",
      "start": 389.36,
      "duration": 3.839
    },
    {
      "text": "which we are going to look at the first",
      "start": 391.36,
      "duration": 4.6
    },
    {
      "text": "is extracting the responses from the llm",
      "start": 393.199,
      "duration": 5.081
    },
    {
      "text": "the second is qualitative evaluation and",
      "start": 395.96,
      "duration": 4.679
    },
    {
      "text": "the third is scoring the responses so",
      "start": 398.28,
      "duration": 4.52
    },
    {
      "text": "let's start implementing the first step",
      "start": 400.639,
      "duration": 4.761
    },
    {
      "text": "which is extracting the",
      "start": 402.8,
      "duration": 5.04
    },
    {
      "text": "responses uh extracting and saving the",
      "start": 405.4,
      "duration": 4.799
    },
    {
      "text": "responses from the",
      "start": 407.84,
      "duration": 5.84
    },
    {
      "text": "llm now u in the code I've mentioned",
      "start": 410.199,
      "duration": 4.921
    },
    {
      "text": "this as step number six which is",
      "start": 413.68,
      "duration": 4.48
    },
    {
      "text": "extracting and saving responses right",
      "start": 415.12,
      "duration": 5.479
    },
    {
      "text": "okay so as I mentioned after finetuning",
      "start": 418.16,
      "duration": 4.159
    },
    {
      "text": "the llm on the training portion of the",
      "start": 420.599,
      "duration": 4.081
    },
    {
      "text": "instruction data set we now proceed to",
      "start": 422.319,
      "duration": 4.56
    },
    {
      "text": "evaluate its performance on the testing",
      "start": 424.68,
      "duration": 4.919
    },
    {
      "text": "data set to accomplish this first we",
      "start": 426.879,
      "duration": 5.32
    },
    {
      "text": "need to extract the responses generated",
      "start": 429.599,
      "duration": 4.681
    },
    {
      "text": "for the model for every input right in",
      "start": 432.199,
      "duration": 3.801
    },
    {
      "text": "the test data set and collect them for",
      "start": 434.28,
      "duration": 4.4
    },
    {
      "text": "manual analysis so if you look at this",
      "start": 436.0,
      "duration": 5.36
    },
    {
      "text": "data set right now this entire data set",
      "start": 438.68,
      "duration": 4.0
    },
    {
      "text": "is",
      "start": 441.36,
      "duration": 4.44
    },
    {
      "text": "1100 uh instruction input output Pairs",
      "start": 442.68,
      "duration": 5.44
    },
    {
      "text": "and out of this I'm using 10% as test",
      "start": 445.8,
      "duration": 4.76
    },
    {
      "text": "data so what I now need to do is that I",
      "start": 448.12,
      "duration": 4.68
    },
    {
      "text": "need to take my f tuned llm and I need",
      "start": 450.56,
      "duration": 5.4
    },
    {
      "text": "to I need to give it the test data and I",
      "start": 452.8,
      "duration": 4.88
    },
    {
      "text": "need to generate the responses for all",
      "start": 455.96,
      "duration": 4.0
    },
    {
      "text": "of the test data and only then I will be",
      "start": 457.68,
      "duration": 4.28
    },
    {
      "text": "able to compare it with the output right",
      "start": 459.96,
      "duration": 3.959
    },
    {
      "text": "currently I showed you only one response",
      "start": 461.96,
      "duration": 4.32
    },
    {
      "text": "over here which is the llm response for",
      "start": 463.919,
      "duration": 4.72
    },
    {
      "text": "one instruction output pair but now I",
      "start": 466.28,
      "duration": 4.52
    },
    {
      "text": "want to collect the llm responses for",
      "start": 468.639,
      "duration": 5.56
    },
    {
      "text": "all the instruction input in the test",
      "start": 470.8,
      "duration": 5.32
    },
    {
      "text": "data and then I will compare it with the",
      "start": 474.199,
      "duration": 3.44
    },
    {
      "text": "ground Roo",
      "start": 476.12,
      "duration": 6.039
    },
    {
      "text": "data awesome so so uh let's do just that",
      "start": 477.639,
      "duration": 6.801
    },
    {
      "text": "before that before we do it for all the",
      "start": 482.159,
      "duration": 5.48
    },
    {
      "text": "different uh instruction output pairs I",
      "start": 484.44,
      "duration": 5.68
    },
    {
      "text": "just want to show you the results of our",
      "start": 487.639,
      "duration": 4.84
    },
    {
      "text": "llm for three test set samples which are",
      "start": 490.12,
      "duration": 4.359
    },
    {
      "text": "there in the data set so what I'm going",
      "start": 492.479,
      "duration": 4.921
    },
    {
      "text": "to do here is that I'm going to uh look",
      "start": 494.479,
      "duration": 5.961
    },
    {
      "text": "at three instructions in the test data",
      "start": 497.4,
      "duration": 4.68
    },
    {
      "text": "set and I'm going to run the fine tuned",
      "start": 500.44,
      "duration": 3.96
    },
    {
      "text": "llm on these instructions and then we",
      "start": 502.08,
      "duration": 3.76
    },
    {
      "text": "are going to look at the generated",
      "start": 504.4,
      "duration": 3.4
    },
    {
      "text": "response so here you can see we are",
      "start": 505.84,
      "duration": 3.919
    },
    {
      "text": "looping over the test data and then we",
      "start": 507.8,
      "duration": 5.039
    },
    {
      "text": "are looking at the first three entries",
      "start": 509.759,
      "duration": 5.601
    },
    {
      "text": "um then what we are doing is that we are",
      "start": 512.839,
      "duration": 4.161
    },
    {
      "text": "generating the output from our fine",
      "start": 515.36,
      "duration": 4.08
    },
    {
      "text": "tuned llm we are going to convert the",
      "start": 517.0,
      "duration": 4.24
    },
    {
      "text": "token IDs into text which will give us",
      "start": 519.44,
      "duration": 4.12
    },
    {
      "text": "the final generated output and then",
      "start": 521.24,
      "duration": 4.4
    },
    {
      "text": "remember that the final output consists",
      "start": 523.56,
      "duration": 4.24
    },
    {
      "text": "of the instruction input as well we need",
      "start": 525.64,
      "duration": 3.879
    },
    {
      "text": "to remove all that and just need to",
      "start": 527.8,
      "duration": 3.56
    },
    {
      "text": "consider the response which is given by",
      "start": 529.519,
      "duration": 4.561
    },
    {
      "text": "the large language model and then we are",
      "start": 531.36,
      "duration": 6.56
    },
    {
      "text": "printing out the input text which is the",
      "start": 534.08,
      "duration": 5.56
    },
    {
      "text": "instruction then we are ALS also",
      "start": 537.92,
      "duration": 3.24
    },
    {
      "text": "printing the correct response and then",
      "start": 539.64,
      "duration": 3.759
    },
    {
      "text": "we are printing the model response for",
      "start": 541.16,
      "duration": 5.0
    },
    {
      "text": "three test samples so let's look at",
      "start": 543.399,
      "duration": 4.88
    },
    {
      "text": "these let's look at the model",
      "start": 546.16,
      "duration": 4.56
    },
    {
      "text": "predictions and the true responses for",
      "start": 548.279,
      "duration": 5.24
    },
    {
      "text": "these three test samples so the first",
      "start": 550.72,
      "duration": 4.96
    },
    {
      "text": "test sample is rewrite the sentence",
      "start": 553.519,
      "duration": 4.76
    },
    {
      "text": "using a simile and the input is the car",
      "start": 555.68,
      "duration": 5.159
    },
    {
      "text": "is very fast so simile is basically",
      "start": 558.279,
      "duration": 4.161
    },
    {
      "text": "using similar",
      "start": 560.839,
      "duration": 4.481
    },
    {
      "text": "words so the correct answer to this car",
      "start": 562.44,
      "duration": 4.68
    },
    {
      "text": "is very fast the correct response is the",
      "start": 565.32,
      "duration": 4.68
    },
    {
      "text": "car is as fast as Lightning whereas our",
      "start": 567.12,
      "duration": 5.159
    },
    {
      "text": "fine tuned llm predicted that the car is",
      "start": 570.0,
      "duration": 3.8
    },
    {
      "text": "as fast as a",
      "start": 572.279,
      "duration": 4.56
    },
    {
      "text": "bullet um that's the first now here you",
      "start": 573.8,
      "duration": 5.36
    },
    {
      "text": "can see that this is quite good right",
      "start": 576.839,
      "duration": 3.841
    },
    {
      "text": "the correct response and the model",
      "start": 579.16,
      "duration": 3.16
    },
    {
      "text": "response are actually very close to each",
      "start": 580.68,
      "duration": 4.24
    },
    {
      "text": "other that's awesome now let's look at",
      "start": 582.32,
      "duration": 4.84
    },
    {
      "text": "the second instruction what type of",
      "start": 584.92,
      "duration": 5.0
    },
    {
      "text": "cloud is typically associated with",
      "start": 587.16,
      "duration": 4.88
    },
    {
      "text": "thunderstorms the correct response to",
      "start": 589.92,
      "duration": 4.039
    },
    {
      "text": "this was the type of cloud typically",
      "start": 592.04,
      "duration": 4.84
    },
    {
      "text": "associated with thunderstorms is",
      "start": 593.959,
      "duration": 5.841
    },
    {
      "text": "cumulonimbus cumulonimbus",
      "start": 596.88,
      "duration": 4.959
    },
    {
      "text": "uh whereas the model response is a",
      "start": 599.8,
      "duration": 3.84
    },
    {
      "text": "thunderstorm is a type of cloud that",
      "start": 601.839,
      "duration": 3.68
    },
    {
      "text": "typically forms in the atmosphere or a",
      "start": 603.64,
      "duration": 4.12
    },
    {
      "text": "region of high pressure it typically",
      "start": 605.519,
      "duration": 4.521
    },
    {
      "text": "produces a strong wind that blows across",
      "start": 607.76,
      "duration": 5.0
    },
    {
      "text": "the area creating a dense dense Cloud so",
      "start": 610.04,
      "duration": 4.44
    },
    {
      "text": "clearly we can see that the model",
      "start": 612.76,
      "duration": 4.36
    },
    {
      "text": "response is not good right because we",
      "start": 614.48,
      "duration": 4.16
    },
    {
      "text": "wanted a type of cloud which is",
      "start": 617.12,
      "duration": 3.6
    },
    {
      "text": "associated with thunderstorms so just",
      "start": 618.64,
      "duration": 3.96
    },
    {
      "text": "qualitatively I can look at the correct",
      "start": 620.72,
      "duration": 3.48
    },
    {
      "text": "response and I can look at my model",
      "start": 622.6,
      "duration": 3.2
    },
    {
      "text": "response and I can see that it's not",
      "start": 624.2,
      "duration": 4.56
    },
    {
      "text": "really doing what it's asked to do and",
      "start": 625.8,
      "duration": 4.479
    },
    {
      "text": "let's look at the the third test which",
      "start": 628.76,
      "duration": 3.96
    },
    {
      "text": "is the final test example name the",
      "start": 630.279,
      "duration": 4.481
    },
    {
      "text": "author of Pride and Prejudice the",
      "start": 632.72,
      "duration": 4.16
    },
    {
      "text": "correct response is Jane Austin but the",
      "start": 634.76,
      "duration": 3.879
    },
    {
      "text": "model response is the author of Pride",
      "start": 636.88,
      "duration": 4.04
    },
    {
      "text": "ande Prejudice is George Bernard Shaw",
      "start": 638.639,
      "duration": 4.76
    },
    {
      "text": "clearly this is a mistake so out of",
      "start": 640.92,
      "duration": 4.64
    },
    {
      "text": "these three test samples without even",
      "start": 643.399,
      "duration": 4.081
    },
    {
      "text": "looking at any evaluation metric",
      "start": 645.56,
      "duration": 4.32
    },
    {
      "text": "qualitatively I can say that for the",
      "start": 647.48,
      "duration": 4.68
    },
    {
      "text": "first test sample it's very close for",
      "start": 649.88,
      "duration": 3.92
    },
    {
      "text": "the second test sample it's not",
      "start": 652.16,
      "duration": 3.96
    },
    {
      "text": "technically wrong but it does not answer",
      "start": 653.8,
      "duration": 4.8
    },
    {
      "text": "correctly and for the third test sample",
      "start": 656.12,
      "duration": 5.12
    },
    {
      "text": "it's fully wrong",
      "start": 658.6,
      "duration": 2.64
    },
    {
      "text": "U so firstly the reason why the llm is",
      "start": 661.279,
      "duration": 4.081
    },
    {
      "text": "making mistakes here is because we have",
      "start": 663.8,
      "duration": 3.56
    },
    {
      "text": "trained it only on one Epoch just",
      "start": 665.36,
      "duration": 4.599
    },
    {
      "text": "increasing the epox to two will really",
      "start": 667.36,
      "duration": 4.4
    },
    {
      "text": "improve the quality of the",
      "start": 669.959,
      "duration": 4.281
    },
    {
      "text": "responses uh even with these number of",
      "start": 671.76,
      "duration": 4.84
    },
    {
      "text": "epox now you can see that actually",
      "start": 674.24,
      "duration": 4.36
    },
    {
      "text": "measuring the performance is quite hard",
      "start": 676.6,
      "duration": 4.039
    },
    {
      "text": "right even for me as a person I can",
      "start": 678.6,
      "duration": 4.28
    },
    {
      "text": "answer this qualitatively but how can I",
      "start": 680.639,
      "duration": 4.76
    },
    {
      "text": "give a performance score how do I give",
      "start": 682.88,
      "duration": 4.88
    },
    {
      "text": "an evaluation score as machine learning",
      "start": 685.399,
      "duration": 4.361
    },
    {
      "text": "Engineers we always like score right",
      "start": 687.76,
      "duration": 4.04
    },
    {
      "text": "such as accuracy such as the",
      "start": 689.76,
      "duration": 4.199
    },
    {
      "text": "classification loss how do I give an",
      "start": 691.8,
      "duration": 5.24
    },
    {
      "text": "evaluation score to these",
      "start": 693.959,
      "duration": 5.68
    },
    {
      "text": "responses so as we can see based on the",
      "start": 697.04,
      "duration": 5.359
    },
    {
      "text": "test instructions the model performs",
      "start": 699.639,
      "duration": 5.32
    },
    {
      "text": "relatively well the answer to the first",
      "start": 702.399,
      "duration": 4.321
    },
    {
      "text": "instruction is clearly correct while the",
      "start": 704.959,
      "duration": 3.481
    },
    {
      "text": "second answer and the third answers are",
      "start": 706.72,
      "duration": 3.76
    },
    {
      "text": "not correct this is because we have done",
      "start": 708.44,
      "duration": 3.959
    },
    {
      "text": "the fine tuning only for one Epoch due",
      "start": 710.48,
      "duration": 4.159
    },
    {
      "text": "to Hardware limitations to get better",
      "start": 712.399,
      "duration": 4.0
    },
    {
      "text": "results we need to increase the EPO to",
      "start": 714.639,
      "duration": 4.081
    },
    {
      "text": "at least two that's number one number",
      "start": 716.399,
      "duration": 5.161
    },
    {
      "text": "two is that that model evaluation is not",
      "start": 718.72,
      "duration": 4.96
    },
    {
      "text": "straightforward in previously we had",
      "start": 721.56,
      "duration": 5.64
    },
    {
      "text": "done a f classification fine tuning for",
      "start": 723.68,
      "duration": 5.88
    },
    {
      "text": "spam and no spam emails right there it's",
      "start": 727.2,
      "duration": 5.84
    },
    {
      "text": "so easy to evaluate the accuracy if the",
      "start": 729.56,
      "duration": 4.959
    },
    {
      "text": "correct answer is Spam and the model",
      "start": 733.04,
      "duration": 3.599
    },
    {
      "text": "predicts no spam it's not accurate if",
      "start": 734.519,
      "duration": 4.281
    },
    {
      "text": "the correct answer is Spam and the model",
      "start": 736.639,
      "duration": 5.76
    },
    {
      "text": "predict spam it's good but in this case",
      "start": 738.8,
      "duration": 5.76
    },
    {
      "text": "it's not as straight forward so what do",
      "start": 742.399,
      "duration": 4.44
    },
    {
      "text": "researchers do there is a whole field of",
      "start": 744.56,
      "duration": 4.639
    },
    {
      "text": "llm evaluation which is becoming",
      "start": 746.839,
      "duration": 4.201
    },
    {
      "text": "extremely important because people are",
      "start": 749.199,
      "duration": 4.361
    },
    {
      "text": "realizing that along with training llms",
      "start": 751.04,
      "duration": 5.359
    },
    {
      "text": "and fine-tuning llms an equally",
      "start": 753.56,
      "duration": 5.639
    },
    {
      "text": "important field to pay attention to is",
      "start": 756.399,
      "duration": 5.68
    },
    {
      "text": "llm evaluation Because unless we have a",
      "start": 759.199,
      "duration": 5.121
    },
    {
      "text": "metric to evaluate llms we will not know",
      "start": 762.079,
      "duration": 4.56
    },
    {
      "text": "what is good and what is bad then how do",
      "start": 764.32,
      "duration": 4.04
    },
    {
      "text": "we know whether research is happening in",
      "start": 766.639,
      "duration": 3.241
    },
    {
      "text": "a successful manner or in an",
      "start": 768.36,
      "duration": 2.96
    },
    {
      "text": "unsuccessful",
      "start": 769.88,
      "duration": 4.519
    },
    {
      "text": "manner so re researchers have basically",
      "start": 771.32,
      "duration": 5.4
    },
    {
      "text": "developed three methods for evaluating",
      "start": 774.399,
      "duration": 5.401
    },
    {
      "text": "llms this is still a very new field and",
      "start": 776.72,
      "duration": 5.32
    },
    {
      "text": "Rapid progress is happening as we speak",
      "start": 779.8,
      "duration": 4.039
    },
    {
      "text": "but these are the three most common ways",
      "start": 782.04,
      "duration": 4.84
    },
    {
      "text": "of evaluating instruction fine tuned",
      "start": 783.839,
      "duration": 6.12
    },
    {
      "text": "llms so the first way is which is a very",
      "start": 786.88,
      "duration": 5.759
    },
    {
      "text": "common way is by giving the llm a",
      "start": 789.959,
      "duration": 4.44
    },
    {
      "text": "general knowledge test and by",
      "start": 792.639,
      "duration": 3.801
    },
    {
      "text": "benchmarking it on actual known",
      "start": 794.399,
      "duration": 4.641
    },
    {
      "text": "responses and this is known as measuring",
      "start": 796.44,
      "duration": 5.04
    },
    {
      "text": "massive multitask language understanding",
      "start": 799.04,
      "duration": 7.12
    },
    {
      "text": "or M mlu so MML score is a very popular",
      "start": 801.48,
      "duration": 7.0
    },
    {
      "text": "term now and the simplest way to",
      "start": 806.16,
      "duration": 4.0
    },
    {
      "text": "understand it is that that this is a",
      "start": 808.48,
      "duration": 3.52
    },
    {
      "text": "score which tests the general knowledge",
      "start": 810.16,
      "duration": 4.44
    },
    {
      "text": "of a model so this test measures the",
      "start": 812.0,
      "duration": 4.36
    },
    {
      "text": "model performance on a huge range of",
      "start": 814.6,
      "duration": 5.28
    },
    {
      "text": "questions in various fields and then uh",
      "start": 816.36,
      "duration": 5.36
    },
    {
      "text": "you can get the score of the model so",
      "start": 819.88,
      "duration": 4.199
    },
    {
      "text": "for example if we have our fine tuned",
      "start": 821.72,
      "duration": 5.84
    },
    {
      "text": "llm right uh to pass this MML test we",
      "start": 824.079,
      "duration": 6.241
    },
    {
      "text": "have to ask those 57 questions which",
      "start": 827.56,
      "duration": 4.279
    },
    {
      "text": "have mention which are mentioned in this",
      "start": 830.32,
      "duration": 3.519
    },
    {
      "text": "paper and we'll note down the responses",
      "start": 831.839,
      "duration": 4.481
    },
    {
      "text": "given by our fine tuned llm then we'll",
      "start": 833.839,
      "duration": 4.321
    },
    {
      "text": "compare it with the actual responses and",
      "start": 836.32,
      "duration": 4.16
    },
    {
      "text": "based on that a score will be generated",
      "start": 838.16,
      "duration": 5.359
    },
    {
      "text": "so for example here's the paper majoring",
      "start": 840.48,
      "duration": 6.039
    },
    {
      "text": "massive multitask language understanding",
      "start": 843.519,
      "duration": 4.0
    },
    {
      "text": "M",
      "start": 846.519,
      "duration": 3.88
    },
    {
      "text": "mlu and if you scroll down below here",
      "start": 847.519,
      "duration": 4.721
    },
    {
      "text": "you can see that these are the 57",
      "start": 850.399,
      "duration": 4.12
    },
    {
      "text": "questions which are tested in this in",
      "start": 852.24,
      "duration": 4.719
    },
    {
      "text": "abstract algebra Anatomy astronomy",
      "start": 854.519,
      "duration": 4.801
    },
    {
      "text": "business ethics high school biology",
      "start": 856.959,
      "duration": 4.56
    },
    {
      "text": "chemistry computer science then",
      "start": 859.32,
      "duration": 4.959
    },
    {
      "text": "international law management marketing",
      "start": 861.519,
      "duration": 5.88
    },
    {
      "text": "moral disputes public relations security",
      "start": 864.279,
      "duration": 5.041
    },
    {
      "text": "relations in all of these fields there",
      "start": 867.399,
      "duration": 5.0
    },
    {
      "text": "are number of questions which are asked",
      "start": 869.32,
      "duration": 5.4
    },
    {
      "text": "so essentially these are the 57 tasks",
      "start": 872.399,
      "duration": 5.36
    },
    {
      "text": "which the model needs to be tested on I",
      "start": 874.72,
      "duration": 5.799
    },
    {
      "text": "asked to summarize this paper to a",
      "start": 877.759,
      "duration": 5.0
    },
    {
      "text": "notebook LM which is an awesome tool",
      "start": 880.519,
      "duration": 3.76
    },
    {
      "text": "developed by Google to summarize",
      "start": 882.759,
      "duration": 3.601
    },
    {
      "text": "research papers and here's what it",
      "start": 884.279,
      "duration": 5.841
    },
    {
      "text": "showed so MML uh is a benchmark for",
      "start": 886.36,
      "duration": 5.76
    },
    {
      "text": "measuring language model",
      "start": 890.12,
      "duration": 4.6
    },
    {
      "text": "knowledge it is designed to evaluate a",
      "start": 892.12,
      "duration": 4.639
    },
    {
      "text": "text model's ability to learn and apply",
      "start": 894.72,
      "duration": 4.32
    },
    {
      "text": "Knowledge from various domains the",
      "start": 896.759,
      "duration": 4.0
    },
    {
      "text": "authors of this paper argue that",
      "start": 899.04,
      "duration": 4.52
    },
    {
      "text": "existing NLP benchmarks while useful do",
      "start": 900.759,
      "duration": 5.601
    },
    {
      "text": "not ACC adequately assess the breadth",
      "start": 903.56,
      "duration": 4.6
    },
    {
      "text": "and depth of knowledge that language",
      "start": 906.36,
      "duration": 4.599
    },
    {
      "text": "models are exposed that is why mlu",
      "start": 908.16,
      "duration": 6.16
    },
    {
      "text": "consists of 57 tasks covering stem",
      "start": 910.959,
      "duration": 5.961
    },
    {
      "text": "Humanity social sciences and other areas",
      "start": 914.32,
      "duration": 4.68
    },
    {
      "text": "ranging from elementary to professional",
      "start": 916.92,
      "duration": 5.0
    },
    {
      "text": "levels of difficulty we can use MML to",
      "start": 919.0,
      "duration": 4.8
    },
    {
      "text": "evaluate our custom instruction fine",
      "start": 921.92,
      "duration": 4.56
    },
    {
      "text": "tuned llm by testing its performance on",
      "start": 923.8,
      "duration": 5.44
    },
    {
      "text": "these 57 tasks in fact many of the",
      "start": 926.48,
      "duration": 4.32
    },
    {
      "text": "research papers if you read in the",
      "start": 929.24,
      "duration": 4.159
    },
    {
      "text": "instruction finetuned llm space they",
      "start": 930.8,
      "duration": 5.2
    },
    {
      "text": "show the mlu score which means how the",
      "start": 933.399,
      "duration": 4.36
    },
    {
      "text": "llm is performing on all of these",
      "start": 936.0,
      "duration": 3.279
    },
    {
      "text": "general knowledge",
      "start": 937.759,
      "duration": 4.121
    },
    {
      "text": "tasks that's the first approach for",
      "start": 939.279,
      "duration": 5.281
    },
    {
      "text": "evaluating llms the second approach is",
      "start": 941.88,
      "duration": 4.8
    },
    {
      "text": "human preference comparison where",
      "start": 944.56,
      "duration": 4.36
    },
    {
      "text": "basically humans look at the answers",
      "start": 946.68,
      "duration": 4.159
    },
    {
      "text": "given by multiple llms and then they",
      "start": 948.92,
      "duration": 4.159
    },
    {
      "text": "compare the performance between the",
      "start": 950.839,
      "duration": 4.041
    },
    {
      "text": "large language models so this is",
      "start": 953.079,
      "duration": 3.921
    },
    {
      "text": "basically having a human in the loop and",
      "start": 954.88,
      "duration": 5.439
    },
    {
      "text": "the human using their own intuition and",
      "start": 957.0,
      "duration": 4.839
    },
    {
      "text": "understanding they Benchmark or compare",
      "start": 960.319,
      "duration": 4.52
    },
    {
      "text": "llms that's the second way the Third Way",
      "start": 961.839,
      "duration": 5.961
    },
    {
      "text": "which is also fairly common is using a",
      "start": 964.839,
      "duration": 5.881
    },
    {
      "text": "large language model itself to evaluate",
      "start": 967.8,
      "duration": 5.64
    },
    {
      "text": "how close the llm responses to the",
      "start": 970.72,
      "duration": 5.44
    },
    {
      "text": "actual output so let's say here I have",
      "start": 973.44,
      "duration": 4.16
    },
    {
      "text": "collected a file which has the",
      "start": 976.16,
      "duration": 3.64
    },
    {
      "text": "instruction input output but it also has",
      "start": 977.6,
      "duration": 4.4
    },
    {
      "text": "the model response so in the third",
      "start": 979.8,
      "duration": 3.92
    },
    {
      "text": "category what is done is basically you",
      "start": 982.0,
      "duration": 3.92
    },
    {
      "text": "look at the true data which is the true",
      "start": 983.72,
      "duration": 4.4
    },
    {
      "text": "output which we expected and we look at",
      "start": 985.92,
      "duration": 3.56
    },
    {
      "text": "the model response",
      "start": 988.12,
      "duration": 3.399
    },
    {
      "text": "and then we ask a large language model",
      "start": 989.48,
      "duration": 4.12
    },
    {
      "text": "itself to compare between the output and",
      "start": 991.519,
      "duration": 4.76
    },
    {
      "text": "the model response and to assign a score",
      "start": 993.6,
      "duration": 4.679
    },
    {
      "text": "so this is very uh this is fairly",
      "start": 996.279,
      "duration": 4.04
    },
    {
      "text": "straightforward to do because it's fully",
      "start": 998.279,
      "duration": 4.601
    },
    {
      "text": "automated we just look at the output we",
      "start": 1000.319,
      "duration": 4.161
    },
    {
      "text": "look at the model response and then we",
      "start": 1002.88,
      "duration": 3.68
    },
    {
      "text": "ask a trained a very massive large",
      "start": 1004.48,
      "duration": 5.12
    },
    {
      "text": "language model to uh compare these two",
      "start": 1006.56,
      "duration": 5.92
    },
    {
      "text": "and find the score if you think about it",
      "start": 1009.6,
      "duration": 4.56
    },
    {
      "text": "this is also like taking the easy way",
      "start": 1012.48,
      "duration": 3.96
    },
    {
      "text": "out because we don't know how the llm is",
      "start": 1014.16,
      "duration": 4.76
    },
    {
      "text": "evaluating right what are the metrics",
      "start": 1016.44,
      "duration": 3.759
    },
    {
      "text": "through which the llm itself is",
      "start": 1018.92,
      "duration": 3.2
    },
    {
      "text": "comparing between the output the true",
      "start": 1020.199,
      "duration": 4.161
    },
    {
      "text": "output and the model's response we are",
      "start": 1022.12,
      "duration": 4.52
    },
    {
      "text": "trusting the llm is extremely and",
      "start": 1024.36,
      "duration": 6.04
    },
    {
      "text": "supremely smart to do this to get this",
      "start": 1026.64,
      "duration": 6.52
    },
    {
      "text": "score for us since this method is simple",
      "start": 1030.4,
      "duration": 4.279
    },
    {
      "text": "we are also going to implement this",
      "start": 1033.16,
      "duration": 4.44
    },
    {
      "text": "method in this particular video or in",
      "start": 1034.679,
      "duration": 5.16
    },
    {
      "text": "this particular lecture Series where",
      "start": 1037.6,
      "duration": 4.0
    },
    {
      "text": "what we are going to do is that we have",
      "start": 1039.839,
      "duration": 4.321
    },
    {
      "text": "the output and the model response we are",
      "start": 1041.6,
      "duration": 4.479
    },
    {
      "text": "going to ask an llm to look at the",
      "start": 1044.16,
      "duration": 3.799
    },
    {
      "text": "output to look at the model response to",
      "start": 1046.079,
      "duration": 3.761
    },
    {
      "text": "compare and to ass a",
      "start": 1047.959,
      "duration": 4.401
    },
    {
      "text": "score but it's very important for you",
      "start": 1049.84,
      "duration": 4.56
    },
    {
      "text": "all to be aware of these three types of",
      "start": 1052.36,
      "duration": 4.76
    },
    {
      "text": "llm evaluation which is extremely",
      "start": 1054.4,
      "duration": 5.08
    },
    {
      "text": "important uh so considering the scale of",
      "start": 1057.12,
      "duration": 4.439
    },
    {
      "text": "the task at hand we will Implement an",
      "start": 1059.48,
      "duration": 3.88
    },
    {
      "text": "approach similar to method three which",
      "start": 1061.559,
      "duration": 3.801
    },
    {
      "text": "involves evaluating the responses",
      "start": 1063.36,
      "duration": 4.72
    },
    {
      "text": "automatically using another llm this",
      "start": 1065.36,
      "duration": 4.76
    },
    {
      "text": "will allow us to efficiently assess the",
      "start": 1068.08,
      "duration": 4.2
    },
    {
      "text": "quality of the generated responses",
      "start": 1070.12,
      "duration": 4.52
    },
    {
      "text": "without the need for extensive human",
      "start": 1072.28,
      "duration": 4.8
    },
    {
      "text": "involvement thereby saving time and",
      "start": 1074.64,
      "duration": 4.2
    },
    {
      "text": "resources while still obtaining",
      "start": 1077.08,
      "duration": 5.24
    },
    {
      "text": "meaningful performance indicators if you",
      "start": 1078.84,
      "duration": 5.959
    },
    {
      "text": "have this code and if you can run this",
      "start": 1082.32,
      "duration": 5.96
    },
    {
      "text": "code using the mlu test uh that would be",
      "start": 1084.799,
      "duration": 5.841
    },
    {
      "text": "awesome and I would really like to see",
      "start": 1088.28,
      "duration": 6.6
    },
    {
      "text": "if someone works on that part of the",
      "start": 1090.64,
      "duration": 6.72
    },
    {
      "text": "code and take the instruction find T llm",
      "start": 1094.88,
      "duration": 5.72
    },
    {
      "text": "and run the MML test on it awesome now",
      "start": 1097.36,
      "duration": 5.799
    },
    {
      "text": "the next step is that we need to",
      "start": 1100.6,
      "duration": 4.76
    },
    {
      "text": "basically collect the responses for the",
      "start": 1103.159,
      "duration": 5.241
    },
    {
      "text": "entire test file right earlier we only",
      "start": 1105.36,
      "duration": 5.679
    },
    {
      "text": "saw the responses for three for the",
      "start": 1108.4,
      "duration": 4.96
    },
    {
      "text": "first three examples of the test data",
      "start": 1111.039,
      "duration": 3.841
    },
    {
      "text": "now what we need to do is that we need",
      "start": 1113.36,
      "duration": 4.439
    },
    {
      "text": "to collect model responses for all of",
      "start": 1114.88,
      "duration": 5.84
    },
    {
      "text": "the uh all of the instructions in the",
      "start": 1117.799,
      "duration": 4.681
    },
    {
      "text": "test data set and we need to collect",
      "start": 1120.72,
      "duration": 4.36
    },
    {
      "text": "these responses in a separate file so",
      "start": 1122.48,
      "duration": 5.12
    },
    {
      "text": "what we are doing now is that we'll",
      "start": 1125.08,
      "duration": 4.56
    },
    {
      "text": "prepare the responses for the evaluation",
      "start": 1127.6,
      "duration": 5.0
    },
    {
      "text": "process and we will essentially",
      "start": 1129.64,
      "duration": 4.8
    },
    {
      "text": "construct a new file or create a new",
      "start": 1132.6,
      "duration": 3.959
    },
    {
      "text": "file which is titled instruction data",
      "start": 1134.44,
      "duration": 5.479
    },
    {
      "text": "with response. Json for record keeping",
      "start": 1136.559,
      "duration": 5.041
    },
    {
      "text": "this file will essentially contain the",
      "start": 1139.919,
      "duration": 4.0
    },
    {
      "text": "instruction input the true output and",
      "start": 1141.6,
      "duration": 3.76
    },
    {
      "text": "also the model",
      "start": 1143.919,
      "duration": 3.841
    },
    {
      "text": "response um so to give you a visual this",
      "start": 1145.36,
      "duration": 4.64
    },
    {
      "text": "is how that file will look like now if",
      "start": 1147.76,
      "duration": 4.0
    },
    {
      "text": "you look at this first file which is",
      "start": 1150.0,
      "duration": 3.919
    },
    {
      "text": "just the instruction data. Json file",
      "start": 1151.76,
      "duration": 4.799
    },
    {
      "text": "over here instruction data. Json this",
      "start": 1153.919,
      "duration": 4.961
    },
    {
      "text": "only con consists of the instruction the",
      "start": 1156.559,
      "duration": 4.401
    },
    {
      "text": "input and the true output it does not",
      "start": 1158.88,
      "duration": 4.44
    },
    {
      "text": "contain the model response but now we",
      "start": 1160.96,
      "duration": 4.28
    },
    {
      "text": "are developing one more file or creating",
      "start": 1163.32,
      "duration": 3.56
    },
    {
      "text": "one more file which is called",
      "start": 1165.24,
      "duration": 4.16
    },
    {
      "text": "instruction data with response so this",
      "start": 1166.88,
      "duration": 4.279
    },
    {
      "text": "Con consists of the instruction the",
      "start": 1169.4,
      "duration": 3.96
    },
    {
      "text": "input output and it also consists of the",
      "start": 1171.159,
      "duration": 4.801
    },
    {
      "text": "model response this will make it very",
      "start": 1173.36,
      "duration": 4.88
    },
    {
      "text": "easy for us to later evaluate the",
      "start": 1175.96,
      "duration": 4.4
    },
    {
      "text": "performance of the model because then we",
      "start": 1178.24,
      "duration": 3.919
    },
    {
      "text": "simply have to compare the output and",
      "start": 1180.36,
      "duration": 3.92
    },
    {
      "text": "the model response for every instruction",
      "start": 1182.159,
      "duration": 3.441
    },
    {
      "text": "and assign a",
      "start": 1184.28,
      "duration": 3.96
    },
    {
      "text": "score so in this piece of code what is",
      "start": 1185.6,
      "duration": 5.52
    },
    {
      "text": "done here is that uh in this piece of",
      "start": 1188.24,
      "duration": 5.319
    },
    {
      "text": "code if you see we are looking at all of",
      "start": 1191.12,
      "duration": 4.72
    },
    {
      "text": "the in all of the input instructions in",
      "start": 1193.559,
      "duration": 4.721
    },
    {
      "text": "the test data and we are generating the",
      "start": 1195.84,
      "duration": 4.44
    },
    {
      "text": "response for all of the inputs in the",
      "start": 1198.28,
      "duration": 4.12
    },
    {
      "text": "test data and then we are collecting the",
      "start": 1200.28,
      "duration": 4.0
    },
    {
      "text": "responses in a file called instruction",
      "start": 1202.4,
      "duration": 4.12
    },
    {
      "text": "data with response that's all so when",
      "start": 1204.28,
      "duration": 4.48
    },
    {
      "text": "you run this code the generate function",
      "start": 1206.52,
      "duration": 4.2
    },
    {
      "text": "will be called on all of the instruction",
      "start": 1208.76,
      "duration": 4.799
    },
    {
      "text": "input pairs in the test data the",
      "start": 1210.72,
      "duration": 5.48
    },
    {
      "text": "responses will be generated and only the",
      "start": 1213.559,
      "duration": 4.321
    },
    {
      "text": "response will be collected and then it",
      "start": 1216.2,
      "duration": 3.92
    },
    {
      "text": "will be appended to the instruction data",
      "start": 1217.88,
      "duration": 4.32
    },
    {
      "text": "with response file so overall the",
      "start": 1220.12,
      "duration": 4.08
    },
    {
      "text": "instruction data with response file will",
      "start": 1222.2,
      "duration": 3.839
    },
    {
      "text": "look exactly like what I'm showing on",
      "start": 1224.2,
      "duration": 4.68
    },
    {
      "text": "the screen right now where in the",
      "start": 1226.039,
      "duration": 4.801
    },
    {
      "text": "instruction input and output dictionary",
      "start": 1228.88,
      "duration": 3.88
    },
    {
      "text": "the model response will also be appended",
      "start": 1230.84,
      "duration": 5.8
    },
    {
      "text": "now for every single uh instruction",
      "start": 1232.76,
      "duration": 5.96
    },
    {
      "text": "input output",
      "start": 1236.64,
      "duration": 4.6
    },
    {
      "text": "pair awesome now if you run this process",
      "start": 1238.72,
      "duration": 4.199
    },
    {
      "text": "it will take some time for me it took",
      "start": 1241.24,
      "duration": 3.679
    },
    {
      "text": "around 10 to 15 minutes to create this",
      "start": 1242.919,
      "duration": 4.161
    },
    {
      "text": "instruction data with response file but",
      "start": 1244.919,
      "duration": 3.681
    },
    {
      "text": "it will be created and then it will be",
      "start": 1247.08,
      "duration": 5.56
    },
    {
      "text": "stored for you so remember now this test",
      "start": 1248.6,
      "duration": 7.68
    },
    {
      "text": "data is the dictionary which consists of",
      "start": 1252.64,
      "duration": 5.44
    },
    {
      "text": "the instruction input output and the",
      "start": 1256.28,
      "duration": 4.399
    },
    {
      "text": "model response also we can test this by",
      "start": 1258.08,
      "duration": 4.32
    },
    {
      "text": "printing the first element of the test",
      "start": 1260.679,
      "duration": 4.321
    },
    {
      "text": "data dictionary so if you print out the",
      "start": 1262.4,
      "duration": 4.159
    },
    {
      "text": "first element you'll see that we have",
      "start": 1265.0,
      "duration": 3.72
    },
    {
      "text": "the instruction we have the input we",
      "start": 1266.559,
      "duration": 3.641
    },
    {
      "text": "have the output and then we have the",
      "start": 1268.72,
      "duration": 4.16
    },
    {
      "text": "model response as well the the output is",
      "start": 1270.2,
      "duration": 5.64
    },
    {
      "text": "here and then the model response is here",
      "start": 1272.88,
      "duration": 5.44
    },
    {
      "text": "so the test data dictionary essentially",
      "start": 1275.84,
      "duration": 5.719
    },
    {
      "text": "now uh consists of everything which we",
      "start": 1278.32,
      "duration": 5.12
    },
    {
      "text": "need it consists of the instruction the",
      "start": 1281.559,
      "duration": 4.441
    },
    {
      "text": "input output and the model",
      "start": 1283.44,
      "duration": 5.2
    },
    {
      "text": "response now what we can do is that once",
      "start": 1286.0,
      "duration": 5.2
    },
    {
      "text": "we have uh we have this file so",
      "start": 1288.64,
      "duration": 4.48
    },
    {
      "text": "everything is ready for us to evaluate",
      "start": 1291.2,
      "duration": 4.4
    },
    {
      "text": "the output and the model response now",
      "start": 1293.12,
      "duration": 4.6
    },
    {
      "text": "what we'll do is that we'll just save",
      "start": 1295.6,
      "duration": 4.48
    },
    {
      "text": "our fine tune model this is extremely",
      "start": 1297.72,
      "duration": 4.0
    },
    {
      "text": "important because if you accidentally",
      "start": 1300.08,
      "duration": 4.32
    },
    {
      "text": "close your working session then you",
      "start": 1301.72,
      "duration": 4.52
    },
    {
      "text": "don't want to fine tune again right",
      "start": 1304.4,
      "duration": 3.759
    },
    {
      "text": "remember fine tuning took four hours for",
      "start": 1306.24,
      "duration": 4.799
    },
    {
      "text": "me on my PC and I just want to I just",
      "start": 1308.159,
      "duration": 4.52
    },
    {
      "text": "want to reuse the fine tune weights",
      "start": 1311.039,
      "duration": 3.481
    },
    {
      "text": "again when I start my session the next",
      "start": 1312.679,
      "duration": 4.081
    },
    {
      "text": "time so please don't forget about this",
      "start": 1314.52,
      "duration": 4.279
    },
    {
      "text": "step it's very simple you just have to",
      "start": 1316.76,
      "duration": 4.68
    },
    {
      "text": "use the tor. save command and model.",
      "start": 1318.799,
      "duration": 4.921
    },
    {
      "text": "state dictionary which will ensure that",
      "start": 1321.44,
      "duration": 4.56
    },
    {
      "text": "all the train parameters will be stored",
      "start": 1323.72,
      "duration": 5.4
    },
    {
      "text": "in this file name which is gpt2 medium",
      "start": 1326.0,
      "duration": 7.279
    },
    {
      "text": "355 million sf. pth this is the file",
      "start": 1329.12,
      "duration": 6.159
    },
    {
      "text": "where we are storing the model and to",
      "start": 1333.279,
      "duration": 4.321
    },
    {
      "text": "load the model in a future session you",
      "start": 1335.279,
      "duration": 4.921
    },
    {
      "text": "simply have to do load State dict so two",
      "start": 1337.6,
      "duration": 5.319
    },
    {
      "text": "commands are important tor. save model.",
      "start": 1340.2,
      "duration": 6.04
    },
    {
      "text": "State dict and tor. save or load State",
      "start": 1342.919,
      "duration": 5.321
    },
    {
      "text": "dict rather the first command is model",
      "start": 1346.24,
      "duration": 5.08
    },
    {
      "text": "do State dict so saving this and the",
      "start": 1348.24,
      "duration": 4.919
    },
    {
      "text": "second is load State dict and then you",
      "start": 1351.32,
      "duration": 4.76
    },
    {
      "text": "have to load the file in which you have",
      "start": 1353.159,
      "duration": 4.841
    },
    {
      "text": "stored the",
      "start": 1356.08,
      "duration": 4.76
    },
    {
      "text": "parameters awesome I hope everyone of",
      "start": 1358.0,
      "duration": 5.32
    },
    {
      "text": "you is with me until this point so we",
      "start": 1360.84,
      "duration": 4.92
    },
    {
      "text": "have reached this stage where we have",
      "start": 1363.32,
      "duration": 4.8
    },
    {
      "text": "successfully collected the responses in",
      "start": 1365.76,
      "duration": 4.039
    },
    {
      "text": "a file called",
      "start": 1368.12,
      "duration": 3.84
    },
    {
      "text": "instruction instruction data with",
      "start": 1369.799,
      "duration": 4.12
    },
    {
      "text": "response. Json we have collected the",
      "start": 1371.96,
      "duration": 4.76
    },
    {
      "text": "responses in this file and we have also",
      "start": 1373.919,
      "duration": 4.321
    },
    {
      "text": "discussed about a way in which we are",
      "start": 1376.72,
      "duration": 3.16
    },
    {
      "text": "going to compare the output and the",
      "start": 1378.24,
      "duration": 3.76
    },
    {
      "text": "model response we have not discussed too",
      "start": 1379.88,
      "duration": 4.279
    },
    {
      "text": "many details about this but we have seen",
      "start": 1382.0,
      "duration": 4.6
    },
    {
      "text": "the three Frameworks for evaluation and",
      "start": 1384.159,
      "duration": 4.441
    },
    {
      "text": "we have shortlisted this last framework",
      "start": 1386.6,
      "duration": 4.72
    },
    {
      "text": "where we'll use use an llm to compare",
      "start": 1388.6,
      "duration": 5.24
    },
    {
      "text": "between the output and the model",
      "start": 1391.32,
      "duration": 4.719
    },
    {
      "text": "response now we are ready to move to the",
      "start": 1393.84,
      "duration": 3.56
    },
    {
      "text": "next part which is essentially",
      "start": 1396.039,
      "duration": 4.0
    },
    {
      "text": "evaluating the fine tuned large language",
      "start": 1397.4,
      "duration": 6.0
    },
    {
      "text": "model so the evaluation process",
      "start": 1400.039,
      "duration": 4.76
    },
    {
      "text": "essentially",
      "start": 1403.4,
      "duration": 4.72
    },
    {
      "text": "will will come in this building block",
      "start": 1404.799,
      "duration": 5.88
    },
    {
      "text": "what we we have seen Okay so until now",
      "start": 1408.12,
      "duration": 4.559
    },
    {
      "text": "we saw extracting the responses and we",
      "start": 1410.679,
      "duration": 4.12
    },
    {
      "text": "have also seen qualitative evaluation",
      "start": 1412.679,
      "duration": 4.081
    },
    {
      "text": "where we looked at the response so let's",
      "start": 1414.799,
      "duration": 4.801
    },
    {
      "text": "say for example I can qualitatively look",
      "start": 1416.76,
      "duration": 5.2
    },
    {
      "text": "at the output and I can look at the",
      "start": 1419.6,
      "duration": 3.84
    },
    {
      "text": "response and I can say whether it's",
      "start": 1421.96,
      "duration": 3.56
    },
    {
      "text": "correct or not qualitatively right but",
      "start": 1423.44,
      "duration": 4.2
    },
    {
      "text": "we have not yet mathematically or",
      "start": 1425.52,
      "duration": 5.12
    },
    {
      "text": "Quantified scoring the responses this is",
      "start": 1427.64,
      "duration": 4.48
    },
    {
      "text": "the part which we'll Implement in",
      "start": 1430.64,
      "duration": 4.76
    },
    {
      "text": "evaluating the llm so after extracting",
      "start": 1432.12,
      "duration": 5.679
    },
    {
      "text": "the responses by our fine tuned llm we",
      "start": 1435.4,
      "duration": 4.6
    },
    {
      "text": "will use another large language model to",
      "start": 1437.799,
      "duration": 5.24
    },
    {
      "text": "automatically evaluate these responses",
      "start": 1440.0,
      "duration": 4.76
    },
    {
      "text": "and let's see how we are going to do",
      "start": 1443.039,
      "duration": 3.441
    },
    {
      "text": "that in practice which is the large",
      "start": 1444.76,
      "duration": 3.72
    },
    {
      "text": "language model which we are going to",
      "start": 1446.48,
      "duration": 4.559
    },
    {
      "text": "use so this brings us to the step number",
      "start": 1448.48,
      "duration": 4.52
    },
    {
      "text": "seven in the evaluating the fine tuned",
      "start": 1451.039,
      "duration": 4.561
    },
    {
      "text": "llm and as I mentioned in this section",
      "start": 1453.0,
      "duration": 4.52
    },
    {
      "text": "we will Implement a method to automate",
      "start": 1455.6,
      "duration": 3.64
    },
    {
      "text": "the response evaluation of the fine",
      "start": 1457.52,
      "duration": 5.399
    },
    {
      "text": "tuned llm using a another larger llm so",
      "start": 1459.24,
      "duration": 5.88
    },
    {
      "text": "we'll use a bigger larger llm which is",
      "start": 1462.919,
      "duration": 4.561
    },
    {
      "text": "pre-trained and it's extremely supremely",
      "start": 1465.12,
      "duration": 4.72
    },
    {
      "text": "knowledgeable to compare our model",
      "start": 1467.48,
      "duration": 3.96
    },
    {
      "text": "response and the true response and to",
      "start": 1469.84,
      "duration": 2.839
    },
    {
      "text": "assign a",
      "start": 1471.44,
      "duration": 4.08
    },
    {
      "text": "score to implement this evaluation step",
      "start": 1472.679,
      "duration": 6.48
    },
    {
      "text": "we are going to use a software or it can",
      "start": 1475.52,
      "duration": 5.32
    },
    {
      "text": "be called as an application which is",
      "start": 1479.159,
      "duration": 2.88
    },
    {
      "text": "called as",
      "start": 1480.84,
      "duration": 5.36
    },
    {
      "text": "AMA so let me take you to the AMA",
      "start": 1482.039,
      "duration": 7.161
    },
    {
      "text": "application so here if you go to ama.com",
      "start": 1486.2,
      "duration": 4.719
    },
    {
      "text": "you can just type it",
      "start": 1489.2,
      "duration": 4.88
    },
    {
      "text": "ama.com you'll see that this interface",
      "start": 1490.919,
      "duration": 5.321
    },
    {
      "text": "essentially comes up and the simplest",
      "start": 1494.08,
      "duration": 4.44
    },
    {
      "text": "way to think about AMA is that it's an",
      "start": 1496.24,
      "duration": 4.559
    },
    {
      "text": "efficient application to run large",
      "start": 1498.52,
      "duration": 4.8
    },
    {
      "text": "language models on your laptop so you",
      "start": 1500.799,
      "duration": 4.561
    },
    {
      "text": "can learn you can run various large",
      "start": 1503.32,
      "duration": 4.599
    },
    {
      "text": "language models you can run Lama 3 which",
      "start": 1505.36,
      "duration": 5.799
    },
    {
      "text": "is developed by meta you can run 53",
      "start": 1507.919,
      "duration": 5.321
    },
    {
      "text": "developed by Microsoft you can run",
      "start": 1511.159,
      "duration": 5.161
    },
    {
      "text": "Mistral you can leun gamma 2 similarly",
      "start": 1513.24,
      "duration": 5.64
    },
    {
      "text": "you can leun several models on your PC",
      "start": 1516.32,
      "duration": 4.8
    },
    {
      "text": "and remember using o Lama you do not do",
      "start": 1518.88,
      "duration": 4.039
    },
    {
      "text": "pre-training you just do inference which",
      "start": 1521.12,
      "duration": 3.08
    },
    {
      "text": "means that the model is already",
      "start": 1522.919,
      "duration": 3.681
    },
    {
      "text": "pre-trained you just look at the",
      "start": 1524.2,
      "duration": 6.12
    },
    {
      "text": "responses or uh you look at the output",
      "start": 1526.6,
      "duration": 5.16
    },
    {
      "text": "which is given by the",
      "start": 1530.32,
      "duration": 3.92
    },
    {
      "text": "model you use the model in inference",
      "start": 1531.76,
      "duration": 4.519
    },
    {
      "text": "mode using AMA you do not use it in",
      "start": 1534.24,
      "duration": 4.039
    },
    {
      "text": "pre-training mode so what we are going",
      "start": 1536.279,
      "duration": 4.361
    },
    {
      "text": "to do is that we are going to implement",
      "start": 1538.279,
      "duration": 6.0
    },
    {
      "text": "the evaluation step uh by utilizing an",
      "start": 1540.64,
      "duration": 6.2
    },
    {
      "text": "instruction ftuned 88 billion parameter",
      "start": 1544.279,
      "duration": 5.801
    },
    {
      "text": "Lama 3 Model so we are going to be using",
      "start": 1546.84,
      "duration": 7.24
    },
    {
      "text": "an 8 billion parameter Lama 3 Model uh",
      "start": 1550.08,
      "duration": 6.68
    },
    {
      "text": "and that's and we are going to access",
      "start": 1554.08,
      "duration": 5.199
    },
    {
      "text": "that through olama",
      "start": 1556.76,
      "duration": 5.24
    },
    {
      "text": "so the reason we are going to utilize",
      "start": 1559.279,
      "duration": 5.081
    },
    {
      "text": "this instruction fine tuned model is",
      "start": 1562.0,
      "duration": 4.159
    },
    {
      "text": "that because it's already fine tuned on",
      "start": 1564.36,
      "duration": 4.52
    },
    {
      "text": "a huge number of instructions so if you",
      "start": 1566.159,
      "duration": 6.12
    },
    {
      "text": "search about this this is the Lama 38",
      "start": 1568.88,
      "duration": 6.0
    },
    {
      "text": "billion instruction fine tuning model",
      "start": 1572.279,
      "duration": 4.601
    },
    {
      "text": "and here the par 8 billion parameters",
      "start": 1574.88,
      "duration": 4.72
    },
    {
      "text": "are already optimized which means that",
      "start": 1576.88,
      "duration": 4.64
    },
    {
      "text": "this model is already trained to follow",
      "start": 1579.6,
      "duration": 4.52
    },
    {
      "text": "instructions and it's supremely smart so",
      "start": 1581.52,
      "duration": 3.96
    },
    {
      "text": "what we are going to do is that we are",
      "start": 1584.12,
      "duration": 4.48
    },
    {
      "text": "going to utilize this model to compare",
      "start": 1585.48,
      "duration": 6.64
    },
    {
      "text": "between the True Result and the our llm",
      "start": 1588.6,
      "duration": 6.36
    },
    {
      "text": "model output so using this Lama 38",
      "start": 1592.12,
      "duration": 4.4
    },
    {
      "text": "billion we are going to compare the",
      "start": 1594.96,
      "duration": 4.319
    },
    {
      "text": "actual output and our model response and",
      "start": 1596.52,
      "duration": 5.44
    },
    {
      "text": "we are going to assign a score so we are",
      "start": 1599.279,
      "duration": 5.721
    },
    {
      "text": "going to tell this instruction finetune",
      "start": 1601.96,
      "duration": 5.199
    },
    {
      "text": "Lama model that your next instruction is",
      "start": 1605.0,
      "duration": 3.64
    },
    {
      "text": "that look at the output look at the",
      "start": 1607.159,
      "duration": 3.64
    },
    {
      "text": "model response and assign a score to how",
      "start": 1608.64,
      "duration": 5.0
    },
    {
      "text": "well my model is doing and since this",
      "start": 1610.799,
      "duration": 4.641
    },
    {
      "text": "llama model is already trained for",
      "start": 1613.64,
      "duration": 4.32
    },
    {
      "text": "instruction for following instruction it",
      "start": 1615.44,
      "duration": 4.8
    },
    {
      "text": "will do a great job at this new",
      "start": 1617.96,
      "duration": 3.92
    },
    {
      "text": "instruction which is essentially finding",
      "start": 1620.24,
      "duration": 3.36
    },
    {
      "text": "an evaluation",
      "start": 1621.88,
      "duration": 4.12
    },
    {
      "text": "score this is also a great time for all",
      "start": 1623.6,
      "duration": 4.52
    },
    {
      "text": "of you to learn about AMA which is a",
      "start": 1626.0,
      "duration": 5.44
    },
    {
      "text": "very commonly used uh llm inference",
      "start": 1628.12,
      "duration": 6.84
    },
    {
      "text": "application okay so uh one thing to",
      "start": 1631.44,
      "duration": 5.719
    },
    {
      "text": "remember is that ama is only a tool for",
      "start": 1634.96,
      "duration": 4.68
    },
    {
      "text": "generating text using llm inference and",
      "start": 1637.159,
      "duration": 3.961
    },
    {
      "text": "it does not support training or",
      "start": 1639.64,
      "duration": 4.68
    },
    {
      "text": "fine-tuning llm so let me now show you",
      "start": 1641.12,
      "duration": 5.6
    },
    {
      "text": "uh the download process for AMA so that",
      "start": 1644.32,
      "duration": 3.88
    },
    {
      "text": "you can follow the similar instruction",
      "start": 1646.72,
      "duration": 4.0
    },
    {
      "text": "functions on your laptop all right so",
      "start": 1648.2,
      "duration": 5.0
    },
    {
      "text": "the next step for us is to download",
      "start": 1650.72,
      "duration": 5.839
    },
    {
      "text": "olama um so I'm using a Mac here so I'm",
      "start": 1653.2,
      "duration": 5.64
    },
    {
      "text": "going to show you how to install it and",
      "start": 1656.559,
      "duration": 4.081
    },
    {
      "text": "run it on Mac and I'll also give you",
      "start": 1658.84,
      "duration": 4.6
    },
    {
      "text": "instructions if you're using Windows so",
      "start": 1660.64,
      "duration": 5.639
    },
    {
      "text": "you have to go to ama.com so let me type",
      "start": 1663.44,
      "duration": 5.119
    },
    {
      "text": "ama.com here and you have to just click",
      "start": 1666.279,
      "duration": 4.76
    },
    {
      "text": "on download over here once you click on",
      "start": 1668.559,
      "duration": 6.041
    },
    {
      "text": "download uh the entire so if you are on",
      "start": 1671.039,
      "duration": 5.601
    },
    {
      "text": "Mac OS or Linux or Windows you can",
      "start": 1674.6,
      "duration": 4.319
    },
    {
      "text": "download the Appo",
      "start": 1676.64,
      "duration": 4.72
    },
    {
      "text": "rate version for you so I have clicked",
      "start": 1678.919,
      "duration": 4.36
    },
    {
      "text": "on download for mac o and you can see",
      "start": 1681.36,
      "duration": 3.84
    },
    {
      "text": "that the download process starts here",
      "start": 1683.279,
      "duration": 3.88
    },
    {
      "text": "it's a file which is",
      "start": 1685.2,
      "duration": 6.599
    },
    {
      "text": "177 uh 177 MB so you can download it and",
      "start": 1687.159,
      "duration": 6.561
    },
    {
      "text": "then you can open it follow instructions",
      "start": 1691.799,
      "duration": 4.72
    },
    {
      "text": "click on next next and next and then AMA",
      "start": 1693.72,
      "duration": 4.76
    },
    {
      "text": "will be installed it's pretty simple the",
      "start": 1696.519,
      "duration": 3.64
    },
    {
      "text": "installation process does not take too",
      "start": 1698.48,
      "duration": 3.48
    },
    {
      "text": "much time I'm going to cancel this",
      "start": 1700.159,
      "duration": 3.481
    },
    {
      "text": "download over here because I've already",
      "start": 1701.96,
      "duration": 4.04
    },
    {
      "text": "installed it then what you can do is",
      "start": 1703.64,
      "duration": 4.2
    },
    {
      "text": "that then you have to open your terminal",
      "start": 1706.0,
      "duration": 4.0
    },
    {
      "text": "so so here you can see Ive opened my Mac",
      "start": 1707.84,
      "duration": 4.959
    },
    {
      "text": "terminal over here and then what you",
      "start": 1710.0,
      "duration": 4.399
    },
    {
      "text": "have to essentially type is that you",
      "start": 1712.799,
      "duration": 4.681
    },
    {
      "text": "have to type O Lama run Lama 3 so this",
      "start": 1714.399,
      "duration": 6.241
    },
    {
      "text": "is the correct command o Lama run Lama 3",
      "start": 1717.48,
      "duration": 7.799
    },
    {
      "text": "and let me type it over here also uh o",
      "start": 1720.64,
      "duration": 7.68
    },
    {
      "text": "Lama run Lama 3 this is the command",
      "start": 1725.279,
      "duration": 5.081
    },
    {
      "text": "which you have to type on the terminal",
      "start": 1728.32,
      "duration": 3.599
    },
    {
      "text": "and if you are using a Mac you can",
      "start": 1730.36,
      "duration": 3.52
    },
    {
      "text": "directly type this command if you are",
      "start": 1731.919,
      "duration": 4.401
    },
    {
      "text": "using Windows then the command which you",
      "start": 1733.88,
      "duration": 6.44
    },
    {
      "text": "might need to type might be o Lama",
      "start": 1736.32,
      "duration": 6.56
    },
    {
      "text": "serve so if you're using Windows type",
      "start": 1740.32,
      "duration": 4.64
    },
    {
      "text": "this command first o Lama serve and then",
      "start": 1742.88,
      "duration": 8.72
    },
    {
      "text": "type O Lama run Lama 3 okay um so type",
      "start": 1744.96,
      "duration": 8.28
    },
    {
      "text": "these commands in the sequence if you're",
      "start": 1751.6,
      "duration": 3.88
    },
    {
      "text": "using Mac you can directly type or o",
      "start": 1753.24,
      "duration": 4.439
    },
    {
      "text": "Lama run Lama 3 let me show you my",
      "start": 1755.48,
      "duration": 4.439
    },
    {
      "text": "terminal again so here you can see I",
      "start": 1757.679,
      "duration": 5.36
    },
    {
      "text": "have typed o Lama run Lama let me expand",
      "start": 1759.919,
      "duration": 5.841
    },
    {
      "text": "this let me expand my terminal so that",
      "start": 1763.039,
      "duration": 5.52
    },
    {
      "text": "you can see it in more detail so here",
      "start": 1765.76,
      "duration": 5.039
    },
    {
      "text": "you can see that Ive run o Lama run Lama",
      "start": 1768.559,
      "duration": 6.641
    },
    {
      "text": "3 and the when you try to run Lama 3 the",
      "start": 1770.799,
      "duration": 6.24
    },
    {
      "text": "files which are downloaded are of size",
      "start": 1775.2,
      "duration": 5.76
    },
    {
      "text": "4.7 GB so it's an 8 billion parameter",
      "start": 1777.039,
      "duration": 6.041
    },
    {
      "text": "model right so it takes a lot of space",
      "start": 1780.96,
      "duration": 4.439
    },
    {
      "text": "and memory to download this so it took",
      "start": 1783.08,
      "duration": 4.319
    },
    {
      "text": "around 15 to 20 minutes for me to",
      "start": 1785.399,
      "duration": 6.321
    },
    {
      "text": "download this on my desktop uh but as",
      "start": 1787.399,
      "duration": 6.241
    },
    {
      "text": "the downloading is happening you'll see",
      "start": 1791.72,
      "duration": 3.76
    },
    {
      "text": "all of these instructions being printed",
      "start": 1793.64,
      "duration": 3.759
    },
    {
      "text": "out on your terminal and at the end you",
      "start": 1795.48,
      "duration": 4.199
    },
    {
      "text": "will see success U when you see the",
      "start": 1797.399,
      "duration": 4.4
    },
    {
      "text": "success these arrows will appear these",
      "start": 1799.679,
      "duration": 4.12
    },
    {
      "text": "right hand side arrows which means that",
      "start": 1801.799,
      "duration": 3.76
    },
    {
      "text": "now the Lama 3 is loaded which means you",
      "start": 1803.799,
      "duration": 3.641
    },
    {
      "text": "can interact with Lama 3 large language",
      "start": 1805.559,
      "duration": 4.041
    },
    {
      "text": "model you can ask any question to this",
      "start": 1807.44,
      "duration": 3.959
    },
    {
      "text": "so for example here I have asked what do",
      "start": 1809.6,
      "duration": 4.919
    },
    {
      "text": "llamas eat and now the llm which will",
      "start": 1811.399,
      "duration": 5.64
    },
    {
      "text": "respond to you is not chat GPT or any",
      "start": 1814.519,
      "duration": 4.561
    },
    {
      "text": "other llm the llm which will respond is",
      "start": 1817.039,
      "duration": 4.081
    },
    {
      "text": "this Lama 38 billion",
      "start": 1819.08,
      "duration": 4.199
    },
    {
      "text": "instruct this is that llm which will",
      "start": 1821.12,
      "duration": 5.559
    },
    {
      "text": "respond to you um so that's what AMA",
      "start": 1823.279,
      "duration": 5.081
    },
    {
      "text": "helps you to do so currently have run",
      "start": 1826.679,
      "duration": 3.6
    },
    {
      "text": "Lama 3 over here right similarly you can",
      "start": 1828.36,
      "duration": 5.679
    },
    {
      "text": "use o Lama to run uh any other llm also",
      "start": 1830.279,
      "duration": 7.321
    },
    {
      "text": "you can use o Lama to run 53 you can use",
      "start": 1834.039,
      "duration": 6.681
    },
    {
      "text": "it to run mistal GMA 2 Etc on your",
      "start": 1837.6,
      "duration": 5.04
    },
    {
      "text": "laptop so I'm running this on my laptop",
      "start": 1840.72,
      "duration": 3.36
    },
    {
      "text": "and all these results which I'm seeing",
      "start": 1842.64,
      "duration": 5.2
    },
    {
      "text": "are on my laptop as well awesome so once",
      "start": 1844.08,
      "duration": 5.959
    },
    {
      "text": "you reach this stage it will mean that o",
      "start": 1847.84,
      "duration": 4.679
    },
    {
      "text": "am I successfully running for you so",
      "start": 1850.039,
      "duration": 4.081
    },
    {
      "text": "when when I'm going to implement the",
      "start": 1852.519,
      "duration": 4.52
    },
    {
      "text": "next part of this code please keep o",
      "start": 1854.12,
      "duration": 4.919
    },
    {
      "text": "Lama running if you shut down this",
      "start": 1857.039,
      "duration": 3.841
    },
    {
      "text": "terminal AMA will not be running and",
      "start": 1859.039,
      "duration": 4.441
    },
    {
      "text": "then your code won't execute so please",
      "start": 1860.88,
      "duration": 4.399
    },
    {
      "text": "keep AMA running the simplest way to",
      "start": 1863.48,
      "duration": 4.799
    },
    {
      "text": "test is that just make sure to ask some",
      "start": 1865.279,
      "duration": 4.4
    },
    {
      "text": "question over here and if you get a",
      "start": 1868.279,
      "duration": 3.561
    },
    {
      "text": "response it means that ama has been",
      "start": 1869.679,
      "duration": 3.441
    },
    {
      "text": "running",
      "start": 1871.84,
      "duration": 3.439
    },
    {
      "text": "successfully awesome now I'm going to",
      "start": 1873.12,
      "duration": 4.399
    },
    {
      "text": "move back or switch back to code to",
      "start": 1875.279,
      "duration": 5.28
    },
    {
      "text": "explain the rest to you okay I hope you",
      "start": 1877.519,
      "duration": 5.921
    },
    {
      "text": "have installed AMA now and you have run",
      "start": 1880.559,
      "duration": 5.08
    },
    {
      "text": "the O Lama run Lama 3 command on the",
      "start": 1883.44,
      "duration": 4.839
    },
    {
      "text": "terminal which I have demonstrated I",
      "start": 1885.639,
      "duration": 4.64
    },
    {
      "text": "have just provided a simple code block",
      "start": 1888.279,
      "duration": 4.12
    },
    {
      "text": "here which verifies that the AMA session",
      "start": 1890.279,
      "duration": 4.921
    },
    {
      "text": "is running properly before we use AMA to",
      "start": 1892.399,
      "duration": 5.361
    },
    {
      "text": "evaluate the test set responses so our",
      "start": 1895.2,
      "duration": 5.52
    },
    {
      "text": "final goal is to use o Lama and Lama 3",
      "start": 1897.76,
      "duration": 6.12
    },
    {
      "text": "especially to compare the output and the",
      "start": 1900.72,
      "duration": 5.4
    },
    {
      "text": "model response to assign a score but",
      "start": 1903.88,
      "duration": 3.679
    },
    {
      "text": "before doing that we need to check",
      "start": 1906.12,
      "duration": 3.6
    },
    {
      "text": "whether o Lama is running or not so you",
      "start": 1907.559,
      "duration": 4.681
    },
    {
      "text": "can run this code and here it should",
      "start": 1909.72,
      "duration": 4.679
    },
    {
      "text": "come O Lama running is equal to true if",
      "start": 1912.24,
      "duration": 4.24
    },
    {
      "text": "this comes false which means that ama is",
      "start": 1914.399,
      "duration": 4.801
    },
    {
      "text": "not successfully running",
      "start": 1916.48,
      "duration": 4.84
    },
    {
      "text": "uh now I showed you the O Lama run",
      "start": 1919.2,
      "duration": 4.0
    },
    {
      "text": "command on the terminal right there is",
      "start": 1921.32,
      "duration": 3.68
    },
    {
      "text": "an alternative for this command instead",
      "start": 1923.2,
      "duration": 3.839
    },
    {
      "text": "of going to the terminal each time we",
      "start": 1925.0,
      "duration": 4.36
    },
    {
      "text": "can interact with the Lama model uh",
      "start": 1927.039,
      "duration": 4.801
    },
    {
      "text": "using the API through python so we can",
      "start": 1929.36,
      "duration": 4.0
    },
    {
      "text": "create a function which is called as the",
      "start": 1931.84,
      "duration": 3.76
    },
    {
      "text": "query model what this function does is",
      "start": 1933.36,
      "duration": 4.679
    },
    {
      "text": "that uh you can pass in the model so",
      "start": 1935.6,
      "duration": 4.959
    },
    {
      "text": "here I'm using Lama 3 so this function",
      "start": 1938.039,
      "duration": 4.401
    },
    {
      "text": "when it's called it will pass a query to",
      "start": 1940.559,
      "duration": 4.12
    },
    {
      "text": "Lama 3 and the query will also be",
      "start": 1942.44,
      "duration": 4.68
    },
    {
      "text": "provided by us so we'll be providing the",
      "start": 1944.679,
      "duration": 4.48
    },
    {
      "text": "query",
      "start": 1947.12,
      "duration": 3.72
    },
    {
      "text": "uh we'll be providing the query as the",
      "start": 1949.159,
      "duration": 3.841
    },
    {
      "text": "prompt so when you call this function",
      "start": 1950.84,
      "duration": 3.64
    },
    {
      "text": "query model you'll have to provide a",
      "start": 1953.0,
      "duration": 2.679
    },
    {
      "text": "prompt and you'll have to provide a",
      "start": 1954.48,
      "duration": 3.679
    },
    {
      "text": "model and what this function will do is",
      "start": 1955.679,
      "duration": 4.72
    },
    {
      "text": "that this this function will send a",
      "start": 1958.159,
      "duration": 5.041
    },
    {
      "text": "request uh will send an API request to",
      "start": 1960.399,
      "duration": 4.961
    },
    {
      "text": "this Lama 3 Model based on the prompt",
      "start": 1963.2,
      "duration": 4.68
    },
    {
      "text": "and then the response will be generated",
      "start": 1965.36,
      "duration": 4.48
    },
    {
      "text": "and then the response will be returned",
      "start": 1967.88,
      "duration": 4.24
    },
    {
      "text": "so instead of writing or instead of",
      "start": 1969.84,
      "duration": 3.36
    },
    {
      "text": "running",
      "start": 1972.12,
      "duration": 5.88
    },
    {
      "text": "the uh o Lama run Lama 3 on the the",
      "start": 1973.2,
      "duration": 6.68
    },
    {
      "text": "terminal each time we can simply do an",
      "start": 1978.0,
      "duration": 4.2
    },
    {
      "text": "API call where we can fetch the results",
      "start": 1979.88,
      "duration": 4.48
    },
    {
      "text": "and bring it into our jupyter notebook",
      "start": 1982.2,
      "duration": 4.12
    },
    {
      "text": "interface I'm not going through this",
      "start": 1984.36,
      "duration": 4.12
    },
    {
      "text": "code in detail because this will",
      "start": 1986.32,
      "duration": 3.959
    },
    {
      "text": "distract us from the main purpose of",
      "start": 1988.48,
      "duration": 5.4
    },
    {
      "text": "this whole video lecture which is to uh",
      "start": 1990.279,
      "duration": 6.841
    },
    {
      "text": "perform the model evaluation so simply",
      "start": 1993.88,
      "duration": 5.2
    },
    {
      "text": "note that what this query model does is",
      "start": 1997.12,
      "duration": 5.48
    },
    {
      "text": "that it helps us to pass in a prompt to",
      "start": 1999.08,
      "duration": 5.52
    },
    {
      "text": "Lama 3 or whichever llm which we are",
      "start": 2002.6,
      "duration": 4.039
    },
    {
      "text": "using through o Lama and to give us the",
      "start": 2004.6,
      "duration": 3.919
    },
    {
      "text": "responses that's it",
      "start": 2006.639,
      "duration": 4.04
    },
    {
      "text": "so now what we'll do is that the prompt",
      "start": 2008.519,
      "duration": 4.441
    },
    {
      "text": "which we are going to give here is that",
      "start": 2010.679,
      "duration": 3.801
    },
    {
      "text": "these are this is the output this is the",
      "start": 2012.96,
      "duration": 4.12
    },
    {
      "text": "model response compare these and assign",
      "start": 2014.48,
      "duration": 4.4
    },
    {
      "text": "a score that's it that's all we are",
      "start": 2017.08,
      "duration": 3.52
    },
    {
      "text": "going to do but this function query",
      "start": 2018.88,
      "duration": 3.32
    },
    {
      "text": "model will be very important to us",
      "start": 2020.6,
      "duration": 2.919
    },
    {
      "text": "because we are going to pass in the",
      "start": 2022.2,
      "duration": 3.599
    },
    {
      "text": "prompt through this",
      "start": 2023.519,
      "duration": 4.481
    },
    {
      "text": "function okay now again I want to",
      "start": 2025.799,
      "duration": 3.6
    },
    {
      "text": "mention that before running the",
      "start": 2028.0,
      "duration": 3.76
    },
    {
      "text": "subsequent code cells ensure that all",
      "start": 2029.399,
      "duration": 4.681
    },
    {
      "text": "Lama is still running the previous code",
      "start": 2031.76,
      "duration": 4.72
    },
    {
      "text": "which I showed you over here should",
      "start": 2034.08,
      "duration": 5.0
    },
    {
      "text": "print o Lama running to be equal to",
      "start": 2036.48,
      "duration": 5.0
    },
    {
      "text": "True uh to confirm that the model is",
      "start": 2039.08,
      "duration": 4.68
    },
    {
      "text": "active and ready to receive requests so",
      "start": 2041.48,
      "duration": 4.199
    },
    {
      "text": "now here is just a small demo of how the",
      "start": 2043.76,
      "duration": 4.56
    },
    {
      "text": "query model works so in the query model",
      "start": 2045.679,
      "duration": 5.881
    },
    {
      "text": "here you can prescribe the uh input",
      "start": 2048.32,
      "duration": 5.319
    },
    {
      "text": "which is what do llamas eat and then you",
      "start": 2051.56,
      "duration": 3.799
    },
    {
      "text": "can also mention the model so here I'm",
      "start": 2053.639,
      "duration": 4.401
    },
    {
      "text": "using Lama 3 in O Lama you can use",
      "start": 2055.359,
      "duration": 5.32
    },
    {
      "text": "various models so you can search models",
      "start": 2058.04,
      "duration": 5.24
    },
    {
      "text": "here and view all so you can see the",
      "start": 2060.679,
      "duration": 4.48
    },
    {
      "text": "different types of models which are",
      "start": 2063.28,
      "duration": 7.319
    },
    {
      "text": "present um in in ama so you can do gamma",
      "start": 2065.159,
      "duration": 8.52
    },
    {
      "text": "2 also here so I can",
      "start": 2070.599,
      "duration": 6.0
    },
    {
      "text": "do I can do gamma 2 also over here but",
      "start": 2073.679,
      "duration": 4.601
    },
    {
      "text": "the reason we are sticking with Lama 3",
      "start": 2076.599,
      "duration": 3.28
    },
    {
      "text": "is that we are using this 8 billion",
      "start": 2078.28,
      "duration": 3.839
    },
    {
      "text": "instruction fine tuned model since it's",
      "start": 2079.879,
      "duration": 4.121
    },
    {
      "text": "already fine tuned on the instruction or",
      "start": 2082.119,
      "duration": 3.76
    },
    {
      "text": "use instruction data",
      "start": 2084.0,
      "duration": 5.0
    },
    {
      "text": "set okay so uh to the query model",
      "start": 2085.879,
      "duration": 5.361
    },
    {
      "text": "function you pass in any prompt and the",
      "start": 2089.0,
      "duration": 4.399
    },
    {
      "text": "model and that llm will look at the",
      "start": 2091.24,
      "duration": 4.08
    },
    {
      "text": "prompt and generate a response so you",
      "start": 2093.399,
      "duration": 4.401
    },
    {
      "text": "can print out the response over here now",
      "start": 2095.32,
      "duration": 4.92
    },
    {
      "text": "please note here that this process also",
      "start": 2097.8,
      "duration": 5.279
    },
    {
      "text": "takes a lot of compute memory so to",
      "start": 2100.24,
      "duration": 5.04
    },
    {
      "text": "print out one result here it took a long",
      "start": 2103.079,
      "duration": 5.04
    },
    {
      "text": "time for me because I on CPU I highly",
      "start": 2105.28,
      "duration": 5.24
    },
    {
      "text": "encourage if you have a GPU access for",
      "start": 2108.119,
      "duration": 5.24
    },
    {
      "text": "all of you to uh use the GPU wherever",
      "start": 2110.52,
      "duration": 5.12
    },
    {
      "text": "possible if you have a CPU it's fine I'm",
      "start": 2113.359,
      "duration": 3.841
    },
    {
      "text": "showing you only those things which can",
      "start": 2115.64,
      "duration": 3.88
    },
    {
      "text": "run on a CPU but just make sure that",
      "start": 2117.2,
      "duration": 4.48
    },
    {
      "text": "this will take time and at least free up",
      "start": 2119.52,
      "duration": 4.52
    },
    {
      "text": "memory space free up at least 15 to 20",
      "start": 2121.68,
      "duration": 5.8
    },
    {
      "text": "GB on your desktop for this entire code",
      "start": 2124.04,
      "duration": 5.76
    },
    {
      "text": "to run so this query and printing out",
      "start": 2127.48,
      "duration": 4.879
    },
    {
      "text": "this result is not as straightforward as",
      "start": 2129.8,
      "duration": 4.72
    },
    {
      "text": "I've shown it here it took a long time",
      "start": 2132.359,
      "duration": 6.201
    },
    {
      "text": "to generate this text and to to query so",
      "start": 2134.52,
      "duration": 5.599
    },
    {
      "text": "let's say I've just passed in one query",
      "start": 2138.56,
      "duration": 3.279
    },
    {
      "text": "here right if I passed in three or four",
      "start": 2140.119,
      "duration": 4.321
    },
    {
      "text": "queries my laptop is does not run my",
      "start": 2141.839,
      "duration": 5.161
    },
    {
      "text": "laptop hangs so that's why I can pass a",
      "start": 2144.44,
      "duration": 4.6
    },
    {
      "text": "maximum of two queries in one session",
      "start": 2147.0,
      "duration": 3.92
    },
    {
      "text": "simultaneously otherwise my laptop just",
      "start": 2149.04,
      "duration": 3.96
    },
    {
      "text": "hangs because the processing speed or",
      "start": 2150.92,
      "duration": 4.04
    },
    {
      "text": "the processing power is not",
      "start": 2153.0,
      "duration": 4.359
    },
    {
      "text": "there so now what we can do is that",
      "start": 2154.96,
      "duration": 4.48
    },
    {
      "text": "using using the query model function we",
      "start": 2157.359,
      "duration": 4.0
    },
    {
      "text": "can evaluate the responses which are",
      "start": 2159.44,
      "duration": 4.6
    },
    {
      "text": "generated by our fine tune model with a",
      "start": 2161.359,
      "duration": 5.24
    },
    {
      "text": "prompt and in that prompt we have to ask",
      "start": 2164.04,
      "duration": 4.76
    },
    {
      "text": "the Lama 3 Model to rate our fine tuned",
      "start": 2166.599,
      "duration": 5.041
    },
    {
      "text": "models responses on a scale of 1 12 100",
      "start": 2168.8,
      "duration": 5.36
    },
    {
      "text": "based on the given test response so let",
      "start": 2171.64,
      "duration": 3.959
    },
    {
      "text": "me show you the prompt which you are",
      "start": 2174.16,
      "duration": 3.439
    },
    {
      "text": "going to give so the prompt which now we",
      "start": 2175.599,
      "duration": 3.881
    },
    {
      "text": "are going to give to the query model is",
      "start": 2177.599,
      "duration": 5.161
    },
    {
      "text": "that uh given the input and the input is",
      "start": 2179.48,
      "duration": 6.92
    },
    {
      "text": "going to be this given this",
      "start": 2182.76,
      "duration": 6.04
    },
    {
      "text": "input uh",
      "start": 2186.4,
      "duration": 4.4
    },
    {
      "text": "actually given the input so it's the",
      "start": 2188.8,
      "duration": 3.799
    },
    {
      "text": "format input function so the input is",
      "start": 2190.8,
      "duration": 4.92
    },
    {
      "text": "going to be actually the instruction",
      "start": 2192.599,
      "duration": 4.921
    },
    {
      "text": "along with the input this is going to be",
      "start": 2195.72,
      "duration": 4.399
    },
    {
      "text": "the input over here so given this input",
      "start": 2197.52,
      "duration": 4.88
    },
    {
      "text": "and the correct output so the correct",
      "start": 2200.119,
      "duration": 3.801
    },
    {
      "text": "output is this",
      "start": 2202.4,
      "duration": 4.52
    },
    {
      "text": "output",
      "start": 2203.92,
      "duration": 3.0
    },
    {
      "text": "um so given the instruction and the",
      "start": 2207.52,
      "duration": 5.68
    },
    {
      "text": "correct output score the model response",
      "start": 2210.359,
      "duration": 5.24
    },
    {
      "text": "and the model response is the model",
      "start": 2213.2,
      "duration": 4.6
    },
    {
      "text": "response in this Json file which is this",
      "start": 2215.599,
      "duration": 5.72
    },
    {
      "text": "model response score the model response",
      "start": 2217.8,
      "duration": 6.24
    },
    {
      "text": "on a scale of 0 to 100 where 100 is the",
      "start": 2221.319,
      "duration": 5.081
    },
    {
      "text": "best score so what the llm will do now",
      "start": 2224.04,
      "duration": 3.96
    },
    {
      "text": "is that it will look at the output it",
      "start": 2226.4,
      "duration": 3.32
    },
    {
      "text": "will look at the model response based on",
      "start": 2228.0,
      "duration": 3.4
    },
    {
      "text": "this instruction and the input and then",
      "start": 2229.72,
      "duration": 5.08
    },
    {
      "text": "it will assign a score so we are using",
      "start": 2231.4,
      "duration": 4.88
    },
    {
      "text": "the query model function which we",
      "start": 2234.8,
      "duration": 3.12
    },
    {
      "text": "defined earlier and then we are passing",
      "start": 2236.28,
      "duration": 3.68
    },
    {
      "text": "this prompt you see how we are using",
      "start": 2237.92,
      "duration": 4.12
    },
    {
      "text": "another large language model to evaluate",
      "start": 2239.96,
      "duration": 5.0
    },
    {
      "text": "our large language model um so we are",
      "start": 2242.04,
      "duration": 5.76
    },
    {
      "text": "using the Lama 3B large Lang language",
      "start": 2244.96,
      "duration": 4.879
    },
    {
      "text": "model Lama 38 billion large language",
      "start": 2247.8,
      "duration": 5.84
    },
    {
      "text": "model to evaluate our fine tuned llm and",
      "start": 2249.839,
      "duration": 5.441
    },
    {
      "text": "here we have specified the model so we",
      "start": 2253.64,
      "duration": 4.6
    },
    {
      "text": "don't need to specify it again um when",
      "start": 2255.28,
      "duration": 5.0
    },
    {
      "text": "we call the query model because if it's",
      "start": 2258.24,
      "duration": 4.119
    },
    {
      "text": "not specified by default the query model",
      "start": 2260.28,
      "duration": 4.68
    },
    {
      "text": "will use Lama 3 when I share this code",
      "start": 2262.359,
      "duration": 4.121
    },
    {
      "text": "file with you you can feel free to",
      "start": 2264.96,
      "duration": 3.84
    },
    {
      "text": "explore with other llms also the sky is",
      "start": 2266.48,
      "duration": 5.32
    },
    {
      "text": "the limit here it's it's an exploratory",
      "start": 2268.8,
      "duration": 5.319
    },
    {
      "text": "notebook and I've got good decent",
      "start": 2271.8,
      "duration": 4.4
    },
    {
      "text": "results here but you can of course feel",
      "start": 2274.119,
      "duration": 3.921
    },
    {
      "text": "free to explore with larger models",
      "start": 2276.2,
      "duration": 5.119
    },
    {
      "text": "different models as well so here you can",
      "start": 2278.04,
      "duration": 5.12
    },
    {
      "text": "see that I'm testing for three data",
      "start": 2281.319,
      "duration": 4.0
    },
    {
      "text": "points or three testing data here and",
      "start": 2283.16,
      "duration": 5.12
    },
    {
      "text": "I'm printing the model I'm printing the",
      "start": 2285.319,
      "duration": 4.401
    },
    {
      "text": "model response and I'm printing the",
      "start": 2288.28,
      "duration": 3.88
    },
    {
      "text": "score also which is given by the this",
      "start": 2289.72,
      "duration": 4.399
    },
    {
      "text": "Lama 8 billion",
      "start": 2292.16,
      "duration": 4.88
    },
    {
      "text": "model again when I ran this code on my",
      "start": 2294.119,
      "duration": 5.521
    },
    {
      "text": "laptop my laptop crashed initially and",
      "start": 2297.04,
      "duration": 5.68
    },
    {
      "text": "then it took around 35 minutes to print",
      "start": 2299.64,
      "duration": 5.6
    },
    {
      "text": "out the responses for the",
      "start": 2302.72,
      "duration": 5.399
    },
    {
      "text": "three um instruction input output pair",
      "start": 2305.24,
      "duration": 5.2
    },
    {
      "text": "so three samples in the test data so for",
      "start": 2308.119,
      "duration": 4.321
    },
    {
      "text": "me it was impossible to run this code on",
      "start": 2310.44,
      "duration": 4.879
    },
    {
      "text": "the entire test data set because I could",
      "start": 2312.44,
      "duration": 5.12
    },
    {
      "text": "only do it for two to three samples so",
      "start": 2315.319,
      "duration": 5.201
    },
    {
      "text": "here is the response so for the first",
      "start": 2317.56,
      "duration": 5.279
    },
    {
      "text": "query for the first instruction the",
      "start": 2320.52,
      "duration": 6.0
    },
    {
      "text": "rewrite the sentence using a Sim the our",
      "start": 2322.839,
      "duration": 6.201
    },
    {
      "text": "model response was the car is as fast as",
      "start": 2326.52,
      "duration": 4.28
    },
    {
      "text": "a bullet and the data set which is the",
      "start": 2329.04,
      "duration": 3.68
    },
    {
      "text": "actual response is the car is as fast as",
      "start": 2330.8,
      "duration": 4.2
    },
    {
      "text": "lighting so here is the qualitative",
      "start": 2332.72,
      "duration": 4.84
    },
    {
      "text": "evaluation which is given by the llm and",
      "start": 2335.0,
      "duration": 4.319
    },
    {
      "text": "rate the model response the car is as",
      "start": 2337.56,
      "duration": 4.68
    },
    {
      "text": "fast as a bullet 85 out of 100 here's",
      "start": 2339.319,
      "duration": 5.561
    },
    {
      "text": "why the response uses simil correctly",
      "start": 2342.24,
      "duration": 4.119
    },
    {
      "text": "comparing the speed of the car to",
      "start": 2344.88,
      "duration": 3.719
    },
    {
      "text": "something else in this case a bullet the",
      "start": 2346.359,
      "duration": 4.24
    },
    {
      "text": "comparison is relevant and makes sense",
      "start": 2348.599,
      "duration": 3.801
    },
    {
      "text": "as bullets are known for their High",
      "start": 2350.599,
      "duration": 4.601
    },
    {
      "text": "Velocity the phrase as fast as is used",
      "start": 2352.4,
      "duration": 4.4
    },
    {
      "text": "correctly to introduce the",
      "start": 2355.2,
      "duration": 3.84
    },
    {
      "text": "simile the only reason I wouldn't give",
      "start": 2356.8,
      "duration": 3.96
    },
    {
      "text": "it a perfect score is that some people",
      "start": 2359.04,
      "duration": 3.6
    },
    {
      "text": "might find the comparison slightly less",
      "start": 2360.76,
      "duration": 3.24
    },
    {
      "text": "Vivid or",
      "start": 2362.64,
      "duration": 3.76
    },
    {
      "text": "evocative uh for example comparing",
      "start": 2364.0,
      "duration": 4.0
    },
    {
      "text": "something to lightning",
      "start": 2366.4,
      "duration": 3.64
    },
    {
      "text": "as in the actual response can be more",
      "start": 2368.0,
      "duration": 4.72
    },
    {
      "text": "dramatic dramatic however as fast as a",
      "start": 2370.04,
      "duration": 4.96
    },
    {
      "text": "bullet is strong and effective simile",
      "start": 2372.72,
      "duration": 4.92
    },
    {
      "text": "that effectively conveys so it says that",
      "start": 2375.0,
      "duration": 6.839
    },
    {
      "text": "although the um response can be more",
      "start": 2377.64,
      "duration": 6.56
    },
    {
      "text": "dramatic it still captures the essence",
      "start": 2381.839,
      "duration": 4.121
    },
    {
      "text": "and it still captures the grammatical",
      "start": 2384.2,
      "duration": 4.28
    },
    {
      "text": "meaning of a simile so overall the model",
      "start": 2385.96,
      "duration": 4.56
    },
    {
      "text": "did a great job and the score is 85 out",
      "start": 2388.48,
      "duration": 4.72
    },
    {
      "text": "of 100 that's for the first for the",
      "start": 2390.52,
      "duration": 5.28
    },
    {
      "text": "second uh for the second input the",
      "start": 2393.2,
      "duration": 4.52
    },
    {
      "text": "actual response was the type of Cloud",
      "start": 2395.8,
      "duration": 3.799
    },
    {
      "text": "typically associated with thunderstorms",
      "start": 2397.72,
      "duration": 3.56
    },
    {
      "text": "is",
      "start": 2399.599,
      "duration": 4.881
    },
    {
      "text": "cumulonimbus cumulonimbus but the model",
      "start": 2401.28,
      "duration": 5.16
    },
    {
      "text": "response is a thunderstorm is a type of",
      "start": 2404.48,
      "duration": 4.879
    },
    {
      "text": "cloud Etc it's pretty long so the score",
      "start": 2406.44,
      "duration": 5.6
    },
    {
      "text": "for this model as given by the Llama llm",
      "start": 2409.359,
      "duration": 5.161
    },
    {
      "text": "is 20 out of 100 and the reason is",
      "start": 2412.04,
      "duration": 4.12
    },
    {
      "text": "because the response doesn't directly",
      "start": 2414.52,
      "duration": 3.36
    },
    {
      "text": "answer the question about what type of",
      "start": 2416.16,
      "duration": 3.439
    },
    {
      "text": "cloud is typically associated with",
      "start": 2417.88,
      "duration": 3.76
    },
    {
      "text": "thunderstorms instead it provides a",
      "start": 2419.599,
      "duration": 4.161
    },
    {
      "text": "general description of thunderstorms",
      "start": 2421.64,
      "duration": 3.76
    },
    {
      "text": "which is not relevant to the original",
      "start": 2423.76,
      "duration": 3.839
    },
    {
      "text": "questions at all the response of also",
      "start": 2425.4,
      "duration": 3.719
    },
    {
      "text": "contain some inaccuracies such as",
      "start": 2427.599,
      "duration": 3.401
    },
    {
      "text": "stating that thunderstorms for more high",
      "start": 2429.119,
      "duration": 4.521
    },
    {
      "text": "pressure regions whereas the llm pointed",
      "start": 2431.0,
      "duration": 4.4
    },
    {
      "text": "out that thunderstorms can actually",
      "start": 2433.64,
      "duration": 3.92
    },
    {
      "text": "occur in various weather",
      "start": 2435.4,
      "duration": 4.48
    },
    {
      "text": "patterns that's why the score is 20 out",
      "start": 2437.56,
      "duration": 5.4
    },
    {
      "text": "of 100 in the third question the actual",
      "start": 2439.88,
      "duration": 5.04
    },
    {
      "text": "response was Jane Austin but the",
      "start": 2442.96,
      "duration": 3.84
    },
    {
      "text": "response of the model was the author of",
      "start": 2444.92,
      "duration": 3.64
    },
    {
      "text": "Pride and Prejudice is George Bernard",
      "start": 2446.8,
      "duration": 4.6
    },
    {
      "text": "Shaw this is wrong so the llm rates is",
      "start": 2448.56,
      "duration": 5.039
    },
    {
      "text": "zero out of 100 the correct answer is",
      "start": 2451.4,
      "duration": 4.64
    },
    {
      "text": "Jane Austin not George Bernard Shaw",
      "start": 2453.599,
      "duration": 4.081
    },
    {
      "text": "George Bernard Shaw was an Irish",
      "start": 2456.04,
      "duration": 3.2
    },
    {
      "text": "playright and author but he did not",
      "start": 2457.68,
      "duration": 3.96
    },
    {
      "text": "write Pride and Prejudice the response",
      "start": 2459.24,
      "duration": 4.079
    },
    {
      "text": "is completely Incorrect and does not",
      "start": 2461.64,
      "duration": 4.0
    },
    {
      "text": "provide any relevant information this is",
      "start": 2463.319,
      "duration": 4.121
    },
    {
      "text": "awesome right it looks that the llm",
      "start": 2465.64,
      "duration": 4.12
    },
    {
      "text": "which we are using is pretty smart and",
      "start": 2467.44,
      "duration": 4.44
    },
    {
      "text": "it's doing an awesome job at qualitative",
      "start": 2469.76,
      "duration": 4.64
    },
    {
      "text": "or even quantitative evaluation so as",
      "start": 2471.88,
      "duration": 4.56
    },
    {
      "text": "human I can think of some",
      "start": 2474.4,
      "duration": 4.52
    },
    {
      "text": "qualitative uh similarities or",
      "start": 2476.44,
      "duration": 4.0
    },
    {
      "text": "qualitatively I can think how to",
      "start": 2478.92,
      "duration": 3.52
    },
    {
      "text": "evaluate the evaluate the model's",
      "start": 2480.44,
      "duration": 4.2
    },
    {
      "text": "response but even the qualitative",
      "start": 2482.44,
      "duration": 3.84
    },
    {
      "text": "evaluation of the llm is actually",
      "start": 2484.64,
      "duration": 4.84
    },
    {
      "text": "amazing the kind of points which the llm",
      "start": 2486.28,
      "duration": 5.559
    },
    {
      "text": "introduced do not come naturally to me",
      "start": 2489.48,
      "duration": 4.2
    },
    {
      "text": "for example as a human I did not think",
      "start": 2491.839,
      "duration": 3.721
    },
    {
      "text": "about this dramatic",
      "start": 2493.68,
      "duration": 5.32
    },
    {
      "text": "effect um as a human I did not know",
      "start": 2495.56,
      "duration": 7.16
    },
    {
      "text": "that uh thunderstorms can also occur in",
      "start": 2499.0,
      "duration": 6.319
    },
    {
      "text": "various weather patterns and uh stating",
      "start": 2502.72,
      "duration": 4.16
    },
    {
      "text": "that thunderstorms form over high",
      "start": 2505.319,
      "duration": 3.881
    },
    {
      "text": "pressure regions might be inaccurate so",
      "start": 2506.88,
      "duration": 4.16
    },
    {
      "text": "as humans I have some limitations even",
      "start": 2509.2,
      "duration": 4.399
    },
    {
      "text": "in qualitative qualitative evaluation",
      "start": 2511.04,
      "duration": 4.92
    },
    {
      "text": "and the llm is filling those limitations",
      "start": 2513.599,
      "duration": 4.441
    },
    {
      "text": "it's doing an awesome job at qualitative",
      "start": 2515.96,
      "duration": 4.399
    },
    {
      "text": "evaluation and although the quantitative",
      "start": 2518.04,
      "duration": 4.2
    },
    {
      "text": "evaluation I'm not still sure why it's",
      "start": 2520.359,
      "duration": 4.96
    },
    {
      "text": "85 why it's 20 it kind of makes sense",
      "start": 2522.24,
      "duration": 5.04
    },
    {
      "text": "here it's 20 because most of the answer",
      "start": 2525.319,
      "duration": 5.201
    },
    {
      "text": "was wrong here it's zero because it's a",
      "start": 2527.28,
      "duration": 4.76
    },
    {
      "text": "factual question so the answer is",
      "start": 2530.52,
      "duration": 3.88
    },
    {
      "text": "factually incorrect and here it's 85",
      "start": 2532.04,
      "duration": 3.84
    },
    {
      "text": "because most of the answer seems to be",
      "start": 2534.4,
      "duration": 3.76
    },
    {
      "text": "generally correct so I would trust",
      "start": 2535.88,
      "duration": 4.04
    },
    {
      "text": "actually the responses which have been",
      "start": 2538.16,
      "duration": 3.919
    },
    {
      "text": "given by this large language model and I",
      "start": 2539.92,
      "duration": 3.72
    },
    {
      "text": "would say that this evaluation is being",
      "start": 2542.079,
      "duration": 3.48
    },
    {
      "text": "done nicely we even have a quantitative",
      "start": 2543.64,
      "duration": 5.4
    },
    {
      "text": "metric now",
      "start": 2545.559,
      "duration": 3.481
    },
    {
      "text": "great so based on the generated response",
      "start": 2551.24,
      "duration": 3.76
    },
    {
      "text": "we can observe that the Lama 3 Model",
      "start": 2553.16,
      "duration": 4.36
    },
    {
      "text": "provides reasonable evaluations and is",
      "start": 2555.0,
      "duration": 4.96
    },
    {
      "text": "capable of also assigning partial points",
      "start": 2557.52,
      "duration": 4.52
    },
    {
      "text": "or even zero points when the answer is",
      "start": 2559.96,
      "duration": 4.359
    },
    {
      "text": "incorrect the previous prompts however",
      "start": 2562.04,
      "duration": 4.84
    },
    {
      "text": "returned highly detailed evaluations we",
      "start": 2564.319,
      "duration": 4.481
    },
    {
      "text": "don't want these qualitative evaluations",
      "start": 2566.88,
      "duration": 4.0
    },
    {
      "text": "we just want a score so then we can",
      "start": 2568.8,
      "duration": 4.2
    },
    {
      "text": "change the prompt by saying that respond",
      "start": 2570.88,
      "duration": 4.679
    },
    {
      "text": "with integer number only and when you do",
      "start": 2573.0,
      "duration": 4.92
    },
    {
      "text": "that the the result you can see is only",
      "start": 2575.559,
      "duration": 5.0
    },
    {
      "text": "with integer numbers and this is now the",
      "start": 2577.92,
      "duration": 4.76
    },
    {
      "text": "metric which we are evaluating instead",
      "start": 2580.559,
      "duration": 3.881
    },
    {
      "text": "of reporting the model accuracy the",
      "start": 2582.68,
      "duration": 4.12
    },
    {
      "text": "classification accuracy we are using",
      "start": 2584.44,
      "duration": 5.52
    },
    {
      "text": "another llm to evaluate our llm and we",
      "start": 2586.8,
      "duration": 5.44
    },
    {
      "text": "are going to assign a quantitative score",
      "start": 2589.96,
      "duration": 5.04
    },
    {
      "text": "it's still not as robust as a",
      "start": 2592.24,
      "duration": 5.4
    },
    {
      "text": "classification accuracy because many",
      "start": 2595.0,
      "duration": 4.48
    },
    {
      "text": "people might argue that why not assign a",
      "start": 2597.64,
      "duration": 4.76
    },
    {
      "text": "score of 88 or 90 and this is still",
      "start": 2599.48,
      "duration": 6.079
    },
    {
      "text": "subjective but it generally makes sense",
      "start": 2602.4,
      "duration": 5.0
    },
    {
      "text": "and I would like to go ahead with with",
      "start": 2605.559,
      "duration": 4.081
    },
    {
      "text": "this evaluation although there might be",
      "start": 2607.4,
      "duration": 4.919
    },
    {
      "text": "other evaluations which also make sense",
      "start": 2609.64,
      "duration": 4.32
    },
    {
      "text": "and that's why this field of llm",
      "start": 2612.319,
      "duration": 3.921
    },
    {
      "text": "evaluation is a field of such",
      "start": 2613.96,
      "duration": 5.32
    },
    {
      "text": "a um such an open research because there",
      "start": 2616.24,
      "duration": 5.76
    },
    {
      "text": "is a huge scope for what are the metrics",
      "start": 2619.28,
      "duration": 4.24
    },
    {
      "text": "which we should use for",
      "start": 2622.0,
      "duration": 3.96
    },
    {
      "text": "evaluations uh does it make sense to use",
      "start": 2623.52,
      "duration": 4.52
    },
    {
      "text": "mlu does it make sense to use another",
      "start": 2625.96,
      "duration": 5.76
    },
    {
      "text": "llm to evaluate one llm it just leaves",
      "start": 2628.04,
      "duration": 6.079
    },
    {
      "text": "room open for a lot of subjectivity when",
      "start": 2631.72,
      "duration": 3.68
    },
    {
      "text": "you are doing a brain tumor",
      "start": 2634.119,
      "duration": 3.561
    },
    {
      "text": "classification problem the accuracy just",
      "start": 2635.4,
      "duration": 4.28
    },
    {
      "text": "has one value based on how many correct",
      "start": 2637.68,
      "duration": 3.6
    },
    {
      "text": "answers are there and based on how many",
      "start": 2639.68,
      "duration": 3.96
    },
    {
      "text": "wrong answers are there but in this case",
      "start": 2641.28,
      "duration": 5.16
    },
    {
      "text": "there is no one correct value right um",
      "start": 2643.64,
      "duration": 4.52
    },
    {
      "text": "there can be many correct values for",
      "start": 2646.44,
      "duration": 4.08
    },
    {
      "text": "this there is a lot of subjectivity in",
      "start": 2648.16,
      "duration": 4.56
    },
    {
      "text": "the evaluation over here and this",
      "start": 2650.52,
      "duration": 4.36
    },
    {
      "text": "subjectivity itself leaves room for a",
      "start": 2652.72,
      "duration": 5.28
    },
    {
      "text": "lot of research and lot of",
      "start": 2654.88,
      "duration": 6.199
    },
    {
      "text": "Explorations U if you have compute power",
      "start": 2658.0,
      "duration": 5.52
    },
    {
      "text": "on your system or if you're using GPU",
      "start": 2661.079,
      "duration": 4.24
    },
    {
      "text": "here is a function which I have written",
      "start": 2663.52,
      "duration": 3.599
    },
    {
      "text": "which actually goes through the entire",
      "start": 2665.319,
      "duration": 4.24
    },
    {
      "text": "test data set and gives and calculates",
      "start": 2667.119,
      "duration": 5.521
    },
    {
      "text": "this kind of a score for the for all of",
      "start": 2669.559,
      "duration": 6.161
    },
    {
      "text": "the um instructions in the test data set",
      "start": 2672.64,
      "duration": 5.439
    },
    {
      "text": "and then finds an average",
      "start": 2675.72,
      "duration": 5.16
    },
    {
      "text": "score uh so I'm not actually running the",
      "start": 2678.079,
      "duration": 4.441
    },
    {
      "text": "above function because of Hardware",
      "start": 2680.88,
      "duration": 3.52
    },
    {
      "text": "Hardware limitations I'm using MacBook",
      "start": 2682.52,
      "duration": 2.839
    },
    {
      "text": "Air",
      "start": 2684.4,
      "duration": 4.0
    },
    {
      "text": "2020 if you have a M3 MacBook Air it",
      "start": 2685.359,
      "duration": 5.24
    },
    {
      "text": "takes about 1 minute on that if you have",
      "start": 2688.4,
      "duration": 4.719
    },
    {
      "text": "a GPU it might be of a similar speed it",
      "start": 2690.599,
      "duration": 3.76
    },
    {
      "text": "will be very",
      "start": 2693.119,
      "duration": 3.601
    },
    {
      "text": "fast so when you run the above code you",
      "start": 2694.359,
      "duration": 5.0
    },
    {
      "text": "will see see that our model the fine",
      "start": 2696.72,
      "duration": 4.639
    },
    {
      "text": "tune model achieves an average score",
      "start": 2699.359,
      "duration": 4.441
    },
    {
      "text": "above 50 which provides a useful",
      "start": 2701.359,
      "duration": 4.281
    },
    {
      "text": "Benchmark for comparison against other",
      "start": 2703.8,
      "duration": 3.48
    },
    {
      "text": "models or experimenting with different",
      "start": 2705.64,
      "duration": 3.12
    },
    {
      "text": "training",
      "start": 2707.28,
      "duration": 3.44
    },
    {
      "text": "configurations it's worth noting that",
      "start": 2708.76,
      "duration": 4.48
    },
    {
      "text": "ama is not entirely deterministic which",
      "start": 2710.72,
      "duration": 4.639
    },
    {
      "text": "means that the scores which you obtain",
      "start": 2713.24,
      "duration": 3.879
    },
    {
      "text": "versus the scores which I have shown",
      "start": 2715.359,
      "duration": 3.321
    },
    {
      "text": "over here they might be slightly",
      "start": 2717.119,
      "duration": 4.281
    },
    {
      "text": "different but in any case we now have a",
      "start": 2718.68,
      "duration": 5.52
    },
    {
      "text": "framework for a quantitative evaluation",
      "start": 2721.4,
      "duration": 4.56
    },
    {
      "text": "as has been mentioned over here in a lot",
      "start": 2724.2,
      "duration": 3.08
    },
    {
      "text": "of detail there are number more of",
      "start": 2725.96,
      "duration": 2.72
    },
    {
      "text": "things which we can do to improve the",
      "start": 2727.28,
      "duration": 3.92
    },
    {
      "text": "fine tuning performance right so if you",
      "start": 2728.68,
      "duration": 4.679
    },
    {
      "text": "run the fine tuning on the entire test",
      "start": 2731.2,
      "duration": 3.6
    },
    {
      "text": "data set you will see that the average",
      "start": 2733.359,
      "duration": 4.24
    },
    {
      "text": "score is just above 50 and which is not",
      "start": 2734.8,
      "duration": 5.64
    },
    {
      "text": "that great so if you look at so here is",
      "start": 2737.599,
      "duration": 4.96
    },
    {
      "text": "the instruction input output and model",
      "start": 2740.44,
      "duration": 5.48
    },
    {
      "text": "response uh where the model has been run",
      "start": 2742.559,
      "duration": 5.441
    },
    {
      "text": "for all of the test data sets using M3",
      "start": 2745.92,
      "duration": 4.84
    },
    {
      "text": "MacBook Air and now if you see some",
      "start": 2748.0,
      "duration": 4.839
    },
    {
      "text": "answers are correct but some answers are",
      "start": 2750.76,
      "duration": 5.2
    },
    {
      "text": "wrong so for example let's see some",
      "start": 2752.839,
      "duration": 5.801
    },
    {
      "text": "sentences",
      "start": 2755.96,
      "duration": 4.48
    },
    {
      "text": "rewrite the sentence the lecture was",
      "start": 2758.64,
      "duration": 4.08
    },
    {
      "text": "delivered in a clear manner the actual",
      "start": 2760.44,
      "duration": 4.0
    },
    {
      "text": "output was the lecture was delivered",
      "start": 2762.72,
      "duration": 3.96
    },
    {
      "text": "clearly but our model response is the",
      "start": 2764.44,
      "duration": 4.0
    },
    {
      "text": "lecture was delivered in a clear manner",
      "start": 2766.68,
      "duration": 3.52
    },
    {
      "text": "so the model response is the same as the",
      "start": 2768.44,
      "duration": 3.84
    },
    {
      "text": "input so in this case the model is",
      "start": 2770.2,
      "duration": 5.879
    },
    {
      "text": "making a mistake right uh in many such",
      "start": 2772.28,
      "duration": 5.48
    },
    {
      "text": "cases you'll find that the model makes a",
      "start": 2776.079,
      "duration": 3.401
    },
    {
      "text": "mistake but in many cases the model is",
      "start": 2777.76,
      "duration": 4.04
    },
    {
      "text": "also good so that's why the accuracy",
      "start": 2779.48,
      "duration": 3.76
    },
    {
      "text": "which you have obtained the evaluation",
      "start": 2781.8,
      "duration": 2.96
    },
    {
      "text": "accuracy is around",
      "start": 2783.24,
      "duration": 4.04
    },
    {
      "text": "55% so here what I have written is there",
      "start": 2784.76,
      "duration": 4.12
    },
    {
      "text": "can be several ways to improve the",
      "start": 2787.28,
      "duration": 3.92
    },
    {
      "text": "model's performance first of all you can",
      "start": 2788.88,
      "duration": 3.92
    },
    {
      "text": "adjust the hyper parameters such as",
      "start": 2791.2,
      "duration": 3.44
    },
    {
      "text": "learning rate batch size or number of",
      "start": 2792.8,
      "duration": 4.44
    },
    {
      "text": "epo secondly you can increase the size",
      "start": 2794.64,
      "duration": 4.8
    },
    {
      "text": "of the training data set we only used",
      "start": 2797.24,
      "duration": 5.079
    },
    {
      "text": "1100 instruction input output pairs",
      "start": 2799.44,
      "duration": 5.6
    },
    {
      "text": "that's why maybe and we are using 85% as",
      "start": 2802.319,
      "duration": 4.681
    },
    {
      "text": "training data so in a sense we are only",
      "start": 2805.04,
      "duration": 4.44
    },
    {
      "text": "training on 800 instruction input output",
      "start": 2807.0,
      "duration": 3.96
    },
    {
      "text": "pairs those are not",
      "start": 2809.48,
      "duration": 3.8
    },
    {
      "text": "enough so you can increase the size of",
      "start": 2810.96,
      "duration": 4.399
    },
    {
      "text": "the training data set one thing which I",
      "start": 2813.28,
      "duration": 4.279
    },
    {
      "text": "can recommend you to do is this",
      "start": 2815.359,
      "duration": 6.521
    },
    {
      "text": "alpaka um so let me type this over here",
      "start": 2817.559,
      "duration": 6.76
    },
    {
      "text": "Stanford alpaka so if you go to this",
      "start": 2821.88,
      "duration": 4.0
    },
    {
      "text": "repository over here they have a",
      "start": 2824.319,
      "duration": 4.081
    },
    {
      "text": "repository which contains data set of",
      "start": 2825.88,
      "duration": 5.32
    },
    {
      "text": "52,000 instruction output pairs so",
      "start": 2828.4,
      "duration": 4.88
    },
    {
      "text": "that's 50 times higher than the length",
      "start": 2831.2,
      "duration": 4.52
    },
    {
      "text": "of the data set which we used if you use",
      "start": 2833.28,
      "duration": 4.36
    },
    {
      "text": "such a data set you might get better",
      "start": 2835.72,
      "duration": 5.24
    },
    {
      "text": "responses and I would be very I would be",
      "start": 2837.64,
      "duration": 5.88
    },
    {
      "text": "encouraging all of you to try the same",
      "start": 2840.96,
      "duration": 5.159
    },
    {
      "text": "code but on a much larger data set and I",
      "start": 2843.52,
      "duration": 4.48
    },
    {
      "text": "have even showed you this data set over",
      "start": 2846.119,
      "duration": 5.601
    },
    {
      "text": "here the alpaka fine tuning data set",
      "start": 2848.0,
      "duration": 6.64
    },
    {
      "text": "I'll share the repository Link in the",
      "start": 2851.72,
      "duration": 4.76
    },
    {
      "text": "video description",
      "start": 2854.64,
      "duration": 4.16
    },
    {
      "text": "also okay then what you can do is you",
      "start": 2856.48,
      "duration": 4.44
    },
    {
      "text": "can experiment with different forms",
      "start": 2858.8,
      "duration": 4.12
    },
    {
      "text": "different prompts or instruction formats",
      "start": 2860.92,
      "duration": 4.28
    },
    {
      "text": "to guide the model responses more",
      "start": 2862.92,
      "duration": 4.6
    },
    {
      "text": "effectively um and then finally you can",
      "start": 2865.2,
      "duration": 4.2
    },
    {
      "text": "consider the use of a larger pre-train",
      "start": 2867.52,
      "duration": 4.039
    },
    {
      "text": "model pre-train model which may have",
      "start": 2869.4,
      "duration": 4.28
    },
    {
      "text": "greater capacity to comp to capture",
      "start": 2871.559,
      "duration": 3.76
    },
    {
      "text": "complex patterns and generate more",
      "start": 2873.68,
      "duration": 4.159
    },
    {
      "text": "accurate responses in our case we used a",
      "start": 2875.319,
      "duration": 5.721
    },
    {
      "text": "gpt2 355 million right so let me show",
      "start": 2877.839,
      "duration": 6.121
    },
    {
      "text": "you the model which we have used in our",
      "start": 2881.04,
      "duration": 5.799
    },
    {
      "text": "case yeah we have used a gpt2 medium",
      "start": 2883.96,
      "duration": 5.48
    },
    {
      "text": "which is 355 million parameters again if",
      "start": 2886.839,
      "duration": 4.28
    },
    {
      "text": "you have the compute power or if you",
      "start": 2889.44,
      "duration": 4.6
    },
    {
      "text": "have access to GPU you can use gpt2",
      "start": 2891.119,
      "duration": 5.72
    },
    {
      "text": "large which has 774 million parameters",
      "start": 2894.04,
      "duration": 5.4
    },
    {
      "text": "and you can use gpt2 Excel which has",
      "start": 2896.839,
      "duration": 4.921
    },
    {
      "text": "more than a billion parameters but again",
      "start": 2899.44,
      "duration": 3.76
    },
    {
      "text": "you should do this only if you have",
      "start": 2901.76,
      "duration": 4.44
    },
    {
      "text": "compute power for example if you use",
      "start": 2903.2,
      "duration": 6.0
    },
    {
      "text": "gpt2 Excel the number of Transformers is",
      "start": 2906.2,
      "duration": 4.919
    },
    {
      "text": "48 and the number of attention heads in",
      "start": 2909.2,
      "duration": 3.84
    },
    {
      "text": "each Transformer is",
      "start": 2911.119,
      "duration": 4.561
    },
    {
      "text": "25 uh so that's one configuration which",
      "start": 2913.04,
      "duration": 6.0
    },
    {
      "text": "you can definitely vary you can vary",
      "start": 2915.68,
      "duration": 6.919
    },
    {
      "text": "the you can vary the length or the size",
      "start": 2919.04,
      "duration": 5.559
    },
    {
      "text": "of the pre-train model a larger",
      "start": 2922.599,
      "duration": 3.601
    },
    {
      "text": "pre-train model may have greater",
      "start": 2924.599,
      "duration": 3.641
    },
    {
      "text": "capacity to capture complex patterns and",
      "start": 2926.2,
      "duration": 3.68
    },
    {
      "text": "generate more accurate",
      "start": 2928.24,
      "duration": 3.92
    },
    {
      "text": "responses lastly we can also use",
      "start": 2929.88,
      "duration": 4.6
    },
    {
      "text": "parameter efficient fine tuning so the",
      "start": 2932.16,
      "duration": 4.0
    },
    {
      "text": "fine tuning technique which I showed you",
      "start": 2934.48,
      "duration": 5.72
    },
    {
      "text": "is pretty simp simple it just simply uh",
      "start": 2936.16,
      "duration": 5.76
    },
    {
      "text": "fine tuning or updating all of the",
      "start": 2940.2,
      "duration": 3.56
    },
    {
      "text": "parameters in the architecture based on",
      "start": 2941.92,
      "duration": 4.24
    },
    {
      "text": "the specific data set but there are",
      "start": 2943.76,
      "duration": 4.0
    },
    {
      "text": "parameter efficient fine tuning",
      "start": 2946.16,
      "duration": 2.959
    },
    {
      "text": "techniques which have been developed",
      "start": 2947.76,
      "duration": 2.24
    },
    {
      "text": "such as",
      "start": 2949.119,
      "duration": 4.761
    },
    {
      "text": "Laura uh and uh so let me just show you",
      "start": 2950.0,
      "duration": 7.16
    },
    {
      "text": "that Laura parameter efficient fine",
      "start": 2953.88,
      "duration": 6.08
    },
    {
      "text": "tuning and you can see that this is the",
      "start": 2957.16,
      "duration": 5.52
    },
    {
      "text": "kind of fine tuning where uh the fine",
      "start": 2959.96,
      "duration": 4.76
    },
    {
      "text": "tuning process is made much more",
      "start": 2962.68,
      "duration": 5.399
    },
    {
      "text": "efficient than normal fine tuning I did",
      "start": 2964.72,
      "duration": 5.24
    },
    {
      "text": "not have time to cover this fine tuning",
      "start": 2968.079,
      "duration": 3.881
    },
    {
      "text": "process in this lecture or the previous",
      "start": 2969.96,
      "duration": 5.119
    },
    {
      "text": "lectures but you I'll share",
      "start": 2971.96,
      "duration": 5.76
    },
    {
      "text": "the uh information related to this in",
      "start": 2975.079,
      "duration": 4.881
    },
    {
      "text": "subsequent lectures you can also search",
      "start": 2977.72,
      "duration": 3.879
    },
    {
      "text": "a bit about it and if you have time you",
      "start": 2979.96,
      "duration": 3.24
    },
    {
      "text": "can go ahead and Implement parameter",
      "start": 2981.599,
      "duration": 3.841
    },
    {
      "text": "efficient fine tuning it's not very",
      "start": 2983.2,
      "duration": 3.48
    },
    {
      "text": "difficult but it just leads to",
      "start": 2985.44,
      "duration": 3.04
    },
    {
      "text": "computational speed up a lot and",
      "start": 2986.68,
      "duration": 4.48
    },
    {
      "text": "sometimes it even leads to speed up in",
      "start": 2988.48,
      "duration": 5.48
    },
    {
      "text": "accuracy uh okay so with that we have",
      "start": 2991.16,
      "duration": 4.52
    },
    {
      "text": "come to the end of this lecture where we",
      "start": 2993.96,
      "duration": 4.96
    },
    {
      "text": "have successfully in fact uh we have",
      "start": 2995.68,
      "duration": 4.679
    },
    {
      "text": "started from scratch and we have",
      "start": 2998.92,
      "duration": 2.56
    },
    {
      "text": "successfully",
      "start": 3000.359,
      "duration": 3.48
    },
    {
      "text": "fine-tuned the model on the instruction",
      "start": 3001.48,
      "duration": 4.76
    },
    {
      "text": "data set and now we can say that a model",
      "start": 3003.839,
      "duration": 5.0
    },
    {
      "text": "can answer instruction successfully but",
      "start": 3006.24,
      "duration": 4.599
    },
    {
      "text": "more importantly we have also seen the",
      "start": 3008.839,
      "duration": 4.641
    },
    {
      "text": "evaluation metrics predominantly we saw",
      "start": 3010.839,
      "duration": 4.76
    },
    {
      "text": "that there are three evaluation metrics",
      "start": 3013.48,
      "duration": 5.359
    },
    {
      "text": "for uh instruction data instruction",
      "start": 3015.599,
      "duration": 6.52
    },
    {
      "text": "ftuned llms the first is mlu which is",
      "start": 3018.839,
      "duration": 5.76
    },
    {
      "text": "measuring massive multitask language",
      "start": 3022.119,
      "duration": 5.68
    },
    {
      "text": "understanding the second is uh",
      "start": 3024.599,
      "duration": 5.881
    },
    {
      "text": "human preference comparison such as such",
      "start": 3027.799,
      "duration": 6.0
    },
    {
      "text": "as asking humans to rate llm output and",
      "start": 3030.48,
      "duration": 5.16
    },
    {
      "text": "the third one which we have used in this",
      "start": 3033.799,
      "duration": 3.961
    },
    {
      "text": "lecture is using another larger large",
      "start": 3035.64,
      "duration": 5.199
    },
    {
      "text": "language model to evaluate our llm",
      "start": 3037.76,
      "duration": 5.96
    },
    {
      "text": "performance so we have used the meta",
      "start": 3040.839,
      "duration": 6.041
    },
    {
      "text": "Lama 8 billion instruct model to",
      "start": 3043.72,
      "duration": 6.32
    },
    {
      "text": "evaluate the performance of our model",
      "start": 3046.88,
      "duration": 5.16
    },
    {
      "text": "and this was an automatic or this was an",
      "start": 3050.04,
      "duration": 3.96
    },
    {
      "text": "automated way and this proceeded in a",
      "start": 3052.04,
      "duration": 3.96
    },
    {
      "text": "simpler manner that's why we use this",
      "start": 3054.0,
      "duration": 3.16
    },
    {
      "text": "third method",
      "start": 3056.0,
      "duration": 2.799
    },
    {
      "text": "but if you have time I would highly",
      "start": 3057.16,
      "duration": 4.72
    },
    {
      "text": "encourage you to use our own code but",
      "start": 3058.799,
      "duration": 5.161
    },
    {
      "text": "use benchmarks such as measuring massive",
      "start": 3061.88,
      "duration": 3.679
    },
    {
      "text": "multitask language understanding you",
      "start": 3063.96,
      "duration": 3.76
    },
    {
      "text": "will learn so much through this process",
      "start": 3065.559,
      "duration": 4.481
    },
    {
      "text": "this mlu paper is highly impactful and",
      "start": 3067.72,
      "duration": 4.76
    },
    {
      "text": "highly relevant and if you want to",
      "start": 3070.04,
      "duration": 4.96
    },
    {
      "text": "contribute to llm evaluation research",
      "start": 3072.48,
      "duration": 4.079
    },
    {
      "text": "there is a lot of scope for this",
      "start": 3075.0,
      "duration": 4.24
    },
    {
      "text": "contribution because this field is novel",
      "start": 3076.559,
      "duration": 3.481
    },
    {
      "text": "it's",
      "start": 3079.24,
      "duration": 3.72
    },
    {
      "text": "evolving and I think that there is there",
      "start": 3080.04,
      "duration": 4.4
    },
    {
      "text": "are still a lot of breakthroughs",
      "start": 3082.96,
      "duration": 2.839
    },
    {
      "text": "remaining to happen in the llm",
      "start": 3084.44,
      "duration": 3.359
    },
    {
      "text": "evaluation space",
      "start": 3085.799,
      "duration": 3.881
    },
    {
      "text": "so thanks a lot everyone we come to the",
      "start": 3087.799,
      "duration": 5.081
    },
    {
      "text": "end of this lecture where we covered the",
      "start": 3089.68,
      "duration": 6.08
    },
    {
      "text": "entire instruction F tuning",
      "start": 3092.88,
      "duration": 5.479
    },
    {
      "text": "pipeline um let me just show you that",
      "start": 3095.76,
      "duration": 4.16
    },
    {
      "text": "pipeline once and what all we have",
      "start": 3098.359,
      "duration": 3.2
    },
    {
      "text": "covered yeah so we have essentially",
      "start": 3099.92,
      "duration": 3.399
    },
    {
      "text": "covered these nine steps which have been",
      "start": 3101.559,
      "duration": 5.0
    },
    {
      "text": "showed over here uh we started out with",
      "start": 3103.319,
      "duration": 5.121
    },
    {
      "text": "stage one which is preparing the data",
      "start": 3106.559,
      "duration": 4.641
    },
    {
      "text": "set which was very important then we",
      "start": 3108.44,
      "duration": 4.6
    },
    {
      "text": "fine tuned the large language model and",
      "start": 3111.2,
      "duration": 4.44
    },
    {
      "text": "we did not stop there we even evaluated",
      "start": 3113.04,
      "duration": 4.24
    },
    {
      "text": "the large language model model we",
      "start": 3115.64,
      "duration": 3.919
    },
    {
      "text": "extracted responses did a qualitative",
      "start": 3117.28,
      "duration": 4.16
    },
    {
      "text": "evaluation and then even did a",
      "start": 3119.559,
      "duration": 4.601
    },
    {
      "text": "quantitative scoring using",
      "start": 3121.44,
      "duration": 5.08
    },
    {
      "text": "AMA through this we also learned a new",
      "start": 3124.16,
      "duration": 6.36
    },
    {
      "text": "tool called AMA and uh with this we have",
      "start": 3126.52,
      "duration": 5.88
    },
    {
      "text": "now come to the end of the instruction",
      "start": 3130.52,
      "duration": 4.559
    },
    {
      "text": "finetuning llm lectures these were five",
      "start": 3132.4,
      "duration": 4.399
    },
    {
      "text": "to six very comprehensive and highly",
      "start": 3135.079,
      "duration": 3.76
    },
    {
      "text": "detailed lectures which taught you",
      "start": 3136.799,
      "duration": 4.28
    },
    {
      "text": "everything about the fine tuning process",
      "start": 3138.839,
      "duration": 4.24
    },
    {
      "text": "thanks a lot everyone I hope you have",
      "start": 3141.079,
      "duration": 4.921
    },
    {
      "text": "liked these lectures which were a mix of",
      "start": 3143.079,
      "duration": 5.441
    },
    {
      "text": "whiteboard approach plus coding I don't",
      "start": 3146.0,
      "duration": 4.079
    },
    {
      "text": "think there are any other videos out",
      "start": 3148.52,
      "duration": 3.079
    },
    {
      "text": "there which show about fine tuning",
      "start": 3150.079,
      "duration": 3.601
    },
    {
      "text": "pre-training in so much detail the",
      "start": 3151.599,
      "duration": 3.921
    },
    {
      "text": "reason I'm showing you all these details",
      "start": 3153.68,
      "duration": 4.6
    },
    {
      "text": "is that I firmly believe that a student",
      "start": 3155.52,
      "duration": 5.44
    },
    {
      "text": "who knows the nuts and bolts of how",
      "start": 3158.28,
      "duration": 5.36
    },
    {
      "text": "models work of how exactly things are",
      "start": 3160.96,
      "duration": 4.44
    },
    {
      "text": "assembled those will be the students who",
      "start": 3163.64,
      "duration": 4.36
    },
    {
      "text": "will be strong ml Engineers strong llm",
      "start": 3165.4,
      "duration": 4.52
    },
    {
      "text": "engineers and those will be the students",
      "start": 3168.0,
      "duration": 3.92
    },
    {
      "text": "who contribute to Noel research or",
      "start": 3169.92,
      "duration": 4.12
    },
    {
      "text": "breakthroughs thank you so much everyone",
      "start": 3171.92,
      "duration": 3.8
    },
    {
      "text": "I look forward to seeing you in the next",
      "start": 3174.04,
      "duration": 4.4
    },
    {
      "text": "lecture",
      "start": 3175.72,
      "duration": 2.72
    }
  ],
  "full_text": "[Music] hello everyone and welcome to this lecture in the build large language models from scratch Series today it's a very interesting and exciting lecture because we are going to complete the instruction fine tuning project which we have started in the previous lecture we fine tuned the large language model on this instruction input and output data set and we proved that after the fineable the llm is much better at responding to instructions so the previous lecture was an awesome lecture and we optained a great result at the end of it if you have not seen the previous lecture I would highly encourage you to go back check that lecture again so let me just show you in the FL map where we are in the stage of the fine tuning process so we have finished the stage number one so let me change to purple ink we have finished the stage number one which is preparation of the data set and this involved data set download batching the data set and creating data loaders just to revise this is the data set which we have been using which consists of 1100 instruction input and output Pairs and we are telling the llm that hey we know that you are pre-trained very well but you are not very good at responding to instructions currently CCT currently and I want to train you again so that you are better at responding to instructions here is the data which I have and I want you to learn from this data so that you get better at responding to instructions so this is the data set we need we spent a lot of time earlier on batching the data set so we needed to implement several steps so that within each batch all of the input samples have the same number of token IDs and after the batching is done we created the training we created the training the testing and the validation data loaders remember that we used 85 % of the data for 85% of the data for training 10% of the data for testing and 5% of the data for validation and when I say data I mean this instruction data file instruction input and output pairs right then we also completed stage number two in stage number two we loaded a pre-trained llm so we loaded the weights which are publicly available from gpt2 so that the model has a good foundational base and then on top of it in the previous lecture we find in tuned the llm which means that we trained the llm again um so that it learns based on this instruction input output data set and then we also saw the loss function in the code so just let me take you to code right now and show you the loss function which we obtained so here's the training and the validation loss function which we obtained in the previous lecture due to the memory and the compute limitations on my CPU I just ran it for one Epoch but as I mentioned in the last lecture as well if you have access to a GPU or if you have a stronger PC such as a Macbook M3 for example you can definitely try running this code for two apox or even more we are using a GPT gpt2 model with 355 million weights so it's a pretty huge model and that's why a lot of llm computations really depend on the architecture of the CPU whether you're using a GPU how fast is your computer Etc anyways I I have shown you this code on a computer with minimal configuration so if I am able to run this on my PC I'm pretty sure all of you would be able to run up till here on your PC as well so let's look at the response which we have got we tested using one one test or one validation sample and the validation sample was that write a response that appropriately completes the request and the request was convert the active sentence to passive convert the active sentence to passive and the sentence was the chef Cooks the me every day the response given by our large language model was the meal is prepared by the chef every day now this was not the correct answer right the correct answer is the meal is cooked by the chef every day whereas our llm used prepared and that's probably because we did not train it for multiple epochs due to the compute limitations in fact I know for a fact that if you run it for two epochs instead of one this output changes to the meal is cooked every day by the chef even with one Epoch we can see that the LM is doing much better instead of just answering randomly before fine tuning so this is the stage we are at right now and now we have to move to the third stage which is evaluating large language models so you might be thinking that we fine tuned the model and now LM is responding to instructions that's great right well the things do not stop here because there is one big challenge now you can see that this response the meal is prepared by the every day by the chef is not exactly correct because the correct answer should be the meal is cooked every day by the chef in fact let's look at the correct answer um so Chef so let me type Chef here and let's go to the actual instruction yeah I think this was the actual instruction which we had given convert the active sentence to passive the chef Cooks the meal every day and this is the actual ground truth answer the meal is scooped by the chef every day whereas the llm which we have trained or fine tuned answered the meal is prepared every day by the chef so it's clearly not correct right the real question which we want to ask in the evaluation stage of the llm is that how do we know how correct are we so how to measure how to measure the llm performance it's not a straightforward answer as a yes or no by binary classification right we have to compare between two sentences and we have to compare and say that okay ideally uh ideally it should have been cooked but instead my llm is saying prepared so I deduct some points for it but how do we Define the scoring system how do we compare between two sentences and understand whether the llm is doing a good job or the llm is not doing a good job and that's why we have to devote a whole separate lecture to evaluating the large language model so this itself consists of three stages which we are going to look at the first is extracting the responses from the llm the second is qualitative evaluation and the third is scoring the responses so let's start implementing the first step which is extracting the responses uh extracting and saving the responses from the llm now u in the code I've mentioned this as step number six which is extracting and saving responses right okay so as I mentioned after finetuning the llm on the training portion of the instruction data set we now proceed to evaluate its performance on the testing data set to accomplish this first we need to extract the responses generated for the model for every input right in the test data set and collect them for manual analysis so if you look at this data set right now this entire data set is 1100 uh instruction input output Pairs and out of this I'm using 10% as test data so what I now need to do is that I need to take my f tuned llm and I need to I need to give it the test data and I need to generate the responses for all of the test data and only then I will be able to compare it with the output right currently I showed you only one response over here which is the llm response for one instruction output pair but now I want to collect the llm responses for all the instruction input in the test data and then I will compare it with the ground Roo data awesome so so uh let's do just that before that before we do it for all the different uh instruction output pairs I just want to show you the results of our llm for three test set samples which are there in the data set so what I'm going to do here is that I'm going to uh look at three instructions in the test data set and I'm going to run the fine tuned llm on these instructions and then we are going to look at the generated response so here you can see we are looping over the test data and then we are looking at the first three entries um then what we are doing is that we are generating the output from our fine tuned llm we are going to convert the token IDs into text which will give us the final generated output and then remember that the final output consists of the instruction input as well we need to remove all that and just need to consider the response which is given by the large language model and then we are printing out the input text which is the instruction then we are ALS also printing the correct response and then we are printing the model response for three test samples so let's look at these let's look at the model predictions and the true responses for these three test samples so the first test sample is rewrite the sentence using a simile and the input is the car is very fast so simile is basically using similar words so the correct answer to this car is very fast the correct response is the car is as fast as Lightning whereas our fine tuned llm predicted that the car is as fast as a bullet um that's the first now here you can see that this is quite good right the correct response and the model response are actually very close to each other that's awesome now let's look at the second instruction what type of cloud is typically associated with thunderstorms the correct response to this was the type of cloud typically associated with thunderstorms is cumulonimbus cumulonimbus uh whereas the model response is a thunderstorm is a type of cloud that typically forms in the atmosphere or a region of high pressure it typically produces a strong wind that blows across the area creating a dense dense Cloud so clearly we can see that the model response is not good right because we wanted a type of cloud which is associated with thunderstorms so just qualitatively I can look at the correct response and I can look at my model response and I can see that it's not really doing what it's asked to do and let's look at the the third test which is the final test example name the author of Pride and Prejudice the correct response is Jane Austin but the model response is the author of Pride ande Prejudice is George Bernard Shaw clearly this is a mistake so out of these three test samples without even looking at any evaluation metric qualitatively I can say that for the first test sample it's very close for the second test sample it's not technically wrong but it does not answer correctly and for the third test sample it's fully wrong U so firstly the reason why the llm is making mistakes here is because we have trained it only on one Epoch just increasing the epox to two will really improve the quality of the responses uh even with these number of epox now you can see that actually measuring the performance is quite hard right even for me as a person I can answer this qualitatively but how can I give a performance score how do I give an evaluation score as machine learning Engineers we always like score right such as accuracy such as the classification loss how do I give an evaluation score to these responses so as we can see based on the test instructions the model performs relatively well the answer to the first instruction is clearly correct while the second answer and the third answers are not correct this is because we have done the fine tuning only for one Epoch due to Hardware limitations to get better results we need to increase the EPO to at least two that's number one number two is that that model evaluation is not straightforward in previously we had done a f classification fine tuning for spam and no spam emails right there it's so easy to evaluate the accuracy if the correct answer is Spam and the model predicts no spam it's not accurate if the correct answer is Spam and the model predict spam it's good but in this case it's not as straight forward so what do researchers do there is a whole field of llm evaluation which is becoming extremely important because people are realizing that along with training llms and fine-tuning llms an equally important field to pay attention to is llm evaluation Because unless we have a metric to evaluate llms we will not know what is good and what is bad then how do we know whether research is happening in a successful manner or in an unsuccessful manner so re researchers have basically developed three methods for evaluating llms this is still a very new field and Rapid progress is happening as we speak but these are the three most common ways of evaluating instruction fine tuned llms so the first way is which is a very common way is by giving the llm a general knowledge test and by benchmarking it on actual known responses and this is known as measuring massive multitask language understanding or M mlu so MML score is a very popular term now and the simplest way to understand it is that that this is a score which tests the general knowledge of a model so this test measures the model performance on a huge range of questions in various fields and then uh you can get the score of the model so for example if we have our fine tuned llm right uh to pass this MML test we have to ask those 57 questions which have mention which are mentioned in this paper and we'll note down the responses given by our fine tuned llm then we'll compare it with the actual responses and based on that a score will be generated so for example here's the paper majoring massive multitask language understanding M mlu and if you scroll down below here you can see that these are the 57 questions which are tested in this in abstract algebra Anatomy astronomy business ethics high school biology chemistry computer science then international law management marketing moral disputes public relations security relations in all of these fields there are number of questions which are asked so essentially these are the 57 tasks which the model needs to be tested on I asked to summarize this paper to a notebook LM which is an awesome tool developed by Google to summarize research papers and here's what it showed so MML uh is a benchmark for measuring language model knowledge it is designed to evaluate a text model's ability to learn and apply Knowledge from various domains the authors of this paper argue that existing NLP benchmarks while useful do not ACC adequately assess the breadth and depth of knowledge that language models are exposed that is why mlu consists of 57 tasks covering stem Humanity social sciences and other areas ranging from elementary to professional levels of difficulty we can use MML to evaluate our custom instruction fine tuned llm by testing its performance on these 57 tasks in fact many of the research papers if you read in the instruction finetuned llm space they show the mlu score which means how the llm is performing on all of these general knowledge tasks that's the first approach for evaluating llms the second approach is human preference comparison where basically humans look at the answers given by multiple llms and then they compare the performance between the large language models so this is basically having a human in the loop and the human using their own intuition and understanding they Benchmark or compare llms that's the second way the Third Way which is also fairly common is using a large language model itself to evaluate how close the llm responses to the actual output so let's say here I have collected a file which has the instruction input output but it also has the model response so in the third category what is done is basically you look at the true data which is the true output which we expected and we look at the model response and then we ask a large language model itself to compare between the output and the model response and to assign a score so this is very uh this is fairly straightforward to do because it's fully automated we just look at the output we look at the model response and then we ask a trained a very massive large language model to uh compare these two and find the score if you think about it this is also like taking the easy way out because we don't know how the llm is evaluating right what are the metrics through which the llm itself is comparing between the output the true output and the model's response we are trusting the llm is extremely and supremely smart to do this to get this score for us since this method is simple we are also going to implement this method in this particular video or in this particular lecture Series where what we are going to do is that we have the output and the model response we are going to ask an llm to look at the output to look at the model response to compare and to ass a score but it's very important for you all to be aware of these three types of llm evaluation which is extremely important uh so considering the scale of the task at hand we will Implement an approach similar to method three which involves evaluating the responses automatically using another llm this will allow us to efficiently assess the quality of the generated responses without the need for extensive human involvement thereby saving time and resources while still obtaining meaningful performance indicators if you have this code and if you can run this code using the mlu test uh that would be awesome and I would really like to see if someone works on that part of the code and take the instruction find T llm and run the MML test on it awesome now the next step is that we need to basically collect the responses for the entire test file right earlier we only saw the responses for three for the first three examples of the test data now what we need to do is that we need to collect model responses for all of the uh all of the instructions in the test data set and we need to collect these responses in a separate file so what we are doing now is that we'll prepare the responses for the evaluation process and we will essentially construct a new file or create a new file which is titled instruction data with response. Json for record keeping this file will essentially contain the instruction input the true output and also the model response um so to give you a visual this is how that file will look like now if you look at this first file which is just the instruction data. Json file over here instruction data. Json this only con consists of the instruction the input and the true output it does not contain the model response but now we are developing one more file or creating one more file which is called instruction data with response so this Con consists of the instruction the input output and it also consists of the model response this will make it very easy for us to later evaluate the performance of the model because then we simply have to compare the output and the model response for every instruction and assign a score so in this piece of code what is done here is that uh in this piece of code if you see we are looking at all of the in all of the input instructions in the test data and we are generating the response for all of the inputs in the test data and then we are collecting the responses in a file called instruction data with response that's all so when you run this code the generate function will be called on all of the instruction input pairs in the test data the responses will be generated and only the response will be collected and then it will be appended to the instruction data with response file so overall the instruction data with response file will look exactly like what I'm showing on the screen right now where in the instruction input and output dictionary the model response will also be appended now for every single uh instruction input output pair awesome now if you run this process it will take some time for me it took around 10 to 15 minutes to create this instruction data with response file but it will be created and then it will be stored for you so remember now this test data is the dictionary which consists of the instruction input output and the model response also we can test this by printing the first element of the test data dictionary so if you print out the first element you'll see that we have the instruction we have the input we have the output and then we have the model response as well the the output is here and then the model response is here so the test data dictionary essentially now uh consists of everything which we need it consists of the instruction the input output and the model response now what we can do is that once we have uh we have this file so everything is ready for us to evaluate the output and the model response now what we'll do is that we'll just save our fine tune model this is extremely important because if you accidentally close your working session then you don't want to fine tune again right remember fine tuning took four hours for me on my PC and I just want to I just want to reuse the fine tune weights again when I start my session the next time so please don't forget about this step it's very simple you just have to use the tor. save command and model. state dictionary which will ensure that all the train parameters will be stored in this file name which is gpt2 medium 355 million sf. pth this is the file where we are storing the model and to load the model in a future session you simply have to do load State dict so two commands are important tor. save model. State dict and tor. save or load State dict rather the first command is model do State dict so saving this and the second is load State dict and then you have to load the file in which you have stored the parameters awesome I hope everyone of you is with me until this point so we have reached this stage where we have successfully collected the responses in a file called instruction instruction data with response. Json we have collected the responses in this file and we have also discussed about a way in which we are going to compare the output and the model response we have not discussed too many details about this but we have seen the three Frameworks for evaluation and we have shortlisted this last framework where we'll use use an llm to compare between the output and the model response now we are ready to move to the next part which is essentially evaluating the fine tuned large language model so the evaluation process essentially will will come in this building block what we we have seen Okay so until now we saw extracting the responses and we have also seen qualitative evaluation where we looked at the response so let's say for example I can qualitatively look at the output and I can look at the response and I can say whether it's correct or not qualitatively right but we have not yet mathematically or Quantified scoring the responses this is the part which we'll Implement in evaluating the llm so after extracting the responses by our fine tuned llm we will use another large language model to automatically evaluate these responses and let's see how we are going to do that in practice which is the large language model which we are going to use so this brings us to the step number seven in the evaluating the fine tuned llm and as I mentioned in this section we will Implement a method to automate the response evaluation of the fine tuned llm using a another larger llm so we'll use a bigger larger llm which is pre-trained and it's extremely supremely knowledgeable to compare our model response and the true response and to assign a score to implement this evaluation step we are going to use a software or it can be called as an application which is called as AMA so let me take you to the AMA application so here if you go to ama.com you can just type it ama.com you'll see that this interface essentially comes up and the simplest way to think about AMA is that it's an efficient application to run large language models on your laptop so you can learn you can run various large language models you can run Lama 3 which is developed by meta you can run 53 developed by Microsoft you can run Mistral you can leun gamma 2 similarly you can leun several models on your PC and remember using o Lama you do not do pre-training you just do inference which means that the model is already pre-trained you just look at the responses or uh you look at the output which is given by the model you use the model in inference mode using AMA you do not use it in pre-training mode so what we are going to do is that we are going to implement the evaluation step uh by utilizing an instruction ftuned 88 billion parameter Lama 3 Model so we are going to be using an 8 billion parameter Lama 3 Model uh and that's and we are going to access that through olama so the reason we are going to utilize this instruction fine tuned model is that because it's already fine tuned on a huge number of instructions so if you search about this this is the Lama 38 billion instruction fine tuning model and here the par 8 billion parameters are already optimized which means that this model is already trained to follow instructions and it's supremely smart so what we are going to do is that we are going to utilize this model to compare between the True Result and the our llm model output so using this Lama 38 billion we are going to compare the actual output and our model response and we are going to assign a score so we are going to tell this instruction finetune Lama model that your next instruction is that look at the output look at the model response and assign a score to how well my model is doing and since this llama model is already trained for instruction for following instruction it will do a great job at this new instruction which is essentially finding an evaluation score this is also a great time for all of you to learn about AMA which is a very commonly used uh llm inference application okay so uh one thing to remember is that ama is only a tool for generating text using llm inference and it does not support training or fine-tuning llm so let me now show you uh the download process for AMA so that you can follow the similar instruction functions on your laptop all right so the next step for us is to download olama um so I'm using a Mac here so I'm going to show you how to install it and run it on Mac and I'll also give you instructions if you're using Windows so you have to go to ama.com so let me type ama.com here and you have to just click on download over here once you click on download uh the entire so if you are on Mac OS or Linux or Windows you can download the Appo rate version for you so I have clicked on download for mac o and you can see that the download process starts here it's a file which is 177 uh 177 MB so you can download it and then you can open it follow instructions click on next next and next and then AMA will be installed it's pretty simple the installation process does not take too much time I'm going to cancel this download over here because I've already installed it then what you can do is that then you have to open your terminal so so here you can see Ive opened my Mac terminal over here and then what you have to essentially type is that you have to type O Lama run Lama 3 so this is the correct command o Lama run Lama 3 and let me type it over here also uh o Lama run Lama 3 this is the command which you have to type on the terminal and if you are using a Mac you can directly type this command if you are using Windows then the command which you might need to type might be o Lama serve so if you're using Windows type this command first o Lama serve and then type O Lama run Lama 3 okay um so type these commands in the sequence if you're using Mac you can directly type or o Lama run Lama 3 let me show you my terminal again so here you can see I have typed o Lama run Lama let me expand this let me expand my terminal so that you can see it in more detail so here you can see that Ive run o Lama run Lama 3 and the when you try to run Lama 3 the files which are downloaded are of size 4.7 GB so it's an 8 billion parameter model right so it takes a lot of space and memory to download this so it took around 15 to 20 minutes for me to download this on my desktop uh but as the downloading is happening you'll see all of these instructions being printed out on your terminal and at the end you will see success U when you see the success these arrows will appear these right hand side arrows which means that now the Lama 3 is loaded which means you can interact with Lama 3 large language model you can ask any question to this so for example here I have asked what do llamas eat and now the llm which will respond to you is not chat GPT or any other llm the llm which will respond is this Lama 38 billion instruct this is that llm which will respond to you um so that's what AMA helps you to do so currently have run Lama 3 over here right similarly you can use o Lama to run uh any other llm also you can use o Lama to run 53 you can use it to run mistal GMA 2 Etc on your laptop so I'm running this on my laptop and all these results which I'm seeing are on my laptop as well awesome so once you reach this stage it will mean that o am I successfully running for you so when when I'm going to implement the next part of this code please keep o Lama running if you shut down this terminal AMA will not be running and then your code won't execute so please keep AMA running the simplest way to test is that just make sure to ask some question over here and if you get a response it means that ama has been running successfully awesome now I'm going to move back or switch back to code to explain the rest to you okay I hope you have installed AMA now and you have run the O Lama run Lama 3 command on the terminal which I have demonstrated I have just provided a simple code block here which verifies that the AMA session is running properly before we use AMA to evaluate the test set responses so our final goal is to use o Lama and Lama 3 especially to compare the output and the model response to assign a score but before doing that we need to check whether o Lama is running or not so you can run this code and here it should come O Lama running is equal to true if this comes false which means that ama is not successfully running uh now I showed you the O Lama run command on the terminal right there is an alternative for this command instead of going to the terminal each time we can interact with the Lama model uh using the API through python so we can create a function which is called as the query model what this function does is that uh you can pass in the model so here I'm using Lama 3 so this function when it's called it will pass a query to Lama 3 and the query will also be provided by us so we'll be providing the query uh we'll be providing the query as the prompt so when you call this function query model you'll have to provide a prompt and you'll have to provide a model and what this function will do is that this this function will send a request uh will send an API request to this Lama 3 Model based on the prompt and then the response will be generated and then the response will be returned so instead of writing or instead of running the uh o Lama run Lama 3 on the the terminal each time we can simply do an API call where we can fetch the results and bring it into our jupyter notebook interface I'm not going through this code in detail because this will distract us from the main purpose of this whole video lecture which is to uh perform the model evaluation so simply note that what this query model does is that it helps us to pass in a prompt to Lama 3 or whichever llm which we are using through o Lama and to give us the responses that's it so now what we'll do is that the prompt which we are going to give here is that these are this is the output this is the model response compare these and assign a score that's it that's all we are going to do but this function query model will be very important to us because we are going to pass in the prompt through this function okay now again I want to mention that before running the subsequent code cells ensure that all Lama is still running the previous code which I showed you over here should print o Lama running to be equal to True uh to confirm that the model is active and ready to receive requests so now here is just a small demo of how the query model works so in the query model here you can prescribe the uh input which is what do llamas eat and then you can also mention the model so here I'm using Lama 3 in O Lama you can use various models so you can search models here and view all so you can see the different types of models which are present um in in ama so you can do gamma 2 also here so I can do I can do gamma 2 also over here but the reason we are sticking with Lama 3 is that we are using this 8 billion instruction fine tuned model since it's already fine tuned on the instruction or use instruction data set okay so uh to the query model function you pass in any prompt and the model and that llm will look at the prompt and generate a response so you can print out the response over here now please note here that this process also takes a lot of compute memory so to print out one result here it took a long time for me because I on CPU I highly encourage if you have a GPU access for all of you to uh use the GPU wherever possible if you have a CPU it's fine I'm showing you only those things which can run on a CPU but just make sure that this will take time and at least free up memory space free up at least 15 to 20 GB on your desktop for this entire code to run so this query and printing out this result is not as straightforward as I've shown it here it took a long time to generate this text and to to query so let's say I've just passed in one query here right if I passed in three or four queries my laptop is does not run my laptop hangs so that's why I can pass a maximum of two queries in one session simultaneously otherwise my laptop just hangs because the processing speed or the processing power is not there so now what we can do is that using using the query model function we can evaluate the responses which are generated by our fine tune model with a prompt and in that prompt we have to ask the Lama 3 Model to rate our fine tuned models responses on a scale of 1 12 100 based on the given test response so let me show you the prompt which you are going to give so the prompt which now we are going to give to the query model is that uh given the input and the input is going to be this given this input uh actually given the input so it's the format input function so the input is going to be actually the instruction along with the input this is going to be the input over here so given this input and the correct output so the correct output is this output um so given the instruction and the correct output score the model response and the model response is the model response in this Json file which is this model response score the model response on a scale of 0 to 100 where 100 is the best score so what the llm will do now is that it will look at the output it will look at the model response based on this instruction and the input and then it will assign a score so we are using the query model function which we defined earlier and then we are passing this prompt you see how we are using another large language model to evaluate our large language model um so we are using the Lama 3B large Lang language model Lama 38 billion large language model to evaluate our fine tuned llm and here we have specified the model so we don't need to specify it again um when we call the query model because if it's not specified by default the query model will use Lama 3 when I share this code file with you you can feel free to explore with other llms also the sky is the limit here it's it's an exploratory notebook and I've got good decent results here but you can of course feel free to explore with larger models different models as well so here you can see that I'm testing for three data points or three testing data here and I'm printing the model I'm printing the model response and I'm printing the score also which is given by the this Lama 8 billion model again when I ran this code on my laptop my laptop crashed initially and then it took around 35 minutes to print out the responses for the three um instruction input output pair so three samples in the test data so for me it was impossible to run this code on the entire test data set because I could only do it for two to three samples so here is the response so for the first query for the first instruction the rewrite the sentence using a Sim the our model response was the car is as fast as a bullet and the data set which is the actual response is the car is as fast as lighting so here is the qualitative evaluation which is given by the llm and rate the model response the car is as fast as a bullet 85 out of 100 here's why the response uses simil correctly comparing the speed of the car to something else in this case a bullet the comparison is relevant and makes sense as bullets are known for their High Velocity the phrase as fast as is used correctly to introduce the simile the only reason I wouldn't give it a perfect score is that some people might find the comparison slightly less Vivid or evocative uh for example comparing something to lightning as in the actual response can be more dramatic dramatic however as fast as a bullet is strong and effective simile that effectively conveys so it says that although the um response can be more dramatic it still captures the essence and it still captures the grammatical meaning of a simile so overall the model did a great job and the score is 85 out of 100 that's for the first for the second uh for the second input the actual response was the type of Cloud typically associated with thunderstorms is cumulonimbus cumulonimbus but the model response is a thunderstorm is a type of cloud Etc it's pretty long so the score for this model as given by the Llama llm is 20 out of 100 and the reason is because the response doesn't directly answer the question about what type of cloud is typically associated with thunderstorms instead it provides a general description of thunderstorms which is not relevant to the original questions at all the response of also contain some inaccuracies such as stating that thunderstorms for more high pressure regions whereas the llm pointed out that thunderstorms can actually occur in various weather patterns that's why the score is 20 out of 100 in the third question the actual response was Jane Austin but the response of the model was the author of Pride and Prejudice is George Bernard Shaw this is wrong so the llm rates is zero out of 100 the correct answer is Jane Austin not George Bernard Shaw George Bernard Shaw was an Irish playright and author but he did not write Pride and Prejudice the response is completely Incorrect and does not provide any relevant information this is awesome right it looks that the llm which we are using is pretty smart and it's doing an awesome job at qualitative or even quantitative evaluation so as human I can think of some qualitative uh similarities or qualitatively I can think how to evaluate the evaluate the model's response but even the qualitative evaluation of the llm is actually amazing the kind of points which the llm introduced do not come naturally to me for example as a human I did not think about this dramatic effect um as a human I did not know that uh thunderstorms can also occur in various weather patterns and uh stating that thunderstorms form over high pressure regions might be inaccurate so as humans I have some limitations even in qualitative qualitative evaluation and the llm is filling those limitations it's doing an awesome job at qualitative evaluation and although the quantitative evaluation I'm not still sure why it's 85 why it's 20 it kind of makes sense here it's 20 because most of the answer was wrong here it's zero because it's a factual question so the answer is factually incorrect and here it's 85 because most of the answer seems to be generally correct so I would trust actually the responses which have been given by this large language model and I would say that this evaluation is being done nicely we even have a quantitative metric now great so based on the generated response we can observe that the Lama 3 Model provides reasonable evaluations and is capable of also assigning partial points or even zero points when the answer is incorrect the previous prompts however returned highly detailed evaluations we don't want these qualitative evaluations we just want a score so then we can change the prompt by saying that respond with integer number only and when you do that the the result you can see is only with integer numbers and this is now the metric which we are evaluating instead of reporting the model accuracy the classification accuracy we are using another llm to evaluate our llm and we are going to assign a quantitative score it's still not as robust as a classification accuracy because many people might argue that why not assign a score of 88 or 90 and this is still subjective but it generally makes sense and I would like to go ahead with with this evaluation although there might be other evaluations which also make sense and that's why this field of llm evaluation is a field of such a um such an open research because there is a huge scope for what are the metrics which we should use for evaluations uh does it make sense to use mlu does it make sense to use another llm to evaluate one llm it just leaves room open for a lot of subjectivity when you are doing a brain tumor classification problem the accuracy just has one value based on how many correct answers are there and based on how many wrong answers are there but in this case there is no one correct value right um there can be many correct values for this there is a lot of subjectivity in the evaluation over here and this subjectivity itself leaves room for a lot of research and lot of Explorations U if you have compute power on your system or if you're using GPU here is a function which I have written which actually goes through the entire test data set and gives and calculates this kind of a score for the for all of the um instructions in the test data set and then finds an average score uh so I'm not actually running the above function because of Hardware Hardware limitations I'm using MacBook Air 2020 if you have a M3 MacBook Air it takes about 1 minute on that if you have a GPU it might be of a similar speed it will be very fast so when you run the above code you will see see that our model the fine tune model achieves an average score above 50 which provides a useful Benchmark for comparison against other models or experimenting with different training configurations it's worth noting that ama is not entirely deterministic which means that the scores which you obtain versus the scores which I have shown over here they might be slightly different but in any case we now have a framework for a quantitative evaluation as has been mentioned over here in a lot of detail there are number more of things which we can do to improve the fine tuning performance right so if you run the fine tuning on the entire test data set you will see that the average score is just above 50 and which is not that great so if you look at so here is the instruction input output and model response uh where the model has been run for all of the test data sets using M3 MacBook Air and now if you see some answers are correct but some answers are wrong so for example let's see some sentences rewrite the sentence the lecture was delivered in a clear manner the actual output was the lecture was delivered clearly but our model response is the lecture was delivered in a clear manner so the model response is the same as the input so in this case the model is making a mistake right uh in many such cases you'll find that the model makes a mistake but in many cases the model is also good so that's why the accuracy which you have obtained the evaluation accuracy is around 55% so here what I have written is there can be several ways to improve the model's performance first of all you can adjust the hyper parameters such as learning rate batch size or number of epo secondly you can increase the size of the training data set we only used 1100 instruction input output pairs that's why maybe and we are using 85% as training data so in a sense we are only training on 800 instruction input output pairs those are not enough so you can increase the size of the training data set one thing which I can recommend you to do is this alpaka um so let me type this over here Stanford alpaka so if you go to this repository over here they have a repository which contains data set of 52,000 instruction output pairs so that's 50 times higher than the length of the data set which we used if you use such a data set you might get better responses and I would be very I would be encouraging all of you to try the same code but on a much larger data set and I have even showed you this data set over here the alpaka fine tuning data set I'll share the repository Link in the video description also okay then what you can do is you can experiment with different forms different prompts or instruction formats to guide the model responses more effectively um and then finally you can consider the use of a larger pre-train model pre-train model which may have greater capacity to comp to capture complex patterns and generate more accurate responses in our case we used a gpt2 355 million right so let me show you the model which we have used in our case yeah we have used a gpt2 medium which is 355 million parameters again if you have the compute power or if you have access to GPU you can use gpt2 large which has 774 million parameters and you can use gpt2 Excel which has more than a billion parameters but again you should do this only if you have compute power for example if you use gpt2 Excel the number of Transformers is 48 and the number of attention heads in each Transformer is 25 uh so that's one configuration which you can definitely vary you can vary the you can vary the length or the size of the pre-train model a larger pre-train model may have greater capacity to capture complex patterns and generate more accurate responses lastly we can also use parameter efficient fine tuning so the fine tuning technique which I showed you is pretty simp simple it just simply uh fine tuning or updating all of the parameters in the architecture based on the specific data set but there are parameter efficient fine tuning techniques which have been developed such as Laura uh and uh so let me just show you that Laura parameter efficient fine tuning and you can see that this is the kind of fine tuning where uh the fine tuning process is made much more efficient than normal fine tuning I did not have time to cover this fine tuning process in this lecture or the previous lectures but you I'll share the uh information related to this in subsequent lectures you can also search a bit about it and if you have time you can go ahead and Implement parameter efficient fine tuning it's not very difficult but it just leads to computational speed up a lot and sometimes it even leads to speed up in accuracy uh okay so with that we have come to the end of this lecture where we have successfully in fact uh we have started from scratch and we have successfully fine-tuned the model on the instruction data set and now we can say that a model can answer instruction successfully but more importantly we have also seen the evaluation metrics predominantly we saw that there are three evaluation metrics for uh instruction data instruction ftuned llms the first is mlu which is measuring massive multitask language understanding the second is uh human preference comparison such as such as asking humans to rate llm output and the third one which we have used in this lecture is using another larger large language model to evaluate our llm performance so we have used the meta Lama 8 billion instruct model to evaluate the performance of our model and this was an automatic or this was an automated way and this proceeded in a simpler manner that's why we use this third method but if you have time I would highly encourage you to use our own code but use benchmarks such as measuring massive multitask language understanding you will learn so much through this process this mlu paper is highly impactful and highly relevant and if you want to contribute to llm evaluation research there is a lot of scope for this contribution because this field is novel it's evolving and I think that there is there are still a lot of breakthroughs remaining to happen in the llm evaluation space so thanks a lot everyone we come to the end of this lecture where we covered the entire instruction F tuning pipeline um let me just show you that pipeline once and what all we have covered yeah so we have essentially covered these nine steps which have been showed over here uh we started out with stage one which is preparing the data set which was very important then we fine tuned the large language model and we did not stop there we even evaluated the large language model model we extracted responses did a qualitative evaluation and then even did a quantitative scoring using AMA through this we also learned a new tool called AMA and uh with this we have now come to the end of the instruction finetuning llm lectures these were five to six very comprehensive and highly detailed lectures which taught you everything about the fine tuning process thanks a lot everyone I hope you have liked these lectures which were a mix of whiteboard approach plus coding I don't think there are any other videos out there which show about fine tuning pre-training in so much detail the reason I'm showing you all these details is that I firmly believe that a student who knows the nuts and bolts of how models work of how exactly things are assembled those will be the students who will be strong ml Engineers strong llm engineers and those will be the students who contribute to Noel research or breakthroughs thank you so much everyone I look forward to seeing you in the next lecture"
}