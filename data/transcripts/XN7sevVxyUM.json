{
  "video": {
    "video_id": "XN7sevVxyUM",
    "title": "Lecture 13: Introduction to the Attention Mechanism in Large Language Models (LLMs)",
    "duration": 3085.0,
    "index": 12
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 3.51
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.04,
      "duration": 5.32
    },
    {
      "text": "in the build large language models from",
      "start": 7.759,
      "duration": 6.08
    },
    {
      "text": "scratch series up till now in this",
      "start": 10.36,
      "duration": 5.32
    },
    {
      "text": "series we have looked at a number of",
      "start": 13.839,
      "duration": 4.481
    },
    {
      "text": "things in particular we have spent a lot",
      "start": 15.68,
      "duration": 5.4
    },
    {
      "text": "of time discussing the data preparation",
      "start": 18.32,
      "duration": 5.119
    },
    {
      "text": "and the sampling stage of building a",
      "start": 21.08,
      "duration": 5.16
    },
    {
      "text": "large language model in particular if",
      "start": 23.439,
      "duration": 5.121
    },
    {
      "text": "you want to build an entire large",
      "start": 26.24,
      "duration": 4.32
    },
    {
      "text": "language model pipeline it has to be",
      "start": 28.56,
      "duration": 5.08
    },
    {
      "text": "done in three stages in stage one you",
      "start": 30.56,
      "duration": 5.159
    },
    {
      "text": "have to look at the data preparation",
      "start": 33.64,
      "duration": 4.04
    },
    {
      "text": "then comes attention mechanism then",
      "start": 35.719,
      "duration": 4.921
    },
    {
      "text": "comes the llm architecture in stage two",
      "start": 37.68,
      "duration": 4.64
    },
    {
      "text": "we have the training and the model",
      "start": 40.64,
      "duration": 4.439
    },
    {
      "text": "evaluation and in stage three we have",
      "start": 42.32,
      "duration": 5.719
    },
    {
      "text": "the fine tuning so we have spent around",
      "start": 45.079,
      "duration": 4.681
    },
    {
      "text": "five to six lectures on the data",
      "start": 48.039,
      "duration": 3.801
    },
    {
      "text": "preparation and sampling part where we",
      "start": 49.76,
      "duration": 7.279
    },
    {
      "text": "looked at um word embedding we looked at",
      "start": 51.84,
      "duration": 7.519
    },
    {
      "text": "tokenization we looked at bite pair",
      "start": 57.039,
      "duration": 4.561
    },
    {
      "text": "encoding we looked at positional",
      "start": 59.359,
      "duration": 5.561
    },
    {
      "text": "encoding essentially we have looked at",
      "start": 61.6,
      "duration": 5.72
    },
    {
      "text": "the entire data pre-processing pipeline",
      "start": 64.92,
      "duration": 5.68
    },
    {
      "text": "of the llm in a lot of detail now it's",
      "start": 67.32,
      "duration": 5.4
    },
    {
      "text": "time to move to the second building",
      "start": 70.6,
      "duration": 5.4
    },
    {
      "text": "block of stage one and that is the",
      "start": 72.72,
      "duration": 4.48
    },
    {
      "text": "attention",
      "start": 76.0,
      "duration": 3.68
    },
    {
      "text": "mechanism in one of the earlier lectures",
      "start": 77.2,
      "duration": 5.64
    },
    {
      "text": "of this series I told all of you that I",
      "start": 79.68,
      "duration": 5.32
    },
    {
      "text": "think of Transformers as the secret",
      "start": 82.84,
      "duration": 6.48
    },
    {
      "text": "Source behind the llm so if Transformers",
      "start": 85.0,
      "duration": 8.04
    },
    {
      "text": "is like a car then attention mechanism",
      "start": 89.32,
      "duration": 5.88
    },
    {
      "text": "is essentially the engine which drives",
      "start": 93.04,
      "duration": 2.92
    },
    {
      "text": "the",
      "start": 95.2,
      "duration": 3.76
    },
    {
      "text": "car this is the mechanism I think which",
      "start": 95.96,
      "duration": 5.439
    },
    {
      "text": "gives so much power to large language",
      "start": 98.96,
      "duration": 5.04
    },
    {
      "text": "models and that's why chat GPT performs",
      "start": 101.399,
      "duration": 6.201
    },
    {
      "text": "so well uh there are a few lectures on",
      "start": 104.0,
      "duration": 5.719
    },
    {
      "text": "attention mechanism on YouTube but they",
      "start": 107.6,
      "duration": 4.28
    },
    {
      "text": "are not comprehensive at all it's",
      "start": 109.719,
      "duration": 4.161
    },
    {
      "text": "impossible to cover everything related",
      "start": 111.88,
      "duration": 4.159
    },
    {
      "text": "to attention mechanism in one",
      "start": 113.88,
      "duration": 5.0
    },
    {
      "text": "lecture uh it's one of the most",
      "start": 116.039,
      "duration": 4.841
    },
    {
      "text": "important Concepts so I have planned a",
      "start": 118.88,
      "duration": 4.32
    },
    {
      "text": "series of four to five lectures on",
      "start": 120.88,
      "duration": 5.12
    },
    {
      "text": "attention mechanism today's lecture will",
      "start": 123.2,
      "duration": 5.88
    },
    {
      "text": "be a foundational overview where we will",
      "start": 126.0,
      "duration": 4.879
    },
    {
      "text": "understand the introduction to attention",
      "start": 129.08,
      "duration": 4.68
    },
    {
      "text": "mechanism what it is why it is really",
      "start": 130.879,
      "duration": 4.681
    },
    {
      "text": "needed and the types of attention",
      "start": 133.76,
      "duration": 3.96
    },
    {
      "text": "mechanism then what we'll be doing from",
      "start": 135.56,
      "duration": 3.759
    },
    {
      "text": "the next lecture onwards is we'll be",
      "start": 137.72,
      "duration": 3.36
    },
    {
      "text": "coding out the entire attention",
      "start": 139.319,
      "duration": 3.841
    },
    {
      "text": "mechanism completely from scratch we are",
      "start": 141.08,
      "duration": 4.56
    },
    {
      "text": "not going to assume even a single",
      "start": 143.16,
      "duration": 6.32
    },
    {
      "text": "thing um okay so in this lecture we are",
      "start": 145.64,
      "duration": 6.08
    },
    {
      "text": "going to look at the subsection which is",
      "start": 149.48,
      "duration": 4.28
    },
    {
      "text": "essentially uh the subsection on",
      "start": 151.72,
      "duration": 4.519
    },
    {
      "text": "attention mechanism so let me switch my",
      "start": 153.76,
      "duration": 5.199
    },
    {
      "text": "color to I think I'll",
      "start": 156.239,
      "duration": 5.481
    },
    {
      "text": "choose purple here and let's start",
      "start": 158.959,
      "duration": 5.441
    },
    {
      "text": "looking at attention mechanism in detail",
      "start": 161.72,
      "duration": 6.4
    },
    {
      "text": "so first let me motivate uh so that you",
      "start": 164.4,
      "duration": 5.68
    },
    {
      "text": "get an intuition of why this name",
      "start": 168.12,
      "duration": 5.0
    },
    {
      "text": "attention mechanism comes and uh what",
      "start": 170.08,
      "duration": 5.36
    },
    {
      "text": "are we essentially trying to solve here",
      "start": 173.12,
      "duration": 4.96
    },
    {
      "text": "so let's look at this example let's say",
      "start": 175.44,
      "duration": 4.48
    },
    {
      "text": "you are a large language model like G P",
      "start": 178.08,
      "duration": 4.519
    },
    {
      "text": "PT and you have received this sentence",
      "start": 179.92,
      "duration": 5.679
    },
    {
      "text": "the sentence is the cat that was sitting",
      "start": 182.599,
      "duration": 6.401
    },
    {
      "text": "on the mat which was next to the dog",
      "start": 185.599,
      "duration": 6.2
    },
    {
      "text": "jumped now as a human I can say that",
      "start": 189.0,
      "duration": 6.519
    },
    {
      "text": "okay there is maybe a cat here the cat",
      "start": 191.799,
      "duration": 6.881
    },
    {
      "text": "uh was sitting next to a dog and the cat",
      "start": 195.519,
      "duration": 6.241
    },
    {
      "text": "was also on a mat and then uh as a human",
      "start": 198.68,
      "duration": 4.88
    },
    {
      "text": "when I read this I know that the cat",
      "start": 201.76,
      "duration": 4.88
    },
    {
      "text": "jumped okay but as a large language",
      "start": 203.56,
      "duration": 5.28
    },
    {
      "text": "model if you look at the sentence you'll",
      "start": 206.64,
      "duration": 4.599
    },
    {
      "text": "soon realize that this sentence is a bit",
      "start": 208.84,
      "duration": 4.959
    },
    {
      "text": "confusing I can very clearly see that",
      "start": 211.239,
      "duration": 4.92
    },
    {
      "text": "the cat was sitting on the mat if only",
      "start": 213.799,
      "duration": 4.681
    },
    {
      "text": "this were the sentence I could easily",
      "start": 216.159,
      "duration": 5.16
    },
    {
      "text": "analyze that the cat is the main subject",
      "start": 218.48,
      "duration": 5.24
    },
    {
      "text": "in this sentence and the cat was sitting",
      "start": 221.319,
      "duration": 5.241
    },
    {
      "text": "and the object is the mat but the thing",
      "start": 223.72,
      "duration": 5.4
    },
    {
      "text": "is when there are such complex sentences",
      "start": 226.56,
      "duration": 4.0
    },
    {
      "text": "which are also called long-term",
      "start": 229.12,
      "duration": 3.479
    },
    {
      "text": "dependencies where there is this second",
      "start": 230.56,
      "duration": 3.56
    },
    {
      "text": "sentence which is",
      "start": 232.599,
      "duration": 4.121
    },
    {
      "text": "attached so then it becomes a bit",
      "start": 234.12,
      "duration": 4.72
    },
    {
      "text": "difficult for the large language model",
      "start": 236.72,
      "duration": 3.879
    },
    {
      "text": "because uh",
      "start": 238.84,
      "duration": 3.479
    },
    {
      "text": "so after this sentence there will be a",
      "start": 240.599,
      "duration": 4.2
    },
    {
      "text": "number of other sentences right but the",
      "start": 242.319,
      "duration": 3.961
    },
    {
      "text": "main thing which the large language",
      "start": 244.799,
      "duration": 3.36
    },
    {
      "text": "model needs to really understand from",
      "start": 246.28,
      "duration": 4.76
    },
    {
      "text": "this sentence is that the cat which is",
      "start": 248.159,
      "duration": 3.961
    },
    {
      "text": "the main",
      "start": 251.04,
      "duration": 4.879
    },
    {
      "text": "subject that subject actually",
      "start": 252.12,
      "duration": 7.16
    },
    {
      "text": "jumped so the main the action which the",
      "start": 255.919,
      "duration": 5.0
    },
    {
      "text": "subject perform is",
      "start": 259.28,
      "duration": 5.56
    },
    {
      "text": "jumping so the llm should understand",
      "start": 260.919,
      "duration": 7.401
    },
    {
      "text": "that when it looks at cat the word which",
      "start": 264.84,
      "duration": 5.639
    },
    {
      "text": "it should be paying the most attention",
      "start": 268.32,
      "duration": 3.68
    },
    {
      "text": "to is",
      "start": 270.479,
      "duration": 4.521
    },
    {
      "text": "jumped notice how I use the word",
      "start": 272.0,
      "duration": 7.32
    },
    {
      "text": "attention so when I look at the word cat",
      "start": 275.0,
      "duration": 6.84
    },
    {
      "text": "um of course sitting is also important",
      "start": 279.32,
      "duration": 4.24
    },
    {
      "text": "because the cat was earlier sitting on",
      "start": 281.84,
      "duration": 4.6
    },
    {
      "text": "the mat but now the cat has jumped so",
      "start": 283.56,
      "duration": 5.56
    },
    {
      "text": "there are few words in this sentence",
      "start": 286.44,
      "duration": 5.96
    },
    {
      "text": "which the llm needs to pay the most",
      "start": 289.12,
      "duration": 7.2
    },
    {
      "text": "attention to in association with",
      "start": 292.4,
      "duration": 6.6
    },
    {
      "text": "cat and if you don't introduce the",
      "start": 296.32,
      "duration": 4.719
    },
    {
      "text": "attention me mechanism it's very",
      "start": 299.0,
      "duration": 4.16
    },
    {
      "text": "difficult for the llm to know that the",
      "start": 301.039,
      "duration": 6.401
    },
    {
      "text": "cat is the one who has jumped uh maybe",
      "start": 303.16,
      "duration": 6.4
    },
    {
      "text": "if the attention was attention mechanism",
      "start": 307.44,
      "duration": 3.68
    },
    {
      "text": "was not there the llm would have been",
      "start": 309.56,
      "duration": 3.479
    },
    {
      "text": "confused and it might think oh the dog",
      "start": 311.12,
      "duration": 4.28
    },
    {
      "text": "has jumped or it might think that the",
      "start": 313.039,
      "duration": 5.0
    },
    {
      "text": "main main part of this sentence is the",
      "start": 315.4,
      "duration": 5.04
    },
    {
      "text": "cat is on a mat so if the attention",
      "start": 318.039,
      "duration": 5.321
    },
    {
      "text": "mechanism was not there maybe the llm",
      "start": 320.44,
      "duration": 4.72
    },
    {
      "text": "would have thought that the cat is on",
      "start": 323.36,
      "duration": 4.6
    },
    {
      "text": "the mat that's it it would not know that",
      "start": 325.16,
      "duration": 5.039
    },
    {
      "text": "I have to give a lot of atten ention to",
      "start": 327.96,
      "duration": 5.72
    },
    {
      "text": "jump in association with the cat this is",
      "start": 330.199,
      "duration": 6.0
    },
    {
      "text": "the broad level intuition why we need to",
      "start": 333.68,
      "duration": 5.16
    },
    {
      "text": "learn about the attention mechanism when",
      "start": 336.199,
      "duration": 4.84
    },
    {
      "text": "you have sentences such as this and then",
      "start": 338.84,
      "duration": 4.919
    },
    {
      "text": "there is a big story after this the LM",
      "start": 341.039,
      "duration": 4.641
    },
    {
      "text": "needs to analyze this sentence and it",
      "start": 343.759,
      "duration": 4.521
    },
    {
      "text": "needs to process in relation to a",
      "start": 345.68,
      "duration": 4.48
    },
    {
      "text": "particular word let's say in relation to",
      "start": 348.28,
      "duration": 4.479
    },
    {
      "text": "cat which other word should I pay the",
      "start": 350.16,
      "duration": 5.479
    },
    {
      "text": "most attention to and that's where the",
      "start": 352.759,
      "duration": 4.56
    },
    {
      "text": "attention mechanism comes into the",
      "start": 355.639,
      "duration": 4.28
    },
    {
      "text": "picture it turns out that with without",
      "start": 357.319,
      "duration": 4.361
    },
    {
      "text": "attention mechanism if you used a",
      "start": 359.919,
      "duration": 4.481
    },
    {
      "text": "recurrent neural network uh or some",
      "start": 361.68,
      "duration": 5.48
    },
    {
      "text": "other neural network it does not capture",
      "start": 364.4,
      "duration": 5.6
    },
    {
      "text": "the longterm dependencies between",
      "start": 367.16,
      "duration": 4.759
    },
    {
      "text": "sentences that's the broad level",
      "start": 370.0,
      "duration": 5.039
    },
    {
      "text": "intuition now let's dive deeper into",
      "start": 371.919,
      "duration": 4.56
    },
    {
      "text": "what all we will be covering about",
      "start": 375.039,
      "duration": 3.72
    },
    {
      "text": "attention in the subsequent",
      "start": 376.479,
      "duration": 4.961
    },
    {
      "text": "lectures so if you look at the attention",
      "start": 378.759,
      "duration": 4.84
    },
    {
      "text": "mechanism itself there are essentially",
      "start": 381.44,
      "duration": 4.28
    },
    {
      "text": "four types of attention",
      "start": 383.599,
      "duration": 5.081
    },
    {
      "text": "mechanism uh the main attention",
      "start": 385.72,
      "duration": 6.039
    },
    {
      "text": "mechanism which was which is used in GPT",
      "start": 388.68,
      "duration": 5.16
    },
    {
      "text": "uh generative pre-train Transformer and",
      "start": 391.759,
      "duration": 5.081
    },
    {
      "text": "all the modern llms is this multi-head",
      "start": 393.84,
      "duration": 6.72
    },
    {
      "text": "attention and many YouTube videos and um",
      "start": 396.84,
      "duration": 6.359
    },
    {
      "text": "courses all many courses just directly",
      "start": 400.56,
      "duration": 4.88
    },
    {
      "text": "start with multi-head attention it's a",
      "start": 403.199,
      "duration": 4.081
    },
    {
      "text": "very difficult concept to understand if",
      "start": 405.44,
      "duration": 4.039
    },
    {
      "text": "you directly start learning this so you",
      "start": 407.28,
      "duration": 4.6
    },
    {
      "text": "have to go in a sequential manner so",
      "start": 409.479,
      "duration": 4.361
    },
    {
      "text": "what I'll be covering in this SE in the",
      "start": 411.88,
      "duration": 3.879
    },
    {
      "text": "series of lectures is first I'll start",
      "start": 413.84,
      "duration": 3.68
    },
    {
      "text": "with something called simplified self",
      "start": 415.759,
      "duration": 4.321
    },
    {
      "text": "attention so this is the pure EST and",
      "start": 417.52,
      "duration": 4.359
    },
    {
      "text": "the most basic form of the attention",
      "start": 420.08,
      "duration": 4.44
    },
    {
      "text": "technique so that you understand what is",
      "start": 421.879,
      "duration": 5.241
    },
    {
      "text": "attention then we will move to self",
      "start": 424.52,
      "duration": 5.359
    },
    {
      "text": "attention so here we will also introduce",
      "start": 427.12,
      "duration": 6.0
    },
    {
      "text": "train trainable weights which form the",
      "start": 429.879,
      "duration": 5.281
    },
    {
      "text": "basis of the actual mechanism which is",
      "start": 433.12,
      "duration": 4.519
    },
    {
      "text": "used in the llms until this part we are",
      "start": 435.16,
      "duration": 5.12
    },
    {
      "text": "still not at the actual mechanism but we",
      "start": 437.639,
      "duration": 4.081
    },
    {
      "text": "are building up",
      "start": 440.28,
      "duration": 4.28
    },
    {
      "text": "slowly after I cover self attention the",
      "start": 441.72,
      "duration": 4.72
    },
    {
      "text": "next thing which I'll move to is causal",
      "start": 444.56,
      "duration": 3.88
    },
    {
      "text": "attention this is when things really",
      "start": 446.44,
      "duration": 3.599
    },
    {
      "text": "start to get interesting",
      "start": 448.44,
      "duration": 4.159
    },
    {
      "text": "we are predicting the next World right",
      "start": 450.039,
      "duration": 4.6
    },
    {
      "text": "by looking at the past world so what",
      "start": 452.599,
      "duration": 3.72
    },
    {
      "text": "causal attention does is that it's a",
      "start": 454.639,
      "duration": 4.361
    },
    {
      "text": "type of self attention uh that allows",
      "start": 456.319,
      "duration": 5.32
    },
    {
      "text": "the model to consider only the previous",
      "start": 459.0,
      "duration": 4.599
    },
    {
      "text": "and the current inputs in a sequence and",
      "start": 461.639,
      "duration": 4.761
    },
    {
      "text": "it masks out the future inputs no need",
      "start": 463.599,
      "duration": 4.961
    },
    {
      "text": "to uh pay too much attention to this",
      "start": 466.4,
      "duration": 4.32
    },
    {
      "text": "right now I'm just giving you a broad",
      "start": 468.56,
      "duration": 3.88
    },
    {
      "text": "overview of what all I'll cover in the",
      "start": 470.72,
      "duration": 3.52
    },
    {
      "text": "subsequent lectures when we look at",
      "start": 472.44,
      "duration": 3.879
    },
    {
      "text": "attention today we are not going to",
      "start": 474.24,
      "duration": 3.919
    },
    {
      "text": "cover all of these today we are just",
      "start": 476.319,
      "duration": 4.961
    },
    {
      "text": "going to look at uh more details about",
      "start": 478.159,
      "duration": 5.72
    },
    {
      "text": "the history of how attention came into",
      "start": 481.28,
      "duration": 4.68
    },
    {
      "text": "the picture why it is needed why it's",
      "start": 483.879,
      "duration": 3.6
    },
    {
      "text": "better than RNN",
      "start": 485.96,
      "duration": 4.16
    },
    {
      "text": "Etc and then finally we'll move to",
      "start": 487.479,
      "duration": 5.28
    },
    {
      "text": "multi-ad attention only when you have",
      "start": 490.12,
      "duration": 4.4
    },
    {
      "text": "understood causal attention and self",
      "start": 492.759,
      "duration": 4.481
    },
    {
      "text": "attention and simplified self attention",
      "start": 494.52,
      "duration": 3.92
    },
    {
      "text": "you will be able to understand",
      "start": 497.24,
      "duration": 3.56
    },
    {
      "text": "multi-head attention this is the main",
      "start": 498.44,
      "duration": 4.24
    },
    {
      "text": "concept which is actually used in",
      "start": 500.8,
      "duration": 2.959
    },
    {
      "text": "building",
      "start": 502.68,
      "duration": 4.0
    },
    {
      "text": "GPD so multi-head attention is just",
      "start": 503.759,
      "duration": 4.761
    },
    {
      "text": "basically a bunch of causal attention",
      "start": 506.68,
      "duration": 3.44
    },
    {
      "text": "heads stacked together",
      "start": 508.52,
      "duration": 3.16
    },
    {
      "text": "and we'll code out this multi-head",
      "start": 510.12,
      "duration": 3.919
    },
    {
      "text": "attention fully from scratch I'll show",
      "start": 511.68,
      "duration": 5.52
    },
    {
      "text": "you the dimensions how they work etc all",
      "start": 514.039,
      "duration": 5.12
    },
    {
      "text": "of that is planned in the subsequent",
      "start": 517.2,
      "duration": 5.079
    },
    {
      "text": "lectures so this multi-head attention is",
      "start": 519.159,
      "duration": 4.76
    },
    {
      "text": "essentially an extension of self",
      "start": 522.279,
      "duration": 4.0
    },
    {
      "text": "attention and causal attention that",
      "start": 523.919,
      "duration": 5.201
    },
    {
      "text": "enables the model to simultaneously",
      "start": 526.279,
      "duration": 5.161
    },
    {
      "text": "attend to information from different",
      "start": 529.12,
      "duration": 4.76
    },
    {
      "text": "representation subspaces don't worry",
      "start": 531.44,
      "duration": 3.839
    },
    {
      "text": "about this just remember that the",
      "start": 533.88,
      "duration": 3.32
    },
    {
      "text": "multi-ad attention allows the llm to",
      "start": 535.279,
      "duration": 4.881
    },
    {
      "text": "look at input data and and then process",
      "start": 537.2,
      "duration": 4.68
    },
    {
      "text": "many parts of that input data in",
      "start": 540.16,
      "duration": 4.04
    },
    {
      "text": "parallel so for example if this is the",
      "start": 541.88,
      "duration": 4.88
    },
    {
      "text": "sentence the multi-ad attention allows",
      "start": 544.2,
      "duration": 5.36
    },
    {
      "text": "the llm to have let's say one attention",
      "start": 546.76,
      "duration": 4.72
    },
    {
      "text": "head looks at this part one attention",
      "start": 549.56,
      "duration": 3.68
    },
    {
      "text": "head looks at this part one attention",
      "start": 551.48,
      "duration": 4.28
    },
    {
      "text": "head looks at this part Etc this is just",
      "start": 553.24,
      "duration": 4.76
    },
    {
      "text": "a crude description so that you get an",
      "start": 555.76,
      "duration": 4.04
    },
    {
      "text": "understanding of what do you mean by",
      "start": 558.0,
      "duration": 3.079
    },
    {
      "text": "multihead",
      "start": 559.8,
      "duration": 3.44
    },
    {
      "text": "attention so I just wanted to show you",
      "start": 561.079,
      "duration": 4.32
    },
    {
      "text": "this overview so that you get an idea of",
      "start": 563.24,
      "duration": 3.76
    },
    {
      "text": "how these four to five lectures are",
      "start": 565.399,
      "duration": 4.641
    },
    {
      "text": "actually planned um it is is impossible",
      "start": 567.0,
      "duration": 4.64
    },
    {
      "text": "as I mentioned to cover all of this in",
      "start": 570.04,
      "duration": 3.479
    },
    {
      "text": "one lecture and that's why I will follow",
      "start": 571.64,
      "duration": 4.16
    },
    {
      "text": "a very comprehensive approach I'll show",
      "start": 573.519,
      "duration": 4.481
    },
    {
      "text": "everything on the Whiteboard and then I",
      "start": 575.8,
      "duration": 4.56
    },
    {
      "text": "have this uh Google collab notebook",
      "start": 578.0,
      "duration": 3.68
    },
    {
      "text": "where everything has already been",
      "start": 580.36,
      "duration": 2.88
    },
    {
      "text": "implemented and we'll go through this",
      "start": 581.68,
      "duration": 4.36
    },
    {
      "text": "entire notebook see hiding future words",
      "start": 583.24,
      "duration": 4.599
    },
    {
      "text": "with causal attention and then I also",
      "start": 586.04,
      "duration": 3.4
    },
    {
      "text": "have a section on",
      "start": 587.839,
      "duration": 4.481
    },
    {
      "text": "U essentially multi-head attention yeah",
      "start": 589.44,
      "duration": 5.0
    },
    {
      "text": "see so at the end of these four to five",
      "start": 592.32,
      "duration": 3.519
    },
    {
      "text": "lectures we'll be implementing this",
      "start": 594.44,
      "duration": 3.36
    },
    {
      "text": "multi-head attention in Python and code",
      "start": 595.839,
      "duration": 4.56
    },
    {
      "text": "it out from scratch",
      "start": 597.8,
      "duration": 4.8
    },
    {
      "text": "okay for now let's continue with today's",
      "start": 600.399,
      "duration": 4.12
    },
    {
      "text": "lecture which is an introduction to the",
      "start": 602.6,
      "duration": 4.88
    },
    {
      "text": "attention mechanism and uh how",
      "start": 604.519,
      "duration": 6.0
    },
    {
      "text": "researchers got to discovering attention",
      "start": 607.48,
      "duration": 5.359
    },
    {
      "text": "so let's go back in time a bit because",
      "start": 610.519,
      "duration": 4.361
    },
    {
      "text": "to always appreciate something new we",
      "start": 612.839,
      "duration": 4.641
    },
    {
      "text": "need to know about the history of how of",
      "start": 614.88,
      "duration": 6.28
    },
    {
      "text": "how we came to this uh",
      "start": 617.48,
      "duration": 6.72
    },
    {
      "text": "Innovation so let's go right at the",
      "start": 621.16,
      "duration": 5.08
    },
    {
      "text": "start where we are modeling let's say",
      "start": 624.2,
      "duration": 4.28
    },
    {
      "text": "long sequences so we have one sequence",
      "start": 626.24,
      "duration": 4.64
    },
    {
      "text": "in English and uh let's say we want to",
      "start": 628.48,
      "duration": 4.88
    },
    {
      "text": "translate it to the German language so",
      "start": 630.88,
      "duration": 5.12
    },
    {
      "text": "what's the problem in modeling long",
      "start": 633.36,
      "duration": 5.32
    },
    {
      "text": "sequences so let's look at this question",
      "start": 636.0,
      "duration": 5.519
    },
    {
      "text": "what is the problem with architectures",
      "start": 638.68,
      "duration": 4.8
    },
    {
      "text": "without the attention mechanism which",
      "start": 641.519,
      "duration": 3.521
    },
    {
      "text": "came before the",
      "start": 643.48,
      "duration": 4.52
    },
    {
      "text": "llms um so for reference we'll start",
      "start": 645.04,
      "duration": 4.72
    },
    {
      "text": "with the language translation model so",
      "start": 648.0,
      "duration": 5.399
    },
    {
      "text": "let's look at this this figure here um",
      "start": 649.76,
      "duration": 5.879
    },
    {
      "text": "so I have words in the English language",
      "start": 653.399,
      "duration": 5.281
    },
    {
      "text": "so can you uh or let's say I have the",
      "start": 655.639,
      "duration": 5.561
    },
    {
      "text": "German inut sentence which I want to",
      "start": 658.68,
      "duration": 5.2
    },
    {
      "text": "translate to English so this is the",
      "start": 661.2,
      "duration": 4.52
    },
    {
      "text": "input sentence I have the first word the",
      "start": 663.88,
      "duration": 4.399
    },
    {
      "text": "second word then I have the third word",
      "start": 665.72,
      "duration": 5.4
    },
    {
      "text": "Etc uh and I want to translate this into",
      "start": 668.279,
      "duration": 6.24
    },
    {
      "text": "English Okay so uh let's say we do a",
      "start": 671.12,
      "duration": 5.56
    },
    {
      "text": "word by word translation if I translate",
      "start": 674.519,
      "duration": 4.56
    },
    {
      "text": "the first word to English it's can if I",
      "start": 676.68,
      "duration": 4.0
    },
    {
      "text": "translate the second word to English",
      "start": 679.079,
      "duration": 3.88
    },
    {
      "text": "it's you if I translate the third word",
      "start": 680.68,
      "duration": 5.08
    },
    {
      "text": "it's me the fourth word help so if you",
      "start": 682.959,
      "duration": 6.601
    },
    {
      "text": "translate every German word word by word",
      "start": 685.76,
      "duration": 5.8
    },
    {
      "text": "the translation comes out to be can you",
      "start": 689.56,
      "duration": 4.719
    },
    {
      "text": "me help this sentence to",
      "start": 691.56,
      "duration": 4.959
    },
    {
      "text": "translate uh that's obviously not",
      "start": 694.279,
      "duration": 5.081
    },
    {
      "text": "correct right so the main takeaway here",
      "start": 696.519,
      "duration": 4.681
    },
    {
      "text": "is that the word by word translation",
      "start": 699.36,
      "duration": 4.84
    },
    {
      "text": "does not work and uh you can also see",
      "start": 701.2,
      "duration": 6.0
    },
    {
      "text": "this in Hindi so if the main text is in",
      "start": 704.2,
      "duration": 5.199
    },
    {
      "text": "English so can you help me and if you",
      "start": 707.2,
      "duration": 4.319
    },
    {
      "text": "want to translate it in",
      "start": 709.399,
      "duration": 5.641
    },
    {
      "text": "Hindi uh so the Hindi translation",
      "start": 711.519,
      "duration": 6.88
    },
    {
      "text": "is so is associated with can that's fine",
      "start": 715.04,
      "duration": 6.64
    },
    {
      "text": "you is associated with tum but Mary is",
      "start": 718.399,
      "duration": 5.801
    },
    {
      "text": "the third word in h in the Hindi",
      "start": 721.68,
      "duration": 4.839
    },
    {
      "text": "translation right but it's actually the",
      "start": 724.2,
      "duration": 4.68
    },
    {
      "text": "fourth word in the English translation",
      "start": 726.519,
      "duration": 4.921
    },
    {
      "text": "similarly madat is the fourth word in",
      "start": 728.88,
      "duration": 5.199
    },
    {
      "text": "the Hindi translation but help is",
      "start": 731.44,
      "duration": 5.639
    },
    {
      "text": "actually the third word in the",
      "start": 734.079,
      "duration": 6.361
    },
    {
      "text": "English so the main point here is that",
      "start": 737.079,
      "duration": 5.401
    },
    {
      "text": "uh word by word translation does not",
      "start": 740.44,
      "duration": 5.28
    },
    {
      "text": "work in this case and uh that was a",
      "start": 742.48,
      "duration": 5.68
    },
    {
      "text": "major realization when people started",
      "start": 745.72,
      "duration": 5.479
    },
    {
      "text": "modeling long sequence es and this is a",
      "start": 748.16,
      "duration": 4.96
    },
    {
      "text": "general problem when you deal with",
      "start": 751.199,
      "duration": 3.921
    },
    {
      "text": "sequences you cannot just do word by",
      "start": 753.12,
      "duration": 4.48
    },
    {
      "text": "word translation you need contextual",
      "start": 755.12,
      "duration": 4.839
    },
    {
      "text": "understanding and grammar",
      "start": 757.6,
      "duration": 4.64
    },
    {
      "text": "alignment so whenever you are developing",
      "start": 759.959,
      "duration": 4.32
    },
    {
      "text": "a model let's say which translates one",
      "start": 762.24,
      "duration": 4.08
    },
    {
      "text": "sequence to another sequence or tries to",
      "start": 764.279,
      "duration": 3.92
    },
    {
      "text": "find the meaning of a sequence or makes",
      "start": 766.32,
      "duration": 4.72
    },
    {
      "text": "the next word prediction from a sequence",
      "start": 768.199,
      "duration": 4.721
    },
    {
      "text": "you need to really understand the",
      "start": 771.04,
      "duration": 4.08
    },
    {
      "text": "context you need to understand how",
      "start": 772.92,
      "duration": 4.64
    },
    {
      "text": "different words relate with each other",
      "start": 775.12,
      "duration": 4.2
    },
    {
      "text": "what's the grammar of that particular",
      "start": 777.56,
      "duration": 4.2
    },
    {
      "text": "language and only then will you be able",
      "start": 779.32,
      "duration": 5.36
    },
    {
      "text": "to uh process",
      "start": 781.76,
      "duration": 6.079
    },
    {
      "text": "sequences or only then you'll be able to",
      "start": 784.68,
      "duration": 5.48
    },
    {
      "text": "model long sequences of textual",
      "start": 787.839,
      "duration": 4.521
    },
    {
      "text": "information that's understanding number",
      "start": 790.16,
      "duration": 5.119
    },
    {
      "text": "one okay with this understanding what",
      "start": 792.36,
      "duration": 5.0
    },
    {
      "text": "people realized is that we cannot just",
      "start": 795.279,
      "duration": 4.961
    },
    {
      "text": "use a normal neural network uh because",
      "start": 797.36,
      "duration": 4.68
    },
    {
      "text": "if you have a normal neural network it",
      "start": 800.24,
      "duration": 4.44
    },
    {
      "text": "does not have memory so we are going to",
      "start": 802.04,
      "duration": 5.4
    },
    {
      "text": "use this word memory a lot just like",
      "start": 804.68,
      "duration": 4.719
    },
    {
      "text": "humans have memory we store information",
      "start": 807.44,
      "duration": 5.44
    },
    {
      "text": "about the past uh in order to do a good",
      "start": 809.399,
      "duration": 5.841
    },
    {
      "text": "job in sequence to sequence translation",
      "start": 812.88,
      "duration": 4.36
    },
    {
      "text": "the models need to have a memory the",
      "start": 815.24,
      "duration": 4.0
    },
    {
      "text": "models need to know what has come in the",
      "start": 817.24,
      "duration": 4.48
    },
    {
      "text": "past why because let's say I have a",
      "start": 819.24,
      "duration": 4.12
    },
    {
      "text": "sentence that Harry Potter went to",
      "start": 821.72,
      "duration": 3.239
    },
    {
      "text": "Station Number 9",
      "start": 823.36,
      "duration": 5.0
    },
    {
      "text": "3x4 uh he did this he did this Etc and",
      "start": 824.959,
      "duration": 6.68
    },
    {
      "text": "then when I come to a for a sentence",
      "start": 828.36,
      "duration": 6.96
    },
    {
      "text": "which is uh three to four sentences",
      "start": 831.639,
      "duration": 5.521
    },
    {
      "text": "after the first sentence which is Harry",
      "start": 835.32,
      "duration": 4.439
    },
    {
      "text": "Potter came to station 9 3x4 I should",
      "start": 837.16,
      "duration": 4.919
    },
    {
      "text": "not forget what came before because the",
      "start": 839.759,
      "duration": 4.801
    },
    {
      "text": "station number 9 3x4 is very important",
      "start": 842.079,
      "duration": 4.44
    },
    {
      "text": "for me to know even if I come at the end",
      "start": 844.56,
      "duration": 4.12
    },
    {
      "text": "of the paragraph So if I'm making some",
      "start": 846.519,
      "duration": 3.88
    },
    {
      "text": "prediction at the end of the paragraph",
      "start": 848.68,
      "duration": 4.0
    },
    {
      "text": "and the word station comes over there I",
      "start": 850.399,
      "duration": 4.68
    },
    {
      "text": "need to go back to the start I need to",
      "start": 852.68,
      "duration": 4.68
    },
    {
      "text": "have memory of what came at the start",
      "start": 855.079,
      "duration": 4.361
    },
    {
      "text": "that it was the station number 9",
      "start": 857.36,
      "duration": 5.0
    },
    {
      "text": "3x4 and this happens a lot with textual",
      "start": 859.44,
      "duration": 5.56
    },
    {
      "text": "data if you want to have meaningful",
      "start": 862.36,
      "duration": 4.719
    },
    {
      "text": "outcomes in terms of text summarization",
      "start": 865.0,
      "duration": 3.519
    },
    {
      "text": "next word prediction language",
      "start": 867.079,
      "duration": 5.241
    },
    {
      "text": "translation you definitely need to have",
      "start": 868.519,
      "duration": 5.481
    },
    {
      "text": "understanding of the meaning and for",
      "start": 872.32,
      "duration": 4.439
    },
    {
      "text": "that you need the model to retain the",
      "start": 874.0,
      "duration": 6.399
    },
    {
      "text": "memory so uh to address this issue that",
      "start": 876.759,
      "duration": 5.64
    },
    {
      "text": "word by word translation does not work",
      "start": 880.399,
      "duration": 4.921
    },
    {
      "text": "in this particular case of translation",
      "start": 882.399,
      "duration": 4.761
    },
    {
      "text": "uh people realize that a normal neural",
      "start": 885.32,
      "duration": 3.92
    },
    {
      "text": "network will not work so they augmented",
      "start": 887.16,
      "duration": 4.56
    },
    {
      "text": "a neural network with two subm modules",
      "start": 889.24,
      "duration": 5.08
    },
    {
      "text": "the first subm module is an encoder and",
      "start": 891.72,
      "duration": 5.359
    },
    {
      "text": "the second sub module is a decoder so",
      "start": 894.32,
      "duration": 5.16
    },
    {
      "text": "what the encoder does is that uh in in",
      "start": 897.079,
      "duration": 4.281
    },
    {
      "text": "the example which we saw it will receive",
      "start": 899.48,
      "duration": 3.84
    },
    {
      "text": "the German",
      "start": 901.36,
      "duration": 4.44
    },
    {
      "text": "text and it will read and process the",
      "start": 903.32,
      "duration": 4.439
    },
    {
      "text": "German text and then it will pass it to",
      "start": 905.8,
      "duration": 4.159
    },
    {
      "text": "the decoder and then the decoder will",
      "start": 907.759,
      "duration": 4.241
    },
    {
      "text": "translate the German text back into",
      "start": 909.959,
      "duration": 4.601
    },
    {
      "text": "English this is the simplest explanation",
      "start": 912.0,
      "duration": 4.839
    },
    {
      "text": "of the encoder decoder and there's a",
      "start": 914.56,
      "duration": 5.68
    },
    {
      "text": "nice animation here which actually shows",
      "start": 916.839,
      "duration": 6.0
    },
    {
      "text": "uh how the encoder decoder works so here",
      "start": 920.24,
      "duration": 4.36
    },
    {
      "text": "you can see the input sequence comes in",
      "start": 922.839,
      "duration": 3.92
    },
    {
      "text": "the German language it goes to the",
      "start": 924.6,
      "duration": 5.64
    },
    {
      "text": "encoder uh a context is generated by the",
      "start": 926.759,
      "duration": 6.2
    },
    {
      "text": "encoder it's called as a context Vector",
      "start": 930.24,
      "duration": 4.599
    },
    {
      "text": "the context Vector essentially captures",
      "start": 932.959,
      "duration": 4.721
    },
    {
      "text": "meaning so it has memory and it captures",
      "start": 934.839,
      "duration": 5.641
    },
    {
      "text": "meaning of okay instead of just word by",
      "start": 937.68,
      "duration": 5.519
    },
    {
      "text": "word translation what does this sentence",
      "start": 940.48,
      "duration": 6.56
    },
    {
      "text": "represent and uh the encoder processes",
      "start": 943.199,
      "duration": 6.241
    },
    {
      "text": "the entire input sequence and sends the",
      "start": 947.04,
      "duration": 4.239
    },
    {
      "text": "context over to the decoder so let me",
      "start": 949.44,
      "duration": 3.959
    },
    {
      "text": "play this again the input sequence comes",
      "start": 951.279,
      "duration": 4.321
    },
    {
      "text": "to the encoder it generates a context",
      "start": 953.399,
      "duration": 5.161
    },
    {
      "text": "Vector which basically encodes meaning",
      "start": 955.6,
      "duration": 4.84
    },
    {
      "text": "and then the encoder transfers the",
      "start": 958.56,
      "duration": 4.04
    },
    {
      "text": "context Vector to the decoder and the",
      "start": 960.44,
      "duration": 4.0
    },
    {
      "text": "decoder generates the output in this",
      "start": 962.6,
      "duration": 4.239
    },
    {
      "text": "case the output is the transl translated",
      "start": 964.44,
      "duration": 4.8
    },
    {
      "text": "English text okay this is how the",
      "start": 966.839,
      "duration": 6.0
    },
    {
      "text": "encoder decoder blocks work and uh the",
      "start": 969.24,
      "duration": 5.519
    },
    {
      "text": "mechanism which really employed the",
      "start": 972.839,
      "duration": 4.56
    },
    {
      "text": "encoder decoder blocks successfully is",
      "start": 974.759,
      "duration": 5.241
    },
    {
      "text": "called recurrent neural networks so",
      "start": 977.399,
      "duration": 4.56
    },
    {
      "text": "before really Transformers came into the",
      "start": 980.0,
      "duration": 4.839
    },
    {
      "text": "picture recurrent neural networks were",
      "start": 981.959,
      "duration": 5.081
    },
    {
      "text": "was that architecture which was",
      "start": 984.839,
      "duration": 4.92
    },
    {
      "text": "extremely popular for language",
      "start": 987.04,
      "duration": 5.719
    },
    {
      "text": "translation and it really uh employed",
      "start": 989.759,
      "duration": 5.76
    },
    {
      "text": "the encoder decoder",
      "start": 992.759,
      "duration": 5.08
    },
    {
      "text": "architecture uh it was implemented in",
      "start": 995.519,
      "duration": 5.081
    },
    {
      "text": "the 1980s so let's look a bit more at",
      "start": 997.839,
      "duration": 5.201
    },
    {
      "text": "how the RNN actually works because when",
      "start": 1000.6,
      "duration": 5.12
    },
    {
      "text": "we if we understand how RNN works that's",
      "start": 1003.04,
      "duration": 4.479
    },
    {
      "text": "when we'll understand the limitations of",
      "start": 1005.72,
      "duration": 3.919
    },
    {
      "text": "recurrent neural networks and that's",
      "start": 1007.519,
      "duration": 4.521
    },
    {
      "text": "when we will really appreciate why the",
      "start": 1009.639,
      "duration": 4.921
    },
    {
      "text": "attention mechanism needed to be",
      "start": 1012.04,
      "duration": 5.039
    },
    {
      "text": "discovered so here's how the encoder",
      "start": 1014.56,
      "duration": 4.8
    },
    {
      "text": "decoder in the RNN actually works",
      "start": 1017.079,
      "duration": 3.921
    },
    {
      "text": "what happens is that you first receive",
      "start": 1019.36,
      "duration": 4.8
    },
    {
      "text": "an input text okay uh and that let's say",
      "start": 1021.0,
      "duration": 6.12
    },
    {
      "text": "is the German uh German text the input",
      "start": 1024.16,
      "duration": 5.2
    },
    {
      "text": "text is passed to the decod to the",
      "start": 1027.12,
      "duration": 4.76
    },
    {
      "text": "encoder what the encoder will do is that",
      "start": 1029.36,
      "duration": 5.679
    },
    {
      "text": "at every step it will take the input and",
      "start": 1031.88,
      "duration": 4.76
    },
    {
      "text": "it will maintain something which is",
      "start": 1035.039,
      "duration": 4.0
    },
    {
      "text": "called as the hidden State this hidden",
      "start": 1036.64,
      "duration": 4.559
    },
    {
      "text": "state was the biggest innovation in the",
      "start": 1039.039,
      "duration": 4.52
    },
    {
      "text": "recurrent neural networks this hidden",
      "start": 1041.199,
      "duration": 4.48
    },
    {
      "text": "State essentially captures the",
      "start": 1043.559,
      "duration": 5.041
    },
    {
      "text": "memory so imagine the uh first input",
      "start": 1045.679,
      "duration": 6.281
    },
    {
      "text": "which is the first German word comes the",
      "start": 1048.6,
      "duration": 5.84
    },
    {
      "text": "encoder augments it or the encoder",
      "start": 1051.96,
      "duration": 5.2
    },
    {
      "text": "maintains a hidden State then you go to",
      "start": 1054.44,
      "duration": 4.72
    },
    {
      "text": "the next iteration then the second input",
      "start": 1057.16,
      "duration": 3.68
    },
    {
      "text": "world comes then the hidden State also",
      "start": 1059.16,
      "duration": 4.48
    },
    {
      "text": "gets updated so as the hidden state gets",
      "start": 1060.84,
      "duration": 5.0
    },
    {
      "text": "updated it receives more and more memory",
      "start": 1063.64,
      "duration": 3.56
    },
    {
      "text": "of what has come",
      "start": 1065.84,
      "duration": 3.88
    },
    {
      "text": "previously and the hidden state gets",
      "start": 1067.2,
      "duration": 4.479
    },
    {
      "text": "updated at each step and then there is a",
      "start": 1069.72,
      "duration": 3.199
    },
    {
      "text": "final hidden",
      "start": 1071.679,
      "duration": 3.601
    },
    {
      "text": "State the final hidden state is",
      "start": 1072.919,
      "duration": 4.681
    },
    {
      "text": "basically the encoder output what we saw",
      "start": 1075.28,
      "duration": 5.08
    },
    {
      "text": "the context Vector over here so when we",
      "start": 1077.6,
      "duration": 4.439
    },
    {
      "text": "looked at the context Vector which is",
      "start": 1080.36,
      "duration": 3.559
    },
    {
      "text": "passed from the encoder to the decoder",
      "start": 1082.039,
      "duration": 3.88
    },
    {
      "text": "let's see over here yeah so here you see",
      "start": 1083.919,
      "duration": 3.601
    },
    {
      "text": "a context Vector is passed from the",
      "start": 1085.919,
      "duration": 3.801
    },
    {
      "text": "encoder to the decoder this context",
      "start": 1087.52,
      "duration": 4.639
    },
    {
      "text": "Vector is the final hidden State this is",
      "start": 1089.72,
      "duration": 3.88
    },
    {
      "text": "basically the encoder telling the",
      "start": 1092.159,
      "duration": 3.241
    },
    {
      "text": "decoder that hey I have looked at the",
      "start": 1093.6,
      "duration": 4.8
    },
    {
      "text": "input text uh here's the meaning of this",
      "start": 1095.4,
      "duration": 5.399
    },
    {
      "text": "this text here's how I encoded it here's",
      "start": 1098.4,
      "duration": 4.279
    },
    {
      "text": "the context Vector take this final",
      "start": 1100.799,
      "duration": 4.561
    },
    {
      "text": "hidden State and try to decode it and",
      "start": 1102.679,
      "duration": 4.801
    },
    {
      "text": "then the decoder uses this final hidden",
      "start": 1105.36,
      "duration": 4.04
    },
    {
      "text": "state to generate the translated",
      "start": 1107.48,
      "duration": 4.439
    },
    {
      "text": "sentence and it generates the translated",
      "start": 1109.4,
      "duration": 4.6
    },
    {
      "text": "sentence one word at a",
      "start": 1111.919,
      "duration": 4.561
    },
    {
      "text": "time uh so here's a schematic which",
      "start": 1114.0,
      "duration": 4.72
    },
    {
      "text": "actually explains this pretty well so I",
      "start": 1116.48,
      "duration": 4.64
    },
    {
      "text": "have an input text here so this is the",
      "start": 1118.72,
      "duration": 4.24
    },
    {
      "text": "first word in German the second word in",
      "start": 1121.12,
      "duration": 4.2
    },
    {
      "text": "German the third word in German and the",
      "start": 1122.96,
      "duration": 5.04
    },
    {
      "text": "fourth word in German what the encoder",
      "start": 1125.32,
      "duration": 5.08
    },
    {
      "text": "block will do is that it will take each",
      "start": 1128.0,
      "duration": 5.039
    },
    {
      "text": "input sequentially and it will maintain",
      "start": 1130.4,
      "duration": 4.84
    },
    {
      "text": "a different hidden state so for the",
      "start": 1133.039,
      "duration": 3.721
    },
    {
      "text": "first input it has the first hidden",
      "start": 1135.24,
      "duration": 3.24
    },
    {
      "text": "State then we move to the next iteration",
      "start": 1136.76,
      "duration": 3.56
    },
    {
      "text": "then the second hidden State then the",
      "start": 1138.48,
      "duration": 4.199
    },
    {
      "text": "third hidden State and then finally when",
      "start": 1140.32,
      "duration": 3.88
    },
    {
      "text": "we have the last input we have this",
      "start": 1142.679,
      "duration": 3.681
    },
    {
      "text": "final hidden State the final hidden",
      "start": 1144.2,
      "duration": 3.56
    },
    {
      "text": "State essentially contains the",
      "start": 1146.36,
      "duration": 3.12
    },
    {
      "text": "accumulation of all previous hidden",
      "start": 1147.76,
      "duration": 4.12
    },
    {
      "text": "state so it contains or encapsulates",
      "start": 1149.48,
      "duration": 4.64
    },
    {
      "text": "memory this is how memory is",
      "start": 1151.88,
      "duration": 4.039
    },
    {
      "text": "incorporated which was missing earlier",
      "start": 1154.12,
      "duration": 4.2
    },
    {
      "text": "with just a normal neural network so",
      "start": 1155.919,
      "duration": 5.161
    },
    {
      "text": "this is the final hidden State and then",
      "start": 1158.32,
      "duration": 4.52
    },
    {
      "text": "this final hidden State essentially",
      "start": 1161.08,
      "duration": 5.68
    },
    {
      "text": "memorizes the entire input and then this",
      "start": 1162.84,
      "duration": 6.68
    },
    {
      "text": "uh hidden state is passed to to the",
      "start": 1166.76,
      "duration": 5.24
    },
    {
      "text": "decoder and then the decoder produces",
      "start": 1169.52,
      "duration": 4.36
    },
    {
      "text": "the final output which is the translated",
      "start": 1172.0,
      "duration": 4.72
    },
    {
      "text": "English sentence so I want to show you",
      "start": 1173.88,
      "duration": 4.799
    },
    {
      "text": "another animation of this so that you",
      "start": 1176.72,
      "duration": 5.199
    },
    {
      "text": "understand it much better um so here's",
      "start": 1178.679,
      "duration": 6.281
    },
    {
      "text": "how the RNN actually works right so see",
      "start": 1181.919,
      "duration": 5.841
    },
    {
      "text": "the input Vector number one is the first",
      "start": 1184.96,
      "duration": 4.719
    },
    {
      "text": "word in German which needs to be",
      "start": 1187.76,
      "duration": 4.039
    },
    {
      "text": "translated so here you will see in this",
      "start": 1189.679,
      "duration": 3.961
    },
    {
      "text": "animation how the hidden state gets",
      "start": 1191.799,
      "duration": 4.12
    },
    {
      "text": "updated so the first word of German",
      "start": 1193.64,
      "duration": 4.44
    },
    {
      "text": "comes then the RNN maintains the hidden",
      "start": 1195.919,
      "duration": 3.521
    },
    {
      "text": "state zero",
      "start": 1198.08,
      "duration": 2.64
    },
    {
      "text": "and here you",
      "start": 1199.44,
      "duration": 4.84
    },
    {
      "text": "see uh the hidden state is the hidden",
      "start": 1200.72,
      "duration": 6.24
    },
    {
      "text": "state zero and the input one is used to",
      "start": 1204.28,
      "duration": 5.08
    },
    {
      "text": "produce the output one and then we also",
      "start": 1206.96,
      "duration": 5.12
    },
    {
      "text": "have a hidden State one then as we move",
      "start": 1209.36,
      "duration": 4.4
    },
    {
      "text": "further we have the hidden state two",
      "start": 1212.08,
      "duration": 3.56
    },
    {
      "text": "hidden state three hidden State four and",
      "start": 1213.76,
      "duration": 4.12
    },
    {
      "text": "final hidden State when the last word",
      "start": 1215.64,
      "duration": 4.159
    },
    {
      "text": "needs to be",
      "start": 1217.88,
      "duration": 4.12
    },
    {
      "text": "processed uh actually I can show you",
      "start": 1219.799,
      "duration": 4.841
    },
    {
      "text": "this again here so this is from a French",
      "start": 1222.0,
      "duration": 4.679
    },
    {
      "text": "to English translation using the",
      "start": 1224.64,
      "duration": 4.039
    },
    {
      "text": "recurrent neural network look look here",
      "start": 1226.679,
      "duration": 3.721
    },
    {
      "text": "so we have French input which is coming",
      "start": 1228.679,
      "duration": 2.961
    },
    {
      "text": "here",
      "start": 1230.4,
      "duration": 4.08
    },
    {
      "text": "justu then uh yeah so the first word",
      "start": 1231.64,
      "duration": 5.08
    },
    {
      "text": "goes into the encoder see now we have",
      "start": 1234.48,
      "duration": 5.16
    },
    {
      "text": "hidden so let me expand it so and play",
      "start": 1236.72,
      "duration": 5.4
    },
    {
      "text": "from the start okay so now the first",
      "start": 1239.64,
      "duration": 4.919
    },
    {
      "text": "word of French which is J goes into the",
      "start": 1242.12,
      "duration": 4.84
    },
    {
      "text": "encoder it has went into the encoder",
      "start": 1244.559,
      "duration": 3.961
    },
    {
      "text": "right now and the first hidden state is",
      "start": 1246.96,
      "duration": 3.92
    },
    {
      "text": "generated see in the orange color great",
      "start": 1248.52,
      "duration": 5.039
    },
    {
      "text": "now this hidden state number one and the",
      "start": 1250.88,
      "duration": 5.48
    },
    {
      "text": "second input is used again look at this",
      "start": 1253.559,
      "duration": 5.24
    },
    {
      "text": "animation again the hidden state state",
      "start": 1256.36,
      "duration": 4.12
    },
    {
      "text": "number one and the second input which is",
      "start": 1258.799,
      "duration": 5.561
    },
    {
      "text": "sui sis s is used U and then we have the",
      "start": 1260.48,
      "duration": 6.079
    },
    {
      "text": "hidden state number two then the hidden",
      "start": 1264.36,
      "duration": 3.92
    },
    {
      "text": "state number two and the input three",
      "start": 1266.559,
      "duration": 3.921
    },
    {
      "text": "which is will be used to produce the",
      "start": 1268.28,
      "duration": 5.16
    },
    {
      "text": "final hidden State great this final",
      "start": 1270.48,
      "duration": 4.76
    },
    {
      "text": "hidden State hidden state number three",
      "start": 1273.44,
      "duration": 4.28
    },
    {
      "text": "essentially contains all the information",
      "start": 1275.24,
      "duration": 5.36
    },
    {
      "text": "in the given sequence plus it also",
      "start": 1277.72,
      "duration": 5.76
    },
    {
      "text": "contains some memory or some",
      "start": 1280.6,
      "duration": 6.319
    },
    {
      "text": "context regarding uh what came in the",
      "start": 1283.48,
      "duration": 6.16
    },
    {
      "text": "past and now this final hidden state is",
      "start": 1286.919,
      "duration": 4.36
    },
    {
      "text": "then passed to the decoder and the",
      "start": 1289.64,
      "duration": 3.639
    },
    {
      "text": "decoder produces the output in English",
      "start": 1291.279,
      "duration": 4.481
    },
    {
      "text": "one word at a time this is exactly how",
      "start": 1293.279,
      "duration": 5.161
    },
    {
      "text": "the recurrent neural network works okay",
      "start": 1295.76,
      "duration": 4.72
    },
    {
      "text": "now you might think that awesome right",
      "start": 1298.44,
      "duration": 3.68
    },
    {
      "text": "this is already doing sequence to",
      "start": 1300.48,
      "duration": 3.079
    },
    {
      "text": "sequence translations and we are",
      "start": 1302.12,
      "duration": 3.28
    },
    {
      "text": "translating from one language to another",
      "start": 1303.559,
      "duration": 4.081
    },
    {
      "text": "language so why do we need attention",
      "start": 1305.4,
      "duration": 5.24
    },
    {
      "text": "mechanism memory is being encoded here",
      "start": 1307.64,
      "duration": 5.6
    },
    {
      "text": "and we are passing in the context which",
      "start": 1310.64,
      "duration": 4.8
    },
    {
      "text": "means that we will be able to identify",
      "start": 1313.24,
      "duration": 3.84
    },
    {
      "text": "how different words of the sequence are",
      "start": 1315.44,
      "duration": 4.52
    },
    {
      "text": "related to each other so why do we need",
      "start": 1317.08,
      "duration": 5.36
    },
    {
      "text": "attention well there is a big problem",
      "start": 1319.96,
      "duration": 5.16
    },
    {
      "text": "with the recurrent neural network and",
      "start": 1322.44,
      "duration": 4.88
    },
    {
      "text": "that problem happens",
      "start": 1325.12,
      "duration": 6.24
    },
    {
      "text": "because um the model the decoder has",
      "start": 1327.32,
      "duration": 6.44
    },
    {
      "text": "essentially no access to the previous",
      "start": 1331.36,
      "duration": 4.76
    },
    {
      "text": "hidden States so if you look at this",
      "start": 1333.76,
      "duration": 5.84
    },
    {
      "text": "video you'll see that uh the decoder has",
      "start": 1336.12,
      "duration": 6.0
    },
    {
      "text": "access to only the final hidden state so",
      "start": 1339.6,
      "duration": 5.079
    },
    {
      "text": "hidden State one hidden State 2 hidden",
      "start": 1342.12,
      "duration": 4.439
    },
    {
      "text": "state three and then hidden state three",
      "start": 1344.679,
      "duration": 3.921
    },
    {
      "text": "is passed to the decoder C the the",
      "start": 1346.559,
      "duration": 4.0
    },
    {
      "text": "decoder has no access to the previous",
      "start": 1348.6,
      "duration": 4.28
    },
    {
      "text": "hidden",
      "start": 1350.559,
      "duration": 2.321
    },
    {
      "text": "States now why is this a big problem um",
      "start": 1353.52,
      "duration": 6.44
    },
    {
      "text": "the reason it's a big problem is",
      "start": 1357.559,
      "duration": 6.041
    },
    {
      "text": "because when we have to process long",
      "start": 1359.96,
      "duration": 6.56
    },
    {
      "text": "sequences if the decoder just relies on",
      "start": 1363.6,
      "duration": 5.6
    },
    {
      "text": "one final hidden State that's a lot of",
      "start": 1366.52,
      "duration": 5.44
    },
    {
      "text": "pressure on the decoder to essentially",
      "start": 1369.2,
      "duration": 5.04
    },
    {
      "text": "that one final hidden State needs to",
      "start": 1371.96,
      "duration": 4.88
    },
    {
      "text": "have the entire information and for long",
      "start": 1374.24,
      "duration": 4.96
    },
    {
      "text": "sequences it usually",
      "start": 1376.84,
      "duration": 4.76
    },
    {
      "text": "fails because it's very hard for one",
      "start": 1379.2,
      "duration": 4.359
    },
    {
      "text": "final hidden state to have the entire",
      "start": 1381.6,
      "duration": 5.48
    },
    {
      "text": "information let me explain this bit more",
      "start": 1383.559,
      "duration": 5.281
    },
    {
      "text": "so as we saw the",
      "start": 1387.08,
      "duration": 4.52
    },
    {
      "text": "encoder let me change my color here I",
      "start": 1388.84,
      "duration": 5.92
    },
    {
      "text": "think let me change it to",
      "start": 1391.6,
      "duration": 3.16
    },
    {
      "text": "Green yeah so as we saw the encoder",
      "start": 1395.44,
      "duration": 6.32
    },
    {
      "text": "processes the entire input text the",
      "start": 1398.44,
      "duration": 5.56
    },
    {
      "text": "encoder processes the entire input text",
      "start": 1401.76,
      "duration": 4.399
    },
    {
      "text": "into one final hidden state which is the",
      "start": 1404.0,
      "duration": 5.32
    },
    {
      "text": "memory cell and then decoder takes this",
      "start": 1406.159,
      "duration": 5.52
    },
    {
      "text": "this hidden State decoder takes this",
      "start": 1409.32,
      "duration": 4.2
    },
    {
      "text": "hidden state to essentially produce an",
      "start": 1411.679,
      "duration": 3.0
    },
    {
      "text": "output",
      "start": 1413.52,
      "duration": 3.8
    },
    {
      "text": "great now here's the biggest issue with",
      "start": 1414.679,
      "duration": 5.081
    },
    {
      "text": "RNN and please play pay very close",
      "start": 1417.32,
      "duration": 4.92
    },
    {
      "text": "attention to this point because if you",
      "start": 1419.76,
      "duration": 3.96
    },
    {
      "text": "understand this you will understand why",
      "start": 1422.24,
      "duration": 3.88
    },
    {
      "text": "attention mechanisms were needed the",
      "start": 1423.72,
      "duration": 4.24
    },
    {
      "text": "biggest issue with the RNN is that a",
      "start": 1426.12,
      "duration": 3.919
    },
    {
      "text": "recurrent neural network cannot directly",
      "start": 1427.96,
      "duration": 4.64
    },
    {
      "text": "access earlier hidden States as we saw",
      "start": 1430.039,
      "duration": 4.561
    },
    {
      "text": "in the video it only accesses the final",
      "start": 1432.6,
      "duration": 5.6
    },
    {
      "text": "hidden state so the RNN can't directly",
      "start": 1434.6,
      "duration": 5.28
    },
    {
      "text": "access earlier hidden States from the",
      "start": 1438.2,
      "duration": 4.04
    },
    {
      "text": "encoder during the decoding",
      "start": 1439.88,
      "duration": 4.96
    },
    {
      "text": "phase it relies only on the current",
      "start": 1442.24,
      "duration": 4.28
    },
    {
      "text": "hidden state which is the final hidden",
      "start": 1444.84,
      "duration": 4.92
    },
    {
      "text": "State and this leads to a loss or this",
      "start": 1446.52,
      "duration": 5.88
    },
    {
      "text": "leads to a loss of context especially in",
      "start": 1449.76,
      "duration": 4.799
    },
    {
      "text": "complex sentences where dependencies",
      "start": 1452.4,
      "duration": 4.759
    },
    {
      "text": "might span long",
      "start": 1454.559,
      "duration": 5.321
    },
    {
      "text": "distances um okay so let me actually",
      "start": 1457.159,
      "duration": 4.921
    },
    {
      "text": "explain this further what does it mean",
      "start": 1459.88,
      "duration": 5.64
    },
    {
      "text": "loss of context right uh so as we saw",
      "start": 1462.08,
      "duration": 5.839
    },
    {
      "text": "the encoder compresses the entire input",
      "start": 1465.52,
      "duration": 3.36
    },
    {
      "text": "sequence",
      "start": 1467.919,
      "duration": 3.841
    },
    {
      "text": "into a single hidden State",
      "start": 1468.88,
      "duration": 5.0
    },
    {
      "text": "Vector I hope you have understood up",
      "start": 1471.76,
      "duration": 4.279
    },
    {
      "text": "till this point now the problem happens",
      "start": 1473.88,
      "duration": 3.88
    },
    {
      "text": "let's say if the sentence if the input",
      "start": 1476.039,
      "duration": 4.201
    },
    {
      "text": "sentence is very long if the input",
      "start": 1477.76,
      "duration": 4.48
    },
    {
      "text": "sentence is very long it really becomes",
      "start": 1480.24,
      "duration": 3.72
    },
    {
      "text": "very difficult for the recurrent neural",
      "start": 1482.24,
      "duration": 3.4
    },
    {
      "text": "network to capture all of that",
      "start": 1483.96,
      "duration": 3.959
    },
    {
      "text": "information in one single final hidden",
      "start": 1485.64,
      "duration": 4.919
    },
    {
      "text": "state that becomes very difficult and",
      "start": 1487.919,
      "duration": 4.961
    },
    {
      "text": "this is the main drawback of the RNN so",
      "start": 1490.559,
      "duration": 5.441
    },
    {
      "text": "for example let's take a practical case",
      "start": 1492.88,
      "duration": 5.519
    },
    {
      "text": "so let's say uh we take the example",
      "start": 1496.0,
      "duration": 3.76
    },
    {
      "text": "which we looked at at the start of the",
      "start": 1498.399,
      "duration": 3.201
    },
    {
      "text": "lecture which is the cat that was",
      "start": 1499.76,
      "duration": 4.36
    },
    {
      "text": "sitting on the mat which was next to the",
      "start": 1501.6,
      "duration": 5.0
    },
    {
      "text": "dog jumped and let's say we want to",
      "start": 1504.12,
      "duration": 4.159
    },
    {
      "text": "convert this English into a French",
      "start": 1506.6,
      "duration": 3.679
    },
    {
      "text": "translation okay so the French",
      "start": 1508.279,
      "duration": 4.64
    },
    {
      "text": "translation will be lat whatever I",
      "start": 1510.279,
      "duration": 4.561
    },
    {
      "text": "cannot spell this out fully but this",
      "start": 1512.919,
      "duration": 3.601
    },
    {
      "text": "will be the French translation for this",
      "start": 1514.84,
      "duration": 3.839
    },
    {
      "text": "English sequence now as I mentioned to",
      "start": 1516.52,
      "duration": 3.72
    },
    {
      "text": "you before this English sequence is",
      "start": 1518.679,
      "duration": 6.921
    },
    {
      "text": "pretty long uh the RNN or the encoder",
      "start": 1520.24,
      "duration": 7.679
    },
    {
      "text": "really needs to capture the dependencies",
      "start": 1525.6,
      "duration": 4.559
    },
    {
      "text": "very well so the final hidden State",
      "start": 1527.919,
      "duration": 4.521
    },
    {
      "text": "needs to capture that the cat is the",
      "start": 1530.159,
      "duration": 4.4
    },
    {
      "text": "subject here and the cat is the one who",
      "start": 1532.44,
      "duration": 5.479
    },
    {
      "text": "has jumped and this information this",
      "start": 1534.559,
      "duration": 5.281
    },
    {
      "text": "context needs to be captured by the",
      "start": 1537.919,
      "duration": 4.681
    },
    {
      "text": "final hidden State and that is very hard",
      "start": 1539.84,
      "duration": 4.36
    },
    {
      "text": "if you are putting all the pressure on",
      "start": 1542.6,
      "duration": 3.439
    },
    {
      "text": "one final hidden state to capture all",
      "start": 1544.2,
      "duration": 4.599
    },
    {
      "text": "this context especially in Long",
      "start": 1546.039,
      "duration": 6.0
    },
    {
      "text": "sequences so uh the key action which is",
      "start": 1548.799,
      "duration": 5.841
    },
    {
      "text": "jumped depends on the subject which is",
      "start": 1552.039,
      "duration": 5.921
    },
    {
      "text": "cat but also an understanding longer",
      "start": 1554.64,
      "duration": 5.919
    },
    {
      "text": "depend dependencies so jumped depends on",
      "start": 1557.96,
      "duration": 4.48
    },
    {
      "text": "cat but we also need to understand that",
      "start": 1560.559,
      "duration": 4.041
    },
    {
      "text": "the cat was sitting on the mat and the",
      "start": 1562.44,
      "duration": 4.28
    },
    {
      "text": "cat was also sitting next to the dog",
      "start": 1564.6,
      "duration": 3.799
    },
    {
      "text": "because the dog also might be referred",
      "start": 1566.72,
      "duration": 4.079
    },
    {
      "text": "somewhere else in the big text so we",
      "start": 1568.399,
      "duration": 4.201
    },
    {
      "text": "need to understand many things from this",
      "start": 1570.799,
      "duration": 3.801
    },
    {
      "text": "sentence and these are also called as",
      "start": 1572.6,
      "duration": 4.079
    },
    {
      "text": "longer",
      "start": 1574.6,
      "duration": 3.92
    },
    {
      "text": "dependencies so",
      "start": 1576.679,
      "duration": 4.441
    },
    {
      "text": "jumped the action jumped of course",
      "start": 1578.52,
      "duration": 5.84
    },
    {
      "text": "depends on the subject cat but we Al to",
      "start": 1581.12,
      "duration": 4.6
    },
    {
      "text": "understand this we also need to",
      "start": 1584.36,
      "duration": 2.919
    },
    {
      "text": "understand longer dependencies that the",
      "start": 1585.72,
      "duration": 3.8
    },
    {
      "text": "cat was sitting next to the dog and the",
      "start": 1587.279,
      "duration": 4.0
    },
    {
      "text": "cat was also sitting on the",
      "start": 1589.52,
      "duration": 4.2
    },
    {
      "text": "mat so to capture these longer",
      "start": 1591.279,
      "duration": 4.76
    },
    {
      "text": "dependencies or to capture",
      "start": 1593.72,
      "duration": 5.24
    },
    {
      "text": "this uh longer context or difficult",
      "start": 1596.039,
      "duration": 5.76
    },
    {
      "text": "context the RNN decoder struggles with",
      "start": 1598.96,
      "duration": 5.319
    },
    {
      "text": "this because it just has one final",
      "start": 1601.799,
      "duration": 5.12
    },
    {
      "text": "hidden State uh to get all the",
      "start": 1604.279,
      "duration": 4.0
    },
    {
      "text": "information",
      "start": 1606.919,
      "duration": 4.64
    },
    {
      "text": "from this is called loss of context and",
      "start": 1608.279,
      "duration": 5.321
    },
    {
      "text": "loss of context was one of the biggest",
      "start": 1611.559,
      "duration": 5.321
    },
    {
      "text": "issues because of which RNN was not as",
      "start": 1613.6,
      "duration": 5.6
    },
    {
      "text": "good as the GPT which exist right now",
      "start": 1616.88,
      "duration": 4.24
    },
    {
      "text": "which is based on the attention",
      "start": 1619.2,
      "duration": 4.16
    },
    {
      "text": "mechanism okay so these are the issues",
      "start": 1621.12,
      "duration": 5.279
    },
    {
      "text": "with RNN uh the decoder cannot access",
      "start": 1623.36,
      "duration": 4.76
    },
    {
      "text": "the hidden states of the input which",
      "start": 1626.399,
      "duration": 3.961
    },
    {
      "text": "came in earlier so we cannot capture",
      "start": 1628.12,
      "duration": 4.32
    },
    {
      "text": "long range",
      "start": 1630.36,
      "duration": 4.039
    },
    {
      "text": "dependencies this is where attention",
      "start": 1632.44,
      "duration": 3.479
    },
    {
      "text": "mechanism actually comes into the",
      "start": 1634.399,
      "duration": 4.561
    },
    {
      "text": "picture okay we will capture long range",
      "start": 1635.919,
      "duration": 4.921
    },
    {
      "text": "dependencies with attention mechanisms",
      "start": 1638.96,
      "duration": 5.56
    },
    {
      "text": "and let's see how so RNN work fine for",
      "start": 1640.84,
      "duration": 6.079
    },
    {
      "text": "translating short sentences and they did",
      "start": 1644.52,
      "duration": 4.56
    },
    {
      "text": "work amazingly actually for quite a",
      "start": 1646.919,
      "duration": 4.561
    },
    {
      "text": "while for short sentences but",
      "start": 1649.08,
      "duration": 4.199
    },
    {
      "text": "researchers soon discovered that they",
      "start": 1651.48,
      "duration": 4.16
    },
    {
      "text": "don't work for long text because they",
      "start": 1653.279,
      "duration": 4.481
    },
    {
      "text": "don't have direct access to previous",
      "start": 1655.64,
      "duration": 3.279
    },
    {
      "text": "words in the",
      "start": 1657.76,
      "duration": 4.56
    },
    {
      "text": "input so when an RNN decoder only",
      "start": 1658.919,
      "duration": 6.24
    },
    {
      "text": "receives the final hidden state right",
      "start": 1662.32,
      "duration": 4.4
    },
    {
      "text": "they don't even have access to the the",
      "start": 1665.159,
      "duration": 3.481
    },
    {
      "text": "decoder does not have access to all the",
      "start": 1666.72,
      "duration": 4.319
    },
    {
      "text": "prior Words which came in the input so",
      "start": 1668.64,
      "duration": 4.639
    },
    {
      "text": "let's say I'm decoding",
      "start": 1671.039,
      "duration": 6.161
    },
    {
      "text": "uh uh let's say I'm looking at this word",
      "start": 1673.279,
      "duration": 5.76
    },
    {
      "text": "um jumped right let's say I'm looking at",
      "start": 1677.2,
      "duration": 4.52
    },
    {
      "text": "the word jumped cat is a word which has",
      "start": 1679.039,
      "duration": 5.52
    },
    {
      "text": "come way prior in the sequence so when I",
      "start": 1681.72,
      "duration": 4.679
    },
    {
      "text": "am looking at the word jumped I need to",
      "start": 1684.559,
      "duration": 4.24
    },
    {
      "text": "give a lot of attention to the word cat",
      "start": 1686.399,
      "duration": 4.64
    },
    {
      "text": "but an RNN gets the entire encoded",
      "start": 1688.799,
      "duration": 4.081
    },
    {
      "text": "version of this sentence so how would",
      "start": 1691.039,
      "duration": 4.201
    },
    {
      "text": "the RNN know that jump actually if",
      "start": 1692.88,
      "duration": 3.84
    },
    {
      "text": "you're looking at jumped you should pay",
      "start": 1695.24,
      "duration": 3.72
    },
    {
      "text": "a lot of attention to the word cat it",
      "start": 1696.72,
      "duration": 4.12
    },
    {
      "text": "does not even have access to this input",
      "start": 1698.96,
      "duration": 4.599
    },
    {
      "text": "Vector for cat this is where attention",
      "start": 1700.84,
      "duration": 5.92
    },
    {
      "text": "mechanism actually comes into the",
      "start": 1703.559,
      "duration": 7.401
    },
    {
      "text": "picture uh okay so as I said one of the",
      "start": 1706.76,
      "duration": 6.32
    },
    {
      "text": "major shortcoming in the RNN is that the",
      "start": 1710.96,
      "duration": 4.52
    },
    {
      "text": "RNN must remember the entire encoded",
      "start": 1713.08,
      "duration": 4.599
    },
    {
      "text": "input in a single hidden",
      "start": 1715.48,
      "duration": 5.52
    },
    {
      "text": "State before it passes the encoded input",
      "start": 1717.679,
      "duration": 6.88
    },
    {
      "text": "to the decoder the RNN has to remember",
      "start": 1721.0,
      "duration": 6.159
    },
    {
      "text": "the entire encoded input in a single",
      "start": 1724.559,
      "duration": 5.081
    },
    {
      "text": "hidden state I'm repeating this again",
      "start": 1727.159,
      "duration": 4.201
    },
    {
      "text": "because unless you understand this you",
      "start": 1729.64,
      "duration": 3.72
    },
    {
      "text": "won't understand why we are learning",
      "start": 1731.36,
      "duration": 4.76
    },
    {
      "text": "about attention and it's very hard for",
      "start": 1733.36,
      "duration": 5.6
    },
    {
      "text": "the RNN to encode the entire or to",
      "start": 1736.12,
      "duration": 4.76
    },
    {
      "text": "remember the entire encoded input in a",
      "start": 1738.96,
      "duration": 3.199
    },
    {
      "text": "single hidden",
      "start": 1740.88,
      "duration": 3.399
    },
    {
      "text": "State this is when the researchers",
      "start": 1742.159,
      "duration": 4.961
    },
    {
      "text": "started looking at other mechanisms and",
      "start": 1744.279,
      "duration": 5.321
    },
    {
      "text": "that's when in 2014 researchers",
      "start": 1747.12,
      "duration": 4.88
    },
    {
      "text": "developed the so-called bhano attention",
      "start": 1749.6,
      "duration": 3.64
    },
    {
      "text": "mechanism for",
      "start": 1752.0,
      "duration": 3.76
    },
    {
      "text": "RNN so when people think of attention",
      "start": 1753.24,
      "duration": 5.52
    },
    {
      "text": "mechanism they always think of the 2017",
      "start": 1755.76,
      "duration": 5.2
    },
    {
      "text": "paper right attention is all you need",
      "start": 1758.76,
      "duration": 4.2
    },
    {
      "text": "but actually attention was introduced in",
      "start": 1760.96,
      "duration": 4.64
    },
    {
      "text": "a paper in 2014 which was called neural",
      "start": 1762.96,
      "duration": 5.16
    },
    {
      "text": "machine translation by jointly learning",
      "start": 1765.6,
      "duration": 3.799
    },
    {
      "text": "to align and",
      "start": 1768.12,
      "duration": 4.6
    },
    {
      "text": "translate um sadly many people don't",
      "start": 1769.399,
      "duration": 5.481
    },
    {
      "text": "remember this now everyone just",
      "start": 1772.72,
      "duration": 4.4
    },
    {
      "text": "remembers attention is all you need but",
      "start": 1774.88,
      "duration": 5.799
    },
    {
      "text": "remember that this these authors badano",
      "start": 1777.12,
      "duration": 6.64
    },
    {
      "text": "Benjo and Kung yuno they were the ones",
      "start": 1780.679,
      "duration": 5.961
    },
    {
      "text": "who worked on the first proposition of",
      "start": 1783.76,
      "duration": 4.48
    },
    {
      "text": "the attention",
      "start": 1786.64,
      "duration": 4.36
    },
    {
      "text": "mechanism so I just want to give uh",
      "start": 1788.24,
      "duration": 5.559
    },
    {
      "text": "credit here and this attention mechanism",
      "start": 1791.0,
      "duration": 4.88
    },
    {
      "text": "was called badan attention mechanism",
      "start": 1793.799,
      "duration": 4.281
    },
    {
      "text": "because the author of this paper was was",
      "start": 1795.88,
      "duration": 3.399
    },
    {
      "text": "last name was",
      "start": 1798.08,
      "duration": 4.04
    },
    {
      "text": "Bano so what was the main idea behind",
      "start": 1799.279,
      "duration": 5.201
    },
    {
      "text": "this what the attention mechanism",
      "start": 1802.12,
      "duration": 3.88
    },
    {
      "text": "basically",
      "start": 1804.48,
      "duration": 5.319
    },
    {
      "text": "uh prescribed is that okay let's take",
      "start": 1806.0,
      "duration": 7.159
    },
    {
      "text": "the encoder decoder RNN and let's modify",
      "start": 1809.799,
      "duration": 6.281
    },
    {
      "text": "the encoder decoder RNN so that the",
      "start": 1813.159,
      "duration": 6.721
    },
    {
      "text": "decoder can selectively access different",
      "start": 1816.08,
      "duration": 5.719
    },
    {
      "text": "parts of the input sequence at each",
      "start": 1819.88,
      "duration": 4.08
    },
    {
      "text": "decoding",
      "start": 1821.799,
      "duration": 5.401
    },
    {
      "text": "step uh let me repeat that remember in",
      "start": 1823.96,
      "duration": 5.079
    },
    {
      "text": "the original and and the decoder only",
      "start": 1827.2,
      "duration": 4.52
    },
    {
      "text": "had access to the final hidden state but",
      "start": 1829.039,
      "duration": 5.841
    },
    {
      "text": "the bhano attention mechanism says that",
      "start": 1831.72,
      "duration": 6.679
    },
    {
      "text": "what if the decoder now can selectively",
      "start": 1834.88,
      "duration": 5.96
    },
    {
      "text": "access different parts of the input",
      "start": 1838.399,
      "duration": 5.241
    },
    {
      "text": "sequence at each decoding step and let",
      "start": 1840.84,
      "duration": 4.559
    },
    {
      "text": "me explain to you what this means by",
      "start": 1843.64,
      "duration": 3.879
    },
    {
      "text": "simple figure so let's say we are at",
      "start": 1845.399,
      "duration": 5.76
    },
    {
      "text": "this decoding step where uh we want to",
      "start": 1847.519,
      "duration": 5.441
    },
    {
      "text": "so let's say the word is Mir which is",
      "start": 1851.159,
      "duration": 5.561
    },
    {
      "text": "the uh German word and we want to decode",
      "start": 1852.96,
      "duration": 6.4
    },
    {
      "text": "this so we are at this hidden state",
      "start": 1856.72,
      "duration": 4.439
    },
    {
      "text": "right now and we want to decode this",
      "start": 1859.36,
      "duration": 4.76
    },
    {
      "text": "okay if we use the original RNN we just",
      "start": 1861.159,
      "duration": 5.281
    },
    {
      "text": "had access to this final hidden state",
      "start": 1864.12,
      "duration": 5.08
    },
    {
      "text": "but now what we say is that when you are",
      "start": 1866.44,
      "duration": 7.199
    },
    {
      "text": "uh decoding this uh hidden State what if",
      "start": 1869.2,
      "duration": 6.199
    },
    {
      "text": "you have access to all of the input",
      "start": 1873.639,
      "duration": 4.721
    },
    {
      "text": "tokens so follow this orange curve which",
      "start": 1875.399,
      "duration": 4.88
    },
    {
      "text": "I'm drawing here so what if you have",
      "start": 1878.36,
      "duration": 3.96
    },
    {
      "text": "access to this token what if you have",
      "start": 1880.279,
      "duration": 4.52
    },
    {
      "text": "access to this token what if you have",
      "start": 1882.32,
      "duration": 5.239
    },
    {
      "text": "access to this token and what if you",
      "start": 1884.799,
      "duration": 4.961
    },
    {
      "text": "have access access to this token so",
      "start": 1887.559,
      "duration": 4.36
    },
    {
      "text": "let's say when you're decoding you have",
      "start": 1889.76,
      "duration": 3.96
    },
    {
      "text": "access to all the tokens and you can",
      "start": 1891.919,
      "duration": 3.88
    },
    {
      "text": "decide how much attention to pay to each",
      "start": 1893.72,
      "duration": 5.199
    },
    {
      "text": "token so for example I think the German",
      "start": 1895.799,
      "duration": 6.081
    },
    {
      "text": "uh translation for you is do which is do",
      "start": 1898.919,
      "duration": 5.0
    },
    {
      "text": "so I know that the maximum attention",
      "start": 1901.88,
      "duration": 4.56
    },
    {
      "text": "needs to be paid to this token so that's",
      "start": 1903.919,
      "duration": 4.401
    },
    {
      "text": "why you I have marked this with a thick",
      "start": 1906.44,
      "duration": 4.719
    },
    {
      "text": "line over here for all the other tokens",
      "start": 1908.32,
      "duration": 5.0
    },
    {
      "text": "well we pay less amount of",
      "start": 1911.159,
      "duration": 4.48
    },
    {
      "text": "attention what this does is that it",
      "start": 1913.32,
      "duration": 4.599
    },
    {
      "text": "allows the decoder to access all the to",
      "start": 1915.639,
      "duration": 4.16
    },
    {
      "text": "tokens so if we are dealing with long",
      "start": 1917.919,
      "duration": 4.401
    },
    {
      "text": "sentences we can access all the tokens",
      "start": 1919.799,
      "duration": 4.681
    },
    {
      "text": "even in long sentences and decide which",
      "start": 1922.32,
      "duration": 4.359
    },
    {
      "text": "token we want to pay more attention to",
      "start": 1924.48,
      "duration": 4.36
    },
    {
      "text": "so for example let's take this longer",
      "start": 1926.679,
      "duration": 4.441
    },
    {
      "text": "sentence which we have looked at a lot",
      "start": 1928.84,
      "duration": 5.679
    },
    {
      "text": "in today's lecture yeah so let's",
      "start": 1931.12,
      "duration": 7.24
    },
    {
      "text": "say and we'll look at it once more again",
      "start": 1934.519,
      "duration": 5.601
    },
    {
      "text": "uh yeah so let's say we have look we are",
      "start": 1938.36,
      "duration": 3.88
    },
    {
      "text": "looking at this sentence",
      "start": 1940.12,
      "duration": 5.679
    },
    {
      "text": "the the cat that was sitting on the mat",
      "start": 1942.24,
      "duration": 5.279
    },
    {
      "text": "which was next to the dog jumped let's",
      "start": 1945.799,
      "duration": 4.6
    },
    {
      "text": "say we looking at this sentence now in",
      "start": 1947.519,
      "duration": 4.52
    },
    {
      "text": "the decoder so let's say you are on the",
      "start": 1950.399,
      "duration": 3.481
    },
    {
      "text": "decoder part and you are decoding for",
      "start": 1952.039,
      "duration": 3.841
    },
    {
      "text": "jumped and you translating it into",
      "start": 1953.88,
      "duration": 4.799
    },
    {
      "text": "French what we will do is that instead",
      "start": 1955.88,
      "duration": 4.36
    },
    {
      "text": "of just looking at the final hidden",
      "start": 1958.679,
      "duration": 4.201
    },
    {
      "text": "State we will have access to all of the",
      "start": 1960.24,
      "duration": 4.36
    },
    {
      "text": "words let's say we have access to all of",
      "start": 1962.88,
      "duration": 4.039
    },
    {
      "text": "the input words and not just that we",
      "start": 1964.6,
      "duration": 4.4
    },
    {
      "text": "have access to all of the input words",
      "start": 1966.919,
      "duration": 3.6
    },
    {
      "text": "and we can also decide how much",
      "start": 1969.0,
      "duration": 3.32
    },
    {
      "text": "attention to pay to each of the input",
      "start": 1970.519,
      "duration": 4.721
    },
    {
      "text": "word so now I will say that okay I want",
      "start": 1972.32,
      "duration": 3.88
    },
    {
      "text": "to",
      "start": 1975.24,
      "duration": 3.52
    },
    {
      "text": "translate jump right so of course a lot",
      "start": 1976.2,
      "duration": 4.64
    },
    {
      "text": "of attention should be paid to jumped",
      "start": 1978.76,
      "duration": 3.919
    },
    {
      "text": "but I will also pay a lot of attention",
      "start": 1980.84,
      "duration": 5.52
    },
    {
      "text": "to cat because the cat is the uh one who",
      "start": 1982.679,
      "duration": 6.36
    },
    {
      "text": "has really jumped so I can decide which",
      "start": 1986.36,
      "duration": 4.799
    },
    {
      "text": "tokens to pay the maximum attention to",
      "start": 1989.039,
      "duration": 4.321
    },
    {
      "text": "so when I'm translating jumped I will",
      "start": 1991.159,
      "duration": 4.041
    },
    {
      "text": "pay attention to jump and I will pay",
      "start": 1993.36,
      "duration": 4.36
    },
    {
      "text": "attention to cat also because now I can",
      "start": 1995.2,
      "duration": 4.959
    },
    {
      "text": "access the token for cat this access",
      "start": 1997.72,
      "duration": 4.4
    },
    {
      "text": "itself was not possible in RNN because",
      "start": 2000.159,
      "duration": 3.681
    },
    {
      "text": "we could not access the previous input",
      "start": 2002.12,
      "duration": 4.519
    },
    {
      "text": "tokens in an RNN and this was the main",
      "start": 2003.84,
      "duration": 4.079
    },
    {
      "text": "problem which was solved by the",
      "start": 2006.639,
      "duration": 3.441
    },
    {
      "text": "introduction of the attention",
      "start": 2007.919,
      "duration": 4.281
    },
    {
      "text": "mechanism so this figure actually",
      "start": 2010.08,
      "duration": 3.92
    },
    {
      "text": "explains the general idea behind the",
      "start": 2012.2,
      "duration": 3.68
    },
    {
      "text": "bhan attention",
      "start": 2014.0,
      "duration": 4.88
    },
    {
      "text": "mechanism um let me Zoom onto this",
      "start": 2015.88,
      "duration": 5.36
    },
    {
      "text": "figure once more yeah so if you look at",
      "start": 2018.88,
      "duration": 5.0
    },
    {
      "text": "this figure here uh when you are",
      "start": 2021.24,
      "duration": 5.12
    },
    {
      "text": "translating from do to English you pay",
      "start": 2023.88,
      "duration": 4.44
    },
    {
      "text": "attention to all the input tokens and",
      "start": 2026.36,
      "duration": 3.36
    },
    {
      "text": "then you can have these attention",
      "start": 2028.32,
      "duration": 3.52
    },
    {
      "text": "weights which basically describe how",
      "start": 2029.72,
      "duration": 3.64
    },
    {
      "text": "much you want to pay attention to each",
      "start": 2031.84,
      "duration": 4.199
    },
    {
      "text": "input token and that solves the problem",
      "start": 2033.36,
      "duration": 5.12
    },
    {
      "text": "of the loss of context which was present",
      "start": 2036.039,
      "duration": 3.52
    },
    {
      "text": "in the",
      "start": 2038.48,
      "duration": 4.4
    },
    {
      "text": "RNN so essentially using an attention",
      "start": 2039.559,
      "duration": 6.401
    },
    {
      "text": "mechanism using an attention mechanism",
      "start": 2042.88,
      "duration": 5.199
    },
    {
      "text": "the text generating decoder part of the",
      "start": 2045.96,
      "duration": 6.28
    },
    {
      "text": "network can access all the input tokens",
      "start": 2048.079,
      "duration": 6.681
    },
    {
      "text": "selectively uh and as I mentioned we can",
      "start": 2052.24,
      "duration": 4.52
    },
    {
      "text": "decide how much weight to give to each",
      "start": 2054.76,
      "duration": 4.359
    },
    {
      "text": "input token so this means that some",
      "start": 2056.76,
      "duration": 4.52
    },
    {
      "text": "input tokens are more important than",
      "start": 2059.119,
      "duration": 4.881
    },
    {
      "text": "others for generating a given output",
      "start": 2061.28,
      "duration": 5.319
    },
    {
      "text": "token this importance is determined by",
      "start": 2064.0,
      "duration": 4.639
    },
    {
      "text": "the attention weights so this is where",
      "start": 2066.599,
      "duration": 3.56
    },
    {
      "text": "the attention weights and the attention",
      "start": 2068.639,
      "duration": 3.2
    },
    {
      "text": "score comes into the picture which we",
      "start": 2070.159,
      "duration": 3.92
    },
    {
      "text": "will learn about",
      "start": 2071.839,
      "duration": 4.881
    },
    {
      "text": "later now uh so this paper was",
      "start": 2074.079,
      "duration": 5.161
    },
    {
      "text": "introduced in 2014 right this paper",
      "start": 2076.72,
      "duration": 4.48
    },
    {
      "text": "which is which introduced the bhan",
      "start": 2079.24,
      "duration": 4.159
    },
    {
      "text": "attention mechanism only three years",
      "start": 2081.2,
      "duration": 4.24
    },
    {
      "text": "later the main Transformers paper came",
      "start": 2083.399,
      "duration": 3.841
    },
    {
      "text": "out and that came out in",
      "start": 2085.44,
      "duration": 5.199
    },
    {
      "text": "2017 so only 3 years later researchers",
      "start": 2087.24,
      "duration": 5.76
    },
    {
      "text": "found that RNN architectures are not",
      "start": 2090.639,
      "duration": 4.641
    },
    {
      "text": "required for building deep neural",
      "start": 2093.0,
      "duration": 5.28
    },
    {
      "text": "networks for natural language processing",
      "start": 2095.28,
      "duration": 4.76
    },
    {
      "text": "and uh this is when the researchers",
      "start": 2098.28,
      "duration": 4.079
    },
    {
      "text": "proposed the Transformer",
      "start": 2100.04,
      "duration": 4.68
    },
    {
      "text": "architecture and at the main core of the",
      "start": 2102.359,
      "duration": 4.601
    },
    {
      "text": "Transformer architecture was the badano",
      "start": 2104.72,
      "duration": 3.52
    },
    {
      "text": "attention",
      "start": 2106.96,
      "duration": 3.32
    },
    {
      "text": "mechanism so it was called self",
      "start": 2108.24,
      "duration": 4.32
    },
    {
      "text": "attention mechanism but it was really",
      "start": 2110.28,
      "duration": 4.76
    },
    {
      "text": "inspired a lot from the bhano attention",
      "start": 2112.56,
      "duration": 4.559
    },
    {
      "text": "mechanism we'll come to what self",
      "start": 2115.04,
      "duration": 4.84
    },
    {
      "text": "attention means bhano attention did not",
      "start": 2117.119,
      "duration": 4.801
    },
    {
      "text": "introduce the term self attention but",
      "start": 2119.88,
      "duration": 3.64
    },
    {
      "text": "the Transformers paper which really",
      "start": 2121.92,
      "duration": 3.679
    },
    {
      "text": "changed everything for llms that paper",
      "start": 2123.52,
      "duration": 3.0
    },
    {
      "text": "came in",
      "start": 2125.599,
      "duration": 3.441
    },
    {
      "text": "2017 uh it introduced the Transformer",
      "start": 2126.52,
      "duration": 4.48
    },
    {
      "text": "architecture with a self attention",
      "start": 2129.04,
      "duration": 3.96
    },
    {
      "text": "mechanism that was completely based on",
      "start": 2131.0,
      "duration": 4.079
    },
    {
      "text": "the badano attention mechanism this is",
      "start": 2133.0,
      "duration": 4.88
    },
    {
      "text": "that paper which came out in",
      "start": 2135.079,
      "duration": 5.921
    },
    {
      "text": "2017 and here you'll see uh attention",
      "start": 2137.88,
      "duration": 5.0
    },
    {
      "text": "attention basically the paper itself is",
      "start": 2141.0,
      "duration": 5.92
    },
    {
      "text": "titled attention is all you need um so",
      "start": 2142.88,
      "duration": 6.32
    },
    {
      "text": "this is how attention really came to be",
      "start": 2146.92,
      "duration": 3.679
    },
    {
      "text": "the core building block of large",
      "start": 2149.2,
      "duration": 3.96
    },
    {
      "text": "language models I just want to explain",
      "start": 2150.599,
      "duration": 5.401
    },
    {
      "text": "to you this once more so that I drive",
      "start": 2153.16,
      "duration": 4.679
    },
    {
      "text": "this concept home about what why",
      "start": 2156.0,
      "duration": 4.48
    },
    {
      "text": "attention mechanism helps so let's say",
      "start": 2157.839,
      "duration": 4.201
    },
    {
      "text": "again the same sentence this is the",
      "start": 2160.48,
      "duration": 3.24
    },
    {
      "text": "sentence the cat that was sitting on the",
      "start": 2162.04,
      "duration": 4.24
    },
    {
      "text": "mat which was next to the dog jumped and",
      "start": 2163.72,
      "duration": 5.8
    },
    {
      "text": "this is the French translation uh for",
      "start": 2166.28,
      "duration": 6.039
    },
    {
      "text": "this particular sequence so what the",
      "start": 2169.52,
      "duration": 4.88
    },
    {
      "text": "attention mechanism does is that at each",
      "start": 2172.319,
      "duration": 6.241
    },
    {
      "text": "decoding step so at each decoding step",
      "start": 2174.4,
      "duration": 6.8
    },
    {
      "text": "the model can look back at the entire",
      "start": 2178.56,
      "duration": 3.6
    },
    {
      "text": "input",
      "start": 2181.2,
      "duration": 4.08
    },
    {
      "text": "sequence and decide which parts are most",
      "start": 2182.16,
      "duration": 6.72
    },
    {
      "text": "relevant to generate the current word",
      "start": 2185.28,
      "duration": 8.0
    },
    {
      "text": "so for example uh let's say we are",
      "start": 2188.88,
      "duration": 6.88
    },
    {
      "text": "predicting this word sa which is the",
      "start": 2193.28,
      "duration": 4.799
    },
    {
      "text": "French translation for jumped so when",
      "start": 2195.76,
      "duration": 4.48
    },
    {
      "text": "the decoder is predicting this French",
      "start": 2198.079,
      "duration": 5.04
    },
    {
      "text": "translation s the attention mechanism",
      "start": 2200.24,
      "duration": 5.28
    },
    {
      "text": "allows the decoder to focus on the part",
      "start": 2203.119,
      "duration": 5.48
    },
    {
      "text": "of the input that corresponds to",
      "start": 2205.52,
      "duration": 6.36
    },
    {
      "text": "jump so we can selectively look at which",
      "start": 2208.599,
      "duration": 4.921
    },
    {
      "text": "part of the input to give maximum",
      "start": 2211.88,
      "duration": 2.6
    },
    {
      "text": "attention",
      "start": 2213.52,
      "duration": 3.76
    },
    {
      "text": "to uh and this Dynamic focus on",
      "start": 2214.48,
      "duration": 5.04
    },
    {
      "text": "different parts of the input sequence is",
      "start": 2217.28,
      "duration": 4.039
    },
    {
      "text": "what helps the attention mechanism to",
      "start": 2219.52,
      "duration": 4.04
    },
    {
      "text": "learn long range dependencies more",
      "start": 2221.319,
      "duration": 4.52
    },
    {
      "text": "effectively so remember we looked at how",
      "start": 2223.56,
      "duration": 5.12
    },
    {
      "text": "RNN fail to understand or even learn",
      "start": 2225.839,
      "duration": 4.401
    },
    {
      "text": "longrange",
      "start": 2228.68,
      "duration": 4.399
    },
    {
      "text": "dependencies uh one main or key thing of",
      "start": 2230.24,
      "duration": 4.839
    },
    {
      "text": "the attention mechanism is this word",
      "start": 2233.079,
      "duration": 3.801
    },
    {
      "text": "which is called as Dynamic",
      "start": 2235.079,
      "duration": 5.081
    },
    {
      "text": "Focus so Dynamic Focus which means that",
      "start": 2236.88,
      "duration": 6.04
    },
    {
      "text": "for every decoder for every decoding",
      "start": 2240.16,
      "duration": 4.6
    },
    {
      "text": "step we can selectively choose which",
      "start": 2242.92,
      "duration": 3.439
    },
    {
      "text": "inputs to focus on and how much",
      "start": 2244.76,
      "duration": 3.88
    },
    {
      "text": "attention to give to it each input",
      "start": 2246.359,
      "duration": 4.48
    },
    {
      "text": "that's why it's called Dynamic Focus so",
      "start": 2248.64,
      "duration": 4.16
    },
    {
      "text": "this Dynamic focus on different parts of",
      "start": 2250.839,
      "duration": 4.681
    },
    {
      "text": "the input sequence helps us to learn",
      "start": 2252.8,
      "duration": 4.96
    },
    {
      "text": "long range dependencies more",
      "start": 2255.52,
      "duration": 4.64
    },
    {
      "text": "effectively and that's why the attention",
      "start": 2257.76,
      "duration": 5.44
    },
    {
      "text": "mechanism uh actually works so well",
      "start": 2260.16,
      "duration": 4.679
    },
    {
      "text": "there is another animation which I want",
      "start": 2263.2,
      "duration": 6.159
    },
    {
      "text": "to show you uh so here we saw the RNN",
      "start": 2264.839,
      "duration": 5.841
    },
    {
      "text": "right let me show you again so this is",
      "start": 2269.359,
      "duration": 3.321
    },
    {
      "text": "the recurrent neural network here you",
      "start": 2270.68,
      "duration": 4.439
    },
    {
      "text": "will see that just the final hidden",
      "start": 2272.68,
      "duration": 4.159
    },
    {
      "text": "state which is the hidden state number",
      "start": 2275.119,
      "duration": 3.921
    },
    {
      "text": "three is actually passed to the decoding",
      "start": 2276.839,
      "duration": 5.201
    },
    {
      "text": "stage nothing else is passed but now let",
      "start": 2279.04,
      "duration": 5.039
    },
    {
      "text": "me show you the modification of this",
      "start": 2282.04,
      "duration": 5.039
    },
    {
      "text": "with with the attention mechanism so now",
      "start": 2284.079,
      "duration": 5.401
    },
    {
      "text": "uh if we do this with attention added",
      "start": 2287.079,
      "duration": 4.321
    },
    {
      "text": "what happens is that you'll see that",
      "start": 2289.48,
      "duration": 4.0
    },
    {
      "text": "hidden state number one is generated",
      "start": 2291.4,
      "duration": 3.959
    },
    {
      "text": "hidden state number two is generated",
      "start": 2293.48,
      "duration": 3.839
    },
    {
      "text": "hidden state number three is generated",
      "start": 2295.359,
      "duration": 3.561
    },
    {
      "text": "but all of these hidden states are",
      "start": 2297.319,
      "duration": 3.321
    },
    {
      "text": "passed to the decoder which means that",
      "start": 2298.92,
      "duration": 4.08
    },
    {
      "text": "the decoder at every step has access to",
      "start": 2300.64,
      "duration": 4.4
    },
    {
      "text": "all of the Hidden States not just the",
      "start": 2303.0,
      "duration": 3.8
    },
    {
      "text": "final hidden state number three but even",
      "start": 2305.04,
      "duration": 3.36
    },
    {
      "text": "hidden States number one and hidden",
      "start": 2306.8,
      "duration": 4.039
    },
    {
      "text": "States number two this is an extremely",
      "start": 2308.4,
      "duration": 4.52
    },
    {
      "text": "important point to",
      "start": 2310.839,
      "duration": 4.401
    },
    {
      "text": "remember uh one more thing which I want",
      "start": 2312.92,
      "duration": 5.76
    },
    {
      "text": "to show you here is that uh yeah this",
      "start": 2315.24,
      "duration": 6.72
    },
    {
      "text": "part so let's say",
      "start": 2318.68,
      "duration": 5.399
    },
    {
      "text": "uh uh yeah so let's say we are",
      "start": 2321.96,
      "duration": 4.52
    },
    {
      "text": "translating from French to English over",
      "start": 2324.079,
      "duration": 5.961
    },
    {
      "text": "here I think let me play this",
      "start": 2326.48,
      "duration": 3.56
    },
    {
      "text": "again yeah yeah so let's say we are",
      "start": 2336.28,
      "duration": 3.88
    },
    {
      "text": "translating from French to English here",
      "start": 2338.319,
      "duration": 4.441
    },
    {
      "text": "again let's see what is happening here",
      "start": 2340.16,
      "duration": 4.6
    },
    {
      "text": "so if we are at the decoder stage we",
      "start": 2342.76,
      "duration": 4.12
    },
    {
      "text": "want to generate this word I right for",
      "start": 2344.76,
      "duration": 4.92
    },
    {
      "text": "Jo so this when we are at the decoder",
      "start": 2346.88,
      "duration": 4.8
    },
    {
      "text": "State the decoder has access to all of",
      "start": 2349.68,
      "duration": 4.04
    },
    {
      "text": "these hidden States 1 2 and three when",
      "start": 2351.68,
      "duration": 4.159
    },
    {
      "text": "it's generating the English translation",
      "start": 2353.72,
      "duration": 5.16
    },
    {
      "text": "for J but what it does is that it gives",
      "start": 2355.839,
      "duration": 4.641
    },
    {
      "text": "more preference to the hidden state",
      "start": 2358.88,
      "duration": 3.32
    },
    {
      "text": "number one because it realize that that",
      "start": 2360.48,
      "duration": 3.76
    },
    {
      "text": "Jo is more important for translation to",
      "start": 2362.2,
      "duration": 4.76
    },
    {
      "text": "I similarly when we want to translate",
      "start": 2364.24,
      "duration": 6.32
    },
    {
      "text": "sui s u i s it actually translates to M",
      "start": 2366.96,
      "duration": 5.359
    },
    {
      "text": "so when the decoder is translating this",
      "start": 2370.56,
      "duration": 3.799
    },
    {
      "text": "to English it has access to Hidden State",
      "start": 2372.319,
      "duration": 4.161
    },
    {
      "text": "1 2 and three but it learns that the",
      "start": 2374.359,
      "duration": 3.96
    },
    {
      "text": "hidden state two is most important for",
      "start": 2376.48,
      "duration": 2.839
    },
    {
      "text": "this",
      "start": 2378.319,
      "duration": 2.881
    },
    {
      "text": "translation similarly when it's",
      "start": 2379.319,
      "duration": 4.161
    },
    {
      "text": "translating Aon with the attention",
      "start": 2381.2,
      "duration": 4.28
    },
    {
      "text": "mechanism it has translate it has access",
      "start": 2383.48,
      "duration": 3.839
    },
    {
      "text": "to one two and three hidden state but it",
      "start": 2385.48,
      "duration": 3.16
    },
    {
      "text": "learns that the hidden state number",
      "start": 2387.319,
      "duration": 4.361
    },
    {
      "text": "three is the most important uh Etc so",
      "start": 2388.64,
      "duration": 5.24
    },
    {
      "text": "this is how it actually works so at",
      "start": 2391.68,
      "duration": 4.159
    },
    {
      "text": "every decoding step we have access to",
      "start": 2393.88,
      "duration": 4.239
    },
    {
      "text": "all the different uh hidden",
      "start": 2395.839,
      "duration": 4.921
    },
    {
      "text": "States and one more key thing to mention",
      "start": 2398.119,
      "duration": 4.48
    },
    {
      "text": "here is that the model is isn't just",
      "start": 2400.76,
      "duration": 3.88
    },
    {
      "text": "mindlessly aligning the first word at",
      "start": 2402.599,
      "duration": 3.841
    },
    {
      "text": "the output with the first word at the",
      "start": 2404.64,
      "duration": 4.16
    },
    {
      "text": "input it actually learns from the",
      "start": 2406.44,
      "duration": 4.639
    },
    {
      "text": "training phase how to align the words in",
      "start": 2408.8,
      "duration": 4.559
    },
    {
      "text": "the language pair so remember at the",
      "start": 2411.079,
      "duration": 4.841
    },
    {
      "text": "start we saw that you cannot blindly do",
      "start": 2413.359,
      "duration": 4.72
    },
    {
      "text": "word to word translation that's not",
      "start": 2415.92,
      "duration": 4.199
    },
    {
      "text": "what's happening here in the training",
      "start": 2418.079,
      "duration": 3.76
    },
    {
      "text": "phase the attention mechanism actually",
      "start": 2420.119,
      "duration": 3.921
    },
    {
      "text": "learns which word to align with which",
      "start": 2421.839,
      "duration": 3.881
    },
    {
      "text": "word so which word in the output should",
      "start": 2424.04,
      "duration": 4.6
    },
    {
      "text": "be aligned with which word in the input",
      "start": 2425.72,
      "duration": 5.28
    },
    {
      "text": "so this this graph is actually presented",
      "start": 2428.64,
      "duration": 4.84
    },
    {
      "text": "in the 2014 paper and I think this graph",
      "start": 2431.0,
      "duration": 4.56
    },
    {
      "text": "is very interesting so you can also take",
      "start": 2433.48,
      "duration": 4.639
    },
    {
      "text": "a look at this uh this is figure number",
      "start": 2435.56,
      "duration": 4.44
    },
    {
      "text": "three in the original the badana",
      "start": 2438.119,
      "duration": 4.121
    },
    {
      "text": "attention mechanism paper so here you",
      "start": 2440.0,
      "duration": 5.079
    },
    {
      "text": "can see that let's see European economic",
      "start": 2442.24,
      "duration": 4.599
    },
    {
      "text": "area so let's say we want to translate",
      "start": 2445.079,
      "duration": 4.641
    },
    {
      "text": "European economic area to French it",
      "start": 2446.839,
      "duration": 6.601
    },
    {
      "text": "actually translates to uh European",
      "start": 2449.72,
      "duration": 5.96
    },
    {
      "text": "economic zone so here it says that in",
      "start": 2453.44,
      "duration": 3.919
    },
    {
      "text": "French the order of this word is",
      "start": 2455.68,
      "duration": 4.72
    },
    {
      "text": "reversed as compared to English so let's",
      "start": 2457.359,
      "duration": 4.841
    },
    {
      "text": "see how the model actually learns so",
      "start": 2460.4,
      "duration": 4.04
    },
    {
      "text": "European so it learns this European",
      "start": 2462.2,
      "duration": 4.84
    },
    {
      "text": "happens over here then economic is here",
      "start": 2464.44,
      "duration": 6.2
    },
    {
      "text": "and area is here so basically the point",
      "start": 2467.04,
      "duration": 6.36
    },
    {
      "text": "is that uh when the attention mechanism",
      "start": 2470.64,
      "duration": 5.64
    },
    {
      "text": "learns that European maps to this",
      "start": 2473.4,
      "duration": 5.919
    },
    {
      "text": "European economic maps to economic and",
      "start": 2476.28,
      "duration": 5.64
    },
    {
      "text": "area maps to Zone it does it's it's not",
      "start": 2479.319,
      "duration": 4.28
    },
    {
      "text": "just blindly looking at the order",
      "start": 2481.92,
      "duration": 3.8
    },
    {
      "text": "because if you see the order European",
      "start": 2483.599,
      "duration": 6.561
    },
    {
      "text": "comes at 1 2 3 4 fth but European which",
      "start": 2485.72,
      "duration": 6.68
    },
    {
      "text": "is the French translation comes at 1 2 3",
      "start": 2490.16,
      "duration": 5.4
    },
    {
      "text": "4 5 6 7th so order is different it's not",
      "start": 2492.4,
      "duration": 6.8
    },
    {
      "text": "blindly copying the order or the number",
      "start": 2495.56,
      "duration": 5.16
    },
    {
      "text": "uh at which the word occurs it's",
      "start": 2499.2,
      "duration": 4.2
    },
    {
      "text": "actually learning from the training uh",
      "start": 2500.72,
      "duration": 4.44
    },
    {
      "text": "it's learning the dependencies between",
      "start": 2503.4,
      "duration": 4.24
    },
    {
      "text": "different words and that's how it does",
      "start": 2505.16,
      "duration": 4.439
    },
    {
      "text": "does the",
      "start": 2507.64,
      "duration": 4.28
    },
    {
      "text": "translation um I'll be sharing the link",
      "start": 2509.599,
      "duration": 4.601
    },
    {
      "text": "to this paper in the information section",
      "start": 2511.92,
      "duration": 5.32
    },
    {
      "text": "the 2014 badano attention paper and I'll",
      "start": 2514.2,
      "duration": 5.04
    },
    {
      "text": "also be sharing the link to the 2017",
      "start": 2517.24,
      "duration": 4.359
    },
    {
      "text": "attention is all you need paper before",
      "start": 2519.24,
      "duration": 4.359
    },
    {
      "text": "we move to self attention I actually",
      "start": 2521.599,
      "duration": 4.161
    },
    {
      "text": "want to take a bit of time to tell you",
      "start": 2523.599,
      "duration": 5.52
    },
    {
      "text": "about the history of the attention and",
      "start": 2525.76,
      "duration": 5.76
    },
    {
      "text": "Transformers so that you have a timeline",
      "start": 2529.119,
      "duration": 3.24
    },
    {
      "text": "in",
      "start": 2531.52,
      "duration": 5.24
    },
    {
      "text": "mind so uh it all started in 1980 1980s",
      "start": 2532.359,
      "duration": 6.121
    },
    {
      "text": "is when recurrent neural networks came",
      "start": 2536.76,
      "duration": 4.4
    },
    {
      "text": "into the picture and recurrent neural",
      "start": 2538.48,
      "duration": 4.52
    },
    {
      "text": "networks introduced this hidden layer",
      "start": 2541.16,
      "duration": 3.52
    },
    {
      "text": "the hidden state that was the key",
      "start": 2543.0,
      "duration": 4.72
    },
    {
      "text": "Innovation here in 1997 came long",
      "start": 2544.68,
      "duration": 5.48
    },
    {
      "text": "short-term memory networks and they were",
      "start": 2547.72,
      "duration": 5.0
    },
    {
      "text": "an advancements compared to lnn uh",
      "start": 2550.16,
      "duration": 4.8
    },
    {
      "text": "compared to RNN so recurrent neural",
      "start": 2552.72,
      "duration": 3.68
    },
    {
      "text": "networks had this problem which is",
      "start": 2554.96,
      "duration": 3.48
    },
    {
      "text": "called Vanishing gradients so when you",
      "start": 2556.4,
      "duration": 4.04
    },
    {
      "text": "stack multiple feedback loops together",
      "start": 2558.44,
      "duration": 3.56
    },
    {
      "text": "it leads to the vanishing gradient",
      "start": 2560.44,
      "duration": 3.919
    },
    {
      "text": "problem long short-term memory solved",
      "start": 2562.0,
      "duration": 6.319
    },
    {
      "text": "this so we had a longterm memory route",
      "start": 2564.359,
      "duration": 5.801
    },
    {
      "text": "and a shortterm memory route and that's",
      "start": 2568.319,
      "duration": 4.721
    },
    {
      "text": "how the lstm actually operates but still",
      "start": 2570.16,
      "duration": 4.56
    },
    {
      "text": "both of these had problems with respect",
      "start": 2573.04,
      "duration": 3.68
    },
    {
      "text": "to longer context which was solved by",
      "start": 2574.72,
      "duration": 5.16
    },
    {
      "text": "the B Z Now attention mechanism in 2014",
      "start": 2576.72,
      "duration": 5.8
    },
    {
      "text": "where the decoder can can have access to",
      "start": 2579.88,
      "duration": 4.92
    },
    {
      "text": "each of the input State when it's",
      "start": 2582.52,
      "duration": 4.12
    },
    {
      "text": "performing the decoding operation and it",
      "start": 2584.8,
      "duration": 4.84
    },
    {
      "text": "can selectively decide which input to",
      "start": 2586.64,
      "duration": 5.959
    },
    {
      "text": "give more attention to then came the",
      "start": 2589.64,
      "duration": 6.04
    },
    {
      "text": "Transformers architecture in 2017 and",
      "start": 2592.599,
      "duration": 5.321
    },
    {
      "text": "the Transformers architecture really",
      "start": 2595.68,
      "duration": 5.32
    },
    {
      "text": "used the attention mechanism which was",
      "start": 2597.92,
      "duration": 6.679
    },
    {
      "text": "proposed in 2014 in this paper badana",
      "start": 2601.0,
      "duration": 5.52
    },
    {
      "text": "attention mechanism so this is the",
      "start": 2604.599,
      "duration": 3.48
    },
    {
      "text": "history we we are living in the age",
      "start": 2606.52,
      "duration": 3.079
    },
    {
      "text": "right now where people only know about",
      "start": 2608.079,
      "duration": 3.441
    },
    {
      "text": "attention and Transformers right but",
      "start": 2609.599,
      "duration": 4.081
    },
    {
      "text": "work has been going on in this area for",
      "start": 2611.52,
      "duration": 6.52
    },
    {
      "text": "the past 43 to 45 years or even more uh",
      "start": 2613.68,
      "duration": 8.2
    },
    {
      "text": "so please keep this particular uh figure",
      "start": 2618.04,
      "duration": 7.079
    },
    {
      "text": "in mind or historical mind map so that",
      "start": 2621.88,
      "duration": 6.08
    },
    {
      "text": "you are aware of how lucky we are to be",
      "start": 2625.119,
      "duration": 4.841
    },
    {
      "text": "living in these times where all of the",
      "start": 2627.96,
      "duration": 3.72
    },
    {
      "text": "research work has been accumulated for",
      "start": 2629.96,
      "duration": 3.84
    },
    {
      "text": "50 years and now we are reaping the",
      "start": 2631.68,
      "duration": 3.639
    },
    {
      "text": "benefits of these scientific",
      "start": 2633.8,
      "duration": 3.759
    },
    {
      "text": "advancements these advancements have not",
      "start": 2635.319,
      "duration": 3.881
    },
    {
      "text": "just happened in a period of one year or",
      "start": 2637.559,
      "duration": 3.8
    },
    {
      "text": "two years or 3 years they have taken",
      "start": 2639.2,
      "duration": 4.159
    },
    {
      "text": "time they researchers have worked hard",
      "start": 2641.359,
      "duration": 4.76
    },
    {
      "text": "over the past half a century to get us",
      "start": 2643.359,
      "duration": 4.561
    },
    {
      "text": "to this",
      "start": 2646.119,
      "duration": 4.841
    },
    {
      "text": "statee okay so I hope until now you have",
      "start": 2647.92,
      "duration": 5.36
    },
    {
      "text": "all understood a flavor and an intuition",
      "start": 2650.96,
      "duration": 5.2
    },
    {
      "text": "behind the attention mechanism itself um",
      "start": 2653.28,
      "duration": 4.839
    },
    {
      "text": "what the Transformers paper did is that",
      "start": 2656.16,
      "duration": 3.84
    },
    {
      "text": "it introduced another terminology which",
      "start": 2658.119,
      "duration": 4.801
    },
    {
      "text": "is actually called as self attention so",
      "start": 2660.0,
      "duration": 5.119
    },
    {
      "text": "self attention is a bit different so",
      "start": 2662.92,
      "duration": 5.28
    },
    {
      "text": "self attention um is basically a",
      "start": 2665.119,
      "duration": 5.72
    },
    {
      "text": "mechanism that allows each position of",
      "start": 2668.2,
      "duration": 5.08
    },
    {
      "text": "the input sequence to attend to all",
      "start": 2670.839,
      "duration": 5.321
    },
    {
      "text": "positions in the same",
      "start": 2673.28,
      "duration": 5.039
    },
    {
      "text": "sequence so we are not looking at",
      "start": 2676.16,
      "duration": 3.88
    },
    {
      "text": "different sequences now when we looked",
      "start": 2678.319,
      "duration": 3.881
    },
    {
      "text": "at language translation we looked at",
      "start": 2680.04,
      "duration": 4.279
    },
    {
      "text": "converting from English to French or",
      "start": 2682.2,
      "duration": 3.6
    },
    {
      "text": "German to English right so we are",
      "start": 2684.319,
      "duration": 2.881
    },
    {
      "text": "looking at one sequence and we are",
      "start": 2685.8,
      "duration": 3.4
    },
    {
      "text": "looking at another sequence self",
      "start": 2687.2,
      "duration": 4.08
    },
    {
      "text": "attention is basically different now",
      "start": 2689.2,
      "duration": 4.399
    },
    {
      "text": "instead of giving attention to another",
      "start": 2691.28,
      "duration": 4.4
    },
    {
      "text": "sequence all the attention is directed",
      "start": 2693.599,
      "duration": 4.76
    },
    {
      "text": "inwards so we we are just looking within",
      "start": 2695.68,
      "duration": 4.48
    },
    {
      "text": "a particular sequence so we are looking",
      "start": 2698.359,
      "duration": 3.72
    },
    {
      "text": "at different tokens within a sequence",
      "start": 2700.16,
      "duration": 4.08
    },
    {
      "text": "and see how these tokens are related to",
      "start": 2702.079,
      "duration": 5.081
    },
    {
      "text": "each other that's called as self",
      "start": 2704.24,
      "duration": 5.56
    },
    {
      "text": "attention so self attention is a key",
      "start": 2707.16,
      "duration": 5.24
    },
    {
      "text": "component of contemporary uh large",
      "start": 2709.8,
      "duration": 5.4
    },
    {
      "text": "language models so large language models",
      "start": 2712.4,
      "duration": 4.88
    },
    {
      "text": "remember are predicting the next word in",
      "start": 2715.2,
      "duration": 4.119
    },
    {
      "text": "a given sentence right they can of",
      "start": 2717.28,
      "duration": 4.0
    },
    {
      "text": "course do language translation tasks as",
      "start": 2719.319,
      "duration": 3.8
    },
    {
      "text": "well but they were predominantly trained",
      "start": 2721.28,
      "duration": 5.0
    },
    {
      "text": "for predicting the next World and uh",
      "start": 2723.119,
      "duration": 4.96
    },
    {
      "text": "they are able to do translation tasks",
      "start": 2726.28,
      "duration": 3.24
    },
    {
      "text": "which is also called as emergent",
      "start": 2728.079,
      "duration": 3.321
    },
    {
      "text": "Behavior but they were trained to",
      "start": 2729.52,
      "duration": 5.0
    },
    {
      "text": "predict the next word and that's why",
      "start": 2731.4,
      "duration": 4.919
    },
    {
      "text": "they have to so let's say when you want",
      "start": 2734.52,
      "duration": 3.88
    },
    {
      "text": "to predict the next word in a sentence",
      "start": 2736.319,
      "duration": 4.681
    },
    {
      "text": "you need to know that how different",
      "start": 2738.4,
      "duration": 4.439
    },
    {
      "text": "words are related to each other so let's",
      "start": 2741.0,
      "duration": 4.68
    },
    {
      "text": "say the cat jumped on a wall next to the",
      "start": 2742.839,
      "duration": 5.28
    },
    {
      "text": "dog the dog also jumped something like",
      "start": 2745.68,
      "duration": 4.32
    },
    {
      "text": "that you need to know when you look at",
      "start": 2748.119,
      "duration": 3.72
    },
    {
      "text": "cat which are the other words in this",
      "start": 2750.0,
      "duration": 3.559
    },
    {
      "text": "sentence which you should pay the most",
      "start": 2751.839,
      "duration": 4.201
    },
    {
      "text": "attention to for dog which are the words",
      "start": 2753.559,
      "duration": 4.721
    },
    {
      "text": "you should pay the most attention to So",
      "start": 2756.04,
      "duration": 4.16
    },
    {
      "text": "within a sentence itself you are",
      "start": 2758.28,
      "duration": 4.12
    },
    {
      "text": "deciding how different words of the",
      "start": 2760.2,
      "duration": 4.2
    },
    {
      "text": "sentence are related to each other and",
      "start": 2762.4,
      "duration": 3.76
    },
    {
      "text": "which word you should pay more attention",
      "start": 2764.4,
      "duration": 4.159
    },
    {
      "text": "to when looking at a particular word",
      "start": 2766.16,
      "duration": 3.919
    },
    {
      "text": "that's called self",
      "start": 2768.559,
      "duration": 4.481
    },
    {
      "text": "attention and self attention module is a",
      "start": 2770.079,
      "duration": 4.921
    },
    {
      "text": "key component of contemporary large",
      "start": 2773.04,
      "duration": 5.4
    },
    {
      "text": "language models so if you look at uh how",
      "start": 2775.0,
      "duration": 5.319
    },
    {
      "text": "this Series has progressed up till now",
      "start": 2778.44,
      "duration": 4.2
    },
    {
      "text": "we have looked at pre-processing steps",
      "start": 2780.319,
      "duration": 4.681
    },
    {
      "text": "like uh tokenization word embedding",
      "start": 2782.64,
      "duration": 4.52
    },
    {
      "text": "positional embedding but now we have",
      "start": 2785.0,
      "duration": 4.64
    },
    {
      "text": "started looking at the llm architecture",
      "start": 2787.16,
      "duration": 5.439
    },
    {
      "text": "itself right and one of the key points",
      "start": 2789.64,
      "duration": 4.959
    },
    {
      "text": "here is the self attention module",
      "start": 2792.599,
      "duration": 3.921
    },
    {
      "text": "without really understanding attention",
      "start": 2794.599,
      "duration": 3.881
    },
    {
      "text": "and self attention we cannot proceed to",
      "start": 2796.52,
      "duration": 4.28
    },
    {
      "text": "the llm training which comes in uh phase",
      "start": 2798.48,
      "duration": 6.32
    },
    {
      "text": "number two of building a large language",
      "start": 2800.8,
      "duration": 7.24
    },
    {
      "text": "model okay so one last thing to mention",
      "start": 2804.8,
      "duration": 6.84
    },
    {
      "text": "is that uh in self attention the self",
      "start": 2808.04,
      "duration": 5.279
    },
    {
      "text": "really refers to the attention",
      "start": 2811.64,
      "duration": 4.64
    },
    {
      "text": "mechanism's ability to compute attention",
      "start": 2813.319,
      "duration": 4.601
    },
    {
      "text": "weights by by relating different",
      "start": 2816.28,
      "duration": 4.24
    },
    {
      "text": "positions in a single input sequence so",
      "start": 2817.92,
      "duration": 5.199
    },
    {
      "text": "as I mentioned we are looking at uh just",
      "start": 2820.52,
      "duration": 5.2
    },
    {
      "text": "one input sequence and we are we are",
      "start": 2823.119,
      "duration": 4.0
    },
    {
      "text": "looking at the attention between",
      "start": 2825.72,
      "duration": 4.2
    },
    {
      "text": "different tokens of that",
      "start": 2827.119,
      "duration": 5.521
    },
    {
      "text": "sequence so it learns the relationship",
      "start": 2829.92,
      "duration": 5.04
    },
    {
      "text": "between various parts of the input",
      "start": 2832.64,
      "duration": 4.52
    },
    {
      "text": "itself so this is different than",
      "start": 2834.96,
      "duration": 4.28
    },
    {
      "text": "traditional attention mechanisms like",
      "start": 2837.16,
      "duration": 4.399
    },
    {
      "text": "the translation task which we saw where",
      "start": 2839.24,
      "duration": 4.359
    },
    {
      "text": "the focus is on relationships between",
      "start": 2841.559,
      "duration": 4.52
    },
    {
      "text": "elements of two different sequences so",
      "start": 2843.599,
      "duration": 4.281
    },
    {
      "text": "for example if we want to translate from",
      "start": 2846.079,
      "duration": 4.841
    },
    {
      "text": "English to German German to French uh",
      "start": 2847.88,
      "duration": 4.679
    },
    {
      "text": "English to Hindi Etc we have two",
      "start": 2850.92,
      "duration": 4.0
    },
    {
      "text": "sequences right so traditional attention",
      "start": 2852.559,
      "duration": 4.0
    },
    {
      "text": "mechanisms look at one part of the",
      "start": 2854.92,
      "duration": 3.32
    },
    {
      "text": "sequence and another part of the",
      "start": 2856.559,
      "duration": 3.081
    },
    {
      "text": "sequence and how they are related to",
      "start": 2858.24,
      "duration": 3.72
    },
    {
      "text": "each other in self attention we",
      "start": 2859.64,
      "duration": 5.04
    },
    {
      "text": "basically learn the relationship between",
      "start": 2861.96,
      "duration": 5.2
    },
    {
      "text": "various parts of the input",
      "start": 2864.68,
      "duration": 5.399
    },
    {
      "text": "itself so here we learn the relationship",
      "start": 2867.16,
      "duration": 4.88
    },
    {
      "text": "between various parts of the input",
      "start": 2870.079,
      "duration": 4.321
    },
    {
      "text": "itself uh and that's the difference",
      "start": 2872.04,
      "duration": 4.16
    },
    {
      "text": "between traditional attention mechanisms",
      "start": 2874.4,
      "duration": 4.48
    },
    {
      "text": "and self attention ention",
      "start": 2876.2,
      "duration": 5.8
    },
    {
      "text": "mechanisms okay so this is the end of",
      "start": 2878.88,
      "duration": 4.959
    },
    {
      "text": "this particular lecture where we have",
      "start": 2882.0,
      "duration": 3.88
    },
    {
      "text": "covered so many things we covered the",
      "start": 2883.839,
      "duration": 4.801
    },
    {
      "text": "basic intuition of attention but before",
      "start": 2885.88,
      "duration": 5.36
    },
    {
      "text": "that I explain to you in a lot of detail",
      "start": 2888.64,
      "duration": 4.24
    },
    {
      "text": "about the shortcomings in recurrent",
      "start": 2891.24,
      "duration": 5.079
    },
    {
      "text": "neural networks so just one thing if you",
      "start": 2892.88,
      "duration": 5.479
    },
    {
      "text": "take away from this lecture is that",
      "start": 2896.319,
      "duration": 4.321
    },
    {
      "text": "recurrent neural networks have a major",
      "start": 2898.359,
      "duration": 4.801
    },
    {
      "text": "shortcoming and that shortcoming is that",
      "start": 2900.64,
      "duration": 4.76
    },
    {
      "text": "they have to remember the entire encoded",
      "start": 2903.16,
      "duration": 4.159
    },
    {
      "text": "input in a single hidden state",
      "start": 2905.4,
      "duration": 4.04
    },
    {
      "text": "before passing it to the decoder and",
      "start": 2907.319,
      "duration": 4.121
    },
    {
      "text": "that's a problem when dealing with long",
      "start": 2909.44,
      "duration": 4.399
    },
    {
      "text": "sentences it leads to context",
      "start": 2911.44,
      "duration": 4.8
    },
    {
      "text": "loss because the decoder does not have",
      "start": 2913.839,
      "duration": 4.681
    },
    {
      "text": "access to the previous inputs this is",
      "start": 2916.24,
      "duration": 4.079
    },
    {
      "text": "exactly the problem which is solved by",
      "start": 2918.52,
      "duration": 3.92
    },
    {
      "text": "attention mechanisms in attention",
      "start": 2920.319,
      "duration": 3.721
    },
    {
      "text": "mechanisms when you are decoding a",
      "start": 2922.44,
      "duration": 4.399
    },
    {
      "text": "particular part like let's say when you",
      "start": 2924.04,
      "duration": 5.559
    },
    {
      "text": "are translating do which is German for",
      "start": 2926.839,
      "duration": 5.921
    },
    {
      "text": "you into English the decoder has access",
      "start": 2929.599,
      "duration": 5.52
    },
    {
      "text": "to all of the tokens all of the inputs",
      "start": 2932.76,
      "duration": 4.12
    },
    {
      "text": "and then it decides how much attention",
      "start": 2935.119,
      "duration": 5.24
    },
    {
      "text": "to give to each input this small Insight",
      "start": 2936.88,
      "duration": 6.32
    },
    {
      "text": "leads to a revolution in GPT and as you",
      "start": 2940.359,
      "duration": 5.24
    },
    {
      "text": "can see chat GPT is so awesome the small",
      "start": 2943.2,
      "duration": 4.84
    },
    {
      "text": "Insight is at the heart is the engine of",
      "start": 2945.599,
      "duration": 3.76
    },
    {
      "text": "all that",
      "start": 2948.04,
      "duration": 3.799
    },
    {
      "text": "awesomeness now what I've demonstrated",
      "start": 2949.359,
      "duration": 4.0
    },
    {
      "text": "here is an example of traditional",
      "start": 2951.839,
      "duration": 3.561
    },
    {
      "text": "attention because you are looking at one",
      "start": 2953.359,
      "duration": 4.0
    },
    {
      "text": "sequence you looking at this sequence",
      "start": 2955.4,
      "duration": 3.8
    },
    {
      "text": "which is in German and then you are",
      "start": 2957.359,
      "duration": 3.801
    },
    {
      "text": "looking at this sequence and then you",
      "start": 2959.2,
      "duration": 4.0
    },
    {
      "text": "are seeing which parts of the output",
      "start": 2961.16,
      "duration": 3.84
    },
    {
      "text": "sequence are more related to which parts",
      "start": 2963.2,
      "duration": 3.639
    },
    {
      "text": "of the input sequence this is",
      "start": 2965.0,
      "duration": 4.28
    },
    {
      "text": "traditional attention in self attention",
      "start": 2966.839,
      "duration": 4.081
    },
    {
      "text": "you just look at one sequence and you",
      "start": 2969.28,
      "duration": 3.279
    },
    {
      "text": "look at different parts of that same",
      "start": 2970.92,
      "duration": 3.6
    },
    {
      "text": "sequence and how they are related with",
      "start": 2972.559,
      "duration": 3.8
    },
    {
      "text": "respect to each",
      "start": 2974.52,
      "duration": 4.44
    },
    {
      "text": "other uh as I already mentioned to you",
      "start": 2976.359,
      "duration": 4.561
    },
    {
      "text": "before I have planned three to four more",
      "start": 2978.96,
      "duration": 3.76
    },
    {
      "text": "lectures which are coming up in the",
      "start": 2980.92,
      "duration": 3.919
    },
    {
      "text": "attention series and what we are going",
      "start": 2982.72,
      "duration": 3.839
    },
    {
      "text": "to do in this lectures is that we are",
      "start": 2984.839,
      "duration": 4.441
    },
    {
      "text": "going to uh first start in the next",
      "start": 2986.559,
      "duration": 5.0
    },
    {
      "text": "lecture with a simplified self",
      "start": 2989.28,
      "duration": 4.2
    },
    {
      "text": "attention uh we'll start with the",
      "start": 2991.559,
      "duration": 3.841
    },
    {
      "text": "simplified self attention and we'll code",
      "start": 2993.48,
      "duration": 4.16
    },
    {
      "text": "it out in Python then we will move to",
      "start": 2995.4,
      "duration": 4.959
    },
    {
      "text": "self attention we'll code it in Python",
      "start": 2997.64,
      "duration": 4.84
    },
    {
      "text": "then we'll move to causal attention",
      "start": 3000.359,
      "duration": 3.881
    },
    {
      "text": "we'll code it out in Python and then",
      "start": 3002.48,
      "duration": 3.48
    },
    {
      "text": "we'll move to multi-head attention and",
      "start": 3004.24,
      "duration": 4.96
    },
    {
      "text": "we'll code it out uh and we'll code this",
      "start": 3005.96,
      "duration": 5.2
    },
    {
      "text": "also in Python so all of the different",
      "start": 3009.2,
      "duration": 4.24
    },
    {
      "text": "modules I'll be showing the mathematical",
      "start": 3011.16,
      "duration": 4.32
    },
    {
      "text": "formulations on the Whiteboard and the",
      "start": 3013.44,
      "duration": 4.96
    },
    {
      "text": "rest will code it in Python so in the",
      "start": 3015.48,
      "duration": 5.0
    },
    {
      "text": "next class let me show you where we'll",
      "start": 3018.4,
      "duration": 4.719
    },
    {
      "text": "begin with in the next class we'll begin",
      "start": 3020.48,
      "duration": 4.359
    },
    {
      "text": "with this this part which is",
      "start": 3023.119,
      "duration": 3.601
    },
    {
      "text": "implementing self implementing a",
      "start": 3024.839,
      "duration": 4.841
    },
    {
      "text": "simplified attention mechanism uh so I",
      "start": 3026.72,
      "duration": 4.92
    },
    {
      "text": "also have whiteboards whiteboard notes",
      "start": 3029.68,
      "duration": 3.56
    },
    {
      "text": "for this ready and then I'll take you",
      "start": 3031.64,
      "duration": 3.719
    },
    {
      "text": "through code remember this is a very",
      "start": 3033.24,
      "duration": 4.4
    },
    {
      "text": "serious lecture Series where I I don't",
      "start": 3035.359,
      "duration": 5.0
    },
    {
      "text": "just want to give you the code directly",
      "start": 3037.64,
      "duration": 4.679
    },
    {
      "text": "I want to teach you the theory make your",
      "start": 3040.359,
      "duration": 3.841
    },
    {
      "text": "foundation strong and build everything",
      "start": 3042.319,
      "duration": 4.201
    },
    {
      "text": "from scratch in code we can always use a",
      "start": 3044.2,
      "duration": 5.119
    },
    {
      "text": "package for this Lang chain is there so",
      "start": 3046.52,
      "duration": 4.96
    },
    {
      "text": "many other packages are there we don't",
      "start": 3049.319,
      "duration": 4.76
    },
    {
      "text": "even need to uh go into the attention",
      "start": 3051.48,
      "duration": 4.639
    },
    {
      "text": "mechanism details if we directly use the",
      "start": 3054.079,
      "duration": 4.04
    },
    {
      "text": "package but I don't think that's a good",
      "start": 3056.119,
      "duration": 4.68
    },
    {
      "text": "way to learn about large language models",
      "start": 3058.119,
      "duration": 4.801
    },
    {
      "text": "the best way is to learn from Basics to",
      "start": 3060.799,
      "duration": 3.56
    },
    {
      "text": "understand the nuts and bolts of how",
      "start": 3062.92,
      "duration": 3.8
    },
    {
      "text": "things work and to build things out from",
      "start": 3064.359,
      "duration": 3.841
    },
    {
      "text": "scratch",
      "start": 3066.72,
      "duration": 4.04
    },
    {
      "text": "yourself uh I hope you all are enjoying",
      "start": 3068.2,
      "duration": 4.879
    },
    {
      "text": "this series um thanks a lot everyone",
      "start": 3070.76,
      "duration": 3.88
    },
    {
      "text": "there are number of exciting lectures",
      "start": 3073.079,
      "duration": 2.961
    },
    {
      "text": "which are planned ahead and I look",
      "start": 3074.64,
      "duration": 2.84
    },
    {
      "text": "forward to seeing you all in those",
      "start": 3076.04,
      "duration": 4.44
    },
    {
      "text": "lectures",
      "start": 3077.48,
      "duration": 3.0
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch series up till now in this series we have looked at a number of things in particular we have spent a lot of time discussing the data preparation and the sampling stage of building a large language model in particular if you want to build an entire large language model pipeline it has to be done in three stages in stage one you have to look at the data preparation then comes attention mechanism then comes the llm architecture in stage two we have the training and the model evaluation and in stage three we have the fine tuning so we have spent around five to six lectures on the data preparation and sampling part where we looked at um word embedding we looked at tokenization we looked at bite pair encoding we looked at positional encoding essentially we have looked at the entire data pre-processing pipeline of the llm in a lot of detail now it's time to move to the second building block of stage one and that is the attention mechanism in one of the earlier lectures of this series I told all of you that I think of Transformers as the secret Source behind the llm so if Transformers is like a car then attention mechanism is essentially the engine which drives the car this is the mechanism I think which gives so much power to large language models and that's why chat GPT performs so well uh there are a few lectures on attention mechanism on YouTube but they are not comprehensive at all it's impossible to cover everything related to attention mechanism in one lecture uh it's one of the most important Concepts so I have planned a series of four to five lectures on attention mechanism today's lecture will be a foundational overview where we will understand the introduction to attention mechanism what it is why it is really needed and the types of attention mechanism then what we'll be doing from the next lecture onwards is we'll be coding out the entire attention mechanism completely from scratch we are not going to assume even a single thing um okay so in this lecture we are going to look at the subsection which is essentially uh the subsection on attention mechanism so let me switch my color to I think I'll choose purple here and let's start looking at attention mechanism in detail so first let me motivate uh so that you get an intuition of why this name attention mechanism comes and uh what are we essentially trying to solve here so let's look at this example let's say you are a large language model like G P PT and you have received this sentence the sentence is the cat that was sitting on the mat which was next to the dog jumped now as a human I can say that okay there is maybe a cat here the cat uh was sitting next to a dog and the cat was also on a mat and then uh as a human when I read this I know that the cat jumped okay but as a large language model if you look at the sentence you'll soon realize that this sentence is a bit confusing I can very clearly see that the cat was sitting on the mat if only this were the sentence I could easily analyze that the cat is the main subject in this sentence and the cat was sitting and the object is the mat but the thing is when there are such complex sentences which are also called long-term dependencies where there is this second sentence which is attached so then it becomes a bit difficult for the large language model because uh so after this sentence there will be a number of other sentences right but the main thing which the large language model needs to really understand from this sentence is that the cat which is the main subject that subject actually jumped so the main the action which the subject perform is jumping so the llm should understand that when it looks at cat the word which it should be paying the most attention to is jumped notice how I use the word attention so when I look at the word cat um of course sitting is also important because the cat was earlier sitting on the mat but now the cat has jumped so there are few words in this sentence which the llm needs to pay the most attention to in association with cat and if you don't introduce the attention me mechanism it's very difficult for the llm to know that the cat is the one who has jumped uh maybe if the attention was attention mechanism was not there the llm would have been confused and it might think oh the dog has jumped or it might think that the main main part of this sentence is the cat is on a mat so if the attention mechanism was not there maybe the llm would have thought that the cat is on the mat that's it it would not know that I have to give a lot of atten ention to jump in association with the cat this is the broad level intuition why we need to learn about the attention mechanism when you have sentences such as this and then there is a big story after this the LM needs to analyze this sentence and it needs to process in relation to a particular word let's say in relation to cat which other word should I pay the most attention to and that's where the attention mechanism comes into the picture it turns out that with without attention mechanism if you used a recurrent neural network uh or some other neural network it does not capture the longterm dependencies between sentences that's the broad level intuition now let's dive deeper into what all we will be covering about attention in the subsequent lectures so if you look at the attention mechanism itself there are essentially four types of attention mechanism uh the main attention mechanism which was which is used in GPT uh generative pre-train Transformer and all the modern llms is this multi-head attention and many YouTube videos and um courses all many courses just directly start with multi-head attention it's a very difficult concept to understand if you directly start learning this so you have to go in a sequential manner so what I'll be covering in this SE in the series of lectures is first I'll start with something called simplified self attention so this is the pure EST and the most basic form of the attention technique so that you understand what is attention then we will move to self attention so here we will also introduce train trainable weights which form the basis of the actual mechanism which is used in the llms until this part we are still not at the actual mechanism but we are building up slowly after I cover self attention the next thing which I'll move to is causal attention this is when things really start to get interesting we are predicting the next World right by looking at the past world so what causal attention does is that it's a type of self attention uh that allows the model to consider only the previous and the current inputs in a sequence and it masks out the future inputs no need to uh pay too much attention to this right now I'm just giving you a broad overview of what all I'll cover in the subsequent lectures when we look at attention today we are not going to cover all of these today we are just going to look at uh more details about the history of how attention came into the picture why it is needed why it's better than RNN Etc and then finally we'll move to multi-ad attention only when you have understood causal attention and self attention and simplified self attention you will be able to understand multi-head attention this is the main concept which is actually used in building GPD so multi-head attention is just basically a bunch of causal attention heads stacked together and we'll code out this multi-head attention fully from scratch I'll show you the dimensions how they work etc all of that is planned in the subsequent lectures so this multi-head attention is essentially an extension of self attention and causal attention that enables the model to simultaneously attend to information from different representation subspaces don't worry about this just remember that the multi-ad attention allows the llm to look at input data and and then process many parts of that input data in parallel so for example if this is the sentence the multi-ad attention allows the llm to have let's say one attention head looks at this part one attention head looks at this part one attention head looks at this part Etc this is just a crude description so that you get an understanding of what do you mean by multihead attention so I just wanted to show you this overview so that you get an idea of how these four to five lectures are actually planned um it is is impossible as I mentioned to cover all of this in one lecture and that's why I will follow a very comprehensive approach I'll show everything on the Whiteboard and then I have this uh Google collab notebook where everything has already been implemented and we'll go through this entire notebook see hiding future words with causal attention and then I also have a section on U essentially multi-head attention yeah see so at the end of these four to five lectures we'll be implementing this multi-head attention in Python and code it out from scratch okay for now let's continue with today's lecture which is an introduction to the attention mechanism and uh how researchers got to discovering attention so let's go back in time a bit because to always appreciate something new we need to know about the history of how of how we came to this uh Innovation so let's go right at the start where we are modeling let's say long sequences so we have one sequence in English and uh let's say we want to translate it to the German language so what's the problem in modeling long sequences so let's look at this question what is the problem with architectures without the attention mechanism which came before the llms um so for reference we'll start with the language translation model so let's look at this this figure here um so I have words in the English language so can you uh or let's say I have the German inut sentence which I want to translate to English so this is the input sentence I have the first word the second word then I have the third word Etc uh and I want to translate this into English Okay so uh let's say we do a word by word translation if I translate the first word to English it's can if I translate the second word to English it's you if I translate the third word it's me the fourth word help so if you translate every German word word by word the translation comes out to be can you me help this sentence to translate uh that's obviously not correct right so the main takeaway here is that the word by word translation does not work and uh you can also see this in Hindi so if the main text is in English so can you help me and if you want to translate it in Hindi uh so the Hindi translation is so is associated with can that's fine you is associated with tum but Mary is the third word in h in the Hindi translation right but it's actually the fourth word in the English translation similarly madat is the fourth word in the Hindi translation but help is actually the third word in the English so the main point here is that uh word by word translation does not work in this case and uh that was a major realization when people started modeling long sequence es and this is a general problem when you deal with sequences you cannot just do word by word translation you need contextual understanding and grammar alignment so whenever you are developing a model let's say which translates one sequence to another sequence or tries to find the meaning of a sequence or makes the next word prediction from a sequence you need to really understand the context you need to understand how different words relate with each other what's the grammar of that particular language and only then will you be able to uh process sequences or only then you'll be able to model long sequences of textual information that's understanding number one okay with this understanding what people realized is that we cannot just use a normal neural network uh because if you have a normal neural network it does not have memory so we are going to use this word memory a lot just like humans have memory we store information about the past uh in order to do a good job in sequence to sequence translation the models need to have a memory the models need to know what has come in the past why because let's say I have a sentence that Harry Potter went to Station Number 9 3x4 uh he did this he did this Etc and then when I come to a for a sentence which is uh three to four sentences after the first sentence which is Harry Potter came to station 9 3x4 I should not forget what came before because the station number 9 3x4 is very important for me to know even if I come at the end of the paragraph So if I'm making some prediction at the end of the paragraph and the word station comes over there I need to go back to the start I need to have memory of what came at the start that it was the station number 9 3x4 and this happens a lot with textual data if you want to have meaningful outcomes in terms of text summarization next word prediction language translation you definitely need to have understanding of the meaning and for that you need the model to retain the memory so uh to address this issue that word by word translation does not work in this particular case of translation uh people realize that a normal neural network will not work so they augmented a neural network with two subm modules the first subm module is an encoder and the second sub module is a decoder so what the encoder does is that uh in in the example which we saw it will receive the German text and it will read and process the German text and then it will pass it to the decoder and then the decoder will translate the German text back into English this is the simplest explanation of the encoder decoder and there's a nice animation here which actually shows uh how the encoder decoder works so here you can see the input sequence comes in the German language it goes to the encoder uh a context is generated by the encoder it's called as a context Vector the context Vector essentially captures meaning so it has memory and it captures meaning of okay instead of just word by word translation what does this sentence represent and uh the encoder processes the entire input sequence and sends the context over to the decoder so let me play this again the input sequence comes to the encoder it generates a context Vector which basically encodes meaning and then the encoder transfers the context Vector to the decoder and the decoder generates the output in this case the output is the transl translated English text okay this is how the encoder decoder blocks work and uh the mechanism which really employed the encoder decoder blocks successfully is called recurrent neural networks so before really Transformers came into the picture recurrent neural networks were was that architecture which was extremely popular for language translation and it really uh employed the encoder decoder architecture uh it was implemented in the 1980s so let's look a bit more at how the RNN actually works because when we if we understand how RNN works that's when we'll understand the limitations of recurrent neural networks and that's when we will really appreciate why the attention mechanism needed to be discovered so here's how the encoder decoder in the RNN actually works what happens is that you first receive an input text okay uh and that let's say is the German uh German text the input text is passed to the decod to the encoder what the encoder will do is that at every step it will take the input and it will maintain something which is called as the hidden State this hidden state was the biggest innovation in the recurrent neural networks this hidden State essentially captures the memory so imagine the uh first input which is the first German word comes the encoder augments it or the encoder maintains a hidden State then you go to the next iteration then the second input world comes then the hidden State also gets updated so as the hidden state gets updated it receives more and more memory of what has come previously and the hidden state gets updated at each step and then there is a final hidden State the final hidden state is basically the encoder output what we saw the context Vector over here so when we looked at the context Vector which is passed from the encoder to the decoder let's see over here yeah so here you see a context Vector is passed from the encoder to the decoder this context Vector is the final hidden State this is basically the encoder telling the decoder that hey I have looked at the input text uh here's the meaning of this this text here's how I encoded it here's the context Vector take this final hidden State and try to decode it and then the decoder uses this final hidden state to generate the translated sentence and it generates the translated sentence one word at a time uh so here's a schematic which actually explains this pretty well so I have an input text here so this is the first word in German the second word in German the third word in German and the fourth word in German what the encoder block will do is that it will take each input sequentially and it will maintain a different hidden state so for the first input it has the first hidden State then we move to the next iteration then the second hidden State then the third hidden State and then finally when we have the last input we have this final hidden State the final hidden State essentially contains the accumulation of all previous hidden state so it contains or encapsulates memory this is how memory is incorporated which was missing earlier with just a normal neural network so this is the final hidden State and then this final hidden State essentially memorizes the entire input and then this uh hidden state is passed to to the decoder and then the decoder produces the final output which is the translated English sentence so I want to show you another animation of this so that you understand it much better um so here's how the RNN actually works right so see the input Vector number one is the first word in German which needs to be translated so here you will see in this animation how the hidden state gets updated so the first word of German comes then the RNN maintains the hidden state zero and here you see uh the hidden state is the hidden state zero and the input one is used to produce the output one and then we also have a hidden State one then as we move further we have the hidden state two hidden state three hidden State four and final hidden State when the last word needs to be processed uh actually I can show you this again here so this is from a French to English translation using the recurrent neural network look look here so we have French input which is coming here justu then uh yeah so the first word goes into the encoder see now we have hidden so let me expand it so and play from the start okay so now the first word of French which is J goes into the encoder it has went into the encoder right now and the first hidden state is generated see in the orange color great now this hidden state number one and the second input is used again look at this animation again the hidden state state number one and the second input which is sui sis s is used U and then we have the hidden state number two then the hidden state number two and the input three which is will be used to produce the final hidden State great this final hidden State hidden state number three essentially contains all the information in the given sequence plus it also contains some memory or some context regarding uh what came in the past and now this final hidden state is then passed to the decoder and the decoder produces the output in English one word at a time this is exactly how the recurrent neural network works okay now you might think that awesome right this is already doing sequence to sequence translations and we are translating from one language to another language so why do we need attention mechanism memory is being encoded here and we are passing in the context which means that we will be able to identify how different words of the sequence are related to each other so why do we need attention well there is a big problem with the recurrent neural network and that problem happens because um the model the decoder has essentially no access to the previous hidden States so if you look at this video you'll see that uh the decoder has access to only the final hidden state so hidden State one hidden State 2 hidden state three and then hidden state three is passed to the decoder C the the decoder has no access to the previous hidden States now why is this a big problem um the reason it's a big problem is because when we have to process long sequences if the decoder just relies on one final hidden State that's a lot of pressure on the decoder to essentially that one final hidden State needs to have the entire information and for long sequences it usually fails because it's very hard for one final hidden state to have the entire information let me explain this bit more so as we saw the encoder let me change my color here I think let me change it to Green yeah so as we saw the encoder processes the entire input text the encoder processes the entire input text into one final hidden state which is the memory cell and then decoder takes this this hidden State decoder takes this hidden state to essentially produce an output great now here's the biggest issue with RNN and please play pay very close attention to this point because if you understand this you will understand why attention mechanisms were needed the biggest issue with the RNN is that a recurrent neural network cannot directly access earlier hidden States as we saw in the video it only accesses the final hidden state so the RNN can't directly access earlier hidden States from the encoder during the decoding phase it relies only on the current hidden state which is the final hidden State and this leads to a loss or this leads to a loss of context especially in complex sentences where dependencies might span long distances um okay so let me actually explain this further what does it mean loss of context right uh so as we saw the encoder compresses the entire input sequence into a single hidden State Vector I hope you have understood up till this point now the problem happens let's say if the sentence if the input sentence is very long if the input sentence is very long it really becomes very difficult for the recurrent neural network to capture all of that information in one single final hidden state that becomes very difficult and this is the main drawback of the RNN so for example let's take a practical case so let's say uh we take the example which we looked at at the start of the lecture which is the cat that was sitting on the mat which was next to the dog jumped and let's say we want to convert this English into a French translation okay so the French translation will be lat whatever I cannot spell this out fully but this will be the French translation for this English sequence now as I mentioned to you before this English sequence is pretty long uh the RNN or the encoder really needs to capture the dependencies very well so the final hidden State needs to capture that the cat is the subject here and the cat is the one who has jumped and this information this context needs to be captured by the final hidden State and that is very hard if you are putting all the pressure on one final hidden state to capture all this context especially in Long sequences so uh the key action which is jumped depends on the subject which is cat but also an understanding longer depend dependencies so jumped depends on cat but we also need to understand that the cat was sitting on the mat and the cat was also sitting next to the dog because the dog also might be referred somewhere else in the big text so we need to understand many things from this sentence and these are also called as longer dependencies so jumped the action jumped of course depends on the subject cat but we Al to understand this we also need to understand longer dependencies that the cat was sitting next to the dog and the cat was also sitting on the mat so to capture these longer dependencies or to capture this uh longer context or difficult context the RNN decoder struggles with this because it just has one final hidden State uh to get all the information from this is called loss of context and loss of context was one of the biggest issues because of which RNN was not as good as the GPT which exist right now which is based on the attention mechanism okay so these are the issues with RNN uh the decoder cannot access the hidden states of the input which came in earlier so we cannot capture long range dependencies this is where attention mechanism actually comes into the picture okay we will capture long range dependencies with attention mechanisms and let's see how so RNN work fine for translating short sentences and they did work amazingly actually for quite a while for short sentences but researchers soon discovered that they don't work for long text because they don't have direct access to previous words in the input so when an RNN decoder only receives the final hidden state right they don't even have access to the the decoder does not have access to all the prior Words which came in the input so let's say I'm decoding uh uh let's say I'm looking at this word um jumped right let's say I'm looking at the word jumped cat is a word which has come way prior in the sequence so when I am looking at the word jumped I need to give a lot of attention to the word cat but an RNN gets the entire encoded version of this sentence so how would the RNN know that jump actually if you're looking at jumped you should pay a lot of attention to the word cat it does not even have access to this input Vector for cat this is where attention mechanism actually comes into the picture uh okay so as I said one of the major shortcoming in the RNN is that the RNN must remember the entire encoded input in a single hidden State before it passes the encoded input to the decoder the RNN has to remember the entire encoded input in a single hidden state I'm repeating this again because unless you understand this you won't understand why we are learning about attention and it's very hard for the RNN to encode the entire or to remember the entire encoded input in a single hidden State this is when the researchers started looking at other mechanisms and that's when in 2014 researchers developed the so-called bhano attention mechanism for RNN so when people think of attention mechanism they always think of the 2017 paper right attention is all you need but actually attention was introduced in a paper in 2014 which was called neural machine translation by jointly learning to align and translate um sadly many people don't remember this now everyone just remembers attention is all you need but remember that this these authors badano Benjo and Kung yuno they were the ones who worked on the first proposition of the attention mechanism so I just want to give uh credit here and this attention mechanism was called badan attention mechanism because the author of this paper was was last name was Bano so what was the main idea behind this what the attention mechanism basically uh prescribed is that okay let's take the encoder decoder RNN and let's modify the encoder decoder RNN so that the decoder can selectively access different parts of the input sequence at each decoding step uh let me repeat that remember in the original and and the decoder only had access to the final hidden state but the bhano attention mechanism says that what if the decoder now can selectively access different parts of the input sequence at each decoding step and let me explain to you what this means by simple figure so let's say we are at this decoding step where uh we want to so let's say the word is Mir which is the uh German word and we want to decode this so we are at this hidden state right now and we want to decode this okay if we use the original RNN we just had access to this final hidden state but now what we say is that when you are uh decoding this uh hidden State what if you have access to all of the input tokens so follow this orange curve which I'm drawing here so what if you have access to this token what if you have access to this token what if you have access to this token and what if you have access access to this token so let's say when you're decoding you have access to all the tokens and you can decide how much attention to pay to each token so for example I think the German uh translation for you is do which is do so I know that the maximum attention needs to be paid to this token so that's why you I have marked this with a thick line over here for all the other tokens well we pay less amount of attention what this does is that it allows the decoder to access all the to tokens so if we are dealing with long sentences we can access all the tokens even in long sentences and decide which token we want to pay more attention to so for example let's take this longer sentence which we have looked at a lot in today's lecture yeah so let's say and we'll look at it once more again uh yeah so let's say we have look we are looking at this sentence the the cat that was sitting on the mat which was next to the dog jumped let's say we looking at this sentence now in the decoder so let's say you are on the decoder part and you are decoding for jumped and you translating it into French what we will do is that instead of just looking at the final hidden State we will have access to all of the words let's say we have access to all of the input words and not just that we have access to all of the input words and we can also decide how much attention to pay to each of the input word so now I will say that okay I want to translate jump right so of course a lot of attention should be paid to jumped but I will also pay a lot of attention to cat because the cat is the uh one who has really jumped so I can decide which tokens to pay the maximum attention to so when I'm translating jumped I will pay attention to jump and I will pay attention to cat also because now I can access the token for cat this access itself was not possible in RNN because we could not access the previous input tokens in an RNN and this was the main problem which was solved by the introduction of the attention mechanism so this figure actually explains the general idea behind the bhan attention mechanism um let me Zoom onto this figure once more yeah so if you look at this figure here uh when you are translating from do to English you pay attention to all the input tokens and then you can have these attention weights which basically describe how much you want to pay attention to each input token and that solves the problem of the loss of context which was present in the RNN so essentially using an attention mechanism using an attention mechanism the text generating decoder part of the network can access all the input tokens selectively uh and as I mentioned we can decide how much weight to give to each input token so this means that some input tokens are more important than others for generating a given output token this importance is determined by the attention weights so this is where the attention weights and the attention score comes into the picture which we will learn about later now uh so this paper was introduced in 2014 right this paper which is which introduced the bhan attention mechanism only three years later the main Transformers paper came out and that came out in 2017 so only 3 years later researchers found that RNN architectures are not required for building deep neural networks for natural language processing and uh this is when the researchers proposed the Transformer architecture and at the main core of the Transformer architecture was the badano attention mechanism so it was called self attention mechanism but it was really inspired a lot from the bhano attention mechanism we'll come to what self attention means bhano attention did not introduce the term self attention but the Transformers paper which really changed everything for llms that paper came in 2017 uh it introduced the Transformer architecture with a self attention mechanism that was completely based on the badano attention mechanism this is that paper which came out in 2017 and here you'll see uh attention attention basically the paper itself is titled attention is all you need um so this is how attention really came to be the core building block of large language models I just want to explain to you this once more so that I drive this concept home about what why attention mechanism helps so let's say again the same sentence this is the sentence the cat that was sitting on the mat which was next to the dog jumped and this is the French translation uh for this particular sequence so what the attention mechanism does is that at each decoding step so at each decoding step the model can look back at the entire input sequence and decide which parts are most relevant to generate the current word so for example uh let's say we are predicting this word sa which is the French translation for jumped so when the decoder is predicting this French translation s the attention mechanism allows the decoder to focus on the part of the input that corresponds to jump so we can selectively look at which part of the input to give maximum attention to uh and this Dynamic focus on different parts of the input sequence is what helps the attention mechanism to learn long range dependencies more effectively so remember we looked at how RNN fail to understand or even learn longrange dependencies uh one main or key thing of the attention mechanism is this word which is called as Dynamic Focus so Dynamic Focus which means that for every decoder for every decoding step we can selectively choose which inputs to focus on and how much attention to give to it each input that's why it's called Dynamic Focus so this Dynamic focus on different parts of the input sequence helps us to learn long range dependencies more effectively and that's why the attention mechanism uh actually works so well there is another animation which I want to show you uh so here we saw the RNN right let me show you again so this is the recurrent neural network here you will see that just the final hidden state which is the hidden state number three is actually passed to the decoding stage nothing else is passed but now let me show you the modification of this with with the attention mechanism so now uh if we do this with attention added what happens is that you'll see that hidden state number one is generated hidden state number two is generated hidden state number three is generated but all of these hidden states are passed to the decoder which means that the decoder at every step has access to all of the Hidden States not just the final hidden state number three but even hidden States number one and hidden States number two this is an extremely important point to remember uh one more thing which I want to show you here is that uh yeah this part so let's say uh uh yeah so let's say we are translating from French to English over here I think let me play this again yeah yeah so let's say we are translating from French to English here again let's see what is happening here so if we are at the decoder stage we want to generate this word I right for Jo so this when we are at the decoder State the decoder has access to all of these hidden States 1 2 and three when it's generating the English translation for J but what it does is that it gives more preference to the hidden state number one because it realize that that Jo is more important for translation to I similarly when we want to translate sui s u i s it actually translates to M so when the decoder is translating this to English it has access to Hidden State 1 2 and three but it learns that the hidden state two is most important for this translation similarly when it's translating Aon with the attention mechanism it has translate it has access to one two and three hidden state but it learns that the hidden state number three is the most important uh Etc so this is how it actually works so at every decoding step we have access to all the different uh hidden States and one more key thing to mention here is that the model is isn't just mindlessly aligning the first word at the output with the first word at the input it actually learns from the training phase how to align the words in the language pair so remember at the start we saw that you cannot blindly do word to word translation that's not what's happening here in the training phase the attention mechanism actually learns which word to align with which word so which word in the output should be aligned with which word in the input so this this graph is actually presented in the 2014 paper and I think this graph is very interesting so you can also take a look at this uh this is figure number three in the original the badana attention mechanism paper so here you can see that let's see European economic area so let's say we want to translate European economic area to French it actually translates to uh European economic zone so here it says that in French the order of this word is reversed as compared to English so let's see how the model actually learns so European so it learns this European happens over here then economic is here and area is here so basically the point is that uh when the attention mechanism learns that European maps to this European economic maps to economic and area maps to Zone it does it's it's not just blindly looking at the order because if you see the order European comes at 1 2 3 4 fth but European which is the French translation comes at 1 2 3 4 5 6 7th so order is different it's not blindly copying the order or the number uh at which the word occurs it's actually learning from the training uh it's learning the dependencies between different words and that's how it does does the translation um I'll be sharing the link to this paper in the information section the 2014 badano attention paper and I'll also be sharing the link to the 2017 attention is all you need paper before we move to self attention I actually want to take a bit of time to tell you about the history of the attention and Transformers so that you have a timeline in mind so uh it all started in 1980 1980s is when recurrent neural networks came into the picture and recurrent neural networks introduced this hidden layer the hidden state that was the key Innovation here in 1997 came long short-term memory networks and they were an advancements compared to lnn uh compared to RNN so recurrent neural networks had this problem which is called Vanishing gradients so when you stack multiple feedback loops together it leads to the vanishing gradient problem long short-term memory solved this so we had a longterm memory route and a shortterm memory route and that's how the lstm actually operates but still both of these had problems with respect to longer context which was solved by the B Z Now attention mechanism in 2014 where the decoder can can have access to each of the input State when it's performing the decoding operation and it can selectively decide which input to give more attention to then came the Transformers architecture in 2017 and the Transformers architecture really used the attention mechanism which was proposed in 2014 in this paper badana attention mechanism so this is the history we we are living in the age right now where people only know about attention and Transformers right but work has been going on in this area for the past 43 to 45 years or even more uh so please keep this particular uh figure in mind or historical mind map so that you are aware of how lucky we are to be living in these times where all of the research work has been accumulated for 50 years and now we are reaping the benefits of these scientific advancements these advancements have not just happened in a period of one year or two years or 3 years they have taken time they researchers have worked hard over the past half a century to get us to this statee okay so I hope until now you have all understood a flavor and an intuition behind the attention mechanism itself um what the Transformers paper did is that it introduced another terminology which is actually called as self attention so self attention is a bit different so self attention um is basically a mechanism that allows each position of the input sequence to attend to all positions in the same sequence so we are not looking at different sequences now when we looked at language translation we looked at converting from English to French or German to English right so we are looking at one sequence and we are looking at another sequence self attention is basically different now instead of giving attention to another sequence all the attention is directed inwards so we we are just looking within a particular sequence so we are looking at different tokens within a sequence and see how these tokens are related to each other that's called as self attention so self attention is a key component of contemporary uh large language models so large language models remember are predicting the next word in a given sentence right they can of course do language translation tasks as well but they were predominantly trained for predicting the next World and uh they are able to do translation tasks which is also called as emergent Behavior but they were trained to predict the next word and that's why they have to so let's say when you want to predict the next word in a sentence you need to know that how different words are related to each other so let's say the cat jumped on a wall next to the dog the dog also jumped something like that you need to know when you look at cat which are the other words in this sentence which you should pay the most attention to for dog which are the words you should pay the most attention to So within a sentence itself you are deciding how different words of the sentence are related to each other and which word you should pay more attention to when looking at a particular word that's called self attention and self attention module is a key component of contemporary large language models so if you look at uh how this Series has progressed up till now we have looked at pre-processing steps like uh tokenization word embedding positional embedding but now we have started looking at the llm architecture itself right and one of the key points here is the self attention module without really understanding attention and self attention we cannot proceed to the llm training which comes in uh phase number two of building a large language model okay so one last thing to mention is that uh in self attention the self really refers to the attention mechanism's ability to compute attention weights by by relating different positions in a single input sequence so as I mentioned we are looking at uh just one input sequence and we are we are looking at the attention between different tokens of that sequence so it learns the relationship between various parts of the input itself so this is different than traditional attention mechanisms like the translation task which we saw where the focus is on relationships between elements of two different sequences so for example if we want to translate from English to German German to French uh English to Hindi Etc we have two sequences right so traditional attention mechanisms look at one part of the sequence and another part of the sequence and how they are related to each other in self attention we basically learn the relationship between various parts of the input itself so here we learn the relationship between various parts of the input itself uh and that's the difference between traditional attention mechanisms and self attention ention mechanisms okay so this is the end of this particular lecture where we have covered so many things we covered the basic intuition of attention but before that I explain to you in a lot of detail about the shortcomings in recurrent neural networks so just one thing if you take away from this lecture is that recurrent neural networks have a major shortcoming and that shortcoming is that they have to remember the entire encoded input in a single hidden state before passing it to the decoder and that's a problem when dealing with long sentences it leads to context loss because the decoder does not have access to the previous inputs this is exactly the problem which is solved by attention mechanisms in attention mechanisms when you are decoding a particular part like let's say when you are translating do which is German for you into English the decoder has access to all of the tokens all of the inputs and then it decides how much attention to give to each input this small Insight leads to a revolution in GPT and as you can see chat GPT is so awesome the small Insight is at the heart is the engine of all that awesomeness now what I've demonstrated here is an example of traditional attention because you are looking at one sequence you looking at this sequence which is in German and then you are looking at this sequence and then you are seeing which parts of the output sequence are more related to which parts of the input sequence this is traditional attention in self attention you just look at one sequence and you look at different parts of that same sequence and how they are related with respect to each other uh as I already mentioned to you before I have planned three to four more lectures which are coming up in the attention series and what we are going to do in this lectures is that we are going to uh first start in the next lecture with a simplified self attention uh we'll start with the simplified self attention and we'll code it out in Python then we will move to self attention we'll code it in Python then we'll move to causal attention we'll code it out in Python and then we'll move to multi-head attention and we'll code it out uh and we'll code this also in Python so all of the different modules I'll be showing the mathematical formulations on the Whiteboard and the rest will code it in Python so in the next class let me show you where we'll begin with in the next class we'll begin with this this part which is implementing self implementing a simplified attention mechanism uh so I also have whiteboards whiteboard notes for this ready and then I'll take you through code remember this is a very serious lecture Series where I I don't just want to give you the code directly I want to teach you the theory make your foundation strong and build everything from scratch in code we can always use a package for this Lang chain is there so many other packages are there we don't even need to uh go into the attention mechanism details if we directly use the package but I don't think that's a good way to learn about large language models the best way is to learn from Basics to understand the nuts and bolts of how things work and to build things out from scratch yourself uh I hope you all are enjoying this series um thanks a lot everyone there are number of exciting lectures which are planned ahead and I look forward to seeing you all in those lectures"
}