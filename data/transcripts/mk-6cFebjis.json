{
  "video": {
    "video_id": "mk-6cFebjis",
    "title": "Lecture 12: The entire Data Preprocessing Pipeline of Large Language Models (LLMs)",
    "duration": 5655.0,
    "index": 11
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.28
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.24,
      "duration": 5.319
    },
    {
      "text": "in the build large language models from",
      "start": 8.28,
      "duration": 5.12
    },
    {
      "text": "scratch Series this is a very special",
      "start": 10.559,
      "duration": 5.16
    },
    {
      "text": "lecture because it covers one of the",
      "start": 13.4,
      "duration": 5.0
    },
    {
      "text": "most fundamental aspects for building a",
      "start": 15.719,
      "duration": 5.161
    },
    {
      "text": "large language model and that is",
      "start": 18.4,
      "duration": 4.879
    },
    {
      "text": "building the data processing",
      "start": 20.88,
      "duration": 4.88
    },
    {
      "text": "pipeline when people learn about large",
      "start": 23.279,
      "duration": 5.08
    },
    {
      "text": "language models I think that they do not",
      "start": 25.76,
      "duration": 4.32
    },
    {
      "text": "pay too much attention to data",
      "start": 28.359,
      "duration": 4.001
    },
    {
      "text": "processing but it's really one of the",
      "start": 30.08,
      "duration": 5.96
    },
    {
      "text": "fundamental building blocks of having a",
      "start": 32.36,
      "duration": 6.039
    },
    {
      "text": "strongly performing large language",
      "start": 36.04,
      "duration": 4.999
    },
    {
      "text": "model so what does it mean data",
      "start": 38.399,
      "duration": 5.041
    },
    {
      "text": "processing the main idea behind data",
      "start": 41.039,
      "duration": 5.401
    },
    {
      "text": "processing is that we all know that",
      "start": 43.44,
      "duration": 5.24
    },
    {
      "text": "large language models have sentences",
      "start": 46.44,
      "duration": 4.0
    },
    {
      "text": "right and they deal with text they deal",
      "start": 48.68,
      "duration": 5.399
    },
    {
      "text": "with sentences but if you have a big PDF",
      "start": 50.44,
      "duration": 6.52
    },
    {
      "text": "file that cannot directly be be given as",
      "start": 54.079,
      "duration": 5.201
    },
    {
      "text": "input to the large language model it",
      "start": 56.96,
      "duration": 6.32
    },
    {
      "text": "needs to be processed further and that",
      "start": 59.28,
      "duration": 7.159
    },
    {
      "text": "this whole uh procedure of processing",
      "start": 63.28,
      "duration": 7.12
    },
    {
      "text": "the data before it is fed as input to",
      "start": 66.439,
      "duration": 6.161
    },
    {
      "text": "the large language model is called as",
      "start": 70.4,
      "duration": 3.6
    },
    {
      "text": "data",
      "start": 72.6,
      "duration": 4.199
    },
    {
      "text": "pre-processing so in our case the data",
      "start": 74.0,
      "duration": 5.72
    },
    {
      "text": "is text the data is paragraphs the data",
      "start": 76.799,
      "duration": 6.32
    },
    {
      "text": "is documents what can we do to this data",
      "start": 79.72,
      "duration": 6.28
    },
    {
      "text": "so that it can serve as an input to the",
      "start": 83.119,
      "duration": 5.161
    },
    {
      "text": "large language model and we'll be",
      "start": 86.0,
      "duration": 4.119
    },
    {
      "text": "answering this question in today's",
      "start": 88.28,
      "duration": 4.08
    },
    {
      "text": "lecture we'll answer this question in",
      "start": 90.119,
      "duration": 4.201
    },
    {
      "text": "four steps which are the four",
      "start": 92.36,
      "duration": 5.96
    },
    {
      "text": "fundamental steps needed before any",
      "start": 94.32,
      "duration": 7.159
    },
    {
      "text": "sentence is transformed",
      "start": 98.32,
      "duration": 6.159
    },
    {
      "text": "into a form which is good enough to be",
      "start": 101.479,
      "duration": 6.081
    },
    {
      "text": "an input for a large language model the",
      "start": 104.479,
      "duration": 5.6
    },
    {
      "text": "first of these four steps is called as",
      "start": 107.56,
      "duration": 4.8
    },
    {
      "text": "tokenization the second of these four",
      "start": 110.079,
      "duration": 5.121
    },
    {
      "text": "steps is token embeddings the third is",
      "start": 112.36,
      "duration": 4.759
    },
    {
      "text": "called positional embeddings and the",
      "start": 115.2,
      "duration": 3.8
    },
    {
      "text": "fourth is when we create input",
      "start": 117.119,
      "duration": 4.28
    },
    {
      "text": "embeddings this is the final step and",
      "start": 119.0,
      "duration": 4.479
    },
    {
      "text": "these input embeddings then serve as",
      "start": 121.399,
      "duration": 4.521
    },
    {
      "text": "inputs to the large language model",
      "start": 123.479,
      "duration": 4.521
    },
    {
      "text": "remember that input embeddings are the",
      "start": 125.92,
      "duration": 4.24
    },
    {
      "text": "summation of token embeddings and",
      "start": 128.0,
      "duration": 4.319
    },
    {
      "text": "positional embeddings so we'll learn",
      "start": 130.16,
      "duration": 4.159
    },
    {
      "text": "about token embeddings in part two and",
      "start": 132.319,
      "duration": 3.721
    },
    {
      "text": "we'll learn about positional embeddings",
      "start": 134.319,
      "duration": 4.041
    },
    {
      "text": "in part three and then we'll sum these",
      "start": 136.04,
      "duration": 4.44
    },
    {
      "text": "two to create input",
      "start": 138.36,
      "duration": 4.04
    },
    {
      "text": "embeddings even when we look at",
      "start": 140.48,
      "duration": 4.2
    },
    {
      "text": "tokenization it can be broken down into",
      "start": 142.4,
      "duration": 5.4
    },
    {
      "text": "three parts word based tokenizer subword",
      "start": 144.68,
      "duration": 5.32
    },
    {
      "text": "based tokenizer and character based",
      "start": 147.8,
      "duration": 4.88
    },
    {
      "text": "tokenizer the subord based tokenizer has",
      "start": 150.0,
      "duration": 5.4
    },
    {
      "text": "many types and bite pair encoder is one",
      "start": 152.68,
      "duration": 6.4
    },
    {
      "text": "of them gpt2 and in fact many GPT models",
      "start": 155.4,
      "duration": 6.04
    },
    {
      "text": "use this bite pair encoder tokenizer",
      "start": 159.08,
      "duration": 4.28
    },
    {
      "text": "which is a subword based",
      "start": 161.44,
      "duration": 4.12
    },
    {
      "text": "tokenizer token embeddings is",
      "start": 163.36,
      "duration": 4.92
    },
    {
      "text": "essentially converting token IDs into",
      "start": 165.56,
      "duration": 5.12
    },
    {
      "text": "vectors positional embeddings is",
      "start": 168.28,
      "duration": 5.4
    },
    {
      "text": "encoding information about the position",
      "start": 170.68,
      "duration": 5.479
    },
    {
      "text": "awesome right so let's get started with",
      "start": 173.68,
      "duration": 4.6
    },
    {
      "text": "these four steps which we will cover in",
      "start": 176.159,
      "duration": 4.681
    },
    {
      "text": "detail in today's lecture first we'll",
      "start": 178.28,
      "duration": 4.64
    },
    {
      "text": "start with tokenization so let me zoom",
      "start": 180.84,
      "duration": 5.08
    },
    {
      "text": "in over here so when you want to build a",
      "start": 182.92,
      "duration": 4.599
    },
    {
      "text": "large language model you need to go",
      "start": 185.92,
      "duration": 4.44
    },
    {
      "text": "through three stages stage one is for",
      "start": 187.519,
      "duration": 6.041
    },
    {
      "text": "building the basics of an llm stage two",
      "start": 190.36,
      "duration": 5.879
    },
    {
      "text": "is for pre-training and stage three is",
      "start": 193.56,
      "duration": 5.52
    },
    {
      "text": "for fine tuning today's lecture comes",
      "start": 196.239,
      "duration": 5.481
    },
    {
      "text": "under stage one and more specifically it",
      "start": 199.08,
      "duration": 4.719
    },
    {
      "text": "comes under the data preparation and",
      "start": 201.72,
      "duration": 6.36
    },
    {
      "text": "sampling part um of stage one so as I",
      "start": 203.799,
      "duration": 6.281
    },
    {
      "text": "mentioned first we'll be looking at",
      "start": 208.08,
      "duration": 4.2
    },
    {
      "text": "tokenization and initially we'll be",
      "start": 210.08,
      "duration": 4.519
    },
    {
      "text": "looking only at word based",
      "start": 212.28,
      "duration": 4.92
    },
    {
      "text": "tokenizer so what we'll do right now is",
      "start": 214.599,
      "duration": 5.161
    },
    {
      "text": "that we'll construct our own word based",
      "start": 217.2,
      "duration": 4.239
    },
    {
      "text": "tokenizer from",
      "start": 219.76,
      "duration": 3.72
    },
    {
      "text": "scratch although this is not the",
      "start": 221.439,
      "duration": 4.641
    },
    {
      "text": "tokenizer which is used in GPT models",
      "start": 223.48,
      "duration": 5.6
    },
    {
      "text": "GPT use subord based tokenizer but if we",
      "start": 226.08,
      "duration": 5.079
    },
    {
      "text": "directly move to subord based tokenizer",
      "start": 229.08,
      "duration": 3.64
    },
    {
      "text": "you'll lose the intuition you'll lose",
      "start": 231.159,
      "duration": 3.881
    },
    {
      "text": "the feel of how exactly tokenization is",
      "start": 232.72,
      "duration": 4.719
    },
    {
      "text": "done so first we'll start with word",
      "start": 235.04,
      "duration": 3.24
    },
    {
      "text": "based",
      "start": 237.439,
      "duration": 3.401
    },
    {
      "text": "tokenizer and uh",
      "start": 238.28,
      "duration": 4.72
    },
    {
      "text": "for that we'll need a database right",
      "start": 240.84,
      "duration": 4.28
    },
    {
      "text": "we'll need a data set very simply",
      "start": 243.0,
      "duration": 4.56
    },
    {
      "text": "speaking word based tokenization is just",
      "start": 245.12,
      "duration": 4.52
    },
    {
      "text": "converting the entire data set into",
      "start": 247.56,
      "duration": 5.84
    },
    {
      "text": "chunks of words that's it and I'm going",
      "start": 249.64,
      "duration": 6.12
    },
    {
      "text": "to show you how that is done it's not as",
      "start": 253.4,
      "duration": 5.0
    },
    {
      "text": "easy as it looks so the data set which",
      "start": 255.76,
      "duration": 4.52
    },
    {
      "text": "we are going to use is a book which is",
      "start": 258.4,
      "duration": 4.28
    },
    {
      "text": "called the verdict it's a book which is",
      "start": 260.28,
      "duration": 5.0
    },
    {
      "text": "written in 1908 you can also search",
      "start": 262.68,
      "duration": 6.48
    },
    {
      "text": "about it it has this uh this cover and",
      "start": 265.28,
      "duration": 6.12
    },
    {
      "text": "we are going to to use this as our input",
      "start": 269.16,
      "duration": 4.879
    },
    {
      "text": "data set it has a lot of text as you can",
      "start": 271.4,
      "duration": 3.76
    },
    {
      "text": "see over",
      "start": 274.039,
      "duration": 4.16
    },
    {
      "text": "here so in word based tokenization what",
      "start": 275.16,
      "duration": 5.039
    },
    {
      "text": "our aim would be would be to take this",
      "start": 278.199,
      "duration": 6.041
    },
    {
      "text": "text and to convert it into words and",
      "start": 280.199,
      "duration": 7.321
    },
    {
      "text": "break it down into words right so then",
      "start": 284.24,
      "duration": 5.08
    },
    {
      "text": "uh let's move to python right now",
      "start": 287.52,
      "duration": 4.16
    },
    {
      "text": "immediately and let's see how to code",
      "start": 289.32,
      "duration": 4.08
    },
    {
      "text": "this out from",
      "start": 291.68,
      "duration": 4.28
    },
    {
      "text": "scratch so this is the python notebook",
      "start": 293.4,
      "duration": 4.68
    },
    {
      "text": "which we'll be going through in today's",
      "start": 295.96,
      "duration": 4.28
    },
    {
      "text": "lecture the first step step as I",
      "start": 298.08,
      "duration": 4.88
    },
    {
      "text": "mentioned is creating Word based tokens",
      "start": 300.24,
      "duration": 6.6
    },
    {
      "text": "from scratch so initially what we'll be",
      "start": 302.96,
      "duration": 6.76
    },
    {
      "text": "doing is that we'll open this data set",
      "start": 306.84,
      "duration": 4.76
    },
    {
      "text": "we'll open this document and we'll read",
      "start": 309.72,
      "duration": 4.199
    },
    {
      "text": "this document the way it is done is that",
      "start": 311.6,
      "duration": 4.64
    },
    {
      "text": "you have to use this with open and then",
      "start": 313.919,
      "duration": 4.961
    },
    {
      "text": "you assign a variable called raw text",
      "start": 316.24,
      "duration": 4.64
    },
    {
      "text": "which contains the entire text which is",
      "start": 318.88,
      "duration": 4.52
    },
    {
      "text": "read by python I've actually downloaded",
      "start": 320.88,
      "duration": 5.039
    },
    {
      "text": "this uh book from this particular link",
      "start": 323.4,
      "duration": 4.6
    },
    {
      "text": "you can download it online I'll share",
      "start": 325.919,
      "duration": 4.84
    },
    {
      "text": "the file with you so that you can also",
      "start": 328.0,
      "duration": 5.84
    },
    {
      "text": "run this code on your own so once the",
      "start": 330.759,
      "duration": 4.921
    },
    {
      "text": "data set or the book itself is",
      "start": 333.84,
      "duration": 4.639
    },
    {
      "text": "downloaded or read by python what you",
      "start": 335.68,
      "duration": 4.56
    },
    {
      "text": "can do is that you can check whether it",
      "start": 338.479,
      "duration": 4.0
    },
    {
      "text": "has indeed been read so you can print",
      "start": 340.24,
      "duration": 4.079
    },
    {
      "text": "the total number of characters and you",
      "start": 342.479,
      "duration": 4.401
    },
    {
      "text": "can even print some portion of the text",
      "start": 344.319,
      "duration": 4.481
    },
    {
      "text": "so you can see that the total number of",
      "start": 346.88,
      "duration": 3.599
    },
    {
      "text": "characters is",
      "start": 348.8,
      "duration": 5.72
    },
    {
      "text": "20479 and here here I'm printing the uh",
      "start": 350.479,
      "duration": 6.321
    },
    {
      "text": "first 100 characters of this file so",
      "start": 354.52,
      "duration": 5.399
    },
    {
      "text": "it's I had always thought Jack uh giz",
      "start": 356.8,
      "duration": 4.76
    },
    {
      "text": "rather a cheap genius though a good",
      "start": 359.919,
      "duration": 3.601
    },
    {
      "text": "fellow let's check whether this is",
      "start": 361.56,
      "duration": 4.84
    },
    {
      "text": "actually correct so in the book it's",
      "start": 363.52,
      "duration": 5.0
    },
    {
      "text": "exactly the same I had always thought",
      "start": 366.4,
      "duration": 4.84
    },
    {
      "text": "Jack gisburn rather a cheap genius great",
      "start": 368.52,
      "duration": 5.88
    },
    {
      "text": "so we have read the data set",
      "start": 371.24,
      "duration": 6.32
    },
    {
      "text": "successfully now let's move next to the",
      "start": 374.4,
      "duration": 5.04
    },
    {
      "text": "portion of how do we split this data",
      "start": 377.56,
      "duration": 5.479
    },
    {
      "text": "into tokens I've added a small note here",
      "start": 379.44,
      "duration": 5.039
    },
    {
      "text": "uh which you'll also see when I share",
      "start": 383.039,
      "duration": 4.081
    },
    {
      "text": "the notebook but when actual large",
      "start": 384.479,
      "duration": 4.72
    },
    {
      "text": "language models are constructed the data",
      "start": 387.12,
      "duration": 3.919
    },
    {
      "text": "set which they process is millions of",
      "start": 389.199,
      "duration": 3.84
    },
    {
      "text": "Articles and hundreds of thousands of",
      "start": 391.039,
      "duration": 4.921
    },
    {
      "text": "books but for educational purposes it's",
      "start": 393.039,
      "duration": 4.6
    },
    {
      "text": "sufficient to work with smaller Tech",
      "start": 395.96,
      "duration": 3.72
    },
    {
      "text": "samples like a single book as I'm",
      "start": 397.639,
      "duration": 4.4
    },
    {
      "text": "showing right now because the hardware",
      "start": 399.68,
      "duration": 5.2
    },
    {
      "text": "capacity uh it requires minimum Hardware",
      "start": 402.039,
      "duration": 4.641
    },
    {
      "text": "capacity and it's also much easier to",
      "start": 404.88,
      "duration": 3.92
    },
    {
      "text": "demonstrate but whatever we are seeing",
      "start": 406.68,
      "duration": 4.919
    },
    {
      "text": "right now can easily be scaled up to uh",
      "start": 408.8,
      "duration": 5.44
    },
    {
      "text": "larger larger and larger data",
      "start": 411.599,
      "duration": 5.201
    },
    {
      "text": "sets now the question which we want to",
      "start": 414.24,
      "duration": 4.84
    },
    {
      "text": "ask is how can we best split this text",
      "start": 416.8,
      "duration": 6.16
    },
    {
      "text": "to obtain a list of tokens so uh let's",
      "start": 419.08,
      "duration": 5.679
    },
    {
      "text": "see for that we are going to use the",
      "start": 422.96,
      "duration": 4.519
    },
    {
      "text": "regular expression library in Python so",
      "start": 424.759,
      "duration": 4.88
    },
    {
      "text": "you can head over to this link which I",
      "start": 427.479,
      "duration": 4.44
    },
    {
      "text": "I'll also share in the YouTube video",
      "start": 429.639,
      "duration": 4.721
    },
    {
      "text": "description and let me show you how this",
      "start": 431.919,
      "duration": 6.081
    },
    {
      "text": "Library can be used so uh you first take",
      "start": 434.36,
      "duration": 5.119
    },
    {
      "text": "some random text I've have chosen a",
      "start": 438.0,
      "duration": 3.52
    },
    {
      "text": "sample text here hello world this is a",
      "start": 439.479,
      "duration": 4.681
    },
    {
      "text": "test and then what we do is that we take",
      "start": 441.52,
      "duration": 4.92
    },
    {
      "text": "this test we take this text and using",
      "start": 444.16,
      "duration": 5.039
    },
    {
      "text": "the regular expression Library we split",
      "start": 446.44,
      "duration": 5.52
    },
    {
      "text": "this text based on wherever white space",
      "start": 449.199,
      "duration": 5.041
    },
    {
      "text": "characters are there so what this",
      "start": 451.96,
      "duration": 4.16
    },
    {
      "text": "command here does is that it reads the",
      "start": 454.24,
      "duration": 4.239
    },
    {
      "text": "text and then it splits the text",
      "start": 456.12,
      "duration": 5.0
    },
    {
      "text": "wherever it encounters white spaces so",
      "start": 458.479,
      "duration": 4.761
    },
    {
      "text": "it scans the text from left so first we",
      "start": 461.12,
      "duration": 4.16
    },
    {
      "text": "scan it from left and after hello and",
      "start": 463.24,
      "duration": 4.32
    },
    {
      "text": "comma is encountered there's a white",
      "start": 465.28,
      "duration": 5.639
    },
    {
      "text": "space here see so then it splits here",
      "start": 467.56,
      "duration": 5.599
    },
    {
      "text": "then it goes on scanning further then",
      "start": 470.919,
      "duration": 4.201
    },
    {
      "text": "after World dot there is a white space",
      "start": 473.159,
      "duration": 4.961
    },
    {
      "text": "here there is a white space here here",
      "start": 475.12,
      "duration": 6.24
    },
    {
      "text": "and here so it creates splits at those",
      "start": 478.12,
      "duration": 5.359
    },
    {
      "text": "locations where white spaces are there",
      "start": 481.36,
      "duration": 4.279
    },
    {
      "text": "and you can see this is the result so",
      "start": 483.479,
      "duration": 4.68
    },
    {
      "text": "the result when we split this is hello",
      "start": 485.639,
      "duration": 5.321
    },
    {
      "text": "then white space then World dot then wh",
      "start": 488.159,
      "duration": 5.6
    },
    {
      "text": "space then this comma then white space",
      "start": 490.96,
      "duration": 5.799
    },
    {
      "text": "then is then white space uh then white",
      "start": 493.759,
      "duration": 4.88
    },
    {
      "text": "space and then",
      "start": 496.759,
      "duration": 5.521
    },
    {
      "text": "test uh do you see the problem with this",
      "start": 498.639,
      "duration": 6.96
    },
    {
      "text": "tokenization why can't we stop over here",
      "start": 502.28,
      "duration": 5.08
    },
    {
      "text": "the reason we can't stop over here is",
      "start": 505.599,
      "duration": 3.521
    },
    {
      "text": "that hello and comma are both in a",
      "start": 507.36,
      "duration": 4.119
    },
    {
      "text": "single token I don't want that I want",
      "start": 509.12,
      "duration": 4.56
    },
    {
      "text": "punctuations to be in a separate token",
      "start": 511.479,
      "duration": 4.161
    },
    {
      "text": "because I want my llm to know which are",
      "start": 513.68,
      "duration": 4.479
    },
    {
      "text": "punctuations which are normal words so I",
      "start": 515.64,
      "duration": 4.759
    },
    {
      "text": "want comma to be a separate token I want",
      "start": 518.159,
      "duration": 4.481
    },
    {
      "text": "full stop to be a separate token",
      "start": 520.399,
      "duration": 4.161
    },
    {
      "text": "similarly if you actually go through the",
      "start": 522.64,
      "duration": 3.639
    },
    {
      "text": "text there are so many other characters",
      "start": 524.56,
      "duration": 3.839
    },
    {
      "text": "like there is this character here I'm",
      "start": 526.279,
      "duration": 4.12
    },
    {
      "text": "sure there's a question mark somewhere I",
      "start": 528.399,
      "duration": 3.721
    },
    {
      "text": "want all of these special characters to",
      "start": 530.399,
      "duration": 4.681
    },
    {
      "text": "be separate tokens split from the text",
      "start": 532.12,
      "duration": 5.36
    },
    {
      "text": "that's the first mistake here the second",
      "start": 535.08,
      "duration": 4.56
    },
    {
      "text": "mistake is that I don't want wh space",
      "start": 537.48,
      "duration": 4.88
    },
    {
      "text": "characters at separate tokens now this",
      "start": 539.64,
      "duration": 6.24
    },
    {
      "text": "is a debatable topic because sometimes",
      "start": 542.36,
      "duration": 5.36
    },
    {
      "text": "it's good to have wh space characters",
      "start": 545.88,
      "duration": 3.519
    },
    {
      "text": "for example if you're writing python",
      "start": 547.72,
      "duration": 4.4
    },
    {
      "text": "code indentations are important right wh",
      "start": 549.399,
      "duration": 4.68
    },
    {
      "text": "space characters are important in that",
      "start": 552.12,
      "duration": 3.52
    },
    {
      "text": "case it's good to have wh space",
      "start": 554.079,
      "duration": 3.681
    },
    {
      "text": "characters as separate tokens but for",
      "start": 555.64,
      "duration": 4.04
    },
    {
      "text": "the sake of demonstration I don't want",
      "start": 557.76,
      "duration": 3.96
    },
    {
      "text": "wh space characters so there are two",
      "start": 559.68,
      "duration": 5.04
    },
    {
      "text": "things I will modify here uh I along",
      "start": 561.72,
      "duration": 5.48
    },
    {
      "text": "with splitting here we only split where",
      "start": 564.72,
      "duration": 4.32
    },
    {
      "text": "white spaces are there right I also want",
      "start": 567.2,
      "duration": 4.079
    },
    {
      "text": "to split wherever different characters",
      "start": 569.04,
      "duration": 4.239
    },
    {
      "text": "are encountered like quotation",
      "start": 571.279,
      "duration": 4.481
    },
    {
      "text": "exclamation mark question mark Etc and",
      "start": 573.279,
      "duration": 4.881
    },
    {
      "text": "then I want to remove the white spaces",
      "start": 575.76,
      "duration": 4.44
    },
    {
      "text": "we'll do exactly this right",
      "start": 578.16,
      "duration": 5.44
    },
    {
      "text": "now so uh what I'll be doing right now",
      "start": 580.2,
      "duration": 6.12
    },
    {
      "text": "is that I'll take this text then what",
      "start": 583.6,
      "duration": 4.799
    },
    {
      "text": "I'll be doing is that I'll split but",
      "start": 586.32,
      "duration": 4.519
    },
    {
      "text": "I'll split on all of these characters so",
      "start": 588.399,
      "duration": 4.521
    },
    {
      "text": "earlier we only split on the white space",
      "start": 590.839,
      "duration": 4.601
    },
    {
      "text": "character right but now I'm saying here",
      "start": 592.92,
      "duration": 4.56
    },
    {
      "text": "that whenever you encounter a comma or a",
      "start": 595.44,
      "duration": 4.839
    },
    {
      "text": "full stop or a colon or a semi colon or",
      "start": 597.48,
      "duration": 5.4
    },
    {
      "text": "a question mark or an underscore or an",
      "start": 600.279,
      "duration": 5.721
    },
    {
      "text": "exclamation inverted comma bracket slash",
      "start": 602.88,
      "duration": 5.0
    },
    {
      "text": "you split there that's what I'm",
      "start": 606.0,
      "duration": 3.56
    },
    {
      "text": "instructing this regular expression",
      "start": 607.88,
      "duration": 3.84
    },
    {
      "text": "library to do so this is the first thing",
      "start": 609.56,
      "duration": 4.04
    },
    {
      "text": "I'll be doing the second thing I'll be",
      "start": 611.72,
      "duration": 4.28
    },
    {
      "text": "doing is removing the white spaces so",
      "start": 613.6,
      "duration": 4.239
    },
    {
      "text": "what this line of code does is that it",
      "start": 616.0,
      "duration": 3.68
    },
    {
      "text": "scans through the result so it scans",
      "start": 617.839,
      "duration": 4.881
    },
    {
      "text": "through the tokens and it results a True",
      "start": 619.68,
      "duration": 5.36
    },
    {
      "text": "Value it outputs a True Value only if",
      "start": 622.72,
      "duration": 4.64
    },
    {
      "text": "white space is not there so wherever",
      "start": 625.04,
      "duration": 4.28
    },
    {
      "text": "white spaces are there it outputs a",
      "start": 627.36,
      "duration": 5.039
    },
    {
      "text": "false value so this item. strip item.",
      "start": 629.32,
      "duration": 5.68
    },
    {
      "text": "strip is true only when wh space is not",
      "start": 632.399,
      "duration": 5.521
    },
    {
      "text": "there item. strip is false when a white",
      "start": 635.0,
      "duration": 5.88
    },
    {
      "text": "space is encountered so when this list",
      "start": 637.92,
      "duration": 5.08
    },
    {
      "text": "of tokens is passed through this item.",
      "start": 640.88,
      "duration": 4.519
    },
    {
      "text": "strip function over here only those",
      "start": 643.0,
      "duration": 4.24
    },
    {
      "text": "tokens which are not a white space will",
      "start": 645.399,
      "duration": 4.521
    },
    {
      "text": "be returned so let's print out the",
      "start": 647.24,
      "duration": 4.399
    },
    {
      "text": "result and let's see the result so here",
      "start": 649.92,
      "duration": 4.12
    },
    {
      "text": "you can see Hello is a separate token",
      "start": 651.639,
      "duration": 4.401
    },
    {
      "text": "comma is a separate token world is a",
      "start": 654.04,
      "duration": 4.76
    },
    {
      "text": "separate token dot is a separate token",
      "start": 656.04,
      "duration": 3.919
    },
    {
      "text": "awesome right",
      "start": 658.8,
      "duration": 5.479
    },
    {
      "text": "right so uh yeah so hello is a separate",
      "start": 659.959,
      "duration": 7.481
    },
    {
      "text": "token comma World dot is this this Dash",
      "start": 664.279,
      "duration": 6.641
    },
    {
      "text": "Dash is also a separate token awesome so",
      "start": 667.44,
      "duration": 5.6
    },
    {
      "text": "now all of these separate characters are",
      "start": 670.92,
      "duration": 4.4
    },
    {
      "text": "individual tokens and we have also",
      "start": 673.04,
      "duration": 3.799
    },
    {
      "text": "removed the white spaces see white",
      "start": 675.32,
      "duration": 3.28
    },
    {
      "text": "spaces are not different tokens over",
      "start": 676.839,
      "duration": 4.12
    },
    {
      "text": "here so we have essentially taken a",
      "start": 678.6,
      "duration": 4.4
    },
    {
      "text": "small sentence and we have converted it",
      "start": 680.959,
      "duration": 4.641
    },
    {
      "text": "into individual tokens now we'll apply",
      "start": 683.0,
      "duration": 5.519
    },
    {
      "text": "these same two commands on this entire",
      "start": 685.6,
      "duration": 5.84
    },
    {
      "text": "uh data set which we are",
      "start": 688.519,
      "duration": 5.76
    },
    {
      "text": "reading so exactly the same commands",
      "start": 691.44,
      "duration": 4.76
    },
    {
      "text": "will be used we'll Define a variable",
      "start": 694.279,
      "duration": 3.921
    },
    {
      "text": "called pre-processed which is a list",
      "start": 696.2,
      "duration": 4.04
    },
    {
      "text": "which contains all the tokens of the",
      "start": 698.2,
      "duration": 4.52
    },
    {
      "text": "entire essay so what we are first saying",
      "start": 700.24,
      "duration": 4.88
    },
    {
      "text": "is that you scan through this essay scan",
      "start": 702.72,
      "duration": 4.04
    },
    {
      "text": "through this essay and split wherever",
      "start": 705.12,
      "duration": 3.92
    },
    {
      "text": "these special characters are encountered",
      "start": 706.76,
      "duration": 3.96
    },
    {
      "text": "and also split where white space is",
      "start": 709.04,
      "duration": 5.239
    },
    {
      "text": "encountered in the second uh sentence we",
      "start": 710.72,
      "duration": 5.559
    },
    {
      "text": "are saying that you have take the list",
      "start": 714.279,
      "duration": 3.68
    },
    {
      "text": "which is called as pre-processed right",
      "start": 716.279,
      "duration": 4.441
    },
    {
      "text": "now uh retain all the tokens in this",
      "start": 717.959,
      "duration": 4.801
    },
    {
      "text": "list but get rid of all the white space",
      "start": 720.72,
      "duration": 4.799
    },
    {
      "text": "characters we don't need those as tokens",
      "start": 722.76,
      "duration": 4.16
    },
    {
      "text": "that's what we are saying in this second",
      "start": 725.519,
      "duration": 3.161
    },
    {
      "text": "sentence right now great so",
      "start": 726.92,
      "duration": 3.76
    },
    {
      "text": "pre-processed is the list which contains",
      "start": 728.68,
      "duration": 5.159
    },
    {
      "text": "the tokens which the essay has been",
      "start": 730.68,
      "duration": 6.159
    },
    {
      "text": "broken down into awesome just to make",
      "start": 733.839,
      "duration": 4.641
    },
    {
      "text": "sure we have done the tokenization",
      "start": 736.839,
      "duration": 3.68
    },
    {
      "text": "correctly what I'm going to do is I'm",
      "start": 738.48,
      "duration": 4.64
    },
    {
      "text": "going to print the first 30 tokens in",
      "start": 740.519,
      "duration": 4.88
    },
    {
      "text": "this list so here you can see the first",
      "start": 743.12,
      "duration": 6.399
    },
    {
      "text": "30 tokens are I had always thought Jack",
      "start": 745.399,
      "duration": 7.12
    },
    {
      "text": "gisburn rather so here you can see I had",
      "start": 749.519,
      "duration": 5.0
    },
    {
      "text": "always thought Jack gburn it has been",
      "start": 752.519,
      "duration": 4.281
    },
    {
      "text": "broken down into individual words but",
      "start": 754.519,
      "duration": 4.081
    },
    {
      "text": "even these characters are separate",
      "start": 756.8,
      "duration": 4.44
    },
    {
      "text": "tokens so if you see over here this Dash",
      "start": 758.6,
      "duration": 5.599
    },
    {
      "text": "Dash is a separate token so tokenization",
      "start": 761.24,
      "duration": 4.52
    },
    {
      "text": "is actually pretty simple here we are",
      "start": 764.199,
      "duration": 3.681
    },
    {
      "text": "looking at word based tokenization where",
      "start": 765.76,
      "duration": 4.12
    },
    {
      "text": "every token is a separate word or a",
      "start": 767.88,
      "duration": 4.0
    },
    {
      "text": "character and when I say character I",
      "start": 769.88,
      "duration": 4.56
    },
    {
      "text": "mean special symbols like uh comma",
      "start": 771.88,
      "duration": 5.72
    },
    {
      "text": "exclamation mark full stop um quotation",
      "start": 774.44,
      "duration": 6.24
    },
    {
      "text": "Etc awesome so here you can also print",
      "start": 777.6,
      "duration": 4.72
    },
    {
      "text": "the length of this list which gives you",
      "start": 780.68,
      "duration": 4.0
    },
    {
      "text": "an idea of how many tokens are there so",
      "start": 782.32,
      "duration": 3.92
    },
    {
      "text": "when you print it out you'll see that we",
      "start": 784.68,
      "duration": 5.399
    },
    {
      "text": "have 469 zero tokens over here um and",
      "start": 786.24,
      "duration": 5.599
    },
    {
      "text": "there may be duplications also because",
      "start": 790.079,
      "duration": 3.481
    },
    {
      "text": "we have not gotten rid of duplicates",
      "start": 791.839,
      "duration": 3.321
    },
    {
      "text": "here that's fine I'm just printing out",
      "start": 793.56,
      "duration": 4.639
    },
    {
      "text": "the total length of tokens it's 4690",
      "start": 795.16,
      "duration": 6.44
    },
    {
      "text": "great uh so now we have the entire data",
      "start": 798.199,
      "duration": 5.161
    },
    {
      "text": "set which is an essay we have converted",
      "start": 801.6,
      "duration": 4.76
    },
    {
      "text": "it into a list of tokens but uh we need",
      "start": 803.36,
      "duration": 4.64
    },
    {
      "text": "numerical representations right",
      "start": 806.36,
      "duration": 3.36
    },
    {
      "text": "computers doesn't don't don't understand",
      "start": 808.0,
      "duration": 4.36
    },
    {
      "text": "words so then we need to convert these",
      "start": 809.72,
      "duration": 5.04
    },
    {
      "text": "tokens into token IDs that's the next",
      "start": 812.36,
      "duration": 4.2
    },
    {
      "text": "step all the tokens which we have",
      "start": 814.76,
      "duration": 4.079
    },
    {
      "text": "generated need to be converted into",
      "start": 816.56,
      "duration": 4.719
    },
    {
      "text": "token IDs and there is a specific way to",
      "start": 818.839,
      "duration": 5.12
    },
    {
      "text": "do this first what you do is you take",
      "start": 821.279,
      "duration": 5.161
    },
    {
      "text": "all the tokens you remove the duplicates",
      "start": 823.959,
      "duration": 4.12
    },
    {
      "text": "and you arrange the tokens in an",
      "start": 826.44,
      "duration": 4.04
    },
    {
      "text": "ascending or an alphabetical order so",
      "start": 828.079,
      "duration": 4.241
    },
    {
      "text": "let's say if the data set was the quick",
      "start": 830.48,
      "duration": 4.52
    },
    {
      "text": "brown fox jumps over the lazy dog you",
      "start": 832.32,
      "duration": 4.68
    },
    {
      "text": "break it down into tokens let's say the",
      "start": 835.0,
      "duration": 4.44
    },
    {
      "text": "quick brown fox and you arrange them all",
      "start": 837.0,
      "duration": 5.24
    },
    {
      "text": "in alphabetical order so brown dog fox",
      "start": 839.44,
      "duration": 6.399
    },
    {
      "text": "jumps lazy or quick the and then to each",
      "start": 842.24,
      "duration": 6.92
    },
    {
      "text": "of the tokens you assign a token ID",
      "start": 845.839,
      "duration": 5.0
    },
    {
      "text": "since they are in alphabetical order the",
      "start": 849.16,
      "duration": 4.0
    },
    {
      "text": "first token will have token ID zero the",
      "start": 850.839,
      "duration": 4.841
    },
    {
      "text": "second token will have token ID 1 Etc",
      "start": 853.16,
      "duration": 4.64
    },
    {
      "text": "dot up till the end the last unique",
      "start": 855.68,
      "duration": 4.2
    },
    {
      "text": "token will have the token ID 7 in this",
      "start": 857.8,
      "duration": 5.0
    },
    {
      "text": "case so basically this is called as a",
      "start": 859.88,
      "duration": 6.199
    },
    {
      "text": "vocabulary the vocabulary is essentially",
      "start": 862.8,
      "duration": 6.68
    },
    {
      "text": "a dictionary where every unique token is",
      "start": 866.079,
      "duration": 5.76
    },
    {
      "text": "mapped to a unique integer which is",
      "start": 869.48,
      "duration": 5.2
    },
    {
      "text": "called as the token ID for all practical",
      "start": 871.839,
      "duration": 5.281
    },
    {
      "text": "purposes the computer only cares or the",
      "start": 874.68,
      "duration": 4.519
    },
    {
      "text": "llm rather only cares about the token",
      "start": 877.12,
      "duration": 5.32
    },
    {
      "text": "IDs not the tokens because the token IDs",
      "start": 879.199,
      "duration": 5.521
    },
    {
      "text": "are have a numerical",
      "start": 882.44,
      "duration": 4.399
    },
    {
      "text": "representation so what I showed you here",
      "start": 884.72,
      "duration": 4.0
    },
    {
      "text": "is called constructing the vocabulary",
      "start": 886.839,
      "duration": 3.521
    },
    {
      "text": "and the vocabulary is a dictionary which",
      "start": 888.72,
      "duration": 4.679
    },
    {
      "text": "maps The Tokens to token IDs this is",
      "start": 890.36,
      "duration": 4.88
    },
    {
      "text": "exactly what we'll be doing in code as",
      "start": 893.399,
      "duration": 3.721
    },
    {
      "text": "well so we have the pre-processed right",
      "start": 895.24,
      "duration": 4.08
    },
    {
      "text": "which is a list of all the tokens first",
      "start": 897.12,
      "duration": 5.6
    },
    {
      "text": "we need to uh convert this into a set so",
      "start": 899.32,
      "duration": 6.439
    },
    {
      "text": "we get rid of all the duplicates also",
      "start": 902.72,
      "duration": 5.72
    },
    {
      "text": "and then we sort it in asscending order",
      "start": 905.759,
      "duration": 4.921
    },
    {
      "text": "so you take all the tokens and you sort",
      "start": 908.44,
      "duration": 5.28
    },
    {
      "text": "them in ascending order and uh you can",
      "start": 910.68,
      "duration": 4.839
    },
    {
      "text": "also print out the vocabulary size so",
      "start": 913.72,
      "duration": 3.16
    },
    {
      "text": "here you will see that is",
      "start": 915.519,
      "duration": 5.0
    },
    {
      "text": "1130 now after you uh sort all the",
      "start": 916.88,
      "duration": 5.44
    },
    {
      "text": "tokens in ascending order you have to",
      "start": 920.519,
      "duration": 4.24
    },
    {
      "text": "assign a token ID which is an integer to",
      "start": 922.32,
      "duration": 4.759
    },
    {
      "text": "each token so you can do that that",
      "start": 924.759,
      "duration": 4.361
    },
    {
      "text": "through enumerate so when you use the",
      "start": 927.079,
      "duration": 4.041
    },
    {
      "text": "enumerate function in Python what it",
      "start": 929.12,
      "duration": 4.279
    },
    {
      "text": "does is that it creates integers and",
      "start": 931.12,
      "duration": 5.0
    },
    {
      "text": "tokens so then you create a dictionary",
      "start": 933.399,
      "duration": 6.081
    },
    {
      "text": "which is called as a vocabulary and uh",
      "start": 936.12,
      "duration": 5.12
    },
    {
      "text": "for each token in the dictionary there",
      "start": 939.48,
      "duration": 3.52
    },
    {
      "text": "is essentially an integer associated",
      "start": 941.24,
      "duration": 4.36
    },
    {
      "text": "with it and enumerate anyway lists down",
      "start": 943.0,
      "duration": 5.0
    },
    {
      "text": "all the integers in Alpha in ascending",
      "start": 945.6,
      "duration": 4.4
    },
    {
      "text": "order so the first token will be",
      "start": 948.0,
      "duration": 4.36
    },
    {
      "text": "assigned a token ID of zero the second",
      "start": 950.0,
      "duration": 4.48
    },
    {
      "text": "token will be assign the token ID of one",
      "start": 952.36,
      "duration": 4.76
    },
    {
      "text": "Etc so what we can do over here is we",
      "start": 954.48,
      "duration": 4.76
    },
    {
      "text": "can actually print out the items in the",
      "start": 957.12,
      "duration": 5.48
    },
    {
      "text": "vocabulary and then uh see the first 50",
      "start": 959.24,
      "duration": 5.599
    },
    {
      "text": "items so here you can see first all",
      "start": 962.6,
      "duration": 4.479
    },
    {
      "text": "these uh special characters are there",
      "start": 964.839,
      "duration": 5.36
    },
    {
      "text": "who have the token ID 012 Etc and then",
      "start": 967.079,
      "duration": 6.161
    },
    {
      "text": "all the separate tokens are associated",
      "start": 970.199,
      "duration": 6.0
    },
    {
      "text": "with a token ID so we can see that for",
      "start": 973.24,
      "duration": 6.279
    },
    {
      "text": "has a token ID of 35 has has a token ID",
      "start": 976.199,
      "duration": 7.281
    },
    {
      "text": "of 47 Etc so it's the vocabulary is",
      "start": 979.519,
      "duration": 5.921
    },
    {
      "text": "essentially a dictionary so if someone",
      "start": 983.48,
      "duration": 4.359
    },
    {
      "text": "gives you a token ID you can see what",
      "start": 985.44,
      "duration": 4.399
    },
    {
      "text": "the corresponding token is is if someone",
      "start": 987.839,
      "duration": 3.721
    },
    {
      "text": "gives you a token you can see what the",
      "start": 989.839,
      "duration": 4.521
    },
    {
      "text": "corresponding token ID is awesome right",
      "start": 991.56,
      "duration": 4.92
    },
    {
      "text": "so that's the second step we have",
      "start": 994.36,
      "duration": 5.36
    },
    {
      "text": "converted the tokens into token IDs the",
      "start": 996.48,
      "duration": 6.039
    },
    {
      "text": "third step what we have to do is that",
      "start": 999.72,
      "duration": 6.119
    },
    {
      "text": "uh uh we have to implement a tokenizer",
      "start": 1002.519,
      "duration": 5.68
    },
    {
      "text": "class in Python what this tokenizer",
      "start": 1005.839,
      "duration": 4.521
    },
    {
      "text": "class will do is that it will take the",
      "start": 1008.199,
      "duration": 5.721
    },
    {
      "text": "vocabulary and uh it will have an encode",
      "start": 1010.36,
      "duration": 5.919
    },
    {
      "text": "method and it will have a decode method",
      "start": 1013.92,
      "duration": 4.32
    },
    {
      "text": "what the encode method will do is that",
      "start": 1016.279,
      "duration": 4.0
    },
    {
      "text": "for any given word it will convert the",
      "start": 1018.24,
      "duration": 4.44
    },
    {
      "text": "word into a token ID what the decode",
      "start": 1020.279,
      "duration": 4.16
    },
    {
      "text": "method will do is that for any given",
      "start": 1022.68,
      "duration": 3.8
    },
    {
      "text": "token ID it will convert it back into",
      "start": 1024.439,
      "duration": 5.0
    },
    {
      "text": "the token so let me illustrate this for",
      "start": 1026.48,
      "duration": 6.199
    },
    {
      "text": "you over here so after we uh saw the",
      "start": 1029.439,
      "duration": 5.24
    },
    {
      "text": "concept of token IDs we now need to",
      "start": 1032.679,
      "duration": 5.321
    },
    {
      "text": "implement a tokenizer class in Python so",
      "start": 1034.679,
      "duration": 4.961
    },
    {
      "text": "this to whenever an instance of the",
      "start": 1038.0,
      "duration": 3.72
    },
    {
      "text": "tokenizer class is created we have two",
      "start": 1039.64,
      "duration": 5.0
    },
    {
      "text": "methods we have the encode method which",
      "start": 1041.72,
      "duration": 5.68
    },
    {
      "text": "is tokenizer do encode what this method",
      "start": 1044.64,
      "duration": 4.6
    },
    {
      "text": "will do is pretty simple it we will just",
      "start": 1047.4,
      "duration": 3.84
    },
    {
      "text": "take the text break it down into tokens",
      "start": 1049.24,
      "duration": 4.679
    },
    {
      "text": "and assign a token ID to each token the",
      "start": 1051.24,
      "duration": 5.12
    },
    {
      "text": "second method is the decode method what",
      "start": 1053.919,
      "duration": 4.721
    },
    {
      "text": "this method will do is the exact reverse",
      "start": 1056.36,
      "duration": 4.4
    },
    {
      "text": "it will essentially take token IDs",
      "start": 1058.64,
      "duration": 4.2
    },
    {
      "text": "convert them into tokenized text and",
      "start": 1060.76,
      "duration": 4.6
    },
    {
      "text": "then generate the original sample text",
      "start": 1062.84,
      "duration": 3.839
    },
    {
      "text": "from these token",
      "start": 1065.36,
      "duration": 3.559
    },
    {
      "text": "IDs so that's what we are going to",
      "start": 1066.679,
      "duration": 4.081
    },
    {
      "text": "implement now so you can see that this",
      "start": 1068.919,
      "duration": 4.681
    },
    {
      "text": "is the tokenizer class and whenever an",
      "start": 1070.76,
      "duration": 4.72
    },
    {
      "text": "instance of the class is created we need",
      "start": 1073.6,
      "duration": 3.64
    },
    {
      "text": "to first pass in the vocabulary which",
      "start": 1075.48,
      "duration": 3.6
    },
    {
      "text": "you already saw and then then we",
      "start": 1077.24,
      "duration": 3.919
    },
    {
      "text": "maintain two dictionaries one is the",
      "start": 1079.08,
      "duration": 3.92
    },
    {
      "text": "dictionary string to int which is of",
      "start": 1081.159,
      "duration": 3.201
    },
    {
      "text": "course the vocabulary because the",
      "start": 1083.0,
      "duration": 2.88
    },
    {
      "text": "vocabulary is just the dictionary which",
      "start": 1084.36,
      "duration": 4.04
    },
    {
      "text": "converts the string to an integer and",
      "start": 1085.88,
      "duration": 3.919
    },
    {
      "text": "then we have to maintain a reverse",
      "start": 1088.4,
      "duration": 4.159
    },
    {
      "text": "dictionary which is integer to string so",
      "start": 1089.799,
      "duration": 4.641
    },
    {
      "text": "what this says is that for every string",
      "start": 1092.559,
      "duration": 4.48
    },
    {
      "text": "integer pair in the vocabulary this",
      "start": 1094.44,
      "duration": 4.119
    },
    {
      "text": "dictionary will be the reverse so this",
      "start": 1097.039,
      "duration": 3.441
    },
    {
      "text": "dictionary will be integer mapping to",
      "start": 1098.559,
      "duration": 4.441
    },
    {
      "text": "string so whenever an integer is given",
      "start": 1100.48,
      "duration": 4.319
    },
    {
      "text": "as the lookup this dictionary will",
      "start": 1103.0,
      "duration": 4.559
    },
    {
      "text": "return this a given string so that's the",
      "start": 1104.799,
      "duration": 5.36
    },
    {
      "text": "decode part so then we Define two",
      "start": 1107.559,
      "duration": 5.281
    },
    {
      "text": "methods the first is encode the encode",
      "start": 1110.159,
      "duration": 4.4
    },
    {
      "text": "will have exactly the same things what",
      "start": 1112.84,
      "duration": 4.48
    },
    {
      "text": "we saw here so here we saw the First",
      "start": 1114.559,
      "duration": 4.561
    },
    {
      "text": "Command was splitting the text based on",
      "start": 1117.32,
      "duration": 3.4
    },
    {
      "text": "these characters the second was to get",
      "start": 1119.12,
      "duration": 3.559
    },
    {
      "text": "rid of white spaces that is how we",
      "start": 1120.72,
      "duration": 4.199
    },
    {
      "text": "created tokens right so the encode",
      "start": 1122.679,
      "duration": 4.12
    },
    {
      "text": "method which I'm showing you has exactly",
      "start": 1124.919,
      "duration": 3.721
    },
    {
      "text": "the same thing we first have this",
      "start": 1126.799,
      "duration": 4.281
    },
    {
      "text": "pre-processed list and we have the",
      "start": 1128.64,
      "duration": 4.279
    },
    {
      "text": "whatever text is given as an input to",
      "start": 1131.08,
      "duration": 4.16
    },
    {
      "text": "this encode method we'll first split it",
      "start": 1132.919,
      "duration": 4.041
    },
    {
      "text": "based on these characters we'll get rid",
      "start": 1135.24,
      "duration": 4.12
    },
    {
      "text": "of the white spaces then whatever",
      "start": 1136.96,
      "duration": 4.32
    },
    {
      "text": "remains for all the strings which remain",
      "start": 1139.36,
      "duration": 4.36
    },
    {
      "text": "we'll see the corresponding integer in",
      "start": 1141.28,
      "duration": 5.2
    },
    {
      "text": "the vocabulary and then this method will",
      "start": 1143.72,
      "duration": 6.079
    },
    {
      "text": "return a list of token IDs that's it the",
      "start": 1146.48,
      "duration": 4.96
    },
    {
      "text": "decode method will do exactly the",
      "start": 1149.799,
      "duration": 4.721
    },
    {
      "text": "reverse it will take as input a list of",
      "start": 1151.44,
      "duration": 5.76
    },
    {
      "text": "token IDs and then what it will do is",
      "start": 1154.52,
      "duration": 3.92
    },
    {
      "text": "that it will",
      "start": 1157.2,
      "duration": 3.719
    },
    {
      "text": "convert it will use this integer to",
      "start": 1158.44,
      "duration": 4.44
    },
    {
      "text": "string dictionary and based on the token",
      "start": 1160.919,
      "duration": 3.88
    },
    {
      "text": "IDs it will convert these integers into",
      "start": 1162.88,
      "duration": 3.76
    },
    {
      "text": "Strings and then it will join the",
      "start": 1164.799,
      "duration": 4.12
    },
    {
      "text": "different strings together there is one",
      "start": 1166.64,
      "duration": 4.84
    },
    {
      "text": "more subtlety here that when we join uh",
      "start": 1168.919,
      "duration": 4.281
    },
    {
      "text": "strings like this there is usually a",
      "start": 1171.48,
      "duration": 4.28
    },
    {
      "text": "space before punctuations and we need to",
      "start": 1173.2,
      "duration": 4.359
    },
    {
      "text": "get rid of that space so that's just the",
      "start": 1175.76,
      "duration": 4.12
    },
    {
      "text": "second sentence over here so this is the",
      "start": 1177.559,
      "duration": 4.48
    },
    {
      "text": "decode method which Returns the text",
      "start": 1179.88,
      "duration": 4.919
    },
    {
      "text": "great so this is how a tokenizer class",
      "start": 1182.039,
      "duration": 4.601
    },
    {
      "text": "is created and once we create a",
      "start": 1184.799,
      "duration": 3.681
    },
    {
      "text": "tokenizer class we can actually Define",
      "start": 1186.64,
      "duration": 3.8
    },
    {
      "text": "an instance of this class so I'm",
      "start": 1188.48,
      "duration": 3.48
    },
    {
      "text": "creating an instance of this class",
      "start": 1190.44,
      "duration": 3.239
    },
    {
      "text": "called tokenizer and I pass in the",
      "start": 1191.96,
      "duration": 3.88
    },
    {
      "text": "vocabulary which I already had defined",
      "start": 1193.679,
      "duration": 5.88
    },
    {
      "text": "so if you remember this is my vocabulary",
      "start": 1195.84,
      "duration": 6.0
    },
    {
      "text": "based on the datas based on this data",
      "start": 1199.559,
      "duration": 4.681
    },
    {
      "text": "set so now what I'm doing is I'm",
      "start": 1201.84,
      "duration": 4.56
    },
    {
      "text": "creating an instance of this simple",
      "start": 1204.24,
      "duration": 4.72
    },
    {
      "text": "tokenizer version one class and passing",
      "start": 1206.4,
      "duration": 4.88
    },
    {
      "text": "in the vocabulary and now once an",
      "start": 1208.96,
      "duration": 4.32
    },
    {
      "text": "instance is created I can access two",
      "start": 1211.28,
      "duration": 4.44
    },
    {
      "text": "methods encode and decode but for the",
      "start": 1213.28,
      "duration": 4.84
    },
    {
      "text": "encode method I need some text right so",
      "start": 1215.72,
      "duration": 4.52
    },
    {
      "text": "let's give it some text so it's the last",
      "start": 1218.12,
      "duration": 4.24
    },
    {
      "text": "he painted you know Mrs gisburn said",
      "start": 1220.24,
      "duration": 4.2
    },
    {
      "text": "with pardonable pride let's see where",
      "start": 1222.36,
      "duration": 3.28
    },
    {
      "text": "this text",
      "start": 1224.44,
      "duration": 5.08
    },
    {
      "text": "appears so pardonable yeah so I'm giving",
      "start": 1225.64,
      "duration": 6.159
    },
    {
      "text": "this text it's the last he painted you",
      "start": 1229.52,
      "duration": 5.399
    },
    {
      "text": "know Mrs gban said with pardonable pride",
      "start": 1231.799,
      "duration": 5.441
    },
    {
      "text": "uh so this is the text which I'm giving",
      "start": 1234.919,
      "duration": 4.64
    },
    {
      "text": "as an input to the tokenizer class and",
      "start": 1237.24,
      "duration": 4.439
    },
    {
      "text": "let's see if it can encode it so then I",
      "start": 1239.559,
      "duration": 4.521
    },
    {
      "text": "use the encode method for this text when",
      "start": 1241.679,
      "duration": 4.161
    },
    {
      "text": "this text is passed what will happen is",
      "start": 1244.08,
      "duration": 3.8
    },
    {
      "text": "that first it will split first it will",
      "start": 1245.84,
      "duration": 4.319
    },
    {
      "text": "split based on the characters remove the",
      "start": 1247.88,
      "duration": 4.96
    },
    {
      "text": "white spaces and convert the tokens into",
      "start": 1250.159,
      "duration": 5.481
    },
    {
      "text": "token IDs and when you print the IDS you",
      "start": 1252.84,
      "duration": 4.719
    },
    {
      "text": "will see that these are the token IDs",
      "start": 1255.64,
      "duration": 4.32
    },
    {
      "text": "for each of these tokens tokens so",
      "start": 1257.559,
      "duration": 4.721
    },
    {
      "text": "basically its will have one token ID the",
      "start": 1259.96,
      "duration": 4.079
    },
    {
      "text": "will have one token ID last will have",
      "start": 1262.28,
      "duration": 3.8
    },
    {
      "text": "one token ID and these token IDs are",
      "start": 1264.039,
      "duration": 5.081
    },
    {
      "text": "printed over here now we can also test",
      "start": 1266.08,
      "duration": 4.959
    },
    {
      "text": "the decode method so you can pass in",
      "start": 1269.12,
      "duration": 4.32
    },
    {
      "text": "these IDs back to the decode method and",
      "start": 1271.039,
      "duration": 4.52
    },
    {
      "text": "you'll see we recover the same sentence",
      "start": 1273.44,
      "duration": 3.92
    },
    {
      "text": "it's the last he painted you know Mrs",
      "start": 1275.559,
      "duration": 4.201
    },
    {
      "text": "gban said with pardonable pride awesome",
      "start": 1277.36,
      "duration": 4.559
    },
    {
      "text": "right uh so we have successfully",
      "start": 1279.76,
      "duration": 4.039
    },
    {
      "text": "implemented an encoder method and",
      "start": 1281.919,
      "duration": 3.961
    },
    {
      "text": "decoder method and we have also created",
      "start": 1283.799,
      "duration": 4.921
    },
    {
      "text": "the tokenizer class in Python so you you",
      "start": 1285.88,
      "duration": 5.039
    },
    {
      "text": "must be thinking this is great what are",
      "start": 1288.72,
      "duration": 4.839
    },
    {
      "text": "the disadvantages here why doesn't GPT",
      "start": 1290.919,
      "duration": 4.841
    },
    {
      "text": "models use this kind of a world-based",
      "start": 1293.559,
      "duration": 4.12
    },
    {
      "text": "tokenizer well there is a major",
      "start": 1295.76,
      "duration": 4.6
    },
    {
      "text": "disadvantage if you come across a text",
      "start": 1297.679,
      "duration": 4.521
    },
    {
      "text": "let's say the text is hello do you like",
      "start": 1300.36,
      "duration": 5.0
    },
    {
      "text": "T and if you try to encode this text",
      "start": 1302.2,
      "duration": 6.24
    },
    {
      "text": "with a tokenizer you'll get an error can",
      "start": 1305.36,
      "duration": 4.84
    },
    {
      "text": "you try to think why you have got an",
      "start": 1308.44,
      "duration": 3.719
    },
    {
      "text": "error here you can pause this video for",
      "start": 1310.2,
      "duration": 4.079
    },
    {
      "text": "some time to think about",
      "start": 1312.159,
      "duration": 4.64
    },
    {
      "text": "it the reason you have got an error here",
      "start": 1314.279,
      "duration": 4.601
    },
    {
      "text": "is that these some of these words were",
      "start": 1316.799,
      "duration": 4.641
    },
    {
      "text": "not present in the vocabulary so the",
      "start": 1318.88,
      "duration": 4.399
    },
    {
      "text": "word hello was not present in the",
      "start": 1321.44,
      "duration": 3.76
    },
    {
      "text": "verdict short story at all hence it's",
      "start": 1323.279,
      "duration": 4.041
    },
    {
      "text": "not contained in the vocabulary so if",
      "start": 1325.2,
      "duration": 4.64
    },
    {
      "text": "you contrl f hello here you'll not see",
      "start": 1327.32,
      "duration": 3.599
    },
    {
      "text": "it over",
      "start": 1329.84,
      "duration": 3.439
    },
    {
      "text": "here so this is the main problem with",
      "start": 1330.919,
      "duration": 4.721
    },
    {
      "text": "word based tokenizer it does not know",
      "start": 1333.279,
      "duration": 5.361
    },
    {
      "text": "how to deal with out of vocabulary words",
      "start": 1335.64,
      "duration": 4.84
    },
    {
      "text": "so if you give it if you give it some",
      "start": 1338.64,
      "duration": 3.6
    },
    {
      "text": "text to encode which is not present in",
      "start": 1340.48,
      "duration": 4.4
    },
    {
      "text": "the vocabulary it returns an error",
      "start": 1342.24,
      "duration": 4.2
    },
    {
      "text": "because hello is not present in the",
      "start": 1344.88,
      "duration": 4.2
    },
    {
      "text": "vocabulary so how do I encode it",
      "start": 1346.44,
      "duration": 4.64
    },
    {
      "text": "that's one of the major problems with",
      "start": 1349.08,
      "duration": 5.839
    },
    {
      "text": "the world-based encoding scheme and",
      "start": 1351.08,
      "duration": 5.56
    },
    {
      "text": "that's why we have to add special",
      "start": 1354.919,
      "duration": 3.081
    },
    {
      "text": "context",
      "start": 1356.64,
      "duration": 2.919
    },
    {
      "text": "tokens",
      "start": 1358.0,
      "duration": 3.919
    },
    {
      "text": "so until now we have implemented a",
      "start": 1359.559,
      "duration": 4.641
    },
    {
      "text": "simple tokenizer right but how to deal",
      "start": 1361.919,
      "duration": 5.401
    },
    {
      "text": "with tokens which are not known a simple",
      "start": 1364.2,
      "duration": 5.04
    },
    {
      "text": "way to do this is to augment the",
      "start": 1367.32,
      "duration": 4.479
    },
    {
      "text": "existing vocabulary with two tokens the",
      "start": 1369.24,
      "duration": 4.96
    },
    {
      "text": "first token will be an unknown token so",
      "start": 1371.799,
      "duration": 4.561
    },
    {
      "text": "if a word comes in the input text which",
      "start": 1374.2,
      "duration": 4.599
    },
    {
      "text": "we do not know uh it will be mapped to",
      "start": 1376.36,
      "duration": 4.88
    },
    {
      "text": "this unknown and then the corresponding",
      "start": 1378.799,
      "duration": 4.801
    },
    {
      "text": "token ID the second token which is very",
      "start": 1381.24,
      "duration": 4.919
    },
    {
      "text": "important is called end of text so you",
      "start": 1383.6,
      "duration": 4.4
    },
    {
      "text": "can see here I've added these two tokens",
      "start": 1386.159,
      "duration": 4.64
    },
    {
      "text": "at the end of the vocabulary so unknown",
      "start": 1388.0,
      "duration": 5.039
    },
    {
      "text": "token is simple to understand but uh you",
      "start": 1390.799,
      "duration": 3.88
    },
    {
      "text": "must be thinking what is this end of",
      "start": 1393.039,
      "duration": 4.041
    },
    {
      "text": "text and this end of text token is",
      "start": 1394.679,
      "duration": 4.761
    },
    {
      "text": "actually also used for training GPT when",
      "start": 1397.08,
      "duration": 4.32
    },
    {
      "text": "you work with different documents which",
      "start": 1399.44,
      "duration": 4.56
    },
    {
      "text": "are inputs you need to specify where one",
      "start": 1401.4,
      "duration": 4.32
    },
    {
      "text": "document ends and when other document",
      "start": 1404.0,
      "duration": 5.08
    },
    {
      "text": "starts so when you give inputs",
      "start": 1405.72,
      "duration": 4.959
    },
    {
      "text": "this is how it's done so let's say this",
      "start": 1409.08,
      "duration": 3.64
    },
    {
      "text": "is the text Source One then this is the",
      "start": 1410.679,
      "duration": 4.0
    },
    {
      "text": "text Source 2 text Source three and text",
      "start": 1412.72,
      "duration": 4.0
    },
    {
      "text": "Source 4 let's say this is a news",
      "start": 1414.679,
      "duration": 3.721
    },
    {
      "text": "article these are news article these are",
      "start": 1416.72,
      "duration": 4.12
    },
    {
      "text": "Reddit posts these are blogs and let's",
      "start": 1418.4,
      "duration": 3.96
    },
    {
      "text": "say these are newspaper",
      "start": 1420.84,
      "duration": 3.92
    },
    {
      "text": "clippings so after this first text",
      "start": 1422.36,
      "duration": 4.48
    },
    {
      "text": "source is finished we need to give end",
      "start": 1424.76,
      "duration": 3.799
    },
    {
      "text": "of text at the beginning of the second",
      "start": 1426.84,
      "duration": 4.16
    },
    {
      "text": "text source which says that text Source",
      "start": 1428.559,
      "duration": 5.321
    },
    {
      "text": "One is done now the text Source 2 begins",
      "start": 1431.0,
      "duration": 4.64
    },
    {
      "text": "similarly at the beginning of the text",
      "start": 1433.88,
      "duration": 3.72
    },
    {
      "text": "Source 3 we have to give this end of end",
      "start": 1435.64,
      "duration": 4.24
    },
    {
      "text": "of text token and at the beginning of",
      "start": 1437.6,
      "duration": 4.16
    },
    {
      "text": "text Source 4 also we have to give this",
      "start": 1439.88,
      "duration": 5.399
    },
    {
      "text": "end of text token so uh when M when",
      "start": 1441.76,
      "duration": 5.56
    },
    {
      "text": "working with multiple text sources we",
      "start": 1445.279,
      "duration": 4.801
    },
    {
      "text": "add end of text tokens between these",
      "start": 1447.32,
      "duration": 5.239
    },
    {
      "text": "texts these tokens essentially act as",
      "start": 1450.08,
      "duration": 5.959
    },
    {
      "text": "markers signaling the start U or end of",
      "start": 1452.559,
      "duration": 6.12
    },
    {
      "text": "a particular segment this leads to more",
      "start": 1456.039,
      "duration": 4.481
    },
    {
      "text": "effective processing and understanding",
      "start": 1458.679,
      "duration": 4.88
    },
    {
      "text": "by the large language model and remember",
      "start": 1460.52,
      "duration": 5.96
    },
    {
      "text": "uh when chat GPT pre-processes the input",
      "start": 1463.559,
      "duration": 5.24
    },
    {
      "text": "resources the text resources it also",
      "start": 1466.48,
      "duration": 5.16
    },
    {
      "text": "gives we also give the end of text",
      "start": 1468.799,
      "duration": 5.201
    },
    {
      "text": "tokens so now we are going to deal with",
      "start": 1471.64,
      "duration": 4.24
    },
    {
      "text": "special uh these are called special",
      "start": 1474.0,
      "duration": 4.36
    },
    {
      "text": "context tokens so we will modify the",
      "start": 1475.88,
      "duration": 4.88
    },
    {
      "text": "tokenizer to use an unknown token if it",
      "start": 1478.36,
      "duration": 4.319
    },
    {
      "text": "encounters a word that is not part of",
      "start": 1480.76,
      "duration": 4.039
    },
    {
      "text": "the vocabulary and we will also add a",
      "start": 1482.679,
      "duration": 4.0
    },
    {
      "text": "token between unrelated text called the",
      "start": 1484.799,
      "duration": 4.921
    },
    {
      "text": "end of text token so here's the all",
      "start": 1486.679,
      "duration": 4.921
    },
    {
      "text": "tokens which are basically all the",
      "start": 1489.72,
      "duration": 4.6
    },
    {
      "text": "tokens in our current vocabulary and we",
      "start": 1491.6,
      "duration": 4.84
    },
    {
      "text": "are extending this by adding two tokens",
      "start": 1494.32,
      "duration": 4.12
    },
    {
      "text": "at the end of this the end of text token",
      "start": 1496.44,
      "duration": 4.359
    },
    {
      "text": "and the unknown token and then we create",
      "start": 1498.44,
      "duration": 4.68
    },
    {
      "text": "the vocabulary again it the same token",
      "start": 1500.799,
      "duration": 5.401
    },
    {
      "text": "to token ID or token to integer mapping",
      "start": 1503.12,
      "duration": 5.24
    },
    {
      "text": "but now it has two additional tokens and",
      "start": 1506.2,
      "duration": 5.28
    },
    {
      "text": "token IDs that's why the length is 1132",
      "start": 1508.36,
      "duration": 5.24
    },
    {
      "text": "if you look at the length of the initial",
      "start": 1511.48,
      "duration": 5.72
    },
    {
      "text": "vocabulary that was uh",
      "start": 1513.6,
      "duration": 5.559
    },
    {
      "text": "1130 right so the length has been",
      "start": 1517.2,
      "duration": 4.28
    },
    {
      "text": "increased by two and we can even print",
      "start": 1519.159,
      "duration": 4.361
    },
    {
      "text": "the vocabulary and look at the last two",
      "start": 1521.48,
      "duration": 4.36
    },
    {
      "text": "items in this vocabulary we have the end",
      "start": 1523.52,
      "duration": 5.12
    },
    {
      "text": "of text token mapped to a token ID of",
      "start": 1525.84,
      "duration": 5.6
    },
    {
      "text": "1130 we have an unknown token mapped to",
      "start": 1528.64,
      "duration": 6.2
    },
    {
      "text": "a token ID of 1131",
      "start": 1531.44,
      "duration": 7.04
    },
    {
      "text": "great so uh now we have augmented the",
      "start": 1534.84,
      "duration": 5.12
    },
    {
      "text": "vocabulary with these two additional",
      "start": 1538.48,
      "duration": 3.76
    },
    {
      "text": "tokens and now we are ready to actually",
      "start": 1539.96,
      "duration": 5.28
    },
    {
      "text": "modify our tokenizer class so our simple",
      "start": 1542.24,
      "duration": 5.08
    },
    {
      "text": "tokenizer version one class had an",
      "start": 1545.24,
      "duration": 3.84
    },
    {
      "text": "encode and decode method which we are",
      "start": 1547.32,
      "duration": 3.479
    },
    {
      "text": "going to retain but we are going to",
      "start": 1549.08,
      "duration": 4.04
    },
    {
      "text": "change some things in the encode method",
      "start": 1550.799,
      "duration": 5.041
    },
    {
      "text": "what we are going to say is that if the",
      "start": 1553.12,
      "duration": 4.64
    },
    {
      "text": "item does not so if whatever is",
      "start": 1555.84,
      "duration": 4.36
    },
    {
      "text": "contained in text it does not exist in",
      "start": 1557.76,
      "duration": 6.32
    },
    {
      "text": "this St str2 uh int dictionary what we",
      "start": 1560.2,
      "duration": 5.92
    },
    {
      "text": "can say is that you can replace that",
      "start": 1564.08,
      "duration": 4.28
    },
    {
      "text": "text with unknown and then you can map",
      "start": 1566.12,
      "duration": 4.36
    },
    {
      "text": "this unknown to the corresponding ID",
      "start": 1568.36,
      "duration": 3.96
    },
    {
      "text": "which is there in unknown so unknown",
      "start": 1570.48,
      "duration": 4.04
    },
    {
      "text": "token is there in our vocabulary so if",
      "start": 1572.32,
      "duration": 4.56
    },
    {
      "text": "some item is encountered in the string",
      "start": 1574.52,
      "duration": 5.039
    },
    {
      "text": "to integer conversion which is not there",
      "start": 1576.88,
      "duration": 5.6
    },
    {
      "text": "in the vocabulary we replace that item",
      "start": 1579.559,
      "duration": 5.201
    },
    {
      "text": "or that word with unknown and then we",
      "start": 1582.48,
      "duration": 4.6
    },
    {
      "text": "return return the integer or the token",
      "start": 1584.76,
      "duration": 4.799
    },
    {
      "text": "ID associated with the unknown token",
      "start": 1587.08,
      "duration": 3.76
    },
    {
      "text": "That's How we'll deal with unknown",
      "start": 1589.559,
      "duration": 3.281
    },
    {
      "text": "tokens the decode method actually",
      "start": 1590.84,
      "duration": 5.0
    },
    {
      "text": "exactly Remains the Same so now let us",
      "start": 1592.84,
      "duration": 5.68
    },
    {
      "text": "test this simple tokenizer version 2 so",
      "start": 1595.84,
      "duration": 4.28
    },
    {
      "text": "I create an instance of the simple",
      "start": 1598.52,
      "duration": 4.44
    },
    {
      "text": "tokenizer version 2 and the text one is",
      "start": 1600.12,
      "duration": 5.48
    },
    {
      "text": "hello do you like T the text two is in",
      "start": 1602.96,
      "duration": 5.24
    },
    {
      "text": "the sunlight Terraces of the palace so",
      "start": 1605.6,
      "duration": 4.72
    },
    {
      "text": "now we are also going to merge these two",
      "start": 1608.2,
      "duration": 3.959
    },
    {
      "text": "texts which is the input which I'm going",
      "start": 1610.32,
      "duration": 4.8
    },
    {
      "text": "to test the tokenizer on but when we",
      "start": 1612.159,
      "duration": 4.681
    },
    {
      "text": "merge we are also going to give this end",
      "start": 1615.12,
      "duration": 4.0
    },
    {
      "text": "of text token this is how it's done in",
      "start": 1616.84,
      "duration": 4.4
    },
    {
      "text": "GPT so we could have directly given",
      "start": 1619.12,
      "duration": 4.32
    },
    {
      "text": "these two sentences but we indicate when",
      "start": 1621.24,
      "duration": 3.76
    },
    {
      "text": "the first sentence end and when the",
      "start": 1623.44,
      "duration": 3.719
    },
    {
      "text": "second sentence starts so this is my",
      "start": 1625.0,
      "duration": 4.039
    },
    {
      "text": "input text which I will now feed to my",
      "start": 1627.159,
      "duration": 3.961
    },
    {
      "text": "encode method and you'll see now there",
      "start": 1629.039,
      "duration": 4.441
    },
    {
      "text": "is no error although hello was not",
      "start": 1631.12,
      "duration": 4.76
    },
    {
      "text": "included in the vocabulary what is done",
      "start": 1633.48,
      "duration": 5.36
    },
    {
      "text": "now is that in this uh simple tokenizer",
      "start": 1635.88,
      "duration": 5.08
    },
    {
      "text": "version two hello is not there in the",
      "start": 1638.84,
      "duration": 4.48
    },
    {
      "text": "vocabulary so when Hello is encountered",
      "start": 1640.96,
      "duration": 4.24
    },
    {
      "text": "by this encode method it replaces it",
      "start": 1643.32,
      "duration": 3.64
    },
    {
      "text": "with the unknown token and assigns it",
      "start": 1645.2,
      "duration": 4.12
    },
    {
      "text": "that corresponding token ID which is 113",
      "start": 1646.96,
      "duration": 4.0
    },
    {
      "text": "0",
      "start": 1649.32,
      "duration": 5.44
    },
    {
      "text": "see uh so that actually it's 1131 for",
      "start": 1650.96,
      "duration": 6.199
    },
    {
      "text": "unknown and whenever the end of text is",
      "start": 1654.76,
      "duration": 4.32
    },
    {
      "text": "there so end of text is here we have a",
      "start": 1657.159,
      "duration": 4.961
    },
    {
      "text": "token idea of 11 130 great and now you",
      "start": 1659.08,
      "duration": 5.079
    },
    {
      "text": "can decode it back so when you decode it",
      "start": 1662.12,
      "duration": 4.12
    },
    {
      "text": "back the decoded output will be unknown",
      "start": 1664.159,
      "duration": 4.081
    },
    {
      "text": "do you like T end of text in the",
      "start": 1666.24,
      "duration": 4.24
    },
    {
      "text": "sunlight Terraces of the unknown so",
      "start": 1668.24,
      "duration": 4.039
    },
    {
      "text": "there are actually two unknown words",
      "start": 1670.48,
      "duration": 3.84
    },
    {
      "text": "here hello and Palace because Palace is",
      "start": 1672.279,
      "duration": 5.28
    },
    {
      "text": "also not in the data set but now by by",
      "start": 1674.32,
      "duration": 4.88
    },
    {
      "text": "augmenting the vocabulary with this",
      "start": 1677.559,
      "duration": 4.24
    },
    {
      "text": "additional tokens we are able to encode",
      "start": 1679.2,
      "duration": 5.0
    },
    {
      "text": "and decode even unknown words",
      "start": 1681.799,
      "duration": 4.12
    },
    {
      "text": "successfully",
      "start": 1684.2,
      "duration": 4.44
    },
    {
      "text": "awesome so so far we have discussed",
      "start": 1685.919,
      "duration": 4.76
    },
    {
      "text": "tokenization and we have also seen some",
      "start": 1688.64,
      "duration": 4.44
    },
    {
      "text": "special tokens and we saw the tokens",
      "start": 1690.679,
      "duration": 4.681
    },
    {
      "text": "like unknown and end of text one thing",
      "start": 1693.08,
      "duration": 4.959
    },
    {
      "text": "to remember is that uh depending on the",
      "start": 1695.36,
      "duration": 5.159
    },
    {
      "text": "llm some researchers also consider",
      "start": 1698.039,
      "duration": 4.12
    },
    {
      "text": "additional tokens like beginning of",
      "start": 1700.519,
      "duration": 4.0
    },
    {
      "text": "sequence end of sequence then a padding",
      "start": 1702.159,
      "duration": 4.961
    },
    {
      "text": "token Etc we'll not go into details",
      "start": 1704.519,
      "duration": 4.241
    },
    {
      "text": "about these tokens tokens but it's",
      "start": 1707.12,
      "duration": 4.08
    },
    {
      "text": "important to remember that this unknown",
      "start": 1708.76,
      "duration": 4.68
    },
    {
      "text": "and end of text are not the only special",
      "start": 1711.2,
      "duration": 3.88
    },
    {
      "text": "context tokens there are also other",
      "start": 1713.44,
      "duration": 3.76
    },
    {
      "text": "tokens with some researchers",
      "start": 1715.08,
      "duration": 6.04
    },
    {
      "text": "consider now for GPT uh the tokenizer",
      "start": 1717.2,
      "duration": 5.839
    },
    {
      "text": "used for GPT models doesn't use an",
      "start": 1721.12,
      "duration": 5.12
    },
    {
      "text": "unknown token uh but it does use an end",
      "start": 1723.039,
      "duration": 7.24
    },
    {
      "text": "of text token Okay Okay so until now we",
      "start": 1726.24,
      "duration": 7.039
    },
    {
      "text": "have looked at word based uh",
      "start": 1730.279,
      "duration": 5.24
    },
    {
      "text": "tokenization you can take a pause here",
      "start": 1733.279,
      "duration": 4.201
    },
    {
      "text": "take a break if you want because now",
      "start": 1735.519,
      "duration": 3.841
    },
    {
      "text": "we'll be moving to the next section",
      "start": 1737.48,
      "duration": 4.28
    },
    {
      "text": "which is types of tokenizers and we'll",
      "start": 1739.36,
      "duration": 5.0
    },
    {
      "text": "also look at the tokenizer which is used",
      "start": 1741.76,
      "duration": 5.759
    },
    {
      "text": "by GPT and that's called as bite pair",
      "start": 1744.36,
      "duration": 6.0
    },
    {
      "text": "encoding so GPT models use a bite pair",
      "start": 1747.519,
      "duration": 5.721
    },
    {
      "text": "encoding tokenizer and we'll now learn",
      "start": 1750.36,
      "duration": 4.679
    },
    {
      "text": "about that so if you want to take a",
      "start": 1753.24,
      "duration": 3.88
    },
    {
      "text": "break at this point feel free to pause",
      "start": 1755.039,
      "duration": 5.321
    },
    {
      "text": "and then come back to this second",
      "start": 1757.12,
      "duration": 7.279
    },
    {
      "text": "part okay so uh until now we looked at",
      "start": 1760.36,
      "duration": 6.24
    },
    {
      "text": "word based tokenization and we actually",
      "start": 1764.399,
      "duration": 4.561
    },
    {
      "text": "constructed our own word based",
      "start": 1766.6,
      "duration": 5.199
    },
    {
      "text": "tokenizer and we saw that by adding it",
      "start": 1768.96,
      "duration": 4.64
    },
    {
      "text": "by adding special context tokens it",
      "start": 1771.799,
      "duration": 3.521
    },
    {
      "text": "could also deal with unknown Words which",
      "start": 1773.6,
      "duration": 3.919
    },
    {
      "text": "were not present in the vocabulary so",
      "start": 1775.32,
      "duration": 4.239
    },
    {
      "text": "then your question would be why not this",
      "start": 1777.519,
      "duration": 4.561
    },
    {
      "text": "why this was not used uh for training",
      "start": 1779.559,
      "duration": 4.761
    },
    {
      "text": "GPT and why do we even need to study",
      "start": 1782.08,
      "duration": 5.52
    },
    {
      "text": "about other tokenization algorithms so",
      "start": 1784.32,
      "duration": 5.56
    },
    {
      "text": "word based tokenization as we saw is",
      "start": 1787.6,
      "duration": 3.919
    },
    {
      "text": "just breaking down the sentence into",
      "start": 1789.88,
      "duration": 3.919
    },
    {
      "text": "individual",
      "start": 1791.519,
      "duration": 4.921
    },
    {
      "text": "words uh the main problem with word",
      "start": 1793.799,
      "duration": 4.321
    },
    {
      "text": "based tokenization as we saw is that",
      "start": 1796.44,
      "duration": 3.64
    },
    {
      "text": "that it gets very difficult to deal with",
      "start": 1798.12,
      "duration": 4.279
    },
    {
      "text": "out of vocabulary words we need to add",
      "start": 1800.08,
      "duration": 5.0
    },
    {
      "text": "these special context tokens Etc the",
      "start": 1802.399,
      "duration": 5.841
    },
    {
      "text": "second major problem is that uh let's",
      "start": 1805.08,
      "duration": 5.4
    },
    {
      "text": "say there are two words boy and boys",
      "start": 1808.24,
      "duration": 4.24
    },
    {
      "text": "there is ideally a lot of similarity in",
      "start": 1810.48,
      "duration": 3.88
    },
    {
      "text": "these words right but both of these",
      "start": 1812.48,
      "duration": 3.52
    },
    {
      "text": "words will be assigned separate token",
      "start": 1814.36,
      "duration": 4.679
    },
    {
      "text": "IDs and there is no meaning which is",
      "start": 1816.0,
      "duration": 5.12
    },
    {
      "text": "captured in the",
      "start": 1819.039,
      "duration": 4.24
    },
    {
      "text": "tokenization because the token IDs are",
      "start": 1821.12,
      "duration": 4.679
    },
    {
      "text": "different there is nowhere we get some",
      "start": 1823.279,
      "duration": 4.4
    },
    {
      "text": "kind of an intuitive meaning that these",
      "start": 1825.799,
      "duration": 3.641
    },
    {
      "text": "two are similar they actually contain",
      "start": 1827.679,
      "duration": 4.561
    },
    {
      "text": "the same root word so the second major",
      "start": 1829.44,
      "duration": 6.359
    },
    {
      "text": "problem with this is that we do not",
      "start": 1832.24,
      "duration": 7.08
    },
    {
      "text": "understand the similarity between words",
      "start": 1835.799,
      "duration": 5.561
    },
    {
      "text": "based on their root words all of that",
      "start": 1839.32,
      "duration": 4.64
    },
    {
      "text": "meaning is lost and the third major",
      "start": 1841.36,
      "duration": 4.48
    },
    {
      "text": "problem is that it leads to very large",
      "start": 1843.96,
      "duration": 3.92
    },
    {
      "text": "vocabulary sizes because if you are",
      "start": 1845.84,
      "duration": 4.079
    },
    {
      "text": "training GPT and if you look at the",
      "start": 1847.88,
      "duration": 3.679
    },
    {
      "text": "number of words in the English language",
      "start": 1849.919,
      "duration": 3.081
    },
    {
      "text": "let's actually",
      "start": 1851.559,
      "duration": 4.48
    },
    {
      "text": "see we we only looked at this data set",
      "start": 1853.0,
      "duration": 5.48
    },
    {
      "text": "right but for training GPT you will need",
      "start": 1856.039,
      "duration": 4.64
    },
    {
      "text": "to have a vocabulary with a huge number",
      "start": 1858.48,
      "duration": 3.799
    },
    {
      "text": "of words so number of words in the",
      "start": 1860.679,
      "duration": 3.96
    },
    {
      "text": "English",
      "start": 1862.279,
      "duration": 2.36
    },
    {
      "text": "language so let's see so the number of",
      "start": 1866.24,
      "duration": 4.08
    },
    {
      "text": "words in the English language are",
      "start": 1868.96,
      "duration": 3.599
    },
    {
      "text": "estimated to be around 600,000 to 1",
      "start": 1870.32,
      "duration": 4.64
    },
    {
      "text": "million that's a huge number of words",
      "start": 1872.559,
      "duration": 5.681
    },
    {
      "text": "right so to actually make a vocabulary",
      "start": 1874.96,
      "duration": 6.559
    },
    {
      "text": "for this we would need a vocabulary size",
      "start": 1878.24,
      "duration": 5.6
    },
    {
      "text": "of this much which is really very high",
      "start": 1881.519,
      "duration": 4.441
    },
    {
      "text": "so word based tokenization schemes also",
      "start": 1883.84,
      "duration": 4.16
    },
    {
      "text": "need a very large vocabulary size and",
      "start": 1885.96,
      "duration": 3.28
    },
    {
      "text": "that's a big",
      "start": 1888.0,
      "duration": 4.24
    },
    {
      "text": "problem uh then how to solve these",
      "start": 1889.24,
      "duration": 4.919
    },
    {
      "text": "problems there is another end of the",
      "start": 1892.24,
      "duration": 3.559
    },
    {
      "text": "Spectrum which is essentially character",
      "start": 1894.159,
      "duration": 4.801
    },
    {
      "text": "based tokenization what what's what",
      "start": 1895.799,
      "duration": 4.921
    },
    {
      "text": "happens in character based tokenization",
      "start": 1898.96,
      "duration": 4.76
    },
    {
      "text": "is that when you look at sentences",
      "start": 1900.72,
      "duration": 4.919
    },
    {
      "text": "different words are not tokens but",
      "start": 1903.72,
      "duration": 4.799
    },
    {
      "text": "instead characters are the tokens so",
      "start": 1905.639,
      "duration": 5.28
    },
    {
      "text": "instead of the tokens being my hobby is",
      "start": 1908.519,
      "duration": 7.241
    },
    {
      "text": "playing the tokens are now m y",
      "start": 1910.919,
      "duration": 7.801
    },
    {
      "text": "h that's it the the tokens are",
      "start": 1915.76,
      "duration": 5.12
    },
    {
      "text": "individual characters can you think of",
      "start": 1918.72,
      "duration": 4.64
    },
    {
      "text": "the advantages and disadvantages of this",
      "start": 1920.88,
      "duration": 5.759
    },
    {
      "text": "you can pause the video here if you",
      "start": 1923.36,
      "duration": 6.039
    },
    {
      "text": "want okay so the major advantage of this",
      "start": 1926.639,
      "duration": 4.361
    },
    {
      "text": "approach is that it leads to a very",
      "start": 1929.399,
      "duration": 3.961
    },
    {
      "text": "small vocabulary remember one of the",
      "start": 1931.0,
      "duration": 4.84
    },
    {
      "text": "major problems of Word level word based",
      "start": 1933.36,
      "duration": 4.08
    },
    {
      "text": "tokenization is that we would need a",
      "start": 1935.84,
      "duration": 4.079
    },
    {
      "text": "large vocabulary but the number of",
      "start": 1937.44,
      "duration": 4.0
    },
    {
      "text": "characters in the English language is",
      "start": 1939.919,
      "duration": 4.561
    },
    {
      "text": "just around 256 and every language has a",
      "start": 1941.44,
      "duration": 5.16
    },
    {
      "text": "fixed number of characters so the",
      "start": 1944.48,
      "duration": 4.679
    },
    {
      "text": "vocabulary which we will be using in",
      "start": 1946.6,
      "duration": 4.559
    },
    {
      "text": "character based tokenization is actually",
      "start": 1949.159,
      "duration": 4.041
    },
    {
      "text": "very small that's one of the big",
      "start": 1951.159,
      "duration": 4.281
    },
    {
      "text": "advantage and it actually solves the oov",
      "start": 1953.2,
      "duration": 4.04
    },
    {
      "text": "problem which is the out of vocabulary",
      "start": 1955.44,
      "duration": 4.479
    },
    {
      "text": "problem but there are some other major",
      "start": 1957.24,
      "duration": 5.679
    },
    {
      "text": "issues with character level tokenization",
      "start": 1959.919,
      "duration": 4.801
    },
    {
      "text": "similar to the word based tokenization",
      "start": 1962.919,
      "duration": 3.321
    },
    {
      "text": "the meaning which words carry is",
      "start": 1964.72,
      "duration": 4.48
    },
    {
      "text": "completely lost so for example boy and",
      "start": 1966.24,
      "duration": 5.96
    },
    {
      "text": "boys there is no meaning",
      "start": 1969.2,
      "duration": 5.4
    },
    {
      "text": "encoded the same root word is used in",
      "start": 1972.2,
      "duration": 4.68
    },
    {
      "text": "these two words boy that also is lost",
      "start": 1974.6,
      "duration": 3.48
    },
    {
      "text": "why is it lost because because we",
      "start": 1976.88,
      "duration": 2.919
    },
    {
      "text": "completely destroy the words down to",
      "start": 1978.08,
      "duration": 3.92
    },
    {
      "text": "their individual characters right we get",
      "start": 1979.799,
      "duration": 4.321
    },
    {
      "text": "a small vocabulary size but in essence",
      "start": 1982.0,
      "duration": 5.159
    },
    {
      "text": "we are removing the soul of Words which",
      "start": 1984.12,
      "duration": 4.72
    },
    {
      "text": "is their meaning by breaking it down",
      "start": 1987.159,
      "duration": 3.841
    },
    {
      "text": "into characters so the meaning",
      "start": 1988.84,
      "duration": 5.4
    },
    {
      "text": "associated with words is completely lost",
      "start": 1991.0,
      "duration": 5.36
    },
    {
      "text": "there is another major problem and that",
      "start": 1994.24,
      "duration": 4.0
    },
    {
      "text": "is that the tokenized sequence is much",
      "start": 1996.36,
      "duration": 4.64
    },
    {
      "text": "longer than the initial raw text so for",
      "start": 1998.24,
      "duration": 5.319
    },
    {
      "text": "example hobby right if you look at Hobby",
      "start": 2001.0,
      "duration": 4.44
    },
    {
      "text": "in word based tokenization hobby was a",
      "start": 2003.559,
      "duration": 4.281
    },
    {
      "text": "separate token but now there will be",
      "start": 2005.44,
      "duration": 9.56
    },
    {
      "text": "five tokens for this h o b b y so this",
      "start": 2007.84,
      "duration": 9.24
    },
    {
      "text": "one word is actually converted into five",
      "start": 2015.0,
      "duration": 4.039
    },
    {
      "text": "tokens and this becomes a big problem",
      "start": 2017.08,
      "duration": 5.719
    },
    {
      "text": "when dealing with large data sets the",
      "start": 2019.039,
      "duration": 6.0
    },
    {
      "text": "the tokenized sequence is much longer",
      "start": 2022.799,
      "duration": 4.76
    },
    {
      "text": "than the initial raw text so these are",
      "start": 2025.039,
      "duration": 4.321
    },
    {
      "text": "the problems associated with character",
      "start": 2027.559,
      "duration": 3.321
    },
    {
      "text": "level",
      "start": 2029.36,
      "duration": 3.72
    },
    {
      "text": "tokenization so essentially the major",
      "start": 2030.88,
      "duration": 4.039
    },
    {
      "text": "problems with word based tokenization is",
      "start": 2033.08,
      "duration": 3.92
    },
    {
      "text": "that it needs a large vocabulary the",
      "start": 2034.919,
      "duration": 3.64
    },
    {
      "text": "major problem with character based",
      "start": 2037.0,
      "duration": 3.08
    },
    {
      "text": "tokenization is that the meaning is",
      "start": 2038.559,
      "duration": 4.08
    },
    {
      "text": "completely lost can we think of Best of",
      "start": 2040.08,
      "duration": 5.079
    },
    {
      "text": "Both Worlds can we think of something",
      "start": 2042.639,
      "duration": 4.681
    },
    {
      "text": "which does not need that much vocabulary",
      "start": 2045.159,
      "duration": 4.92
    },
    {
      "text": "but it also retains the essence of words",
      "start": 2047.32,
      "duration": 4.559
    },
    {
      "text": "that's where subword based tokenization",
      "start": 2050.079,
      "duration": 4.121
    },
    {
      "text": "actually comes into the picture and the",
      "start": 2051.879,
      "duration": 5.321
    },
    {
      "text": "bite pair encoder which GPT models are",
      "start": 2054.2,
      "duration": 5.479
    },
    {
      "text": "using that is actually a subword based",
      "start": 2057.2,
      "duration": 4.639
    },
    {
      "text": "tokenization as you must have guessed in",
      "start": 2059.679,
      "duration": 5.081
    },
    {
      "text": "subword based tokenization not full Char",
      "start": 2061.839,
      "duration": 4.76
    },
    {
      "text": "not full words are tokens and neither",
      "start": 2064.76,
      "duration": 3.48
    },
    {
      "text": "full characters are tokens",
      "start": 2066.599,
      "duration": 3.961
    },
    {
      "text": "but subwords are",
      "start": 2068.24,
      "duration": 5.24
    },
    {
      "text": "tokens uh and there are some rules for",
      "start": 2070.56,
      "duration": 5.24
    },
    {
      "text": "subword based tokenization the first",
      "start": 2073.48,
      "duration": 4.76
    },
    {
      "text": "rule is that we do not split frequently",
      "start": 2075.8,
      "duration": 5.4
    },
    {
      "text": "used words into smaller subwords so",
      "start": 2078.24,
      "duration": 4.879
    },
    {
      "text": "those words stay the same but we split",
      "start": 2081.2,
      "duration": 4.199
    },
    {
      "text": "the rare words into smaller meaningful",
      "start": 2083.119,
      "duration": 4.641
    },
    {
      "text": "subwords now what does this mean so for",
      "start": 2085.399,
      "duration": 5.0
    },
    {
      "text": "example if you consider boy and boys boy",
      "start": 2087.76,
      "duration": 4.919
    },
    {
      "text": "may occur many times in the data set so",
      "start": 2090.399,
      "duration": 3.72
    },
    {
      "text": "that will not be split that will be",
      "start": 2092.679,
      "duration": 5.881
    },
    {
      "text": "retained as one word boy but boys",
      "start": 2094.119,
      "duration": 8.561
    },
    {
      "text": "boys will actually be split into boy and",
      "start": 2098.56,
      "duration": 7.48
    },
    {
      "text": "S so boys is a rare word maybe and it is",
      "start": 2102.68,
      "duration": 5.56
    },
    {
      "text": "also derivative of the word boy so it",
      "start": 2106.04,
      "duration": 5.0
    },
    {
      "text": "will be split into two words boy and S",
      "start": 2108.24,
      "duration": 4.56
    },
    {
      "text": "so now instead of having two separate",
      "start": 2111.04,
      "duration": 4.36
    },
    {
      "text": "token for boy and boys we just have one",
      "start": 2112.8,
      "duration": 5.24
    },
    {
      "text": "token which is boy so you can see that",
      "start": 2115.4,
      "duration": 5.16
    },
    {
      "text": "this tokenization retains the root word",
      "start": 2118.04,
      "duration": 5.24
    },
    {
      "text": "so boy and boys both will have the boy",
      "start": 2120.56,
      "duration": 4.96
    },
    {
      "text": "token as common just boys will have an",
      "start": 2123.28,
      "duration": 5.52
    },
    {
      "text": "extra token which is s So subword based",
      "start": 2125.52,
      "duration": 5.52
    },
    {
      "text": "tokenization can also have word entire",
      "start": 2128.8,
      "duration": 4.279
    },
    {
      "text": "word as tokens can also have characters",
      "start": 2131.04,
      "duration": 4.52
    },
    {
      "text": "as tokens and it can also have subwords",
      "start": 2133.079,
      "duration": 5.321
    },
    {
      "text": "as tokens but it follows these two rules",
      "start": 2135.56,
      "duration": 4.36
    },
    {
      "text": "so if there is a word which occurs",
      "start": 2138.4,
      "duration": 3.64
    },
    {
      "text": "frequently that is retained as a single",
      "start": 2139.92,
      "duration": 4.36
    },
    {
      "text": "token but if there is a rare word that",
      "start": 2142.04,
      "duration": 4.319
    },
    {
      "text": "split into smaller meaningful subwords",
      "start": 2144.28,
      "duration": 4.72
    },
    {
      "text": "which retains the root",
      "start": 2146.359,
      "duration": 6.361
    },
    {
      "text": "word uh so the subword splitting ex",
      "start": 2149.0,
      "duration": 6.52
    },
    {
      "text": "actually helps the model learn uh that",
      "start": 2152.72,
      "duration": 5.2
    },
    {
      "text": "different words with the same root word",
      "start": 2155.52,
      "duration": 5.52
    },
    {
      "text": "as token such as tokens and let's say",
      "start": 2157.92,
      "duration": 5.919
    },
    {
      "text": "tokenizing are similar in meaning so",
      "start": 2161.04,
      "duration": 4.36
    },
    {
      "text": "this is an important point which we did",
      "start": 2163.839,
      "duration": 3.561
    },
    {
      "text": "not achieve in word based tokenization",
      "start": 2165.4,
      "duration": 4.12
    },
    {
      "text": "and even character based tokenization",
      "start": 2167.4,
      "duration": 4.08
    },
    {
      "text": "the subword splitting actually helps the",
      "start": 2169.52,
      "duration": 4.4
    },
    {
      "text": "modern learn that different words with",
      "start": 2171.48,
      "duration": 4.96
    },
    {
      "text": "the same root words such as token uh",
      "start": 2173.92,
      "duration": 4.12
    },
    {
      "text": "such as tokens and tokenizing are",
      "start": 2176.44,
      "duration": 3.679
    },
    {
      "text": "similar in meaning that's very important",
      "start": 2178.04,
      "duration": 4.039
    },
    {
      "text": "so this meaning is actually encoded in",
      "start": 2180.119,
      "duration": 3.561
    },
    {
      "text": "subord based",
      "start": 2182.079,
      "duration": 3.881
    },
    {
      "text": "tokenization now how is subword based",
      "start": 2183.68,
      "duration": 4.36
    },
    {
      "text": "tokenization implemented there are",
      "start": 2185.96,
      "duration": 4.24
    },
    {
      "text": "certain algorithms to implement subword",
      "start": 2188.04,
      "duration": 4.279
    },
    {
      "text": "based tokenization and one such",
      "start": 2190.2,
      "duration": 4.68
    },
    {
      "text": "algorithm is called as bite pair",
      "start": 2192.319,
      "duration": 5.201
    },
    {
      "text": "encoding so this algorithm itself was",
      "start": 2194.88,
      "duration": 5.479
    },
    {
      "text": "developed in 1994 and the main purpose",
      "start": 2197.52,
      "duration": 5.799
    },
    {
      "text": "of this algorithm was to compress data",
      "start": 2200.359,
      "duration": 4.801
    },
    {
      "text": "I'll not spend too much time on it but",
      "start": 2203.319,
      "duration": 3.641
    },
    {
      "text": "let me take you through a quick example",
      "start": 2205.16,
      "duration": 3.4
    },
    {
      "text": "so let's say if this was the original",
      "start": 2206.96,
      "duration": 4.0
    },
    {
      "text": "data what happens in this algorithm is",
      "start": 2208.56,
      "duration": 5.24
    },
    {
      "text": "that we actually first look at a bite",
      "start": 2210.96,
      "duration": 4.56
    },
    {
      "text": "pair which occurs the most so in this",
      "start": 2213.8,
      "duration": 3.84
    },
    {
      "text": "case AA is that bite pair right which",
      "start": 2215.52,
      "duration": 4.559
    },
    {
      "text": "occurs the most so we look at the bite",
      "start": 2217.64,
      "duration": 4.4
    },
    {
      "text": "pair which which occurs the most and we",
      "start": 2220.079,
      "duration": 4.681
    },
    {
      "text": "replace the bite pair with with another",
      "start": 2222.04,
      "duration": 4.64
    },
    {
      "text": "variable which does not occur in the",
      "start": 2224.76,
      "duration": 4.48
    },
    {
      "text": "data so a a appears the most so we'll",
      "start": 2226.68,
      "duration": 4.04
    },
    {
      "text": "replace it with another variable which",
      "start": 2229.24,
      "duration": 3.64
    },
    {
      "text": "is called as Zed so now the compressed",
      "start": 2230.72,
      "duration": 4.96
    },
    {
      "text": "data becomes Z AB d z a",
      "start": 2232.88,
      "duration": 5.6
    },
    {
      "text": "a then we look for the next common bite",
      "start": 2235.68,
      "duration": 5.56
    },
    {
      "text": "pair that is AB and that is replaced by",
      "start": 2238.48,
      "duration": 4.92
    },
    {
      "text": "another variable which is y so then the",
      "start": 2241.24,
      "duration": 5.28
    },
    {
      "text": "further compressed data becomes zyd z y",
      "start": 2243.4,
      "duration": 6.439
    },
    {
      "text": "a so only one bite pair is left and",
      "start": 2246.52,
      "duration": 5.16
    },
    {
      "text": "after that we do not do the compression",
      "start": 2249.839,
      "duration": 3.76
    },
    {
      "text": "further there is one more round which",
      "start": 2251.68,
      "duration": 3.679
    },
    {
      "text": "can be done this zedy can be replaced",
      "start": 2253.599,
      "duration": 4.24
    },
    {
      "text": "with w and this zedy can be replaced",
      "start": 2255.359,
      "duration": 4.561
    },
    {
      "text": "with W so that is the last round of",
      "start": 2257.839,
      "duration": 4.161
    },
    {
      "text": "compression this is how the bite pair",
      "start": 2259.92,
      "duration": 4.56
    },
    {
      "text": "encoding algorithm works we look at bite",
      "start": 2262.0,
      "duration": 4.56
    },
    {
      "text": "pairs which occur the most and we go on",
      "start": 2264.48,
      "duration": 3.92
    },
    {
      "text": "replacing them with different variables",
      "start": 2266.56,
      "duration": 3.4
    },
    {
      "text": "and we compress the",
      "start": 2268.4,
      "duration": 4.12
    },
    {
      "text": "data so you must be thinking how is",
      "start": 2269.96,
      "duration": 4.639
    },
    {
      "text": "exactly the bite pair algorithm used for",
      "start": 2272.52,
      "duration": 4.599
    },
    {
      "text": "large language models and how does it",
      "start": 2274.599,
      "duration": 4.641
    },
    {
      "text": "relate to the subord tokenization rules",
      "start": 2277.119,
      "duration": 6.561
    },
    {
      "text": "which we saw so we saw that in uh subord",
      "start": 2279.24,
      "duration": 7.2
    },
    {
      "text": "tokenization um the most commonly used",
      "start": 2283.68,
      "duration": 5.399
    },
    {
      "text": "words are represented as a single token",
      "start": 2286.44,
      "duration": 4.679
    },
    {
      "text": "while rare words are broken down into",
      "start": 2289.079,
      "duration": 4.561
    },
    {
      "text": "two or more subword tokens we'll see",
      "start": 2291.119,
      "duration": 4.361
    },
    {
      "text": "that this is exactly what the bite pair",
      "start": 2293.64,
      "duration": 3.88
    },
    {
      "text": "encoder algorithm achieves when it's",
      "start": 2295.48,
      "duration": 3.599
    },
    {
      "text": "applied to sentences or when it's",
      "start": 2297.52,
      "duration": 4.2
    },
    {
      "text": "applied to vocabulary so essentially",
      "start": 2299.079,
      "duration": 4.921
    },
    {
      "text": "let's take a practical example if you",
      "start": 2301.72,
      "duration": 4.2
    },
    {
      "text": "have this data set of words let's say if",
      "start": 2304.0,
      "duration": 3.72
    },
    {
      "text": "you have this is your vocabulary let's",
      "start": 2305.92,
      "duration": 4.439
    },
    {
      "text": "say old appears seven times older",
      "start": 2307.72,
      "duration": 5.44
    },
    {
      "text": "appears three times finest appears nine",
      "start": 2310.359,
      "duration": 6.081
    },
    {
      "text": "times lowest appears four times so what",
      "start": 2313.16,
      "duration": 5.72
    },
    {
      "text": "you do first is that you add uh another",
      "start": 2316.44,
      "duration": 4.28
    },
    {
      "text": "token which is similar to the end of",
      "start": 2318.88,
      "duration": 4.28
    },
    {
      "text": "text token which we saw earlier this",
      "start": 2320.72,
      "duration": 5.119
    },
    {
      "text": "token indicates that a word has ended so",
      "start": 2323.16,
      "duration": 5.72
    },
    {
      "text": "after each word you add this token so",
      "start": 2325.839,
      "duration": 4.921
    },
    {
      "text": "now this is our",
      "start": 2328.88,
      "duration": 4.64
    },
    {
      "text": "vocabulary now let's see how subord",
      "start": 2330.76,
      "duration": 4.559
    },
    {
      "text": "based tokenization or how bite pair",
      "start": 2333.52,
      "duration": 4.16
    },
    {
      "text": "encoding is actually done first what's",
      "start": 2335.319,
      "duration": 4.161
    },
    {
      "text": "done is that all the words are split",
      "start": 2337.68,
      "duration": 3.679
    },
    {
      "text": "into their character based tokens into",
      "start": 2339.48,
      "duration": 5.0
    },
    {
      "text": "character level so in this vocabulary we",
      "start": 2341.359,
      "duration": 4.681
    },
    {
      "text": "have all of these different character",
      "start": 2344.48,
      "duration": 3.8
    },
    {
      "text": "level tokens and we also mention their",
      "start": 2346.04,
      "duration": 4.799
    },
    {
      "text": "frequency next what is done is that",
      "start": 2348.28,
      "duration": 4.12
    },
    {
      "text": "similar to The Bite pair encoding",
      "start": 2350.839,
      "duration": 4.321
    },
    {
      "text": "algorithm we look for the tokens with",
      "start": 2352.4,
      "duration": 5.439
    },
    {
      "text": "the most common pairing and then we",
      "start": 2355.16,
      "duration": 4.959
    },
    {
      "text": "merge them so let's look at these words",
      "start": 2357.839,
      "duration": 4.52
    },
    {
      "text": "it seems that the most common pairing is",
      "start": 2360.119,
      "duration": 5.321
    },
    {
      "text": "e and s because it comes in finest nine",
      "start": 2362.359,
      "duration": 5.041
    },
    {
      "text": "times and lowest four times so it comes",
      "start": 2365.44,
      "duration": 5.8
    },
    {
      "text": "13 times right so the most common bite",
      "start": 2367.4,
      "duration": 5.88
    },
    {
      "text": "which starts with e is es so this is",
      "start": 2371.24,
      "duration": 4.24
    },
    {
      "text": "actually the bite which occurs the most",
      "start": 2373.28,
      "duration": 3.68
    },
    {
      "text": "so then what we'll do is that we'll",
      "start": 2375.48,
      "duration": 3.56
    },
    {
      "text": "merge e and s and then we'll create a",
      "start": 2376.96,
      "duration": 4.48
    },
    {
      "text": "separate token so now the new token",
      "start": 2379.04,
      "duration": 4.72
    },
    {
      "text": "which is created is es and then we'll",
      "start": 2381.44,
      "duration": 4.48
    },
    {
      "text": "apply the same thing then es is one",
      "start": 2383.76,
      "duration": 4.559
    },
    {
      "text": "token then we look for the next commonly",
      "start": 2385.92,
      "duration": 5.84
    },
    {
      "text": "occurring bite and that is EST so then",
      "start": 2388.319,
      "duration": 6.561
    },
    {
      "text": "EST in the next iteration EST will be a",
      "start": 2391.76,
      "duration": 4.12
    },
    {
      "text": "separate",
      "start": 2394.88,
      "duration": 4.36
    },
    {
      "text": "token in the iteration further than that",
      "start": 2395.88,
      "duration": 5.959
    },
    {
      "text": "EST followed by this slw will be a",
      "start": 2399.24,
      "duration": 5.64
    },
    {
      "text": "separate token then we'll see that the",
      "start": 2401.839,
      "duration": 5.601
    },
    {
      "text": "byes o and L also appear 10 times so",
      "start": 2404.88,
      "duration": 4.6
    },
    {
      "text": "then we'll merge o and L into a separate",
      "start": 2407.44,
      "duration": 5.28
    },
    {
      "text": "token then we'll see that o l and D is",
      "start": 2409.48,
      "duration": 5.119
    },
    {
      "text": "that bite pair which is also occurred 10",
      "start": 2412.72,
      "duration": 4.52
    },
    {
      "text": "times so we'll merge them so finally the",
      "start": 2414.599,
      "duration": 4.361
    },
    {
      "text": "list of tokens which you will have will",
      "start": 2417.24,
      "duration": 3.48
    },
    {
      "text": "look something like",
      "start": 2418.96,
      "duration": 4.32
    },
    {
      "text": "this this is the final list of tokens",
      "start": 2420.72,
      "duration": 4.399
    },
    {
      "text": "after you do four to five iterations of",
      "start": 2423.28,
      "duration": 4.2
    },
    {
      "text": "the most commonly occurring bites and if",
      "start": 2425.119,
      "duration": 4.641
    },
    {
      "text": "you merge them so these will be the list",
      "start": 2427.48,
      "duration": 4.639
    },
    {
      "text": "of tokens now if you analyze these list",
      "start": 2429.76,
      "duration": 4.16
    },
    {
      "text": "of tokens there are also characters in",
      "start": 2432.119,
      "duration": 3.561
    },
    {
      "text": "these tokens but there are also words in",
      "start": 2433.92,
      "duration": 4.12
    },
    {
      "text": "these tokens and the beauty of these",
      "start": 2435.68,
      "duration": 4.24
    },
    {
      "text": "tokens now is that we are retaining the",
      "start": 2438.04,
      "duration": 4.88
    },
    {
      "text": "root words because if you see uh let's",
      "start": 2439.92,
      "duration": 4.24
    },
    {
      "text": "look at our",
      "start": 2442.92,
      "duration": 5.199
    },
    {
      "text": "words uh in the vocabulary EST occurs in",
      "start": 2444.16,
      "duration": 7.64
    },
    {
      "text": "finest and lowest right so EST is a root",
      "start": 2448.119,
      "duration": 6.641
    },
    {
      "text": "word which can occur in many words and",
      "start": 2451.8,
      "duration": 4.799
    },
    {
      "text": "the world based tokenizer and the",
      "start": 2454.76,
      "duration": 3.76
    },
    {
      "text": "character based tokenizer will never",
      "start": 2456.599,
      "duration": 4.881
    },
    {
      "text": "learn this but now we have learned this",
      "start": 2458.52,
      "duration": 5.64
    },
    {
      "text": "even older if you see older and old",
      "start": 2461.48,
      "duration": 4.24
    },
    {
      "text": "these would be two separate tokens in",
      "start": 2464.16,
      "duration": 4.8
    },
    {
      "text": "world based tokenizer but now old is a",
      "start": 2465.72,
      "duration": 6.2
    },
    {
      "text": "token and then e and r are separate",
      "start": 2468.96,
      "duration": 5.48
    },
    {
      "text": "tokens that's exactly how subword based",
      "start": 2471.92,
      "duration": 4.88
    },
    {
      "text": "tokenization actually works we retain",
      "start": 2474.44,
      "duration": 5.6
    },
    {
      "text": "the root words and uh some kind of",
      "start": 2476.8,
      "duration": 6.24
    },
    {
      "text": "meaning is encoded and since it's not",
      "start": 2480.04,
      "duration": 5.44
    },
    {
      "text": "exactly word based tokenization also we",
      "start": 2483.04,
      "duration": 5.96
    },
    {
      "text": "don't need that much amount of tokens to",
      "start": 2485.48,
      "duration": 5.68
    },
    {
      "text": "construct a",
      "start": 2489.0,
      "duration": 5.0
    },
    {
      "text": "vocabulary so now in this in this",
      "start": 2491.16,
      "duration": 4.959
    },
    {
      "text": "simplified example this list of 11",
      "start": 2494.0,
      "duration": 4.599
    },
    {
      "text": "tokens will serve as our vocabulary and",
      "start": 2496.119,
      "duration": 4.161
    },
    {
      "text": "then you must be thinking when when do",
      "start": 2498.599,
      "duration": 3.561
    },
    {
      "text": "this iteration stop when do we stop this",
      "start": 2500.28,
      "duration": 4.96
    },
    {
      "text": "merging we stop this merging of bytes",
      "start": 2502.16,
      "duration": 5.0
    },
    {
      "text": "based on let's say you have a maximum",
      "start": 2505.24,
      "duration": 3.52
    },
    {
      "text": "token count or let's say if you have",
      "start": 2507.16,
      "duration": 3.4
    },
    {
      "text": "already specified the maximum number of",
      "start": 2508.76,
      "duration": 4.4
    },
    {
      "text": "iterations which will happen so this is",
      "start": 2510.56,
      "duration": 4.84
    },
    {
      "text": "a gist of how the bite pair encoder",
      "start": 2513.16,
      "duration": 4.36
    },
    {
      "text": "algorithm actually works we we have an",
      "start": 2515.4,
      "duration": 4.919
    },
    {
      "text": "entire separate lecture devoted to this",
      "start": 2517.52,
      "duration": 4.839
    },
    {
      "text": "uh you can have that you can look at",
      "start": 2520.319,
      "duration": 4.481
    },
    {
      "text": "that lecture it's a 1 hour lecture which",
      "start": 2522.359,
      "duration": 4.881
    },
    {
      "text": "goes into a lot of detail of bite pair",
      "start": 2524.8,
      "duration": 4.799
    },
    {
      "text": "encoder but here I just wanted to give",
      "start": 2527.24,
      "duration": 5.16
    },
    {
      "text": "you an overview of how the bpe algorithm",
      "start": 2529.599,
      "duration": 6.121
    },
    {
      "text": "works and why GPT is using it GPT is",
      "start": 2532.4,
      "duration": 5.48
    },
    {
      "text": "using it because it also does not want a",
      "start": 2535.72,
      "duration": 3.879
    },
    {
      "text": "very large vocabulary size which would",
      "start": 2537.88,
      "duration": 3.04
    },
    {
      "text": "have been needed in world based",
      "start": 2539.599,
      "duration": 4.281
    },
    {
      "text": "tokenizer and we also need to retain",
      "start": 2540.92,
      "duration": 5.08
    },
    {
      "text": "some kind of root meaning like we know",
      "start": 2543.88,
      "duration": 4.679
    },
    {
      "text": "that EST is a root word so we return",
      "start": 2546.0,
      "duration": 4.48
    },
    {
      "text": "that and we don't want a separate word",
      "start": 2548.559,
      "duration": 3.681
    },
    {
      "text": "for old we don't want a separate token",
      "start": 2550.48,
      "duration": 3.8
    },
    {
      "text": "for older instead I'll just have a",
      "start": 2552.24,
      "duration": 3.52
    },
    {
      "text": "common token for",
      "start": 2554.28,
      "duration": 4.44
    },
    {
      "text": "old so bite pair encoding is the best of",
      "start": 2555.76,
      "duration": 5.359
    },
    {
      "text": "both world it's an efficient vocabulary",
      "start": 2558.72,
      "duration": 5.16
    },
    {
      "text": "size it also retains",
      "start": 2561.119,
      "duration": 6.0
    },
    {
      "text": "meaning awesome so that's why actually",
      "start": 2563.88,
      "duration": 6.12
    },
    {
      "text": "uh GPT uses the bite pair encoding what",
      "start": 2567.119,
      "duration": 5.0
    },
    {
      "text": "we can do right now in code is we can",
      "start": 2570.0,
      "duration": 4.04
    },
    {
      "text": "have a simple implementation of the bite",
      "start": 2572.119,
      "duration": 5.081
    },
    {
      "text": "pair encoder algorithm and let's see uh",
      "start": 2574.04,
      "duration": 4.4
    },
    {
      "text": "how this actually",
      "start": 2577.2,
      "duration": 3.919
    },
    {
      "text": "works so there is a python Library which",
      "start": 2578.44,
      "duration": 5.919
    },
    {
      "text": "is called as tick token and uh this is",
      "start": 2581.119,
      "duration": 5.121
    },
    {
      "text": "the this is the bite pair encoding",
      "start": 2584.359,
      "duration": 3.881
    },
    {
      "text": "tokenizer which is used in open AI",
      "start": 2586.24,
      "duration": 4.4
    },
    {
      "text": "models so we are going to use the same",
      "start": 2588.24,
      "duration": 4.599
    },
    {
      "text": "tick token so you need to First install",
      "start": 2590.64,
      "duration": 5.04
    },
    {
      "text": "the tick token library and then uh you",
      "start": 2592.839,
      "duration": 5.52
    },
    {
      "text": "can actually directly create tokenizers",
      "start": 2595.68,
      "duration": 4.28
    },
    {
      "text": "so what I'm doing is I've created a",
      "start": 2598.359,
      "duration": 4.041
    },
    {
      "text": "tokenizer from tick token and I'm using",
      "start": 2599.96,
      "duration": 5.879
    },
    {
      "text": "the same uh encodings which are used in",
      "start": 2602.4,
      "duration": 6.36
    },
    {
      "text": "gpt2 uh remember our previous tokenizer",
      "start": 2605.839,
      "duration": 5.0
    },
    {
      "text": "class had this encode method and the",
      "start": 2608.76,
      "duration": 4.48
    },
    {
      "text": "decode method similarly here also we",
      "start": 2610.839,
      "duration": 4.641
    },
    {
      "text": "have the encode and decode so let's see",
      "start": 2613.24,
      "duration": 4.4
    },
    {
      "text": "let's see we give this text which we",
      "start": 2615.48,
      "duration": 3.48
    },
    {
      "text": "which we had already given in the",
      "start": 2617.64,
      "duration": 4.199
    },
    {
      "text": "previous example and you see in word",
      "start": 2618.96,
      "duration": 4.68
    },
    {
      "text": "based encoding we have to augment the",
      "start": 2621.839,
      "duration": 4.24
    },
    {
      "text": "dictionary with special words right but",
      "start": 2623.64,
      "duration": 3.919
    },
    {
      "text": "as you will see over",
      "start": 2626.079,
      "duration": 4.081
    },
    {
      "text": "here uh G this bite pair encoding",
      "start": 2627.559,
      "duration": 5.161
    },
    {
      "text": "tokenizer deals with this unknown text",
      "start": 2630.16,
      "duration": 4.439
    },
    {
      "text": "so I've given this text here hello do",
      "start": 2632.72,
      "duration": 4.639
    },
    {
      "text": "you like tea in the sunlit Terra of some",
      "start": 2634.599,
      "duration": 4.96
    },
    {
      "text": "unknown Place some unknown place is not",
      "start": 2637.359,
      "duration": 4.561
    },
    {
      "text": "even a word so ideally the tokenizer",
      "start": 2639.559,
      "duration": 4.28
    },
    {
      "text": "should show an error right let's see if",
      "start": 2641.92,
      "duration": 4.159
    },
    {
      "text": "it's indeed the case so now I do",
      "start": 2643.839,
      "duration": 5.28
    },
    {
      "text": "tokenizer do encode text and I allow the",
      "start": 2646.079,
      "duration": 5.24
    },
    {
      "text": "special character end of text so here",
      "start": 2649.119,
      "duration": 4.841
    },
    {
      "text": "you see when we encode there is no error",
      "start": 2651.319,
      "duration": 5.04
    },
    {
      "text": "so how do you think the bpe tokenizer",
      "start": 2653.96,
      "duration": 4.32
    },
    {
      "text": "dealt with this random",
      "start": 2656.359,
      "duration": 4.361
    },
    {
      "text": "word remember the bite pair encoding",
      "start": 2658.28,
      "duration": 4.96
    },
    {
      "text": "tokenizer is a subword based tokenizer",
      "start": 2660.72,
      "duration": 4.56
    },
    {
      "text": "so it has characters and it has subwords",
      "start": 2663.24,
      "duration": 4.48
    },
    {
      "text": "also as tokens so it is very very likely",
      "start": 2665.28,
      "duration": 3.88
    },
    {
      "text": "that this word is actually an",
      "start": 2667.72,
      "duration": 3.359
    },
    {
      "text": "accumulation of these characters or",
      "start": 2669.16,
      "duration": 4.84
    },
    {
      "text": "subwords that's the advantage of that's",
      "start": 2671.079,
      "duration": 4.76
    },
    {
      "text": "another advantage of B pair encoder",
      "start": 2674.0,
      "duration": 4.2
    },
    {
      "text": "tokenizer we don't need to create",
      "start": 2675.839,
      "duration": 5.121
    },
    {
      "text": "special tokens like unknown",
      "start": 2678.2,
      "duration": 5.399
    },
    {
      "text": "tokens with the with the same vocabulary",
      "start": 2680.96,
      "duration": 5.32
    },
    {
      "text": "of tokens which we have it can actually",
      "start": 2683.599,
      "duration": 4.841
    },
    {
      "text": "uh replace any word which is even not",
      "start": 2686.28,
      "duration": 4.319
    },
    {
      "text": "present in the",
      "start": 2688.44,
      "duration": 4.56
    },
    {
      "text": "vocabulary uh so three advantages of",
      "start": 2690.599,
      "duration": 4.361
    },
    {
      "text": "bite pair encoder first the vocabulary",
      "start": 2693.0,
      "duration": 5.28
    },
    {
      "text": "size is not too high second uh it",
      "start": 2694.96,
      "duration": 5.52
    },
    {
      "text": "retains the meaning of root words and",
      "start": 2698.28,
      "duration": 4.6
    },
    {
      "text": "third it automatically deals with",
      "start": 2700.48,
      "duration": 5.119
    },
    {
      "text": "unknown words so the vocabulary size of",
      "start": 2702.88,
      "duration": 5.4
    },
    {
      "text": "bite pair encoder which was used in gpt2",
      "start": 2705.599,
      "duration": 5.641
    },
    {
      "text": "was around 50,000 you can see that it's",
      "start": 2708.28,
      "duration": 4.559
    },
    {
      "text": "much lesser than the total number of",
      "start": 2711.24,
      "duration": 3.64
    },
    {
      "text": "words in the English language which is",
      "start": 2712.839,
      "duration": 3.641
    },
    {
      "text": "what we might have required if we use",
      "start": 2714.88,
      "duration": 4.08
    },
    {
      "text": "the word based tokenizer you can also",
      "start": 2716.48,
      "duration": 4.879
    },
    {
      "text": "Google this",
      "start": 2718.96,
      "duration": 9.359
    },
    {
      "text": "so number of uh or the vocabulary size",
      "start": 2721.359,
      "duration": 6.96
    },
    {
      "text": "uh vocabulary size of DP tokenizer used",
      "start": 2729.04,
      "duration": 7.84
    },
    {
      "text": "for",
      "start": 2735.119,
      "duration": 5.121
    },
    {
      "text": "gpt2 so let's see so the it has 50,000",
      "start": 2736.88,
      "duration": 6.16
    },
    {
      "text": "tokens yeah so close to around 50,000",
      "start": 2740.24,
      "duration": 5.56
    },
    {
      "text": "tokens so awesome so this encoder method",
      "start": 2743.04,
      "duration": 5.6
    },
    {
      "text": "Works without mentioning special unknown",
      "start": 2745.8,
      "duration": 5.2
    },
    {
      "text": "tokens also and now we can also do",
      "start": 2748.64,
      "duration": 4.52
    },
    {
      "text": "decoder and now you can see hello do you",
      "start": 2751.0,
      "duration": 4.2
    },
    {
      "text": "like tea in The sunle Terraces of some",
      "start": 2753.16,
      "duration": 4.48
    },
    {
      "text": "unknown place even the decoder works",
      "start": 2755.2,
      "duration": 4.76
    },
    {
      "text": "perfectly so instead of writing our own",
      "start": 2757.64,
      "duration": 4.479
    },
    {
      "text": "word based tokenizer what actually works",
      "start": 2759.96,
      "duration": 4.44
    },
    {
      "text": "the best is using subword based",
      "start": 2762.119,
      "duration": 4.24
    },
    {
      "text": "tokenizer which employs bite pair",
      "start": 2764.4,
      "duration": 4.56
    },
    {
      "text": "encoding it's already provided to us by",
      "start": 2766.359,
      "duration": 5.801
    },
    {
      "text": "The Tick token Library we can use tick",
      "start": 2768.96,
      "duration": 6.28
    },
    {
      "text": "token. get encoding to create the",
      "start": 2772.16,
      "duration": 5.32
    },
    {
      "text": "tokenizer and then we can test the",
      "start": 2775.24,
      "duration": 4.56
    },
    {
      "text": "encode and decode methods there are some",
      "start": 2777.48,
      "duration": 5.48
    },
    {
      "text": "notes here so the first note which is",
      "start": 2779.8,
      "duration": 5.559
    },
    {
      "text": "important is that the BP tokenizer",
      "start": 2782.96,
      "duration": 4.28
    },
    {
      "text": "encodes and decodes unknown work words",
      "start": 2785.359,
      "duration": 4.561
    },
    {
      "text": "such as some unknown Place correctly it",
      "start": 2787.24,
      "duration": 5.359
    },
    {
      "text": "can handle any unknown word so how does",
      "start": 2789.92,
      "duration": 4.76
    },
    {
      "text": "it achieve this that's the important",
      "start": 2792.599,
      "duration": 3.921
    },
    {
      "text": "question right so the algorithm",
      "start": 2794.68,
      "duration": 4.04
    },
    {
      "text": "underlying BP breaks down words that",
      "start": 2796.52,
      "duration": 4.2
    },
    {
      "text": "aren't in the predefined vocabulary into",
      "start": 2798.72,
      "duration": 3.839
    },
    {
      "text": "smaller subword units or individual",
      "start": 2800.72,
      "duration": 4.68
    },
    {
      "text": "characters as I said and this enables it",
      "start": 2802.559,
      "duration": 5.321
    },
    {
      "text": "to handle out of vocabulary words so",
      "start": 2805.4,
      "duration": 5.0
    },
    {
      "text": "let's say you are given this and you",
      "start": 2807.88,
      "duration": 4.84
    },
    {
      "text": "want to encode it right if you use a",
      "start": 2810.4,
      "duration": 4.28
    },
    {
      "text": "word based tokenizer it will failure",
      "start": 2812.72,
      "duration": 3.44
    },
    {
      "text": "because these words are not there this",
      "start": 2814.68,
      "duration": 2.919
    },
    {
      "text": "is not a word",
      "start": 2816.16,
      "duration": 3.84
    },
    {
      "text": "but if you encode this it will print out",
      "start": 2817.599,
      "duration": 5.321
    },
    {
      "text": "some token IDs why this is the case so I",
      "start": 2820.0,
      "duration": 4.44
    },
    {
      "text": "have a screenshot",
      "start": 2822.92,
      "duration": 4.36
    },
    {
      "text": "here so let's look at the screenshot",
      "start": 2824.44,
      "duration": 5.24
    },
    {
      "text": "what the BP algorithm is actually doing",
      "start": 2827.28,
      "duration": 4.44
    },
    {
      "text": "is that all of these which are",
      "start": 2829.68,
      "duration": 4.24
    },
    {
      "text": "individual subwords or sub characters",
      "start": 2831.72,
      "duration": 5.639
    },
    {
      "text": "are tokens so whenever this new word is",
      "start": 2833.92,
      "duration": 5.199
    },
    {
      "text": "given to us it's usually broken down",
      "start": 2837.359,
      "duration": 3.521
    },
    {
      "text": "into these tokens which are either",
      "start": 2839.119,
      "duration": 4.24
    },
    {
      "text": "subwords or which are characters so",
      "start": 2840.88,
      "duration": 4.32
    },
    {
      "text": "that's why that's how it deals with out",
      "start": 2843.359,
      "duration": 3.72
    },
    {
      "text": "of vocabulary words like this which",
      "start": 2845.2,
      "duration": 4.04
    },
    {
      "text": "don't even mean anything and of course",
      "start": 2847.079,
      "duration": 4.24
    },
    {
      "text": "then the decode method also works and it",
      "start": 2849.24,
      "duration": 4.76
    },
    {
      "text": "returns back the same sentence to",
      "start": 2851.319,
      "duration": 5.681
    },
    {
      "text": "us so here I've done some exploratory",
      "start": 2854.0,
      "duration": 6.599
    },
    {
      "text": "thing like I got the encodings for gpt2",
      "start": 2857.0,
      "duration": 5.079
    },
    {
      "text": "and I got the encodings which are",
      "start": 2860.599,
      "duration": 4.48
    },
    {
      "text": "commonly associated with gpt3 and gp4",
      "start": 2862.079,
      "duration": 5.321
    },
    {
      "text": "and I printed out the vocabulary size so",
      "start": 2865.079,
      "duration": 3.921
    },
    {
      "text": "you can see that the vocabulary size",
      "start": 2867.4,
      "duration": 4.719
    },
    {
      "text": "subsequently increases because as better",
      "start": 2869.0,
      "duration": 5.24
    },
    {
      "text": "and better models of GPT are trained we",
      "start": 2872.119,
      "duration": 3.881
    },
    {
      "text": "have a larger and larger and larger",
      "start": 2874.24,
      "duration": 4.24
    },
    {
      "text": "vocabulary",
      "start": 2876.0,
      "duration": 4.8
    },
    {
      "text": "okay so this is how the bite pair",
      "start": 2878.48,
      "duration": 4.92
    },
    {
      "text": "encoding algorithm actually works we",
      "start": 2880.8,
      "duration": 4.84
    },
    {
      "text": "will not cover character level encoding",
      "start": 2883.4,
      "duration": 4.439
    },
    {
      "text": "because that is not generally employed",
      "start": 2885.64,
      "duration": 5.08
    },
    {
      "text": "in large scale llm",
      "start": 2887.839,
      "duration": 5.561
    },
    {
      "text": "models so now let's go to the start of",
      "start": 2890.72,
      "duration": 4.76
    },
    {
      "text": "this lecture and let us see how much",
      "start": 2893.4,
      "duration": 5.0
    },
    {
      "text": "have we covered so",
      "start": 2895.48,
      "duration": 2.92
    },
    {
      "text": "far okay so yeah right over here so",
      "start": 2899.319,
      "duration": 6.0
    },
    {
      "text": "until now we have covered I think",
      "start": 2903.64,
      "duration": 3.64
    },
    {
      "text": "tokenization that's the first section",
      "start": 2905.319,
      "duration": 3.52
    },
    {
      "text": "which we have covered it took some time",
      "start": 2907.28,
      "duration": 3.839
    },
    {
      "text": "but it's very important now let us move",
      "start": 2908.839,
      "duration": 3.961
    },
    {
      "text": "on to the next Parts which are token",
      "start": 2911.119,
      "duration": 3.761
    },
    {
      "text": "embedding positional embedding and input",
      "start": 2912.8,
      "duration": 5.12
    },
    {
      "text": "embedding so uh if you look at this",
      "start": 2914.88,
      "duration": 5.199
    },
    {
      "text": "figure this is what we are covering up",
      "start": 2917.92,
      "duration": 3.84
    },
    {
      "text": "till now we have finished token",
      "start": 2920.079,
      "duration": 3.561
    },
    {
      "text": "tokenization and we have finished token",
      "start": 2921.76,
      "duration": 4.04
    },
    {
      "text": "IDs and now we will cover token",
      "start": 2923.64,
      "duration": 5.12
    },
    {
      "text": "embeddings and positional embeddings so",
      "start": 2925.8,
      "duration": 4.559
    },
    {
      "text": "if you want to take a break over here",
      "start": 2928.76,
      "duration": 3.52
    },
    {
      "text": "please pause this video take a break and",
      "start": 2930.359,
      "duration": 4.48
    },
    {
      "text": "then we'll start with these sections",
      "start": 2932.28,
      "duration": 4.799
    },
    {
      "text": "okay so I hope everyone is with with me",
      "start": 2934.839,
      "duration": 4.76
    },
    {
      "text": "until now it has been a long lecture so",
      "start": 2937.079,
      "duration": 4.681
    },
    {
      "text": "far but I hope you are energized and",
      "start": 2939.599,
      "duration": 5.361
    },
    {
      "text": "excited for the next part and here as I",
      "start": 2941.76,
      "duration": 5.88
    },
    {
      "text": "mentioned we have got the token IDs but",
      "start": 2944.96,
      "duration": 5.359
    },
    {
      "text": "these token IDs need to be encoded into",
      "start": 2947.64,
      "duration": 5.439
    },
    {
      "text": "a higher dimensional Vector space uh",
      "start": 2950.319,
      "duration": 4.48
    },
    {
      "text": "that's called token embeddings and then",
      "start": 2953.079,
      "duration": 3.601
    },
    {
      "text": "we need to add positional embeddings to",
      "start": 2954.799,
      "duration": 3.8
    },
    {
      "text": "these token embeddings to create input",
      "start": 2956.68,
      "duration": 4.36
    },
    {
      "text": "embeddings which will be the final input",
      "start": 2958.599,
      "duration": 4.801
    },
    {
      "text": "to the large language model so let's",
      "start": 2961.04,
      "duration": 5.44
    },
    {
      "text": "begin this embedding Journey uh and this",
      "start": 2963.4,
      "duration": 7.159
    },
    {
      "text": "joury Journey Begins with uh token",
      "start": 2966.48,
      "duration": 7.52
    },
    {
      "text": "embedding so okay let me scroll down to",
      "start": 2970.559,
      "duration": 6.841
    },
    {
      "text": "my lecture notes yeah so before actually",
      "start": 2974.0,
      "duration": 5.72
    },
    {
      "text": "coming to token embeddings there is one",
      "start": 2977.4,
      "duration": 4.919
    },
    {
      "text": "more uh small point which I want to",
      "start": 2979.72,
      "duration": 4.119
    },
    {
      "text": "cover and that point is actually",
      "start": 2982.319,
      "duration": 3.601
    },
    {
      "text": "extremely important I could have",
      "start": 2983.839,
      "duration": 3.96
    },
    {
      "text": "directly skipped to the embedding",
      "start": 2985.92,
      "duration": 4.639
    },
    {
      "text": "portion without covering this point but",
      "start": 2987.799,
      "duration": 5.721
    },
    {
      "text": "uh I want this lecture to be as close to",
      "start": 2990.559,
      "duration": 5.201
    },
    {
      "text": "real life as possible so I want to show",
      "start": 2993.52,
      "duration": 4.44
    },
    {
      "text": "you all the Finer Things which need to",
      "start": 2995.76,
      "duration": 3.0
    },
    {
      "text": "be",
      "start": 2997.96,
      "duration": 3.599
    },
    {
      "text": "considered um when dealing with the data",
      "start": 2998.76,
      "duration": 4.88
    },
    {
      "text": "pre-processing and one such thing is",
      "start": 3001.559,
      "duration": 4.161
    },
    {
      "text": "creating input Target",
      "start": 3003.64,
      "duration": 4.4
    },
    {
      "text": "pairs so let's see what this actually",
      "start": 3005.72,
      "duration": 4.96
    },
    {
      "text": "looks like to start discussing about",
      "start": 3008.04,
      "duration": 4.72
    },
    {
      "text": "giving an input to the llm we also need",
      "start": 3010.68,
      "duration": 4.0
    },
    {
      "text": "to have a discussion of what the input",
      "start": 3012.76,
      "duration": 4.599
    },
    {
      "text": "really looks like so we know that llms",
      "start": 3014.68,
      "duration": 5.0
    },
    {
      "text": "predict the next word right that's the",
      "start": 3017.359,
      "duration": 4.561
    },
    {
      "text": "main task which llms do so if you have",
      "start": 3019.68,
      "duration": 4.159
    },
    {
      "text": "the first word llm it will predict the",
      "start": 3021.92,
      "duration": 4.399
    },
    {
      "text": "next word learn if you have llms learn",
      "start": 3023.839,
      "duration": 4.561
    },
    {
      "text": "it predicts the next word two if you",
      "start": 3026.319,
      "duration": 4.321
    },
    {
      "text": "have llms learn to it predicts the next",
      "start": 3028.4,
      "duration": 4.399
    },
    {
      "text": "word predict if you have the llms learn",
      "start": 3030.64,
      "duration": 3.88
    },
    {
      "text": "to predict it predicts the next word",
      "start": 3032.799,
      "duration": 4.681
    },
    {
      "text": "which is one so there is an input and",
      "start": 3034.52,
      "duration": 4.96
    },
    {
      "text": "there is a target but we now need to see",
      "start": 3037.48,
      "duration": 4.079
    },
    {
      "text": "how to represent this in numerical",
      "start": 3039.48,
      "duration": 4.44
    },
    {
      "text": "format and the way this is done is as",
      "start": 3041.559,
      "duration": 4.56
    },
    {
      "text": "follows first we need to determine a",
      "start": 3043.92,
      "duration": 4.879
    },
    {
      "text": "variable which is called as context size",
      "start": 3046.119,
      "duration": 4.761
    },
    {
      "text": "context size is the maximum length of",
      "start": 3048.799,
      "duration": 4.881
    },
    {
      "text": "the input which the llm Sees at one time",
      "start": 3050.88,
      "duration": 4.719
    },
    {
      "text": "which basically means how many tokens",
      "start": 3053.68,
      "duration": 4.159
    },
    {
      "text": "will the llm process C to predict the",
      "start": 3055.599,
      "duration": 4.601
    },
    {
      "text": "next World it can be four tokens it can",
      "start": 3057.839,
      "duration": 5.24
    },
    {
      "text": "be 256 tokens in large models the number",
      "start": 3060.2,
      "duration": 4.84
    },
    {
      "text": "of tokens which the llm sees is usually",
      "start": 3063.079,
      "duration": 4.881
    },
    {
      "text": "pretty large more than 150 more than 200",
      "start": 3065.04,
      "duration": 5.88
    },
    {
      "text": "it can even be more around 500 etc for",
      "start": 3067.96,
      "duration": 4.839
    },
    {
      "text": "now here I'm showing a context size of",
      "start": 3070.92,
      "duration": 4.159
    },
    {
      "text": "four which means that at one time the",
      "start": 3072.799,
      "duration": 4.721
    },
    {
      "text": "llm can see a maximum of",
      "start": 3075.079,
      "duration": 5.72
    },
    {
      "text": "four four tokens so what we do is we",
      "start": 3077.52,
      "duration": 5.48
    },
    {
      "text": "create input output pairs based on this",
      "start": 3080.799,
      "duration": 4.681
    },
    {
      "text": "context size so here is one sample input",
      "start": 3083.0,
      "duration": 4.079
    },
    {
      "text": "output pair",
      "start": 3085.48,
      "duration": 4.48
    },
    {
      "text": "when the llm receives this input it has",
      "start": 3087.079,
      "duration": 4.921
    },
    {
      "text": "to uh predict the next word in the",
      "start": 3089.96,
      "duration": 5.32
    },
    {
      "text": "output so for example if one is an input",
      "start": 3092.0,
      "duration": 4.92
    },
    {
      "text": "the next word which is predicted by the",
      "start": 3095.28,
      "duration": 4.64
    },
    {
      "text": "llm should be word if one and word are",
      "start": 3096.92,
      "duration": 5.639
    },
    {
      "text": "the input the next word predicted by the",
      "start": 3099.92,
      "duration": 5.84
    },
    {
      "text": "LM should be at if one word at is the",
      "start": 3102.559,
      "duration": 5.28
    },
    {
      "text": "input to the llm the next word predicted",
      "start": 3105.76,
      "duration": 5.48
    },
    {
      "text": "is a and if one word at a is the input",
      "start": 3107.839,
      "duration": 5.0
    },
    {
      "text": "to the llm the next word which is",
      "start": 3111.24,
      "duration": 3.079
    },
    {
      "text": "predicted is",
      "start": 3112.839,
      "duration": 4.361
    },
    {
      "text": "time so this is how the input output",
      "start": 3114.319,
      "duration": 5.28
    },
    {
      "text": "pairs need to be constructed so remember",
      "start": 3117.2,
      "duration": 4.44
    },
    {
      "text": "when you look at one input output pair",
      "start": 3119.599,
      "duration": 3.72
    },
    {
      "text": "there are actually multiple prediction",
      "start": 3121.64,
      "duration": 3.919
    },
    {
      "text": "tasks which are happening so if you look",
      "start": 3123.319,
      "duration": 4.201
    },
    {
      "text": "at this input output pair there are four",
      "start": 3125.559,
      "duration": 4.201
    },
    {
      "text": "prediction tasks happening right if one",
      "start": 3127.52,
      "duration": 4.319
    },
    {
      "text": "is the input word is the output if one",
      "start": 3129.76,
      "duration": 4.2
    },
    {
      "text": "word is the input at is the output if",
      "start": 3131.839,
      "duration": 4.921
    },
    {
      "text": "one word at is the input o is the output",
      "start": 3133.96,
      "duration": 4.879
    },
    {
      "text": "if one word at a is the input time is",
      "start": 3136.76,
      "duration": 3.799
    },
    {
      "text": "the output so there are four prediction",
      "start": 3138.839,
      "duration": 3.801
    },
    {
      "text": "tasks so keep this in",
      "start": 3140.559,
      "duration": 5.121
    },
    {
      "text": "mind so what we now need to do right now",
      "start": 3142.64,
      "duration": 5.84
    },
    {
      "text": "is that we have have to if provided a",
      "start": 3145.68,
      "duration": 5.159
    },
    {
      "text": "text sample which is a text sample which",
      "start": 3148.48,
      "duration": 4.76
    },
    {
      "text": "is like this we have to break this text",
      "start": 3150.839,
      "duration": 5.96
    },
    {
      "text": "sample down into input output pairs like",
      "start": 3153.24,
      "duration": 5.44
    },
    {
      "text": "these we have to break down the text",
      "start": 3156.799,
      "duration": 4.241
    },
    {
      "text": "sample into input output pairs like this",
      "start": 3158.68,
      "duration": 4.96
    },
    {
      "text": "based on the context size so let's see",
      "start": 3161.04,
      "duration": 5.079
    },
    {
      "text": "what what this will look",
      "start": 3163.64,
      "duration": 6.199
    },
    {
      "text": "like so our main aim is to uh create the",
      "start": 3166.119,
      "duration": 5.601
    },
    {
      "text": "input output pairs right and the way we",
      "start": 3169.839,
      "duration": 3.641
    },
    {
      "text": "will do it is by using something called",
      "start": 3171.72,
      "duration": 4.2
    },
    {
      "text": "as data loader so before I started",
      "start": 3173.48,
      "duration": 4.56
    },
    {
      "text": "learning LM I have not I had not used",
      "start": 3175.92,
      "duration": 4.919
    },
    {
      "text": "data loader before but it's actually a",
      "start": 3178.04,
      "duration": 4.2
    },
    {
      "text": "very very",
      "start": 3180.839,
      "duration": 4.76
    },
    {
      "text": "useful uh tool I would say which is",
      "start": 3182.24,
      "duration": 5.44
    },
    {
      "text": "provided through P torch and it's just",
      "start": 3185.599,
      "duration": 4.24
    },
    {
      "text": "it just helps to process data in a much",
      "start": 3187.68,
      "duration": 4.48
    },
    {
      "text": "better manner so what we are going to do",
      "start": 3189.839,
      "duration": 3.96
    },
    {
      "text": "here is that we are going to create this",
      "start": 3192.16,
      "duration": 4.04
    },
    {
      "text": "input output Target pairs through a data",
      "start": 3193.799,
      "duration": 4.28
    },
    {
      "text": "loader and we'll use a sliding window",
      "start": 3196.2,
      "duration": 3.72
    },
    {
      "text": "approach don't worry about this",
      "start": 3198.079,
      "duration": 3.681
    },
    {
      "text": "terminologies I'll explain what it",
      "start": 3199.92,
      "duration": 3.96
    },
    {
      "text": "actually means so what we are going to",
      "start": 3201.76,
      "duration": 4.039
    },
    {
      "text": "do here is that let's say this is sample",
      "start": 3203.88,
      "duration": 4.32
    },
    {
      "text": "text in the Heart of the City stood the",
      "start": 3205.799,
      "duration": 4.841
    },
    {
      "text": "old library and let's say we are using a",
      "start": 3208.2,
      "duration": 5.52
    },
    {
      "text": "context size of four okay so I will I",
      "start": 3210.64,
      "duration": 5.52
    },
    {
      "text": "will create two tensors based on the",
      "start": 3213.72,
      "duration": 5.44
    },
    {
      "text": "sample text one will be my input tensor",
      "start": 3216.16,
      "duration": 5.36
    },
    {
      "text": "and one will be my output tensor or the",
      "start": 3219.16,
      "duration": 4.8
    },
    {
      "text": "target tensor let's look at the input",
      "start": 3221.52,
      "duration": 5.36
    },
    {
      "text": "tensor first each row of the input",
      "start": 3223.96,
      "duration": 5.56
    },
    {
      "text": "tensor will be",
      "start": 3226.88,
      "duration": 6.4
    },
    {
      "text": "uh an input to the llm and each row of",
      "start": 3229.52,
      "duration": 5.76
    },
    {
      "text": "the target tensor will be an output of",
      "start": 3233.28,
      "duration": 4.2
    },
    {
      "text": "the llm where is similar to the input",
      "start": 3235.28,
      "duration": 5.16
    },
    {
      "text": "output pair uh which we just saw over",
      "start": 3237.48,
      "duration": 6.119
    },
    {
      "text": "here this is an input output pair so the",
      "start": 3240.44,
      "duration": 6.0
    },
    {
      "text": "first row uh so the first row of the",
      "start": 3243.599,
      "duration": 5.641
    },
    {
      "text": "tensor X and the first row of the tensor",
      "start": 3246.44,
      "duration": 5.08
    },
    {
      "text": "y will be an input output pair the",
      "start": 3249.24,
      "duration": 4.0
    },
    {
      "text": "second row of the tensor X and the",
      "start": 3251.52,
      "duration": 3.319
    },
    {
      "text": "second row of the tensor y will be",
      "start": 3253.24,
      "duration": 4.359
    },
    {
      "text": "another input output pair Etc so using",
      "start": 3254.839,
      "duration": 6.321
    },
    {
      "text": "data loader we'll first collect so using",
      "start": 3257.599,
      "duration": 5.441
    },
    {
      "text": "first using data loaders we'll collect",
      "start": 3261.16,
      "duration": 4.639
    },
    {
      "text": "the inputs in a tensor X where each row",
      "start": 3263.04,
      "duration": 5.6
    },
    {
      "text": "represents one input context and we'll",
      "start": 3265.799,
      "duration": 6.0
    },
    {
      "text": "also create another tensor y Which con",
      "start": 3268.64,
      "duration": 5.04
    },
    {
      "text": "consists of the corresponding prediction",
      "start": 3271.799,
      "duration": 4.121
    },
    {
      "text": "targets so if you look at the prediction",
      "start": 3273.68,
      "duration": 4.48
    },
    {
      "text": "targets they are just the input which is",
      "start": 3275.92,
      "duration": 5.399
    },
    {
      "text": "shifted by one word so remember this is",
      "start": 3278.16,
      "duration": 5.08
    },
    {
      "text": "important the prediction so if you look",
      "start": 3281.319,
      "duration": 4.681
    },
    {
      "text": "at the Target if you look at the first",
      "start": 3283.24,
      "duration": 5.04
    },
    {
      "text": "row of the target it's actually just the",
      "start": 3286.0,
      "duration": 4.839
    },
    {
      "text": "first row of the input but shifted by",
      "start": 3288.28,
      "duration": 4.559
    },
    {
      "text": "one word because we are going to predict",
      "start": 3290.839,
      "duration": 4.601
    },
    {
      "text": "the next word that is important so let's",
      "start": 3292.839,
      "duration": 4.561
    },
    {
      "text": "see how these inut output pairs are",
      "start": 3295.44,
      "duration": 4.6
    },
    {
      "text": "actually created using the data loader",
      "start": 3297.4,
      "duration": 5.28
    },
    {
      "text": "so I'm going to scroll down",
      "start": 3300.04,
      "duration": 6.16
    },
    {
      "text": "um right here yeah so we have uh",
      "start": 3302.68,
      "duration": 5.72
    },
    {
      "text": "implemented this bite pair encoder",
      "start": 3306.2,
      "duration": 4.68
    },
    {
      "text": "tokenizer right and uh using this",
      "start": 3308.4,
      "duration": 5.439
    },
    {
      "text": "tokenizer we encoded some sample text",
      "start": 3310.88,
      "duration": 6.479
    },
    {
      "text": "but now we'll encode the entire uh data",
      "start": 3313.839,
      "duration": 5.28
    },
    {
      "text": "set which we are which we are using in",
      "start": 3317.359,
      "duration": 4.361
    },
    {
      "text": "this lecture so the way to do this again",
      "start": 3319.119,
      "duration": 4.521
    },
    {
      "text": "is we first read this text and store it",
      "start": 3321.72,
      "duration": 3.839
    },
    {
      "text": "in the Raw text and then we use the",
      "start": 3323.64,
      "duration": 4.439
    },
    {
      "text": "tokenizer to encode this raw text this",
      "start": 3325.559,
      "duration": 5.601
    },
    {
      "text": "tokenizer is this bite pair tokenizer",
      "start": 3328.079,
      "duration": 5.04
    },
    {
      "text": "which we have got from tick",
      "start": 3331.16,
      "duration": 4.84
    },
    {
      "text": "token okay so we are encoding this raw",
      "start": 3333.119,
      "duration": 5.361
    },
    {
      "text": "text now and if you print out the length",
      "start": 3336.0,
      "duration": 4.96
    },
    {
      "text": "of the encoding text it is 5145 which",
      "start": 3338.48,
      "duration": 5.72
    },
    {
      "text": "are the number of tokens uh in the",
      "start": 3340.96,
      "duration": 5.639
    },
    {
      "text": "training set after applying the bite",
      "start": 3344.2,
      "duration": 4.879
    },
    {
      "text": "pair encoding tokenizer so now what",
      "start": 3346.599,
      "duration": 4.881
    },
    {
      "text": "we'll be doing here is that uh here I",
      "start": 3349.079,
      "duration": 4.561
    },
    {
      "text": "have just mentioned what the input",
      "start": 3351.48,
      "duration": 5.76
    },
    {
      "text": "output pairs can look like so uh if you",
      "start": 3353.64,
      "duration": 6.199
    },
    {
      "text": "have this X as an input pair and Y as an",
      "start": 3357.24,
      "duration": 6.559
    },
    {
      "text": "output pair if 290 is the input 4920 is",
      "start": 3359.839,
      "duration": 6.841
    },
    {
      "text": "the output if 290a 4920 is the input",
      "start": 3363.799,
      "duration": 5.121
    },
    {
      "text": "2241 is the output so there are",
      "start": 3366.68,
      "duration": 3.879
    },
    {
      "text": "basically four prediction tasks",
      "start": 3368.92,
      "duration": 3.679
    },
    {
      "text": "happening in one input output pair",
      "start": 3370.559,
      "duration": 3.641
    },
    {
      "text": "similar to what we discussed on the",
      "start": 3372.599,
      "duration": 2.361
    },
    {
      "text": "white",
      "start": 3374.2,
      "duration": 3.919
    },
    {
      "text": "board this can also be printed out in",
      "start": 3374.96,
      "duration": 5.76
    },
    {
      "text": "textual format so if and is the input",
      "start": 3378.119,
      "duration": 4.521
    },
    {
      "text": "established is the output if and",
      "start": 3380.72,
      "duration": 3.839
    },
    {
      "text": "established is the input himself is the",
      "start": 3382.64,
      "duration": 4.52
    },
    {
      "text": "output if and EST list himself is the",
      "start": 3384.559,
      "duration": 4.881
    },
    {
      "text": "input in is the output this is what we",
      "start": 3387.16,
      "duration": 5.04
    },
    {
      "text": "want so we want to create input output",
      "start": 3389.44,
      "duration": 5.8
    },
    {
      "text": "pairs such as these so this is an input",
      "start": 3392.2,
      "duration": 5.76
    },
    {
      "text": "output pair and for that we'll use the",
      "start": 3395.24,
      "duration": 4.839
    },
    {
      "text": "data loader so let's see how we'll use",
      "start": 3397.96,
      "duration": 3.2
    },
    {
      "text": "the data",
      "start": 3400.079,
      "duration": 4.121
    },
    {
      "text": "loader uh so it's actually very simple",
      "start": 3401.16,
      "duration": 5.6
    },
    {
      "text": "we first encode the entire text into",
      "start": 3404.2,
      "duration": 5.04
    },
    {
      "text": "token IDs and then what we are going to",
      "start": 3406.76,
      "duration": 5.079
    },
    {
      "text": "do is we are going to chunk this book",
      "start": 3409.24,
      "duration": 5.4
    },
    {
      "text": "into overlapping sequences so this is",
      "start": 3411.839,
      "duration": 4.401
    },
    {
      "text": "one more thing the One More Concept",
      "start": 3414.64,
      "duration": 3.199
    },
    {
      "text": "ccept which I want to explain here is",
      "start": 3416.24,
      "duration": 3.44
    },
    {
      "text": "that before we learn about chunking you",
      "start": 3417.839,
      "duration": 4.361
    },
    {
      "text": "need to understand the concept of",
      "start": 3419.68,
      "duration": 5.96
    },
    {
      "text": "stride so uh let's say here in this case",
      "start": 3422.2,
      "duration": 6.399
    },
    {
      "text": "my first input is in the heart of right",
      "start": 3425.64,
      "duration": 5.439
    },
    {
      "text": "my second input here is the city stood",
      "start": 3428.599,
      "duration": 5.561
    },
    {
      "text": "the so if you uh actually let me scroll",
      "start": 3431.079,
      "duration": 5.121
    },
    {
      "text": "down below where I have the photo",
      "start": 3434.16,
      "duration": 5.159
    },
    {
      "text": "yeah uh so let's look at this second",
      "start": 3436.2,
      "duration": 5.76
    },
    {
      "text": "example first my first input is in the",
      "start": 3439.319,
      "duration": 4.881
    },
    {
      "text": "heart of my second input is the city",
      "start": 3441.96,
      "duration": 4.56
    },
    {
      "text": "stood the so let's mark these two inputs",
      "start": 3444.2,
      "duration": 3.919
    },
    {
      "text": "let's mark the first input and the",
      "start": 3446.52,
      "duration": 5.279
    },
    {
      "text": "second input so here the stride is four",
      "start": 3448.119,
      "duration": 5.641
    },
    {
      "text": "because the input moves by four",
      "start": 3451.799,
      "duration": 4.04
    },
    {
      "text": "positions so if this is the first input",
      "start": 3453.76,
      "duration": 3.799
    },
    {
      "text": "to construct my second input I have to",
      "start": 3455.839,
      "duration": 5.52
    },
    {
      "text": "move 1 2 3 and four then to construct my",
      "start": 3457.559,
      "duration": 6.04
    },
    {
      "text": "third input I will move to the this",
      "start": 3461.359,
      "duration": 4.081
    },
    {
      "text": "position then my third input will be old",
      "start": 3463.599,
      "duration": 4.601
    },
    {
      "text": "library or Relic so this is tried equal",
      "start": 3465.44,
      "duration": 6.56
    },
    {
      "text": "to 4 so here what I'm doing is uh I'm",
      "start": 3468.2,
      "duration": 6.04
    },
    {
      "text": "not missing out any word but between my",
      "start": 3472.0,
      "duration": 4.64
    },
    {
      "text": "different inputs there is no overlap",
      "start": 3474.24,
      "duration": 4.48
    },
    {
      "text": "now in this in this first example you",
      "start": 3476.64,
      "duration": 4.959
    },
    {
      "text": "see stride equal to one and why is this",
      "start": 3478.72,
      "duration": 4.599
    },
    {
      "text": "stride equal to one because the first",
      "start": 3481.599,
      "duration": 4.0
    },
    {
      "text": "input here is in the heart of and the",
      "start": 3483.319,
      "duration": 4.921
    },
    {
      "text": "second input is the heart of the so see",
      "start": 3485.599,
      "duration": 4.2
    },
    {
      "text": "there is an overlap between the first",
      "start": 3488.24,
      "duration": 3.52
    },
    {
      "text": "input and the second input because the",
      "start": 3489.799,
      "duration": 4.56
    },
    {
      "text": "stride is one if the stride is four it",
      "start": 3491.76,
      "duration": 5.279
    },
    {
      "text": "leads to this this second example over",
      "start": 3494.359,
      "duration": 6.0
    },
    {
      "text": "here so the stride actually determines",
      "start": 3497.039,
      "duration": 5.681
    },
    {
      "text": "how much we move from one input batch to",
      "start": 3500.359,
      "duration": 5.0
    },
    {
      "text": "the next input batch so that is another",
      "start": 3502.72,
      "duration": 4.48
    },
    {
      "text": "important variable when we construct the",
      "start": 3505.359,
      "duration": 4.321
    },
    {
      "text": "input data so what so that's why it's",
      "start": 3507.2,
      "duration": 4.68
    },
    {
      "text": "called as a sliding window approach if",
      "start": 3509.68,
      "duration": 4.119
    },
    {
      "text": "the stride is equal to one we just slide",
      "start": 3511.88,
      "duration": 4.28
    },
    {
      "text": "by one to create the next input but if",
      "start": 3513.799,
      "duration": 4.721
    },
    {
      "text": "the stride is equal to four we stride by",
      "start": 3516.16,
      "duration": 5.159
    },
    {
      "text": "four to create the next input so that's",
      "start": 3518.52,
      "duration": 4.92
    },
    {
      "text": "why it's sliding window approach so what",
      "start": 3521.319,
      "duration": 4.841
    },
    {
      "text": "we are doing here is that the final",
      "start": 3523.44,
      "duration": 5.52
    },
    {
      "text": "input tensor will be input IDs and the",
      "start": 3526.16,
      "duration": 5.399
    },
    {
      "text": "target tensor will be Target IDs so to",
      "start": 3528.96,
      "duration": 4.32
    },
    {
      "text": "visualize this you can just think of",
      "start": 3531.559,
      "duration": 4.76
    },
    {
      "text": "these two tensors over here yeah the",
      "start": 3533.28,
      "duration": 3.92
    },
    {
      "text": "input",
      "start": 3536.319,
      "duration": 3.8
    },
    {
      "text": "tensor the input tensor will be X and",
      "start": 3537.2,
      "duration": 5.599
    },
    {
      "text": "the target tensor will be the target ID",
      "start": 3540.119,
      "duration": 4.48
    },
    {
      "text": "and what we are doing here is that we",
      "start": 3542.799,
      "duration": 4.681
    },
    {
      "text": "are appending so we are first looping",
      "start": 3544.599,
      "duration": 5.641
    },
    {
      "text": "over all the token IDs based on the",
      "start": 3547.48,
      "duration": 4.599
    },
    {
      "text": "context length and the stride which we",
      "start": 3550.24,
      "duration": 4.44
    },
    {
      "text": "have and then we are appending the",
      "start": 3552.079,
      "duration": 5.0
    },
    {
      "text": "inputs to the input chunk so we are",
      "start": 3554.68,
      "duration": 5.399
    },
    {
      "text": "appending the input uh input chunks to",
      "start": 3557.079,
      "duration": 5.601
    },
    {
      "text": "the input ID sensor and we're appending",
      "start": 3560.079,
      "duration": 5.401
    },
    {
      "text": "the target chunk to the Target ID sensor",
      "start": 3562.68,
      "duration": 4.679
    },
    {
      "text": "and what is the input chunk is just the",
      "start": 3565.48,
      "duration": 4.04
    },
    {
      "text": "token it's just the four token IDs at",
      "start": 3567.359,
      "duration": 4.361
    },
    {
      "text": "one time that's it and what is the",
      "start": 3569.52,
      "duration": 4.2
    },
    {
      "text": "target chunk it's just the input chunk",
      "start": 3571.72,
      "duration": 4.399
    },
    {
      "text": "shifted by one so let me show this to",
      "start": 3573.72,
      "duration": 5.04
    },
    {
      "text": "you in here so when the code is",
      "start": 3576.119,
      "duration": 6.041
    },
    {
      "text": "implemented uh the this first row is the",
      "start": 3578.76,
      "duration": 5.88
    },
    {
      "text": "input chunk that is appended to the",
      "start": 3582.16,
      "duration": 4.959
    },
    {
      "text": "input tensor this first row of the",
      "start": 3584.64,
      "duration": 4.199
    },
    {
      "text": "target is the target chunk that is",
      "start": 3587.119,
      "duration": 3.96
    },
    {
      "text": "appended in the first iteration in the",
      "start": 3588.839,
      "duration": 4.041
    },
    {
      "text": "second iteration the second row is",
      "start": 3591.079,
      "duration": 3.561
    },
    {
      "text": "appended second row of the input and the",
      "start": 3592.88,
      "duration": 4.159
    },
    {
      "text": "second row of the target similarly we",
      "start": 3594.64,
      "duration": 6.0
    },
    {
      "text": "Loop over the entire text and we append",
      "start": 3597.039,
      "duration": 5.401
    },
    {
      "text": "the input tensor and we append to the",
      "start": 3600.64,
      "duration": 5.199
    },
    {
      "text": "Target tensor so at the end of this data",
      "start": 3602.44,
      "duration": 5.399
    },
    {
      "text": "loader when this data loader is created",
      "start": 3605.839,
      "duration": 4.801
    },
    {
      "text": "we have the inputs and",
      "start": 3607.839,
      "duration": 6.28
    },
    {
      "text": "outputs uh so I hope you are",
      "start": 3610.64,
      "duration": 5.159
    },
    {
      "text": "understanding this part this is a bit of",
      "start": 3614.119,
      "duration": 4.121
    },
    {
      "text": "a detail but it's very very important",
      "start": 3615.799,
      "duration": 5.081
    },
    {
      "text": "because uh what this data loader enables",
      "start": 3618.24,
      "duration": 5.2
    },
    {
      "text": "us to do is enables to include the",
      "start": 3620.88,
      "duration": 4.159
    },
    {
      "text": "stride and later you will see that it",
      "start": 3623.44,
      "duration": 4.32
    },
    {
      "text": "also enables us to to batch processing",
      "start": 3625.039,
      "duration": 5.401
    },
    {
      "text": "parallel processing Etc until now just",
      "start": 3627.76,
      "duration": 4.559
    },
    {
      "text": "remember that we have created input",
      "start": 3630.44,
      "duration": 3.72
    },
    {
      "text": "output pairs like this we have created",
      "start": 3632.319,
      "duration": 3.641
    },
    {
      "text": "input pair and we have created output",
      "start": 3634.16,
      "duration": 4.399
    },
    {
      "text": "pair and now what we have so this is",
      "start": 3635.96,
      "duration": 5.44
    },
    {
      "text": "just the this is the just the data set",
      "start": 3638.559,
      "duration": 4.721
    },
    {
      "text": "so we created the input data set and we",
      "start": 3641.4,
      "duration": 3.959
    },
    {
      "text": "have created the output data set we have",
      "start": 3643.28,
      "duration": 4.279
    },
    {
      "text": "to pass this data set now to the data",
      "start": 3645.359,
      "duration": 4.68
    },
    {
      "text": "loader so what the data loader does is",
      "start": 3647.559,
      "duration": 5.081
    },
    {
      "text": "that it takes the data set uh and then",
      "start": 3650.039,
      "duration": 4.641
    },
    {
      "text": "it also takes some other attributes like",
      "start": 3652.64,
      "duration": 3.88
    },
    {
      "text": "we have to specify the batch size this",
      "start": 3654.68,
      "duration": 4.04
    },
    {
      "text": "is very important batch size is",
      "start": 3656.52,
      "duration": 4.48
    },
    {
      "text": "basically how many input batches do you",
      "start": 3658.72,
      "duration": 3.92
    },
    {
      "text": "want to process at one time before",
      "start": 3661.0,
      "duration": 4.079
    },
    {
      "text": "updating the parameters this this is the",
      "start": 3662.64,
      "duration": 4.24
    },
    {
      "text": "exact same notion as the batch size in",
      "start": 3665.079,
      "duration": 3.28
    },
    {
      "text": "other machine learning",
      "start": 3666.88,
      "duration": 3.959
    },
    {
      "text": "Frameworks so a batch size of one will",
      "start": 3668.359,
      "duration": 5.041
    },
    {
      "text": "mean that only one input output pair is",
      "start": 3670.839,
      "duration": 4.401
    },
    {
      "text": "processed at one time before updating a",
      "start": 3673.4,
      "duration": 4.439
    },
    {
      "text": "parameter if batch size of four it means",
      "start": 3675.24,
      "duration": 4.44
    },
    {
      "text": "four input output pairs are processed",
      "start": 3677.839,
      "duration": 4.24
    },
    {
      "text": "before updating the parameters there are",
      "start": 3679.68,
      "duration": 4.08
    },
    {
      "text": "some other things here such as number of",
      "start": 3682.079,
      "duration": 4.401
    },
    {
      "text": "workers number of workers means how many",
      "start": 3683.76,
      "duration": 5.12
    },
    {
      "text": "threads you want to split the code on",
      "start": 3686.48,
      "duration": 4.16
    },
    {
      "text": "your CPU let's say for parallel",
      "start": 3688.88,
      "duration": 3.8
    },
    {
      "text": "processing you don't need to worry about",
      "start": 3690.64,
      "duration": 4.88
    },
    {
      "text": "this right now so data data loader is",
      "start": 3692.68,
      "duration": 5.119
    },
    {
      "text": "basically something which can be used to",
      "start": 3695.52,
      "duration": 4.279
    },
    {
      "text": "process the data set basically the input",
      "start": 3697.799,
      "duration": 5.441
    },
    {
      "text": "output tensors which we described here",
      "start": 3699.799,
      "duration": 5.201
    },
    {
      "text": "uh what the data loader will do is that",
      "start": 3703.24,
      "duration": 3.92
    },
    {
      "text": "it will go over these input and output",
      "start": 3705.0,
      "duration": 5.599
    },
    {
      "text": "tensors and it will generate uh the",
      "start": 3707.16,
      "duration": 6.72
    },
    {
      "text": "input output batches so after creating",
      "start": 3710.599,
      "duration": 5.041
    },
    {
      "text": "the data loader you can actually print",
      "start": 3713.88,
      "duration": 4.36
    },
    {
      "text": "the input batch you can print the output",
      "start": 3715.64,
      "duration": 5.719
    },
    {
      "text": "batch and this is what it looks like uh",
      "start": 3718.24,
      "duration": 4.799
    },
    {
      "text": "so this is the first batch of input this",
      "start": 3721.359,
      "duration": 3.48
    },
    {
      "text": "is the first batch of output this is the",
      "start": 3723.039,
      "duration": 3.32
    },
    {
      "text": "second batch of input and this is the",
      "start": 3724.839,
      "duration": 4.48
    },
    {
      "text": "second batch of output uh you can also",
      "start": 3726.359,
      "duration": 5.081
    },
    {
      "text": "do a batch size of eight and when you",
      "start": 3729.319,
      "duration": 4.0
    },
    {
      "text": "use the data loader with a batch size of",
      "start": 3731.44,
      "duration": 3.72
    },
    {
      "text": "eight this is the these are the input",
      "start": 3733.319,
      "duration": 4.161
    },
    {
      "text": "output tensors so since we are doing a",
      "start": 3735.16,
      "duration": 5.08
    },
    {
      "text": "batch size of eight it means the LM",
      "start": 3737.48,
      "duration": 4.52
    },
    {
      "text": "parameters which we'll look at later",
      "start": 3740.24,
      "duration": 4.68
    },
    {
      "text": "will be updated only after this entire",
      "start": 3742.0,
      "duration": 4.76
    },
    {
      "text": "batch is processed so you'll see the",
      "start": 3744.92,
      "duration": 4.679
    },
    {
      "text": "input has actually eight eight inputs",
      "start": 3746.76,
      "duration": 5.16
    },
    {
      "text": "and the output has eight rows so each",
      "start": 3749.599,
      "duration": 3.96
    },
    {
      "text": "row so this is the first input and this",
      "start": 3751.92,
      "duration": 3.119
    },
    {
      "text": "is the first output this is a second",
      "start": 3753.559,
      "duration": 3.601
    },
    {
      "text": "input and this is a second output what",
      "start": 3755.039,
      "duration": 3.56
    },
    {
      "text": "this actually means that is that there",
      "start": 3757.16,
      "duration": 3.28
    },
    {
      "text": "are four words in the input and there",
      "start": 3758.599,
      "duration": 3.041
    },
    {
      "text": "are four words which need to be",
      "start": 3760.44,
      "duration": 2.96
    },
    {
      "text": "predicted in the output like the next",
      "start": 3761.64,
      "duration": 4.56
    },
    {
      "text": "word so each input output pair has four",
      "start": 3763.4,
      "duration": 5.879
    },
    {
      "text": "prediction tasks which we learned about",
      "start": 3766.2,
      "duration": 7.48
    },
    {
      "text": "before okay so uh until now what we have",
      "start": 3769.279,
      "duration": 7.361
    },
    {
      "text": "done is that we saw the uh word",
      "start": 3773.68,
      "duration": 5.439
    },
    {
      "text": "embedding we saw the token word",
      "start": 3776.64,
      "duration": 4.199
    },
    {
      "text": "tokenization we saw the bite pair",
      "start": 3779.119,
      "duration": 4.081
    },
    {
      "text": "encoding which is subword tokenization",
      "start": 3780.839,
      "duration": 6.161
    },
    {
      "text": "before coming to uh Vector embedding or",
      "start": 3783.2,
      "duration": 5.159
    },
    {
      "text": "positional embedding it was very",
      "start": 3787.0,
      "duration": 3.119
    },
    {
      "text": "important for me to tell you how the",
      "start": 3788.359,
      "duration": 4.521
    },
    {
      "text": "data itself is loaded how the inputs are",
      "start": 3790.119,
      "duration": 5.761
    },
    {
      "text": "given and how the outputs are given to",
      "start": 3792.88,
      "duration": 5.6
    },
    {
      "text": "the llm and I also wanted to show you",
      "start": 3795.88,
      "duration": 4.64
    },
    {
      "text": "the importance of context size and",
      "start": 3798.48,
      "duration": 4.44
    },
    {
      "text": "stride if you did not understand this",
      "start": 3800.52,
      "duration": 5.36
    },
    {
      "text": "data loader uh data loader part if you",
      "start": 3802.92,
      "duration": 4.639
    },
    {
      "text": "did not understand data sets and data",
      "start": 3805.88,
      "duration": 5.0
    },
    {
      "text": "loaders I'll also share this link in the",
      "start": 3807.559,
      "duration": 6.52
    },
    {
      "text": "uh information section but you can you",
      "start": 3810.88,
      "duration": 6.04
    },
    {
      "text": "can now try to proed proceed to the next",
      "start": 3814.079,
      "duration": 5.441
    },
    {
      "text": "part we don't strictly need to",
      "start": 3816.92,
      "duration": 4.159
    },
    {
      "text": "understand data sets and data loaders",
      "start": 3819.52,
      "duration": 2.839
    },
    {
      "text": "before understanding the token",
      "start": 3821.079,
      "duration": 3.04
    },
    {
      "text": "embeddings which I'm coming to right now",
      "start": 3822.359,
      "duration": 3.92
    },
    {
      "text": "in the next section so again you can",
      "start": 3824.119,
      "duration": 5.72
    },
    {
      "text": "take a break here and uh Focus your full",
      "start": 3826.279,
      "duration": 5.441
    },
    {
      "text": "energy on what is going to happen next",
      "start": 3829.839,
      "duration": 3.161
    },
    {
      "text": "which is token",
      "start": 3831.72,
      "duration": 4.16
    },
    {
      "text": "embeddings okay so now we are ready to",
      "start": 3833.0,
      "duration": 4.599
    },
    {
      "text": "proceed with the next section which is",
      "start": 3835.88,
      "duration": 4.239
    },
    {
      "text": "token embeddings so up till now we have",
      "start": 3837.599,
      "duration": 4.44
    },
    {
      "text": "looked at tokenization we have looked at",
      "start": 3840.119,
      "duration": 4.401
    },
    {
      "text": "converting tokens to token IDs we have",
      "start": 3842.039,
      "duration": 4.161
    },
    {
      "text": "looked at the bite pair encoding we have",
      "start": 3844.52,
      "duration": 3.72
    },
    {
      "text": "looked at data data sets and data",
      "start": 3846.2,
      "duration": 4.32
    },
    {
      "text": "loaders now let's come to the next step",
      "start": 3848.24,
      "duration": 4.0
    },
    {
      "text": "which is token",
      "start": 3850.52,
      "duration": 5.48
    },
    {
      "text": "embeddings um so before actually looking",
      "start": 3852.24,
      "duration": 6.24
    },
    {
      "text": "at the code for token embeddings I also",
      "start": 3856.0,
      "duration": 4.799
    },
    {
      "text": "want to intuitively show you why token",
      "start": 3858.48,
      "duration": 4.799
    },
    {
      "text": "embeddings are needed in the first place",
      "start": 3860.799,
      "duration": 6.0
    },
    {
      "text": "and uh why can't we just use token ID",
      "start": 3863.279,
      "duration": 5.721
    },
    {
      "text": "as inputs so we saw the data sets and",
      "start": 3866.799,
      "duration": 4.04
    },
    {
      "text": "data loaders and we saw that inputs have",
      "start": 3869.0,
      "duration": 4.16
    },
    {
      "text": "token IDs right why can't we just leave",
      "start": 3870.839,
      "duration": 4.681
    },
    {
      "text": "it at that stage why are embeddings",
      "start": 3873.16,
      "duration": 5.24
    },
    {
      "text": "needed so to motivate the concept of why",
      "start": 3875.52,
      "duration": 4.2
    },
    {
      "text": "embeddings are",
      "start": 3878.4,
      "duration": 3.959
    },
    {
      "text": "needed uh let's look at this so",
      "start": 3879.72,
      "duration": 5.48
    },
    {
      "text": "computers need numeric numerical",
      "start": 3882.359,
      "duration": 5.361
    },
    {
      "text": "information right it requires numerical",
      "start": 3885.2,
      "duration": 4.8
    },
    {
      "text": "representation of words so how can we",
      "start": 3887.72,
      "duration": 5.079
    },
    {
      "text": "represent words in numbers you might you",
      "start": 3890.0,
      "duration": 4.359
    },
    {
      "text": "might be saying that this is already",
      "start": 3892.799,
      "duration": 3.881
    },
    {
      "text": "done in token IDs right we can just",
      "start": 3894.359,
      "duration": 4.601
    },
    {
      "text": "assign one token ID to each world and",
      "start": 3896.68,
      "duration": 4.399
    },
    {
      "text": "that's it we can maybe even assign",
      "start": 3898.96,
      "duration": 4.24
    },
    {
      "text": "random numbers to each world so then",
      "start": 3901.079,
      "duration": 4.28
    },
    {
      "text": "every word will be a number and computer",
      "start": 3903.2,
      "duration": 4.96
    },
    {
      "text": "or the llm rather would understand that",
      "start": 3905.359,
      "duration": 5.44
    },
    {
      "text": "information what's the problem with",
      "start": 3908.16,
      "duration": 5.24
    },
    {
      "text": "that one of the major problem with that",
      "start": 3910.799,
      "duration": 4.441
    },
    {
      "text": "is that let's say if you have two words",
      "start": 3913.4,
      "duration": 3.919
    },
    {
      "text": "cat and kitten these words are",
      "start": 3915.24,
      "duration": 4.68
    },
    {
      "text": "semantically related but if you randomly",
      "start": 3917.319,
      "duration": 4.52
    },
    {
      "text": "assign token IDs or numbers to these",
      "start": 3919.92,
      "duration": 4.84
    },
    {
      "text": "words that semantic relation or that",
      "start": 3921.839,
      "duration": 4.641
    },
    {
      "text": "meaning between these words is not",
      "start": 3924.76,
      "duration": 5.0
    },
    {
      "text": "captured at all because let's say the",
      "start": 3926.48,
      "duration": 5.879
    },
    {
      "text": "token ID for cat is 34 let's say or the",
      "start": 3929.76,
      "duration": 4.44
    },
    {
      "text": "number associated with kitten is minus",
      "start": 3932.359,
      "duration": 4.48
    },
    {
      "text": "13 we are nowhere encoding this",
      "start": 3934.2,
      "duration": 4.839
    },
    {
      "text": "information right that cat and kitten",
      "start": 3936.839,
      "duration": 4.48
    },
    {
      "text": "are actually related to each other so",
      "start": 3939.039,
      "duration": 4.361
    },
    {
      "text": "then you might think that what about one",
      "start": 3941.319,
      "duration": 4.201
    },
    {
      "text": "hot encoding what if I create a",
      "start": 3943.4,
      "duration": 4.48
    },
    {
      "text": "dictionary of words and wherever that",
      "start": 3945.52,
      "duration": 4.0
    },
    {
      "text": "particular word appears I'll just",
      "start": 3947.88,
      "duration": 3.56
    },
    {
      "text": "represented by one all others will be",
      "start": 3949.52,
      "duration": 4.88
    },
    {
      "text": "zero so then dog let's say will be 0 00",
      "start": 3951.44,
      "duration": 6.44
    },
    {
      "text": "1 0 0 dot dot dot dot uh puppy would be",
      "start": 3954.4,
      "duration": 6.6
    },
    {
      "text": "something like 00 0 1 0 0 dot dot dot so",
      "start": 3957.88,
      "duration": 4.399
    },
    {
      "text": "the one will appear at different",
      "start": 3961.0,
      "duration": 2.92
    },
    {
      "text": "positions for dog and",
      "start": 3962.279,
      "duration": 4.52
    },
    {
      "text": "puppy the same problem exists for even",
      "start": 3963.92,
      "duration": 5.32
    },
    {
      "text": "this case one hot encoding fails to",
      "start": 3966.799,
      "duration": 4.24
    },
    {
      "text": "capture the semantic relationship",
      "start": 3969.24,
      "duration": 4.799
    },
    {
      "text": "between the words so even if we do one",
      "start": 3971.039,
      "duration": 5.32
    },
    {
      "text": "hot encoding Noh no where we are saying",
      "start": 3974.039,
      "duration": 4.08
    },
    {
      "text": "that dog and puppy are actually related",
      "start": 3976.359,
      "duration": 4.081
    },
    {
      "text": "to each other so the semantic meaning",
      "start": 3978.119,
      "duration": 4.521
    },
    {
      "text": "between the words is lost if we just",
      "start": 3980.44,
      "duration": 5.48
    },
    {
      "text": "used one hot encoding and that's where",
      "start": 3982.64,
      "duration": 5.08
    },
    {
      "text": "token embedding actually comes into the",
      "start": 3985.92,
      "duration": 3.76
    },
    {
      "text": "picture the main idea is let's say if we",
      "start": 3987.72,
      "duration": 5.72
    },
    {
      "text": "have four words dog cat apple and banana",
      "start": 3989.68,
      "duration": 6.439
    },
    {
      "text": "and let's say I represent each word in a",
      "start": 3993.44,
      "duration": 5.48
    },
    {
      "text": "vectorial format and how to get this",
      "start": 3996.119,
      "duration": 4.761
    },
    {
      "text": "Vector let's say each Vector has five",
      "start": 3998.92,
      "duration": 4.199
    },
    {
      "text": "Dimensions based on has a tail is",
      "start": 4000.88,
      "duration": 5.479
    },
    {
      "text": "eatable has four legs makes sound or is",
      "start": 4003.119,
      "duration": 6.081
    },
    {
      "text": "a pet and let's say I fill in the",
      "start": 4006.359,
      "duration": 5.2
    },
    {
      "text": "information for the dog so then dog will",
      "start": 4009.2,
      "duration": 4.04
    },
    {
      "text": "be represented let's say by five",
      "start": 4011.559,
      "duration": 4.081
    },
    {
      "text": "dimensional vector and you can see that",
      "start": 4013.24,
      "duration": 4.839
    },
    {
      "text": "has a tail has four legs makes sound and",
      "start": 4015.64,
      "duration": 5.36
    },
    {
      "text": "is a pet have higher values now when you",
      "start": 4018.079,
      "duration": 5.48
    },
    {
      "text": "encode cat you'll see that has a tail",
      "start": 4021.0,
      "duration": 5.279
    },
    {
      "text": "has high value is eatable has low value",
      "start": 4023.559,
      "duration": 5.081
    },
    {
      "text": "has four legs has high value make sound",
      "start": 4026.279,
      "duration": 4.08
    },
    {
      "text": "has high value and is a pet has high",
      "start": 4028.64,
      "duration": 3.88
    },
    {
      "text": "value now when you look at the vectors",
      "start": 4030.359,
      "duration": 4.521
    },
    {
      "text": "for dog and cat you can see that the",
      "start": 4032.52,
      "duration": 4.36
    },
    {
      "text": "dimensions for which dog is higher is",
      "start": 4034.88,
      "duration": 3.919
    },
    {
      "text": "also the same dimensions for which cat",
      "start": 4036.88,
      "duration": 4.239
    },
    {
      "text": "is higher so there is some meaning which",
      "start": 4038.799,
      "duration": 4.28
    },
    {
      "text": "has been encoded here right you can look",
      "start": 4041.119,
      "duration": 3.48
    },
    {
      "text": "at these vectors and you can say that",
      "start": 4043.079,
      "duration": 3.161
    },
    {
      "text": "these vectors are actually similar to",
      "start": 4044.599,
      "duration": 3.76
    },
    {
      "text": "each other so maybe if you plot these",
      "start": 4046.24,
      "duration": 3.92
    },
    {
      "text": "vectors in five dimensional space they",
      "start": 4048.359,
      "duration": 4.2
    },
    {
      "text": "will be grouped together that that",
      "start": 4050.16,
      "duration": 4.36
    },
    {
      "text": "implies that dog and cat are actually",
      "start": 4052.559,
      "duration": 4.161
    },
    {
      "text": "related they are words which which are",
      "start": 4054.52,
      "duration": 3.519
    },
    {
      "text": "similar to each other they are both",
      "start": 4056.72,
      "duration": 3.72
    },
    {
      "text": "animals they both have four legs they",
      "start": 4058.039,
      "duration": 4.441
    },
    {
      "text": "both make a sound they both are pets",
      "start": 4060.44,
      "duration": 3.28
    },
    {
      "text": "they both have a",
      "start": 4062.48,
      "duration": 4.04
    },
    {
      "text": "tail similarly if you encode apple and",
      "start": 4063.72,
      "duration": 5.879
    },
    {
      "text": "banana for has a tail for has four legs",
      "start": 4066.52,
      "duration": 5.2
    },
    {
      "text": "for mix sound and is AET they will have",
      "start": 4069.599,
      "duration": 5.0
    },
    {
      "text": "lower values but apple and banana will",
      "start": 4071.72,
      "duration": 5.72
    },
    {
      "text": "have much higher Valu for is eatable so",
      "start": 4074.599,
      "duration": 4.361
    },
    {
      "text": "if you look at Apple and if you look at",
      "start": 4077.44,
      "duration": 3.399
    },
    {
      "text": "Banana you'll see that these two vectors",
      "start": 4078.96,
      "duration": 4.04
    },
    {
      "text": "are closer to each other and if you look",
      "start": 4080.839,
      "duration": 3.881
    },
    {
      "text": "at dog and if you look at Apple you'll",
      "start": 4083.0,
      "duration": 3.92
    },
    {
      "text": "see that they are very far apart because",
      "start": 4084.72,
      "duration": 4.319
    },
    {
      "text": "apple has a high value for is eatable",
      "start": 4086.92,
      "duration": 5.639
    },
    {
      "text": "and dog has a low value for that so if",
      "start": 4089.039,
      "duration": 8.32
    },
    {
      "text": "you actually can embed words as vectors",
      "start": 4092.559,
      "duration": 7.441
    },
    {
      "text": "then you can capture or retain the",
      "start": 4097.359,
      "duration": 4.84
    },
    {
      "text": "semantic relationship or meaning between",
      "start": 4100.0,
      "duration": 4.759
    },
    {
      "text": "words this is an extremely important",
      "start": 4102.199,
      "duration": 3.841
    },
    {
      "text": "idea and that that's where token",
      "start": 4104.759,
      "duration": 3.121
    },
    {
      "text": "embeddings actually come into the",
      "start": 4106.04,
      "duration": 4.239
    },
    {
      "text": "picture if we do this if we can",
      "start": 4107.88,
      "duration": 5.72
    },
    {
      "text": "successfully embed words as vectors and",
      "start": 4110.279,
      "duration": 5.48
    },
    {
      "text": "capture the meaning between them and if",
      "start": 4113.6,
      "duration": 4.759
    },
    {
      "text": "you feed these inputs to the llm then",
      "start": 4115.759,
      "duration": 4.761
    },
    {
      "text": "those inputs will be will have a lot",
      "start": 4118.359,
      "duration": 4.4
    },
    {
      "text": "more meaning and that will lead to much",
      "start": 4120.52,
      "duration": 5.36
    },
    {
      "text": "more successful llm models rather than",
      "start": 4122.759,
      "duration": 4.881
    },
    {
      "text": "randomly giving token",
      "start": 4125.88,
      "duration": 4.52
    },
    {
      "text": "IDs so this is the main idea behind",
      "start": 4127.64,
      "duration": 5.039
    },
    {
      "text": "token embedding or vector embedding",
      "start": 4130.4,
      "duration": 4.439
    },
    {
      "text": "vectors can essentially capture semantic",
      "start": 4132.679,
      "duration": 3.721
    },
    {
      "text": "meaning",
      "start": 4134.839,
      "duration": 4.201
    },
    {
      "text": "now you must be thinking that okay uh",
      "start": 4136.4,
      "duration": 4.48
    },
    {
      "text": "this is great but how do I generate",
      "start": 4139.04,
      "duration": 5.36
    },
    {
      "text": "these vectors how do I get the values uh",
      "start": 4140.88,
      "duration": 6.319
    },
    {
      "text": "for these vectors and actually to get",
      "start": 4144.4,
      "duration": 4.439
    },
    {
      "text": "the values for these vectors we have to",
      "start": 4147.199,
      "duration": 3.401
    },
    {
      "text": "train a neural network and we have to",
      "start": 4148.839,
      "duration": 5.041
    },
    {
      "text": "create Vector embeddings so uh if you",
      "start": 4150.6,
      "duration": 5.159
    },
    {
      "text": "look at GPT and if you look at the",
      "start": 4153.88,
      "duration": 3.56
    },
    {
      "text": "vector embeddings it's a huge High",
      "start": 4155.759,
      "duration": 4.6
    },
    {
      "text": "dimensional space and so every word is",
      "start": 4157.44,
      "duration": 4.64
    },
    {
      "text": "converted into that higher dimensional",
      "start": 4160.359,
      "duration": 3.681
    },
    {
      "text": "space vector and neural network is",
      "start": 4162.08,
      "duration": 6.0
    },
    {
      "text": "essentially trained to generate those",
      "start": 4164.04,
      "duration": 6.799
    },
    {
      "text": "vectors now let me come back to the",
      "start": 4168.08,
      "duration": 4.8
    },
    {
      "text": "Whiteboard",
      "start": 4170.839,
      "duration": 4.4
    },
    {
      "text": "again okay up till now hopefully you",
      "start": 4172.88,
      "duration": 4.479
    },
    {
      "text": "have understood why token embeddings are",
      "start": 4175.239,
      "duration": 4.52
    },
    {
      "text": "needed in the first place and now let us",
      "start": 4177.359,
      "duration": 4.521
    },
    {
      "text": "start with the next section that how",
      "start": 4179.759,
      "duration": 4.241
    },
    {
      "text": "token embeddings are actually created",
      "start": 4181.88,
      "duration": 4.2
    },
    {
      "text": "for large language",
      "start": 4184.0,
      "duration": 5.12
    },
    {
      "text": "models so to understand this we we again",
      "start": 4186.08,
      "duration": 5.599
    },
    {
      "text": "need to start from the vocabulary so",
      "start": 4189.12,
      "duration": 5.0
    },
    {
      "text": "let's say that we have a vocabulary of",
      "start": 4191.679,
      "duration": 5.441
    },
    {
      "text": "words so of tokens and there is a token",
      "start": 4194.12,
      "duration": 6.039
    },
    {
      "text": "ID right associated with each token so",
      "start": 4197.12,
      "duration": 6.039
    },
    {
      "text": "we need to convert every token ID into a",
      "start": 4200.159,
      "duration": 5.56
    },
    {
      "text": "vector right so let's look at this",
      "start": 4203.159,
      "duration": 4.921
    },
    {
      "text": "Matrix here so this is a matrix which is",
      "start": 4205.719,
      "duration": 5.041
    },
    {
      "text": "called as the embedding layer uh this is",
      "start": 4208.08,
      "duration": 4.8
    },
    {
      "text": "called as the embedding layer weight",
      "start": 4210.76,
      "duration": 4.52
    },
    {
      "text": "Matrix so what is actually happening in",
      "start": 4212.88,
      "duration": 4.2
    },
    {
      "text": "this Matrix is that if you have token",
      "start": 4215.28,
      "duration": 6.04
    },
    {
      "text": "IDs such as 0 1 2 50257 which are the",
      "start": 4217.08,
      "duration": 7.0
    },
    {
      "text": "number of token IDs for training gpt2",
      "start": 4221.32,
      "duration": 4.919
    },
    {
      "text": "for each of these token IDs you will",
      "start": 4224.08,
      "duration": 4.8
    },
    {
      "text": "have a vector so let's say if the vector",
      "start": 4226.239,
      "duration": 6.0
    },
    {
      "text": "Dimension is 256 for",
      "start": 4228.88,
      "duration": 6.44
    },
    {
      "text": "example uh I think a vector dimension of",
      "start": 4232.239,
      "duration": 6.281
    },
    {
      "text": "768 was actually used for training gpt2",
      "start": 4235.32,
      "duration": 6.28
    },
    {
      "text": "but let's consider 256 for now so each",
      "start": 4238.52,
      "duration": 6.679
    },
    {
      "text": "token ID will be represented as a 256",
      "start": 4241.6,
      "duration": 6.28
    },
    {
      "text": "Dimension Vector so token ID 0 will be a",
      "start": 4245.199,
      "duration": 5.681
    },
    {
      "text": "256 Dimension Vector token ID 1 will be",
      "start": 4247.88,
      "duration": 6.16
    },
    {
      "text": "a 256 dimensional Vector a right up till",
      "start": 4250.88,
      "duration": 6.0
    },
    {
      "text": "token ID of 50 257 that will also be a",
      "start": 4254.04,
      "duration": 6.36
    },
    {
      "text": "256 dimensional Vector so this is now a",
      "start": 4256.88,
      "duration": 5.64
    },
    {
      "text": "embedding layer weight Matrix which has",
      "start": 4260.4,
      "duration": 6.08
    },
    {
      "text": "dimensions of so it has 50257 rows and",
      "start": 4262.52,
      "duration": 5.76
    },
    {
      "text": "it has 256",
      "start": 4266.48,
      "duration": 4.32
    },
    {
      "text": "columns so this embedding Matrix needs",
      "start": 4268.28,
      "duration": 4.959
    },
    {
      "text": "to be constructed and then whenever we",
      "start": 4270.8,
      "duration": 5.08
    },
    {
      "text": "want to fetch an embedding Vector for a",
      "start": 4273.239,
      "duration": 4.641
    },
    {
      "text": "particular ID we just look at that",
      "start": 4275.88,
      "duration": 4.44
    },
    {
      "text": "particular row so for example if we want",
      "start": 4277.88,
      "duration": 5.359
    },
    {
      "text": "the embedding Vector for ID number two",
      "start": 4280.32,
      "duration": 5.16
    },
    {
      "text": "we just look at the third row because p",
      "start": 4283.239,
      "duration": 5.121
    },
    {
      "text": "as zero indexing and then we retrieve",
      "start": 4285.48,
      "duration": 4.8
    },
    {
      "text": "the embedding Vector for that particular",
      "start": 4288.36,
      "duration": 4.359
    },
    {
      "text": "ID that's it that's the simple idea",
      "start": 4290.28,
      "duration": 5.84
    },
    {
      "text": "behind uh the embedding layer weight",
      "start": 4292.719,
      "duration": 6.601
    },
    {
      "text": "Matrix so let us look at that right",
      "start": 4296.12,
      "duration": 6.8
    },
    {
      "text": "now uh",
      "start": 4299.32,
      "duration": 3.6
    },
    {
      "text": "okay so for the sake of Simplicity right",
      "start": 4303.36,
      "duration": 5.56
    },
    {
      "text": "now we are going to look at uh",
      "start": 4306.239,
      "duration": 4.881
    },
    {
      "text": "vocabulary size of six and we are going",
      "start": 4308.92,
      "duration": 4.48
    },
    {
      "text": "to look at a vector dimension of three",
      "start": 4311.12,
      "duration": 4.599
    },
    {
      "text": "so you can think of this table but you",
      "start": 4313.4,
      "duration": 3.92
    },
    {
      "text": "can think of the vector Dimension as",
      "start": 4315.719,
      "duration": 4.761
    },
    {
      "text": "three so there will be three columns and",
      "start": 4317.32,
      "duration": 5.08
    },
    {
      "text": "there will be six rows because the",
      "start": 4320.48,
      "duration": 4.52
    },
    {
      "text": "vocabulary size is six so let's say",
      "start": 4322.4,
      "duration": 6.04
    },
    {
      "text": "these are the input IDs 2 3 5 and 1 and",
      "start": 4325.0,
      "duration": 5.8
    },
    {
      "text": "we want to convert these input IDs into",
      "start": 4328.44,
      "duration": 4.279
    },
    {
      "text": "vectors like each of these input ID will",
      "start": 4330.8,
      "duration": 4.28
    },
    {
      "text": "be converted into a vector so first we",
      "start": 4332.719,
      "duration": 4.48
    },
    {
      "text": "will actually create an embedding layer",
      "start": 4335.08,
      "duration": 5.72
    },
    {
      "text": "and I think I should write it down over",
      "start": 4337.199,
      "duration": 5.801
    },
    {
      "text": "here uh so first we'll create an",
      "start": 4340.8,
      "duration": 5.04
    },
    {
      "text": "embedding layer like this and this",
      "start": 4343.0,
      "duration": 5.28
    },
    {
      "text": "embedding layer will actually have uh",
      "start": 4345.84,
      "duration": 5.839
    },
    {
      "text": "three columns and it will have",
      "start": 4348.28,
      "duration": 6.879
    },
    {
      "text": "one three it will have six",
      "start": 4351.679,
      "duration": 6.201
    },
    {
      "text": "rows why will it have six rows because",
      "start": 4355.159,
      "duration": 4.761
    },
    {
      "text": "we have six token IDs we have token ID",
      "start": 4357.88,
      "duration": 5.72
    },
    {
      "text": "of zero token ID of one token ID of",
      "start": 4359.92,
      "duration": 7.319
    },
    {
      "text": "two uh token ID of two token ID of three",
      "start": 4363.6,
      "duration": 6.559
    },
    {
      "text": "token ID of four and token ID of five so",
      "start": 4367.239,
      "duration": 5.48
    },
    {
      "text": "we have six token IDs and each token ID",
      "start": 4370.159,
      "duration": 4.721
    },
    {
      "text": "will be represented as a three D three",
      "start": 4372.719,
      "duration": 3.601
    },
    {
      "text": "Di dimensional",
      "start": 4374.88,
      "duration": 4.2
    },
    {
      "text": "Vector so this is the embedding layer",
      "start": 4376.32,
      "duration": 5.319
    },
    {
      "text": "Matrix how is this created in Python in",
      "start": 4379.08,
      "duration": 5.96
    },
    {
      "text": "Python it's created by using tor. nn.",
      "start": 4381.639,
      "duration": 5.841
    },
    {
      "text": "embedding and you specify the number of",
      "start": 4385.04,
      "duration": 4.199
    },
    {
      "text": "rows which is the vocabulary size you",
      "start": 4387.48,
      "duration": 4.04
    },
    {
      "text": "specify the number of columns so let me",
      "start": 4389.239,
      "duration": 5.641
    },
    {
      "text": "show you tor. nn. embedding so if you go",
      "start": 4391.52,
      "duration": 5.679
    },
    {
      "text": "to tor. nn. embedding you'll see the",
      "start": 4394.88,
      "duration": 5.92
    },
    {
      "text": "documentation for this basically uh you",
      "start": 4397.199,
      "duration": 5.44
    },
    {
      "text": "can specify the number of embeddings and",
      "start": 4400.8,
      "duration": 3.6
    },
    {
      "text": "the embedding dimensions and it creates",
      "start": 4402.639,
      "duration": 3.361
    },
    {
      "text": "a matrix",
      "start": 4404.4,
      "duration": 4.2
    },
    {
      "text": "uh with randomly assigned values so",
      "start": 4406.0,
      "duration": 4.32
    },
    {
      "text": "after you create the embedding layer you",
      "start": 4408.6,
      "duration": 3.2
    },
    {
      "text": "can print out the embedding layer",
      "start": 4410.32,
      "duration": 3.319
    },
    {
      "text": "weights and you'll see that we have six",
      "start": 4411.8,
      "duration": 3.48
    },
    {
      "text": "rows and three columns with randomly",
      "start": 4413.639,
      "duration": 4.681
    },
    {
      "text": "assigned values so here's the embedding",
      "start": 4415.28,
      "duration": 5.12
    },
    {
      "text": "layer Matrix and random values will be",
      "start": 4418.32,
      "duration": 4.8
    },
    {
      "text": "assigned to all of these so for the",
      "start": 4420.4,
      "duration": 4.96
    },
    {
      "text": "first ID random three values will be",
      "start": 4423.12,
      "duration": 5.119
    },
    {
      "text": "assigned to create uh the vector for the",
      "start": 4425.36,
      "duration": 5.2
    },
    {
      "text": "second ID random values will be assigned",
      "start": 4428.239,
      "duration": 4.601
    },
    {
      "text": "similarly for ID number five random",
      "start": 4430.56,
      "duration": 4.599
    },
    {
      "text": "values will be assigned",
      "start": 4432.84,
      "duration": 4.28
    },
    {
      "text": "one very important thing to note here is",
      "start": 4435.159,
      "duration": 4.401
    },
    {
      "text": "that these values are random currently",
      "start": 4437.12,
      "duration": 4.0
    },
    {
      "text": "right because we don't know the ideal",
      "start": 4439.56,
      "duration": 3.88
    },
    {
      "text": "token embedding even this has to be",
      "start": 4441.12,
      "duration": 4.119
    },
    {
      "text": "learned when we train large language",
      "start": 4443.44,
      "duration": 4.36
    },
    {
      "text": "models so dog and puppy needs to be",
      "start": 4445.239,
      "duration": 5.0
    },
    {
      "text": "closer so the vectors for dog and puppy",
      "start": 4447.8,
      "duration": 4.68
    },
    {
      "text": "should be aligned we cannot have random",
      "start": 4450.239,
      "duration": 5.0
    },
    {
      "text": "values like these and we also have a",
      "start": 4452.48,
      "duration": 4.52
    },
    {
      "text": "neural network for this which needs to",
      "start": 4455.239,
      "duration": 4.121
    },
    {
      "text": "be trained so even token embeddings need",
      "start": 4457.0,
      "duration": 5.159
    },
    {
      "text": "to be trained and uh GPT when it was",
      "start": 4459.36,
      "duration": 5.44
    },
    {
      "text": "trained even the embeddings were trained",
      "start": 4462.159,
      "duration": 4.641
    },
    {
      "text": "all along with the next word prediction",
      "start": 4464.8,
      "duration": 4.439
    },
    {
      "text": "task so these are the if you print out",
      "start": 4466.8,
      "duration": 4.0
    },
    {
      "text": "the embedding layer weights these are",
      "start": 4469.239,
      "duration": 3.881
    },
    {
      "text": "the all the weights and then as I",
      "start": 4470.8,
      "duration": 4.0
    },
    {
      "text": "mentioned you can retrieve the embedding",
      "start": 4473.12,
      "duration": 3.96
    },
    {
      "text": "Vector for any ID which you want so if",
      "start": 4474.8,
      "duration": 4.04
    },
    {
      "text": "you want the embedding Vector for ID",
      "start": 4477.08,
      "duration": 3.68
    },
    {
      "text": "number three you just pass in that",
      "start": 4478.84,
      "duration": 4.799
    },
    {
      "text": "particular ID and then you will get the",
      "start": 4480.76,
      "duration": 4.68
    },
    {
      "text": "vector because then it it's just the",
      "start": 4483.639,
      "duration": 4.52
    },
    {
      "text": "fourth row so if you want to get the",
      "start": 4485.44,
      "duration": 4.719
    },
    {
      "text": "embedding Vector for ID number three you",
      "start": 4488.159,
      "duration": 4.52
    },
    {
      "text": "just pass in this value and then you",
      "start": 4490.159,
      "duration": 4.961
    },
    {
      "text": "will get the vector so one way to think",
      "start": 4492.679,
      "duration": 4.161
    },
    {
      "text": "of this embedding layer matrix it's a",
      "start": 4495.12,
      "duration": 4.84
    },
    {
      "text": "lookup table and you can actually pass",
      "start": 4496.84,
      "duration": 4.92
    },
    {
      "text": "in the ID and you will get the",
      "start": 4499.96,
      "duration": 4.199
    },
    {
      "text": "corresponding embedding Vector so here",
      "start": 4501.76,
      "duration": 4.08
    },
    {
      "text": "you can see I've passed the ID number",
      "start": 4504.159,
      "duration": 4.04
    },
    {
      "text": "three and I've got the token embedding",
      "start": 4505.84,
      "duration": 5.839
    },
    {
      "text": "Vector for that ID we can also pass all",
      "start": 4508.199,
      "duration": 5.281
    },
    {
      "text": "the input IDs which we want the",
      "start": 4511.679,
      "duration": 3.801
    },
    {
      "text": "embedding or the token embedding for at",
      "start": 4513.48,
      "duration": 4.44
    },
    {
      "text": "one point and then based on the",
      "start": 4515.48,
      "duration": 4.96
    },
    {
      "text": "embedding layer weight Matrix we get the",
      "start": 4517.92,
      "duration": 5.0
    },
    {
      "text": "embedding token embedding vectors for",
      "start": 4520.44,
      "duration": 6.88
    },
    {
      "text": "all the input IDs so essentially what we",
      "start": 4522.92,
      "duration": 7.239
    },
    {
      "text": "are doing in this token embedding",
      "start": 4527.32,
      "duration": 6.319
    },
    {
      "text": "section is that every token ID is is",
      "start": 4530.159,
      "duration": 6.681
    },
    {
      "text": "converted into a token into a vector",
      "start": 4533.639,
      "duration": 5.681
    },
    {
      "text": "into an embedding vector and the best",
      "start": 4536.84,
      "duration": 4.879
    },
    {
      "text": "way to do this is to use the embedding",
      "start": 4539.32,
      "duration": 5.6
    },
    {
      "text": "layer in py torch so when you create an",
      "start": 4541.719,
      "duration": 4.92
    },
    {
      "text": "embedding layer you have to specify two",
      "start": 4544.92,
      "duration": 3.719
    },
    {
      "text": "things you have to specify the",
      "start": 4546.639,
      "duration": 3.761
    },
    {
      "text": "vocabulary",
      "start": 4548.639,
      "duration": 4.281
    },
    {
      "text": "size and you have to specify the vector",
      "start": 4550.4,
      "duration": 5.319
    },
    {
      "text": "Dimension why because you have to create",
      "start": 4552.92,
      "duration": 6.12
    },
    {
      "text": "a vector for every uh token ID in the",
      "start": 4555.719,
      "duration": 5.881
    },
    {
      "text": "vocabulary so the number of rows will be",
      "start": 4559.04,
      "duration": 4.639
    },
    {
      "text": "equal to the vocabulary size and the",
      "start": 4561.6,
      "duration": 3.599
    },
    {
      "text": "number of columns will be equal to",
      "start": 4563.679,
      "duration": 3.441
    },
    {
      "text": "Vector Dimension because for every token",
      "start": 4565.199,
      "duration": 5.561
    },
    {
      "text": "ID we will get a vector of those many",
      "start": 4567.12,
      "duration": 5.92
    },
    {
      "text": "dimensions and then it's also important",
      "start": 4570.76,
      "duration": 4.399
    },
    {
      "text": "to note that this embedding layer weight",
      "start": 4573.04,
      "duration": 4.159
    },
    {
      "text": "Matrix is actually optimized during the",
      "start": 4575.159,
      "duration": 4.321
    },
    {
      "text": "llm training process that's very",
      "start": 4577.199,
      "duration": 4.081
    },
    {
      "text": "important so all of these parameters",
      "start": 4579.48,
      "duration": 4.239
    },
    {
      "text": "also need to be optimized and the last",
      "start": 4581.28,
      "duration": 4.0
    },
    {
      "text": "point to note in this this token",
      "start": 4583.719,
      "duration": 3.721
    },
    {
      "text": "embedding section is that the embedding",
      "start": 4585.28,
      "duration": 4.28
    },
    {
      "text": "layer if you look at it it's just a",
      "start": 4587.44,
      "duration": 4.199
    },
    {
      "text": "lookup operation basically that",
      "start": 4589.56,
      "duration": 4.0
    },
    {
      "text": "retrieves rows from the embedding layer",
      "start": 4591.639,
      "duration": 4.201
    },
    {
      "text": "weight Matrix using a token",
      "start": 4593.56,
      "duration": 5.04
    },
    {
      "text": "ID now actually what we can do is that",
      "start": 4595.84,
      "duration": 5.52
    },
    {
      "text": "before we move to position encoding uh",
      "start": 4598.6,
      "duration": 6.96
    },
    {
      "text": "we can we can use the actual value of",
      "start": 4601.36,
      "duration": 7.96
    },
    {
      "text": "the token vocabulary size which is 50257",
      "start": 4605.56,
      "duration": 6.84
    },
    {
      "text": "used in chat used in gpt2 and we can use",
      "start": 4609.32,
      "duration": 5.68
    },
    {
      "text": "a vector dimension of 256 to construct",
      "start": 4612.4,
      "duration": 5.92
    },
    {
      "text": "very real uh real life or practical",
      "start": 4615.0,
      "duration": 5.639
    },
    {
      "text": "embedding layer Matrix so that's what",
      "start": 4618.32,
      "duration": 4.28
    },
    {
      "text": "we'll be doing in code right",
      "start": 4620.639,
      "duration": 5.0
    },
    {
      "text": "now uh so let me go to that particular",
      "start": 4622.6,
      "duration": 5.36
    },
    {
      "text": "section in the",
      "start": 4625.639,
      "duration": 7.441
    },
    {
      "text": "code uh okay so first yeah this is",
      "start": 4627.96,
      "duration": 7.279
    },
    {
      "text": "positional embedding but first I want to",
      "start": 4633.08,
      "duration": 5.36
    },
    {
      "text": "create that uh that Matrix and then",
      "start": 4635.239,
      "duration": 6.041
    },
    {
      "text": "we'll come to positional embedding yeah",
      "start": 4638.44,
      "duration": 4.64
    },
    {
      "text": "okay so now what I will do is that I",
      "start": 4641.28,
      "duration": 4.399
    },
    {
      "text": "will create an embedding layer m",
      "start": 4643.08,
      "duration": 7.119
    },
    {
      "text": "which has 50257 rows and which has 256",
      "start": 4645.679,
      "duration": 6.801
    },
    {
      "text": "Dimension vectors so let's see how we",
      "start": 4650.199,
      "duration": 4.241
    },
    {
      "text": "are going to do that so let's say the",
      "start": 4652.48,
      "duration": 3.52
    },
    {
      "text": "vocabulary size is",
      "start": 4654.44,
      "duration": 4.0
    },
    {
      "text": "50257 and the vector Dimension which we",
      "start": 4656.0,
      "duration": 5.28
    },
    {
      "text": "want for each token ID is 256 so we'll",
      "start": 4658.44,
      "duration": 6.12
    },
    {
      "text": "use the same function tor. nn. embedding",
      "start": 4661.28,
      "duration": 4.84
    },
    {
      "text": "this will create an embedding Matrix",
      "start": 4664.56,
      "duration": 3.56
    },
    {
      "text": "with a vocabulary size which means",
      "start": 4666.12,
      "duration": 4.64
    },
    {
      "text": "number of rows of 50257 and number of",
      "start": 4668.12,
      "duration": 4.44
    },
    {
      "text": "columns of",
      "start": 4670.76,
      "duration": 4.16
    },
    {
      "text": "256 now what we'll do is that we have",
      "start": 4672.56,
      "duration": 4.2
    },
    {
      "text": "already created the input data from the",
      "start": 4674.92,
      "duration": 5.56
    },
    {
      "text": "data loader right uh we will uh we will",
      "start": 4676.76,
      "duration": 6.959
    },
    {
      "text": "get that input data initially and then",
      "start": 4680.48,
      "duration": 5.92
    },
    {
      "text": "uh this input data has token IDs so",
      "start": 4683.719,
      "duration": 5.281
    },
    {
      "text": "we'll pass that those token IDs into the",
      "start": 4686.4,
      "duration": 5.799
    },
    {
      "text": "embedding and then get the corresponding",
      "start": 4689.0,
      "duration": 5.719
    },
    {
      "text": "embedding vectors for those token IDs",
      "start": 4692.199,
      "duration": 4.401
    },
    {
      "text": "it's actually a very simple concept but",
      "start": 4694.719,
      "duration": 3.241
    },
    {
      "text": "you just need to pay attention to",
      "start": 4696.6,
      "duration": 3.639
    },
    {
      "text": "Dimensions here so remember what we are",
      "start": 4697.96,
      "duration": 4.759
    },
    {
      "text": "trying to do here is that we have this",
      "start": 4700.239,
      "duration": 4.92
    },
    {
      "text": "uh embedding layer weight Matrix which",
      "start": 4702.719,
      "duration": 4.241
    },
    {
      "text": "is essentially a lookup table which",
      "start": 4705.159,
      "duration": 3.881
    },
    {
      "text": "means that if you provide a token ID it",
      "start": 4706.96,
      "duration": 5.08
    },
    {
      "text": "will give you the uh embedding",
      "start": 4709.04,
      "duration": 5.639
    },
    {
      "text": "Vector but to get the token IDs we need",
      "start": 4712.04,
      "duration": 4.4
    },
    {
      "text": "to get the input data from the data",
      "start": 4714.679,
      "duration": 4.52
    },
    {
      "text": "loader and the data loader we are going",
      "start": 4716.44,
      "duration": 7.04
    },
    {
      "text": "to uh get input data in batches of eight",
      "start": 4719.199,
      "duration": 6.561
    },
    {
      "text": "and then context length is four which",
      "start": 4723.48,
      "duration": 4.08
    },
    {
      "text": "means that at one time we are going to",
      "start": 4725.76,
      "duration": 3.959
    },
    {
      "text": "look at eight text samples with four",
      "start": 4727.56,
      "duration": 4.639
    },
    {
      "text": "tokens each so if you look at the first",
      "start": 4729.719,
      "duration": 4.401
    },
    {
      "text": "batch of this input data it has four",
      "start": 4732.199,
      "duration": 5.801
    },
    {
      "text": "tokens if you look at the second uh",
      "start": 4734.12,
      "duration": 6.119
    },
    {
      "text": "actually this all is one batch itself I",
      "start": 4738.0,
      "duration": 3.56
    },
    {
      "text": "should rather say if you look at the",
      "start": 4740.239,
      "duration": 3.92
    },
    {
      "text": "first row of the input data or the first",
      "start": 4741.56,
      "duration": 5.24
    },
    {
      "text": "input sequence it has four tokens if you",
      "start": 4744.159,
      "duration": 5.681
    },
    {
      "text": "look at the second row of the input data",
      "start": 4746.8,
      "duration": 5.52
    },
    {
      "text": "uh which is the second input uh sequence",
      "start": 4749.84,
      "duration": 3.76
    },
    {
      "text": "it has four",
      "start": 4752.32,
      "duration": 3.919
    },
    {
      "text": "tokens so we have to essentially get all",
      "start": 4753.6,
      "duration": 4.72
    },
    {
      "text": "of these token IDs each token is one",
      "start": 4756.239,
      "duration": 4.321
    },
    {
      "text": "token ID and we have to convert it into",
      "start": 4758.32,
      "duration": 4.44
    },
    {
      "text": "Vector representation and we are going",
      "start": 4760.56,
      "duration": 4.52
    },
    {
      "text": "to do this in code it's very simple just",
      "start": 4762.76,
      "duration": 4.84
    },
    {
      "text": "keep in mind the dimensions keep in mind",
      "start": 4765.08,
      "duration": 7.36
    },
    {
      "text": "this dimension of 50257 rows and uh 256",
      "start": 4767.6,
      "duration": 6.68
    },
    {
      "text": "columns and here keep in mind that we",
      "start": 4772.44,
      "duration": 3.68
    },
    {
      "text": "have a context length of four which",
      "start": 4774.28,
      "duration": 4.399
    },
    {
      "text": "means four columns and eight",
      "start": 4776.12,
      "duration": 5.32
    },
    {
      "text": "rows okay so we are creating a data",
      "start": 4778.679,
      "duration": 5.321
    },
    {
      "text": "loader here which takes in the Raw text",
      "start": 4781.44,
      "duration": 4.92
    },
    {
      "text": "bat size of eight and we are going to",
      "start": 4784.0,
      "duration": 3.88
    },
    {
      "text": "use a maximum length which is the",
      "start": 4786.36,
      "duration": 3.72
    },
    {
      "text": "context size of equal to 4 and we are",
      "start": 4787.88,
      "duration": 4.359
    },
    {
      "text": "going to generate the inputs and the",
      "start": 4790.08,
      "duration": 4.88
    },
    {
      "text": "targets so let's just look at the input",
      "start": 4792.239,
      "duration": 5.121
    },
    {
      "text": "right now so if you see these inputs are",
      "start": 4794.96,
      "duration": 4.04
    },
    {
      "text": "exactly the same as what I was showing",
      "start": 4797.36,
      "duration": 4.16
    },
    {
      "text": "to you in this white board so every row",
      "start": 4799.0,
      "duration": 4.36
    },
    {
      "text": "essentially has four tokens and we have",
      "start": 4801.52,
      "duration": 3.92
    },
    {
      "text": "eight rows because each batch has",
      "start": 4803.36,
      "duration": 4.72
    },
    {
      "text": "essentially eight input sequences now we",
      "start": 4805.44,
      "duration": 4.759
    },
    {
      "text": "want to convert each of these token IDs",
      "start": 4808.08,
      "duration": 5.8
    },
    {
      "text": "into 256 Dimension Vector so if you see",
      "start": 4810.199,
      "duration": 6.361
    },
    {
      "text": "this is currently 8x4 right this is 8x4",
      "start": 4813.88,
      "duration": 6.24
    },
    {
      "text": "Dimensions now we want to convert all of",
      "start": 4816.56,
      "duration": 7.36
    },
    {
      "text": "these into a 256 dimensional uh vectors",
      "start": 4820.12,
      "duration": 5.24
    },
    {
      "text": "so let's see",
      "start": 4823.92,
      "duration": 3.52
    },
    {
      "text": "how the dimensions will play",
      "start": 4825.36,
      "duration": 4.799
    },
    {
      "text": "out so we have these inputs right so",
      "start": 4827.44,
      "duration": 5.88
    },
    {
      "text": "let's say this is 8x4 Matrix and these",
      "start": 4830.159,
      "duration": 5.121
    },
    {
      "text": "are the input IDs so we have to",
      "start": 4833.32,
      "duration": 5.0
    },
    {
      "text": "essentially take each input ID here uh",
      "start": 4835.28,
      "duration": 5.399
    },
    {
      "text": "go to the lookup table and get the",
      "start": 4838.32,
      "duration": 4.919
    },
    {
      "text": "corresponding Vector for that input ID",
      "start": 4840.679,
      "duration": 5.361
    },
    {
      "text": "right that is what we are going to do uh",
      "start": 4843.239,
      "duration": 5.48
    },
    {
      "text": "so one embedding Vector of length 256 is",
      "start": 4846.04,
      "duration": 5.119
    },
    {
      "text": "generated for each token in this input",
      "start": 4848.719,
      "duration": 6.081
    },
    {
      "text": "Matrix so if you if you actually see how",
      "start": 4851.159,
      "duration": 7.08
    },
    {
      "text": "this looks like it's like this so if you",
      "start": 4854.8,
      "duration": 5.919
    },
    {
      "text": "look at the first token ID this will be",
      "start": 4858.239,
      "duration": 5.041
    },
    {
      "text": "a 256 Dimension Vector if you look at",
      "start": 4860.719,
      "duration": 5.121
    },
    {
      "text": "the second token ID this will be a 256",
      "start": 4863.28,
      "duration": 4.48
    },
    {
      "text": "Dimension Vector if you look at the",
      "start": 4865.84,
      "duration": 4.08
    },
    {
      "text": "third token ID it will be a 256",
      "start": 4867.76,
      "duration": 5.479
    },
    {
      "text": "Dimension Vector so now every single",
      "start": 4869.92,
      "duration": 6.6
    },
    {
      "text": "token ID here is a 256 Dimension Vector",
      "start": 4873.239,
      "duration": 9.321
    },
    {
      "text": "so you can think of this result as 8X 4X",
      "start": 4876.52,
      "duration": 6.04
    },
    {
      "text": "256 so here there are 8x4 entries right",
      "start": 4882.719,
      "duration": 6.0
    },
    {
      "text": "and each of these 8x4 entries has now a",
      "start": 4886.0,
      "duration": 5.96
    },
    {
      "text": "256 Dimension Vector associated with it",
      "start": 4888.719,
      "duration": 5.401
    },
    {
      "text": "so the result of this embedding is that",
      "start": 4891.96,
      "duration": 3.88
    },
    {
      "text": "when we generate this embedding Vector",
      "start": 4894.12,
      "duration": 3.64
    },
    {
      "text": "for each of these inputs we'll have an",
      "start": 4895.84,
      "duration": 5.76
    },
    {
      "text": "8X 4X 256 tensor so that's exactly what",
      "start": 4897.76,
      "duration": 6.16
    },
    {
      "text": "is happening right here so we we have",
      "start": 4901.6,
      "duration": 4.039
    },
    {
      "text": "the token embedding layer and we are",
      "start": 4903.92,
      "duration": 4.48
    },
    {
      "text": "passing these inputs which are the input",
      "start": 4905.639,
      "duration": 5.961
    },
    {
      "text": "idies basically and the result will be",
      "start": 4908.4,
      "duration": 4.799
    },
    {
      "text": "uh the token embeddings which is the",
      "start": 4911.6,
      "duration": 4.32
    },
    {
      "text": "result will be an 8x4 by 256",
      "start": 4913.199,
      "duration": 4.841
    },
    {
      "text": "Vector so this is how the token",
      "start": 4915.92,
      "duration": 4.759
    },
    {
      "text": "embeddings are obtained in practice for",
      "start": 4918.04,
      "duration": 4.92
    },
    {
      "text": "a batch size of eight for a Contex size",
      "start": 4920.679,
      "duration": 4.52
    },
    {
      "text": "of four and for a vector Dimension size",
      "start": 4922.96,
      "duration": 3.12
    },
    {
      "text": "of",
      "start": 4925.199,
      "duration": 3.321
    },
    {
      "text": "256 now we will add the positional",
      "start": 4926.08,
      "duration": 4.88
    },
    {
      "text": "embeddings on top of this these token",
      "start": 4928.52,
      "duration": 5.0
    },
    {
      "text": "embeddings for one batch and then that",
      "start": 4930.96,
      "duration": 5.52
    },
    {
      "text": "will be the input to the large language",
      "start": 4933.52,
      "duration": 5.36
    },
    {
      "text": "model so now we are going to come to",
      "start": 4936.48,
      "duration": 4.36
    },
    {
      "text": "positional embedding or positional",
      "start": 4938.88,
      "duration": 3.759
    },
    {
      "text": "encoding it's used",
      "start": 4940.84,
      "duration": 3.68
    },
    {
      "text": "interchangeably uh again you can pause",
      "start": 4942.639,
      "duration": 5.801
    },
    {
      "text": "pause here for some time um because",
      "start": 4944.52,
      "duration": 5.44
    },
    {
      "text": "positional encoding is actually going to",
      "start": 4948.44,
      "duration": 5.16
    },
    {
      "text": "be the last section of this long lecture",
      "start": 4949.96,
      "duration": 6.52
    },
    {
      "text": "so you need to be energetic and",
      "start": 4953.6,
      "duration": 4.96
    },
    {
      "text": "motivated in this last part",
      "start": 4956.48,
      "duration": 6.199
    },
    {
      "text": "also so I'll just meanwhile scroll to",
      "start": 4958.56,
      "duration": 5.92
    },
    {
      "text": "the positional embedding section on the",
      "start": 4962.679,
      "duration": 5.56
    },
    {
      "text": "Whiteboard yeah okay so thanks a lot",
      "start": 4964.48,
      "duration": 5.719
    },
    {
      "text": "everyone who have stayed until this part",
      "start": 4968.239,
      "duration": 4.201
    },
    {
      "text": "which is the last part of this lecture",
      "start": 4970.199,
      "duration": 4.721
    },
    {
      "text": "and that's on positional encoding so up",
      "start": 4972.44,
      "duration": 3.759
    },
    {
      "text": "till now we have looked at token",
      "start": 4974.92,
      "duration": 4.84
    },
    {
      "text": "embedding and we also saw how to convert",
      "start": 4976.199,
      "duration": 6.361
    },
    {
      "text": "all the input token IDs into token",
      "start": 4979.76,
      "duration": 3.879
    },
    {
      "text": "embedding",
      "start": 4982.56,
      "duration": 3.599
    },
    {
      "text": "vectors the problem with token embedding",
      "start": 4983.639,
      "duration": 4.241
    },
    {
      "text": "is that it does not take into account",
      "start": 4986.159,
      "duration": 3.921
    },
    {
      "text": "the position at all so basically if",
      "start": 4987.88,
      "duration": 4.16
    },
    {
      "text": "there are two sentences the cat sat on",
      "start": 4990.08,
      "duration": 3.96
    },
    {
      "text": "the mat and the second sentence is on",
      "start": 4992.04,
      "duration": 5.04
    },
    {
      "text": "the mat the cat sat so cat the word cat",
      "start": 4994.04,
      "duration": 5.84
    },
    {
      "text": "comes in both sentences right but the",
      "start": 4997.08,
      "duration": 4.76
    },
    {
      "text": "embedding Vector for both these words",
      "start": 4999.88,
      "duration": 4.2
    },
    {
      "text": "will be exactly the same it does not",
      "start": 5001.84,
      "duration": 4.359
    },
    {
      "text": "contain any information about the",
      "start": 5004.08,
      "duration": 4.72
    },
    {
      "text": "position so the cat word comes in",
      "start": 5006.199,
      "duration": 3.881
    },
    {
      "text": "different positions in these two",
      "start": 5008.8,
      "duration": 4.12
    },
    {
      "text": "sentences and ideally where the position",
      "start": 5010.08,
      "duration": 5.2
    },
    {
      "text": "is really matters for understanding the",
      "start": 5012.92,
      "duration": 3.68
    },
    {
      "text": "context of a",
      "start": 5015.28,
      "duration": 3.84
    },
    {
      "text": "sentence so it will it will be very",
      "start": 5016.6,
      "duration": 4.92
    },
    {
      "text": "advantageous for us if along with",
      "start": 5019.12,
      "duration": 4.48
    },
    {
      "text": "encoding the semantic meaning which we",
      "start": 5021.52,
      "duration": 4.44
    },
    {
      "text": "did in tokon embeddings if we also",
      "start": 5023.6,
      "duration": 4.52
    },
    {
      "text": "encode information about the position of",
      "start": 5025.96,
      "duration": 5.56
    },
    {
      "text": "the word um in the sentence so that's",
      "start": 5028.12,
      "duration": 5.16
    },
    {
      "text": "where positional encoding actually comes",
      "start": 5031.52,
      "duration": 4.44
    },
    {
      "text": "into the picture uh the main uh",
      "start": 5033.28,
      "duration": 5.24
    },
    {
      "text": "objective here is that or the main uh",
      "start": 5035.96,
      "duration": 5.44
    },
    {
      "text": "drawback of just token embeddings is",
      "start": 5038.52,
      "duration": 4.92
    },
    {
      "text": "that it does not include positional",
      "start": 5041.4,
      "duration": 4.4
    },
    {
      "text": "information and it is very helpful to",
      "start": 5043.44,
      "duration": 4.36
    },
    {
      "text": "inject additional positional information",
      "start": 5045.8,
      "duration": 4.32
    },
    {
      "text": "to the large language models there are",
      "start": 5047.8,
      "duration": 4.72
    },
    {
      "text": "two types of positional embeddings or",
      "start": 5050.12,
      "duration": 4.519
    },
    {
      "text": "encodings first is the absolute",
      "start": 5052.52,
      "duration": 4.52
    },
    {
      "text": "positional encoding and second is the",
      "start": 5054.639,
      "duration": 3.961
    },
    {
      "text": "relative positional",
      "start": 5057.04,
      "duration": 3.84
    },
    {
      "text": "encoding what happens in an absolute",
      "start": 5058.6,
      "duration": 4.36
    },
    {
      "text": "positional encoding is that for each",
      "start": 5060.88,
      "duration": 4.2
    },
    {
      "text": "position in the input sequence",
      "start": 5062.96,
      "duration": 4.44
    },
    {
      "text": "a unique embedding Vector is added to",
      "start": 5065.08,
      "duration": 5.119
    },
    {
      "text": "the Token embedding so this figure",
      "start": 5067.4,
      "duration": 4.68
    },
    {
      "text": "actually explains it the best so let's",
      "start": 5070.199,
      "duration": 5.841
    },
    {
      "text": "say uh we have tokens such as",
      "start": 5072.08,
      "duration": 7.4
    },
    {
      "text": "cat uh the second token",
      "start": 5076.04,
      "duration": 5.76
    },
    {
      "text": "is let's say",
      "start": 5079.48,
      "duration": 6.4
    },
    {
      "text": "set the third token is",
      "start": 5081.8,
      "duration": 7.76
    },
    {
      "text": "on and uh the fourth token is the let's",
      "start": 5085.88,
      "duration": 5.799
    },
    {
      "text": "say these are the four tokens and for",
      "start": 5089.56,
      "duration": 3.72
    },
    {
      "text": "the sake of Simplicity let's say the",
      "start": 5091.679,
      "duration": 3.44
    },
    {
      "text": "token embedding vector for all of these",
      "start": 5093.28,
      "duration": 4.68
    },
    {
      "text": "four are similar so let's say 11 one one",
      "start": 5095.119,
      "duration": 6.361
    },
    {
      "text": "Etc now uh what happens in absolute",
      "start": 5097.96,
      "duration": 5.88
    },
    {
      "text": "position embedding is that another",
      "start": 5101.48,
      "duration": 4.12
    },
    {
      "text": "embedding Vector is added to the Token",
      "start": 5103.84,
      "duration": 3.799
    },
    {
      "text": "embedding and that is the positional",
      "start": 5105.6,
      "duration": 4.36
    },
    {
      "text": "embedding vector and since these words",
      "start": 5107.639,
      "duration": 4.201
    },
    {
      "text": "have different positions so cat is first",
      "start": 5109.96,
      "duration": 4.4
    },
    {
      "text": "word sat is second word on is third word",
      "start": 5111.84,
      "duration": 4.2
    },
    {
      "text": "or third token and the is the fourth",
      "start": 5114.36,
      "duration": 3.2
    },
    {
      "text": "token they will have different",
      "start": 5116.04,
      "duration": 3.84
    },
    {
      "text": "positional embeddings so for example the",
      "start": 5117.56,
      "duration": 4.559
    },
    {
      "text": "positional embedding for CAT will be 1.1",
      "start": 5119.88,
      "duration": 5.359
    },
    {
      "text": "1.2 1.3 the positional embedding for set",
      "start": 5122.119,
      "duration": 6.361
    },
    {
      "text": "will be 2.1 2.2 2.3 the positional",
      "start": 5125.239,
      "duration": 6.841
    },
    {
      "text": "embedding for on will be 3.1 3.2 3.3 the",
      "start": 5128.48,
      "duration": 6.0
    },
    {
      "text": "positional embedding for the will be 4.1",
      "start": 5132.08,
      "duration": 5.72
    },
    {
      "text": "4.2 and 4.3 and then you add the token",
      "start": 5134.48,
      "duration": 5.28
    },
    {
      "text": "embedding to the positional embedding",
      "start": 5137.8,
      "duration": 3.919
    },
    {
      "text": "and then finally you get input",
      "start": 5139.76,
      "duration": 5.0
    },
    {
      "text": "embeddings and then this will be 2.1 2.2",
      "start": 5141.719,
      "duration": 6.041
    },
    {
      "text": "2.3 for the word cat this will be 3.1",
      "start": 5144.76,
      "duration": 6.72
    },
    {
      "text": "3.2 3.3 for the word set it will be 4.1",
      "start": 5147.76,
      "duration": 7.84
    },
    {
      "text": "4.2 4.3 for on and it will be 5.1 5.2",
      "start": 5151.48,
      "duration": 7.32
    },
    {
      "text": "5.3 for the word the so these are the",
      "start": 5155.6,
      "duration": 4.84
    },
    {
      "text": "input embeddings which will then be",
      "start": 5158.8,
      "duration": 4.0
    },
    {
      "text": "finally used by the llm and if you see",
      "start": 5160.44,
      "duration": 3.799
    },
    {
      "text": "although the token embeddings are",
      "start": 5162.8,
      "duration": 2.879
    },
    {
      "text": "similar the input embeddings are",
      "start": 5164.239,
      "duration": 3.601
    },
    {
      "text": "different from each other because now",
      "start": 5165.679,
      "duration": 4.96
    },
    {
      "text": "the position is encoded so the input",
      "start": 5167.84,
      "duration": 4.799
    },
    {
      "text": "embeddings for cat sat on the mat will",
      "start": 5170.639,
      "duration": 4.6
    },
    {
      "text": "be different than the mat the cat sat on",
      "start": 5172.639,
      "duration": 5.201
    },
    {
      "text": "let's say so here the position is taken",
      "start": 5175.239,
      "duration": 5.241
    },
    {
      "text": "into account so the positional vectors",
      "start": 5177.84,
      "duration": 4.359
    },
    {
      "text": "have the same Dimension as the original",
      "start": 5180.48,
      "duration": 3.36
    },
    {
      "text": "token embeddings because you have to add",
      "start": 5182.199,
      "duration": 3.321
    },
    {
      "text": "the position embedding vectors to the",
      "start": 5183.84,
      "duration": 4.72
    },
    {
      "text": "Token embeddings relative positioning or",
      "start": 5185.52,
      "duration": 5.32
    },
    {
      "text": "relative positional encoding is a bit",
      "start": 5188.56,
      "duration": 4.559
    },
    {
      "text": "different the emphasis here is on the",
      "start": 5190.84,
      "duration": 3.92
    },
    {
      "text": "relative position or the distance",
      "start": 5193.119,
      "duration": 4.801
    },
    {
      "text": "between tokens so rather than uh",
      "start": 5194.76,
      "duration": 5.359
    },
    {
      "text": "focusing on at what exact position the",
      "start": 5197.92,
      "duration": 5.279
    },
    {
      "text": "token is the model instead focuses on",
      "start": 5200.119,
      "duration": 5.761
    },
    {
      "text": "how far apart the different tokens are",
      "start": 5203.199,
      "duration": 4.641
    },
    {
      "text": "so the distance between the token become",
      "start": 5205.88,
      "duration": 3.239
    },
    {
      "text": "very important",
      "start": 5207.84,
      "duration": 4.92
    },
    {
      "text": "here so uh what are the advantages",
      "start": 5209.119,
      "duration": 6.04
    },
    {
      "text": "disadvantages of both these types",
      "start": 5212.76,
      "duration": 4.84
    },
    {
      "text": "the absolute positional encoding is very",
      "start": 5215.159,
      "duration": 4.681
    },
    {
      "text": "much suited when fixed order of tokens",
      "start": 5217.6,
      "duration": 4.639
    },
    {
      "text": "is crucial such as sequence generation",
      "start": 5219.84,
      "duration": 5.24
    },
    {
      "text": "so GPT uses absolute positional encoding",
      "start": 5222.239,
      "duration": 4.721
    },
    {
      "text": "and it is more commonly used than",
      "start": 5225.08,
      "duration": 4.72
    },
    {
      "text": "relative positional encoding even today",
      "start": 5226.96,
      "duration": 4.36
    },
    {
      "text": "we are going to learn about absolute",
      "start": 5229.8,
      "duration": 3.96
    },
    {
      "text": "positional encoding but even relative",
      "start": 5231.32,
      "duration": 4.2
    },
    {
      "text": "positional encoding is actually suitable",
      "start": 5233.76,
      "duration": 4.32
    },
    {
      "text": "for tasks like language modeling or long",
      "start": 5235.52,
      "duration": 4.92
    },
    {
      "text": "sequences where the same phrase can",
      "start": 5238.08,
      "duration": 3.72
    },
    {
      "text": "appear in different parts of the",
      "start": 5240.44,
      "duration": 3.84
    },
    {
      "text": "sequence so for that relative position",
      "start": 5241.8,
      "duration": 4.68
    },
    {
      "text": "positional encoding can also be used but",
      "start": 5244.28,
      "duration": 3.68
    },
    {
      "text": "today we are going to learn about",
      "start": 5246.48,
      "duration": 3.8
    },
    {
      "text": "absolute positional",
      "start": 5247.96,
      "duration": 5.36
    },
    {
      "text": "encoding uh okay so we have until now we",
      "start": 5250.28,
      "duration": 6.399
    },
    {
      "text": "have got this uh token token embeddings",
      "start": 5253.32,
      "duration": 6.6
    },
    {
      "text": "right so we have a tensor of 8X 4X",
      "start": 5256.679,
      "duration": 5.881
    },
    {
      "text": "256 the next step is that we need to",
      "start": 5259.92,
      "duration": 4.48
    },
    {
      "text": "create another embedding layer for",
      "start": 5262.56,
      "duration": 4.159
    },
    {
      "text": "positional em positional encoding or for",
      "start": 5264.4,
      "duration": 4.6
    },
    {
      "text": "positional embedding so remember one",
      "start": 5266.719,
      "duration": 3.841
    },
    {
      "text": "thing which is very important here is",
      "start": 5269.0,
      "duration": 5.52
    },
    {
      "text": "that uh every input essentially just has",
      "start": 5270.56,
      "duration": 6.72
    },
    {
      "text": "four tokens right so the context length",
      "start": 5274.52,
      "duration": 4.119
    },
    {
      "text": "is important when we create the",
      "start": 5277.28,
      "duration": 3.16
    },
    {
      "text": "embedding layer Matrix for positional",
      "start": 5278.639,
      "duration": 4.161
    },
    {
      "text": "encoding the vector dimension of course",
      "start": 5280.44,
      "duration": 4.799
    },
    {
      "text": "can be 256 which is the same same",
      "start": 5282.8,
      "duration": 4.76
    },
    {
      "text": "Dimension as the token embedding and",
      "start": 5285.239,
      "duration": 4.121
    },
    {
      "text": "that needs to be the same because we are",
      "start": 5287.56,
      "duration": 3.88
    },
    {
      "text": "going to add the positional encoding",
      "start": 5289.36,
      "duration": 5.08
    },
    {
      "text": "Vector to the Token encoding Vector but",
      "start": 5291.44,
      "duration": 5.199
    },
    {
      "text": "we actually need this these vectors for",
      "start": 5294.44,
      "duration": 4.84
    },
    {
      "text": "just four positions because the context",
      "start": 5296.639,
      "duration": 5.321
    },
    {
      "text": "length is four the token ID can be",
      "start": 5299.28,
      "duration": 4.439
    },
    {
      "text": "either in position number one position",
      "start": 5301.96,
      "duration": 4.08
    },
    {
      "text": "number two position number three and",
      "start": 5303.719,
      "duration": 3.48
    },
    {
      "text": "position number",
      "start": 5306.04,
      "duration": 3.88
    },
    {
      "text": "four and for each of these positions we",
      "start": 5307.199,
      "duration": 5.401
    },
    {
      "text": "need a separate Vector which is the",
      "start": 5309.92,
      "duration": 5.04
    },
    {
      "text": "positional embedding Vector so the size",
      "start": 5312.6,
      "duration": 4.24
    },
    {
      "text": "of this positional embedding Matrix will",
      "start": 5314.96,
      "duration": 4.679
    },
    {
      "text": "just be four rows and 256 columns",
      "start": 5316.84,
      "duration": 5.12
    },
    {
      "text": "because for each position position 1 2",
      "start": 5319.639,
      "duration": 5.441
    },
    {
      "text": "position 1 2 3 and four we need a",
      "start": 5321.96,
      "duration": 6.04
    },
    {
      "text": "separate Vector which is 256",
      "start": 5325.08,
      "duration": 5.32
    },
    {
      "text": "Dimensions so that is what we are going",
      "start": 5328.0,
      "duration": 5.639
    },
    {
      "text": "to do right now so yeah when the input",
      "start": 5330.4,
      "duration": 6.0
    },
    {
      "text": "batch comes like this uh if we look at",
      "start": 5333.639,
      "duration": 4.841
    },
    {
      "text": "first input it will be something like",
      "start": 5336.4,
      "duration": 3.92
    },
    {
      "text": "this it will have four token IDs and",
      "start": 5338.48,
      "duration": 4.52
    },
    {
      "text": "each will be 256 Dimensions so if our",
      "start": 5340.32,
      "duration": 5.52
    },
    {
      "text": "positional encoding has four vectors and",
      "start": 5343.0,
      "duration": 5.36
    },
    {
      "text": "each is 256 Dimension then we'll just",
      "start": 5345.84,
      "duration": 6.2
    },
    {
      "text": "add it to each input in the batch so",
      "start": 5348.36,
      "duration": 6.64
    },
    {
      "text": "that is what we'll be doing so we need",
      "start": 5352.04,
      "duration": 5.36
    },
    {
      "text": "only four positional embedding vectors",
      "start": 5355.0,
      "duration": 3.8
    },
    {
      "text": "and now what we'll be doing is that",
      "start": 5357.4,
      "duration": 3.08
    },
    {
      "text": "we'll generate the positional embedding",
      "start": 5358.8,
      "duration": 4.439
    },
    {
      "text": "Matrix and then create the four",
      "start": 5360.48,
      "duration": 6.159
    },
    {
      "text": "positional en coding vectors from it as",
      "start": 5363.239,
      "duration": 5.641
    },
    {
      "text": "you can see I sometimes use embedding",
      "start": 5366.639,
      "duration": 4.241
    },
    {
      "text": "and encoding interchangeably and that's",
      "start": 5368.88,
      "duration": 5.44
    },
    {
      "text": "fine so we are creating the positional",
      "start": 5370.88,
      "duration": 5.92
    },
    {
      "text": "embedding layer which has context length",
      "start": 5374.32,
      "duration": 4.2
    },
    {
      "text": "which has four rows because we only need",
      "start": 5376.8,
      "duration": 4.399
    },
    {
      "text": "four vectors and it has 256 columns",
      "start": 5378.52,
      "duration": 5.04
    },
    {
      "text": "because each Vector will have size",
      "start": 5381.199,
      "duration": 4.841
    },
    {
      "text": "256 and then what we do is that we",
      "start": 5383.56,
      "duration": 4.2
    },
    {
      "text": "create the four positional embeddings",
      "start": 5386.04,
      "duration": 4.599
    },
    {
      "text": "from this Matrix that's it so then the",
      "start": 5387.76,
      "duration": 4.6
    },
    {
      "text": "positional embedding layer will be 4X",
      "start": 5390.639,
      "duration": 4.921
    },
    {
      "text": "256 why because we have one 256",
      "start": 5392.36,
      "duration": 5.0
    },
    {
      "text": "dimensional Vector for each",
      "start": 5395.56,
      "duration": 4.52
    },
    {
      "text": "position and now after we create the",
      "start": 5397.36,
      "duration": 4.52
    },
    {
      "text": "positional embedding Vector so we create",
      "start": 5400.08,
      "duration": 4.44
    },
    {
      "text": "this four vectors as the last step what",
      "start": 5401.88,
      "duration": 4.12
    },
    {
      "text": "we have to do is that we have to just",
      "start": 5404.52,
      "duration": 2.96
    },
    {
      "text": "add the token embeddings to the",
      "start": 5406.0,
      "duration": 3.719
    },
    {
      "text": "positional embeddings so the token",
      "start": 5407.48,
      "duration": 6.04
    },
    {
      "text": "embeddings as we saw is 8x 4X 256 tensor",
      "start": 5409.719,
      "duration": 6.681
    },
    {
      "text": "and the positional embedding is 4X 256",
      "start": 5413.52,
      "duration": 4.679
    },
    {
      "text": "so you must be thinking how to actually",
      "start": 5416.4,
      "duration": 3.92
    },
    {
      "text": "add these so when you add these two in",
      "start": 5418.199,
      "duration": 3.681
    },
    {
      "text": "Python it will do the broadcasting",
      "start": 5420.32,
      "duration": 3.68
    },
    {
      "text": "operation which means what it will do is",
      "start": 5421.88,
      "duration": 4.56
    },
    {
      "text": "is that it will convert this 4X 256 into",
      "start": 5424.0,
      "duration": 5.92
    },
    {
      "text": "an 8X 4X 256 by duplicating these value",
      "start": 5426.44,
      "duration": 5.52
    },
    {
      "text": "eight times so what will essentially",
      "start": 5429.92,
      "duration": 3.759
    },
    {
      "text": "happen when you do this summation is",
      "start": 5431.96,
      "duration": 4.279
    },
    {
      "text": "that you first look at the first input",
      "start": 5433.679,
      "duration": 5.48
    },
    {
      "text": "sequence and you add this positional",
      "start": 5436.239,
      "duration": 5.201
    },
    {
      "text": "embedding for that input sequence that",
      "start": 5439.159,
      "duration": 4.681
    },
    {
      "text": "will give you the the first row of the",
      "start": 5441.44,
      "duration": 5.279
    },
    {
      "text": "input embeddings then you look at the",
      "start": 5443.84,
      "duration": 6.359
    },
    {
      "text": "second row even for the second row you",
      "start": 5446.719,
      "duration": 5.721
    },
    {
      "text": "uh you look at the token embedding",
      "start": 5450.199,
      "duration": 4.881
    },
    {
      "text": "second row you again add the same four",
      "start": 5452.44,
      "duration": 5.44
    },
    {
      "text": "positional vectors that will give you",
      "start": 5455.08,
      "duration": 5.079
    },
    {
      "text": "the the second row of the input",
      "start": 5457.88,
      "duration": 4.48
    },
    {
      "text": "embeddings similarly you look at the",
      "start": 5460.159,
      "duration": 4.241
    },
    {
      "text": "eighth row which is the final row of the",
      "start": 5462.36,
      "duration": 4.359
    },
    {
      "text": "token embedding Matrix and again you add",
      "start": 5464.4,
      "duration": 4.52
    },
    {
      "text": "the same four positional embeddings so",
      "start": 5466.719,
      "duration": 3.801
    },
    {
      "text": "then you get the final row which is the",
      "start": 5468.92,
      "duration": 3.84
    },
    {
      "text": "eighth row of the input embedding Matrix",
      "start": 5470.52,
      "duration": 3.639
    },
    {
      "text": "and that is how the input embedding",
      "start": 5472.76,
      "duration": 3.879
    },
    {
      "text": "Matrix is actually",
      "start": 5474.159,
      "duration": 4.801
    },
    {
      "text": "constructed uh so this is the last step",
      "start": 5476.639,
      "duration": 3.961
    },
    {
      "text": "which we'll also be doing in code so",
      "start": 5478.96,
      "duration": 3.32
    },
    {
      "text": "input embeddings is token embeddings",
      "start": 5480.6,
      "duration": 3.559
    },
    {
      "text": "plus positional embeddings and and then",
      "start": 5482.28,
      "duration": 4.52
    },
    {
      "text": "we get the final input embedding one",
      "start": 5484.159,
      "duration": 4.52
    },
    {
      "text": "thing to mention here is that similar to",
      "start": 5486.8,
      "duration": 4.439
    },
    {
      "text": "token embedding even when you initialize",
      "start": 5488.679,
      "duration": 5.161
    },
    {
      "text": "the positional embedding so when you",
      "start": 5491.239,
      "duration": 5.201
    },
    {
      "text": "initialize this positional embedding",
      "start": 5493.84,
      "duration": 6.6
    },
    {
      "text": "Matrix which I think I showed over",
      "start": 5496.44,
      "duration": 4.0
    },
    {
      "text": "here yeah if you initialize this",
      "start": 5501.44,
      "duration": 4.6
    },
    {
      "text": "positional encoding Matrix all the",
      "start": 5503.639,
      "duration": 4.881
    },
    {
      "text": "values will be initialized randomly as I",
      "start": 5506.04,
      "duration": 4.36
    },
    {
      "text": "said the embedding layer in Python just",
      "start": 5508.52,
      "duration": 3.84
    },
    {
      "text": "initializes the value",
      "start": 5510.4,
      "duration": 4.64
    },
    {
      "text": "randomly uh similar to the Token",
      "start": 5512.36,
      "duration": 4.319
    },
    {
      "text": "embedding even the values in the",
      "start": 5515.04,
      "duration": 3.28
    },
    {
      "text": "positional embedding layer need to be",
      "start": 5516.679,
      "duration": 3.361
    },
    {
      "text": "optimized during the training process",
      "start": 5518.32,
      "duration": 3.919
    },
    {
      "text": "itself so actually multiple things are",
      "start": 5520.04,
      "duration": 3.72
    },
    {
      "text": "happening to train the large language",
      "start": 5522.239,
      "duration": 3.041
    },
    {
      "text": "models of course we have to train it",
      "start": 5523.76,
      "duration": 3.879
    },
    {
      "text": "later to predict predict the next word",
      "start": 5525.28,
      "duration": 4.32
    },
    {
      "text": "but even in the data processing pipeline",
      "start": 5527.639,
      "duration": 3.321
    },
    {
      "text": "we have to train the positional",
      "start": 5529.6,
      "duration": 3.28
    },
    {
      "text": "embedding we have to train the token",
      "start": 5530.96,
      "duration": 4.719
    },
    {
      "text": "embedding as well which are then fed as",
      "start": 5532.88,
      "duration": 6.88
    },
    {
      "text": "input to the llm for the llm training",
      "start": 5535.679,
      "duration": 6.841
    },
    {
      "text": "procedure okay so this actually uh",
      "start": 5539.76,
      "duration": 5.64
    },
    {
      "text": "brings us to the end of today's lecture",
      "start": 5542.52,
      "duration": 4.48
    },
    {
      "text": "this is where we started the lecture",
      "start": 5545.4,
      "duration": 4.239
    },
    {
      "text": "with we started with the llm data",
      "start": 5547.0,
      "duration": 4.92
    },
    {
      "text": "pre-processing Pipeline and we discussed",
      "start": 5549.639,
      "duration": 4.08
    },
    {
      "text": "that will cover all the Four Points in",
      "start": 5551.92,
      "duration": 3.56
    },
    {
      "text": "today's lecture we started with",
      "start": 5553.719,
      "duration": 3.801
    },
    {
      "text": "tokenization we understood word based",
      "start": 5555.48,
      "duration": 4.88
    },
    {
      "text": "subord based character based tokenizer",
      "start": 5557.52,
      "duration": 4.599
    },
    {
      "text": "then we actually learned about the bite",
      "start": 5560.36,
      "duration": 4.08
    },
    {
      "text": "pair encoding tokenizer which is used by",
      "start": 5562.119,
      "duration": 4.721
    },
    {
      "text": "GPT and then we learned about token",
      "start": 5564.44,
      "duration": 3.64
    },
    {
      "text": "embeddings which is essentially",
      "start": 5566.84,
      "duration": 3.72
    },
    {
      "text": "converting token IDs to vectors we saw",
      "start": 5568.08,
      "duration": 4.2
    },
    {
      "text": "that it's not enough to just learn about",
      "start": 5570.56,
      "duration": 3.72
    },
    {
      "text": "token embeddings you also Al need to",
      "start": 5572.28,
      "duration": 3.52
    },
    {
      "text": "essentially learn about positional",
      "start": 5574.28,
      "duration": 4.32
    },
    {
      "text": "embeddings as well because these",
      "start": 5575.8,
      "duration": 4.399
    },
    {
      "text": "positional embedding vectors encode",
      "start": 5578.6,
      "duration": 3.84
    },
    {
      "text": "information about the position and then",
      "start": 5580.199,
      "duration": 4.321
    },
    {
      "text": "finally you add the token embeddings",
      "start": 5582.44,
      "duration": 3.719
    },
    {
      "text": "with the positional embeddings to create",
      "start": 5584.52,
      "duration": 2.92
    },
    {
      "text": "the input",
      "start": 5586.159,
      "duration": 4.281
    },
    {
      "text": "embedding if you were to ask me for one",
      "start": 5587.44,
      "duration": 4.759
    },
    {
      "text": "figure to explain today's lecture I",
      "start": 5590.44,
      "duration": 3.679
    },
    {
      "text": "would say in today's lecture we have",
      "start": 5592.199,
      "duration": 3.801
    },
    {
      "text": "covered the entire data pre-processing",
      "start": 5594.119,
      "duration": 5.841
    },
    {
      "text": "pipeline shown here so right from uh",
      "start": 5596.0,
      "duration": 6.52
    },
    {
      "text": "tokenization to token IDs to token",
      "start": 5599.96,
      "duration": 4.32
    },
    {
      "text": "embeddings to positional embeddings and",
      "start": 5602.52,
      "duration": 3.92
    },
    {
      "text": "finally to input embeddings we covered",
      "start": 5604.28,
      "duration": 4.439
    },
    {
      "text": "all of this in today's lecture this",
      "start": 5606.44,
      "duration": 4.4
    },
    {
      "text": "makes the data ready to be fed as an",
      "start": 5608.719,
      "duration": 5.361
    },
    {
      "text": "input to the U llm architecture or to",
      "start": 5610.84,
      "duration": 6.0
    },
    {
      "text": "the llm pre-training",
      "start": 5614.08,
      "duration": 5.4
    },
    {
      "text": "process um we also have separate",
      "start": 5616.84,
      "duration": 4.48
    },
    {
      "text": "lectures on all of these topics but I",
      "start": 5619.48,
      "duration": 3.639
    },
    {
      "text": "thought to make one condensed lecture to",
      "start": 5621.32,
      "duration": 4.76
    },
    {
      "text": "show you all the things in one glance uh",
      "start": 5623.119,
      "duration": 4.481
    },
    {
      "text": "as always I take an approach of the",
      "start": 5626.08,
      "duration": 4.4
    },
    {
      "text": "Whiteboard plus coding in Google collab",
      "start": 5627.6,
      "duration": 5.2
    },
    {
      "text": "so that your the theoretical foundations",
      "start": 5630.48,
      "duration": 4.88
    },
    {
      "text": "are strong plus your coding foundations",
      "start": 5632.8,
      "duration": 4.68
    },
    {
      "text": "are also strong I'll be sharing this",
      "start": 5635.36,
      "duration": 3.799
    },
    {
      "text": "entire code file with you so that you",
      "start": 5637.48,
      "duration": 3.36
    },
    {
      "text": "can play around with it you can run it",
      "start": 5639.159,
      "duration": 4.321
    },
    {
      "text": "on your own browser Etc please ask any",
      "start": 5640.84,
      "duration": 5.04
    },
    {
      "text": "questions which you might be having in",
      "start": 5643.48,
      "duration": 4.84
    },
    {
      "text": "the YouTube comments and I'll be happy",
      "start": 5645.88,
      "duration": 6.239
    },
    {
      "text": "to answer all of them",
      "start": 5648.32,
      "duration": 3.799
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch Series this is a very special lecture because it covers one of the most fundamental aspects for building a large language model and that is building the data processing pipeline when people learn about large language models I think that they do not pay too much attention to data processing but it's really one of the fundamental building blocks of having a strongly performing large language model so what does it mean data processing the main idea behind data processing is that we all know that large language models have sentences right and they deal with text they deal with sentences but if you have a big PDF file that cannot directly be be given as input to the large language model it needs to be processed further and that this whole uh procedure of processing the data before it is fed as input to the large language model is called as data pre-processing so in our case the data is text the data is paragraphs the data is documents what can we do to this data so that it can serve as an input to the large language model and we'll be answering this question in today's lecture we'll answer this question in four steps which are the four fundamental steps needed before any sentence is transformed into a form which is good enough to be an input for a large language model the first of these four steps is called as tokenization the second of these four steps is token embeddings the third is called positional embeddings and the fourth is when we create input embeddings this is the final step and these input embeddings then serve as inputs to the large language model remember that input embeddings are the summation of token embeddings and positional embeddings so we'll learn about token embeddings in part two and we'll learn about positional embeddings in part three and then we'll sum these two to create input embeddings even when we look at tokenization it can be broken down into three parts word based tokenizer subword based tokenizer and character based tokenizer the subord based tokenizer has many types and bite pair encoder is one of them gpt2 and in fact many GPT models use this bite pair encoder tokenizer which is a subword based tokenizer token embeddings is essentially converting token IDs into vectors positional embeddings is encoding information about the position awesome right so let's get started with these four steps which we will cover in detail in today's lecture first we'll start with tokenization so let me zoom in over here so when you want to build a large language model you need to go through three stages stage one is for building the basics of an llm stage two is for pre-training and stage three is for fine tuning today's lecture comes under stage one and more specifically it comes under the data preparation and sampling part um of stage one so as I mentioned first we'll be looking at tokenization and initially we'll be looking only at word based tokenizer so what we'll do right now is that we'll construct our own word based tokenizer from scratch although this is not the tokenizer which is used in GPT models GPT use subord based tokenizer but if we directly move to subord based tokenizer you'll lose the intuition you'll lose the feel of how exactly tokenization is done so first we'll start with word based tokenizer and uh for that we'll need a database right we'll need a data set very simply speaking word based tokenization is just converting the entire data set into chunks of words that's it and I'm going to show you how that is done it's not as easy as it looks so the data set which we are going to use is a book which is called the verdict it's a book which is written in 1908 you can also search about it it has this uh this cover and we are going to to use this as our input data set it has a lot of text as you can see over here so in word based tokenization what our aim would be would be to take this text and to convert it into words and break it down into words right so then uh let's move to python right now immediately and let's see how to code this out from scratch so this is the python notebook which we'll be going through in today's lecture the first step step as I mentioned is creating Word based tokens from scratch so initially what we'll be doing is that we'll open this data set we'll open this document and we'll read this document the way it is done is that you have to use this with open and then you assign a variable called raw text which contains the entire text which is read by python I've actually downloaded this uh book from this particular link you can download it online I'll share the file with you so that you can also run this code on your own so once the data set or the book itself is downloaded or read by python what you can do is that you can check whether it has indeed been read so you can print the total number of characters and you can even print some portion of the text so you can see that the total number of characters is 20479 and here here I'm printing the uh first 100 characters of this file so it's I had always thought Jack uh giz rather a cheap genius though a good fellow let's check whether this is actually correct so in the book it's exactly the same I had always thought Jack gisburn rather a cheap genius great so we have read the data set successfully now let's move next to the portion of how do we split this data into tokens I've added a small note here uh which you'll also see when I share the notebook but when actual large language models are constructed the data set which they process is millions of Articles and hundreds of thousands of books but for educational purposes it's sufficient to work with smaller Tech samples like a single book as I'm showing right now because the hardware capacity uh it requires minimum Hardware capacity and it's also much easier to demonstrate but whatever we are seeing right now can easily be scaled up to uh larger larger and larger data sets now the question which we want to ask is how can we best split this text to obtain a list of tokens so uh let's see for that we are going to use the regular expression library in Python so you can head over to this link which I I'll also share in the YouTube video description and let me show you how this Library can be used so uh you first take some random text I've have chosen a sample text here hello world this is a test and then what we do is that we take this test we take this text and using the regular expression Library we split this text based on wherever white space characters are there so what this command here does is that it reads the text and then it splits the text wherever it encounters white spaces so it scans the text from left so first we scan it from left and after hello and comma is encountered there's a white space here see so then it splits here then it goes on scanning further then after World dot there is a white space here there is a white space here here and here so it creates splits at those locations where white spaces are there and you can see this is the result so the result when we split this is hello then white space then World dot then wh space then this comma then white space then is then white space uh then white space and then test uh do you see the problem with this tokenization why can't we stop over here the reason we can't stop over here is that hello and comma are both in a single token I don't want that I want punctuations to be in a separate token because I want my llm to know which are punctuations which are normal words so I want comma to be a separate token I want full stop to be a separate token similarly if you actually go through the text there are so many other characters like there is this character here I'm sure there's a question mark somewhere I want all of these special characters to be separate tokens split from the text that's the first mistake here the second mistake is that I don't want wh space characters at separate tokens now this is a debatable topic because sometimes it's good to have wh space characters for example if you're writing python code indentations are important right wh space characters are important in that case it's good to have wh space characters as separate tokens but for the sake of demonstration I don't want wh space characters so there are two things I will modify here uh I along with splitting here we only split where white spaces are there right I also want to split wherever different characters are encountered like quotation exclamation mark question mark Etc and then I want to remove the white spaces we'll do exactly this right now so uh what I'll be doing right now is that I'll take this text then what I'll be doing is that I'll split but I'll split on all of these characters so earlier we only split on the white space character right but now I'm saying here that whenever you encounter a comma or a full stop or a colon or a semi colon or a question mark or an underscore or an exclamation inverted comma bracket slash you split there that's what I'm instructing this regular expression library to do so this is the first thing I'll be doing the second thing I'll be doing is removing the white spaces so what this line of code does is that it scans through the result so it scans through the tokens and it results a True Value it outputs a True Value only if white space is not there so wherever white spaces are there it outputs a false value so this item. strip item. strip is true only when wh space is not there item. strip is false when a white space is encountered so when this list of tokens is passed through this item. strip function over here only those tokens which are not a white space will be returned so let's print out the result and let's see the result so here you can see Hello is a separate token comma is a separate token world is a separate token dot is a separate token awesome right right so uh yeah so hello is a separate token comma World dot is this this Dash Dash is also a separate token awesome so now all of these separate characters are individual tokens and we have also removed the white spaces see white spaces are not different tokens over here so we have essentially taken a small sentence and we have converted it into individual tokens now we'll apply these same two commands on this entire uh data set which we are reading so exactly the same commands will be used we'll Define a variable called pre-processed which is a list which contains all the tokens of the entire essay so what we are first saying is that you scan through this essay scan through this essay and split wherever these special characters are encountered and also split where white space is encountered in the second uh sentence we are saying that you have take the list which is called as pre-processed right now uh retain all the tokens in this list but get rid of all the white space characters we don't need those as tokens that's what we are saying in this second sentence right now great so pre-processed is the list which contains the tokens which the essay has been broken down into awesome just to make sure we have done the tokenization correctly what I'm going to do is I'm going to print the first 30 tokens in this list so here you can see the first 30 tokens are I had always thought Jack gisburn rather so here you can see I had always thought Jack gburn it has been broken down into individual words but even these characters are separate tokens so if you see over here this Dash Dash is a separate token so tokenization is actually pretty simple here we are looking at word based tokenization where every token is a separate word or a character and when I say character I mean special symbols like uh comma exclamation mark full stop um quotation Etc awesome so here you can also print the length of this list which gives you an idea of how many tokens are there so when you print it out you'll see that we have 469 zero tokens over here um and there may be duplications also because we have not gotten rid of duplicates here that's fine I'm just printing out the total length of tokens it's 4690 great uh so now we have the entire data set which is an essay we have converted it into a list of tokens but uh we need numerical representations right computers doesn't don't don't understand words so then we need to convert these tokens into token IDs that's the next step all the tokens which we have generated need to be converted into token IDs and there is a specific way to do this first what you do is you take all the tokens you remove the duplicates and you arrange the tokens in an ascending or an alphabetical order so let's say if the data set was the quick brown fox jumps over the lazy dog you break it down into tokens let's say the quick brown fox and you arrange them all in alphabetical order so brown dog fox jumps lazy or quick the and then to each of the tokens you assign a token ID since they are in alphabetical order the first token will have token ID zero the second token will have token ID 1 Etc dot up till the end the last unique token will have the token ID 7 in this case so basically this is called as a vocabulary the vocabulary is essentially a dictionary where every unique token is mapped to a unique integer which is called as the token ID for all practical purposes the computer only cares or the llm rather only cares about the token IDs not the tokens because the token IDs are have a numerical representation so what I showed you here is called constructing the vocabulary and the vocabulary is a dictionary which maps The Tokens to token IDs this is exactly what we'll be doing in code as well so we have the pre-processed right which is a list of all the tokens first we need to uh convert this into a set so we get rid of all the duplicates also and then we sort it in asscending order so you take all the tokens and you sort them in ascending order and uh you can also print out the vocabulary size so here you will see that is 1130 now after you uh sort all the tokens in ascending order you have to assign a token ID which is an integer to each token so you can do that that through enumerate so when you use the enumerate function in Python what it does is that it creates integers and tokens so then you create a dictionary which is called as a vocabulary and uh for each token in the dictionary there is essentially an integer associated with it and enumerate anyway lists down all the integers in Alpha in ascending order so the first token will be assigned a token ID of zero the second token will be assign the token ID of one Etc so what we can do over here is we can actually print out the items in the vocabulary and then uh see the first 50 items so here you can see first all these uh special characters are there who have the token ID 012 Etc and then all the separate tokens are associated with a token ID so we can see that for has a token ID of 35 has has a token ID of 47 Etc so it's the vocabulary is essentially a dictionary so if someone gives you a token ID you can see what the corresponding token is is if someone gives you a token you can see what the corresponding token ID is awesome right so that's the second step we have converted the tokens into token IDs the third step what we have to do is that uh uh we have to implement a tokenizer class in Python what this tokenizer class will do is that it will take the vocabulary and uh it will have an encode method and it will have a decode method what the encode method will do is that for any given word it will convert the word into a token ID what the decode method will do is that for any given token ID it will convert it back into the token so let me illustrate this for you over here so after we uh saw the concept of token IDs we now need to implement a tokenizer class in Python so this to whenever an instance of the tokenizer class is created we have two methods we have the encode method which is tokenizer do encode what this method will do is pretty simple it we will just take the text break it down into tokens and assign a token ID to each token the second method is the decode method what this method will do is the exact reverse it will essentially take token IDs convert them into tokenized text and then generate the original sample text from these token IDs so that's what we are going to implement now so you can see that this is the tokenizer class and whenever an instance of the class is created we need to first pass in the vocabulary which you already saw and then then we maintain two dictionaries one is the dictionary string to int which is of course the vocabulary because the vocabulary is just the dictionary which converts the string to an integer and then we have to maintain a reverse dictionary which is integer to string so what this says is that for every string integer pair in the vocabulary this dictionary will be the reverse so this dictionary will be integer mapping to string so whenever an integer is given as the lookup this dictionary will return this a given string so that's the decode part so then we Define two methods the first is encode the encode will have exactly the same things what we saw here so here we saw the First Command was splitting the text based on these characters the second was to get rid of white spaces that is how we created tokens right so the encode method which I'm showing you has exactly the same thing we first have this pre-processed list and we have the whatever text is given as an input to this encode method we'll first split it based on these characters we'll get rid of the white spaces then whatever remains for all the strings which remain we'll see the corresponding integer in the vocabulary and then this method will return a list of token IDs that's it the decode method will do exactly the reverse it will take as input a list of token IDs and then what it will do is that it will convert it will use this integer to string dictionary and based on the token IDs it will convert these integers into Strings and then it will join the different strings together there is one more subtlety here that when we join uh strings like this there is usually a space before punctuations and we need to get rid of that space so that's just the second sentence over here so this is the decode method which Returns the text great so this is how a tokenizer class is created and once we create a tokenizer class we can actually Define an instance of this class so I'm creating an instance of this class called tokenizer and I pass in the vocabulary which I already had defined so if you remember this is my vocabulary based on the datas based on this data set so now what I'm doing is I'm creating an instance of this simple tokenizer version one class and passing in the vocabulary and now once an instance is created I can access two methods encode and decode but for the encode method I need some text right so let's give it some text so it's the last he painted you know Mrs gisburn said with pardonable pride let's see where this text appears so pardonable yeah so I'm giving this text it's the last he painted you know Mrs gban said with pardonable pride uh so this is the text which I'm giving as an input to the tokenizer class and let's see if it can encode it so then I use the encode method for this text when this text is passed what will happen is that first it will split first it will split based on the characters remove the white spaces and convert the tokens into token IDs and when you print the IDS you will see that these are the token IDs for each of these tokens tokens so basically its will have one token ID the will have one token ID last will have one token ID and these token IDs are printed over here now we can also test the decode method so you can pass in these IDs back to the decode method and you'll see we recover the same sentence it's the last he painted you know Mrs gban said with pardonable pride awesome right uh so we have successfully implemented an encoder method and decoder method and we have also created the tokenizer class in Python so you you must be thinking this is great what are the disadvantages here why doesn't GPT models use this kind of a world-based tokenizer well there is a major disadvantage if you come across a text let's say the text is hello do you like T and if you try to encode this text with a tokenizer you'll get an error can you try to think why you have got an error here you can pause this video for some time to think about it the reason you have got an error here is that these some of these words were not present in the vocabulary so the word hello was not present in the verdict short story at all hence it's not contained in the vocabulary so if you contrl f hello here you'll not see it over here so this is the main problem with word based tokenizer it does not know how to deal with out of vocabulary words so if you give it if you give it some text to encode which is not present in the vocabulary it returns an error because hello is not present in the vocabulary so how do I encode it that's one of the major problems with the world-based encoding scheme and that's why we have to add special context tokens so until now we have implemented a simple tokenizer right but how to deal with tokens which are not known a simple way to do this is to augment the existing vocabulary with two tokens the first token will be an unknown token so if a word comes in the input text which we do not know uh it will be mapped to this unknown and then the corresponding token ID the second token which is very important is called end of text so you can see here I've added these two tokens at the end of the vocabulary so unknown token is simple to understand but uh you must be thinking what is this end of text and this end of text token is actually also used for training GPT when you work with different documents which are inputs you need to specify where one document ends and when other document starts so when you give inputs this is how it's done so let's say this is the text Source One then this is the text Source 2 text Source three and text Source 4 let's say this is a news article these are news article these are Reddit posts these are blogs and let's say these are newspaper clippings so after this first text source is finished we need to give end of text at the beginning of the second text source which says that text Source One is done now the text Source 2 begins similarly at the beginning of the text Source 3 we have to give this end of end of text token and at the beginning of text Source 4 also we have to give this end of text token so uh when M when working with multiple text sources we add end of text tokens between these texts these tokens essentially act as markers signaling the start U or end of a particular segment this leads to more effective processing and understanding by the large language model and remember uh when chat GPT pre-processes the input resources the text resources it also gives we also give the end of text tokens so now we are going to deal with special uh these are called special context tokens so we will modify the tokenizer to use an unknown token if it encounters a word that is not part of the vocabulary and we will also add a token between unrelated text called the end of text token so here's the all tokens which are basically all the tokens in our current vocabulary and we are extending this by adding two tokens at the end of this the end of text token and the unknown token and then we create the vocabulary again it the same token to token ID or token to integer mapping but now it has two additional tokens and token IDs that's why the length is 1132 if you look at the length of the initial vocabulary that was uh 1130 right so the length has been increased by two and we can even print the vocabulary and look at the last two items in this vocabulary we have the end of text token mapped to a token ID of 1130 we have an unknown token mapped to a token ID of 1131 great so uh now we have augmented the vocabulary with these two additional tokens and now we are ready to actually modify our tokenizer class so our simple tokenizer version one class had an encode and decode method which we are going to retain but we are going to change some things in the encode method what we are going to say is that if the item does not so if whatever is contained in text it does not exist in this St str2 uh int dictionary what we can say is that you can replace that text with unknown and then you can map this unknown to the corresponding ID which is there in unknown so unknown token is there in our vocabulary so if some item is encountered in the string to integer conversion which is not there in the vocabulary we replace that item or that word with unknown and then we return return the integer or the token ID associated with the unknown token That's How we'll deal with unknown tokens the decode method actually exactly Remains the Same so now let us test this simple tokenizer version 2 so I create an instance of the simple tokenizer version 2 and the text one is hello do you like T the text two is in the sunlight Terraces of the palace so now we are also going to merge these two texts which is the input which I'm going to test the tokenizer on but when we merge we are also going to give this end of text token this is how it's done in GPT so we could have directly given these two sentences but we indicate when the first sentence end and when the second sentence starts so this is my input text which I will now feed to my encode method and you'll see now there is no error although hello was not included in the vocabulary what is done now is that in this uh simple tokenizer version two hello is not there in the vocabulary so when Hello is encountered by this encode method it replaces it with the unknown token and assigns it that corresponding token ID which is 113 0 see uh so that actually it's 1131 for unknown and whenever the end of text is there so end of text is here we have a token idea of 11 130 great and now you can decode it back so when you decode it back the decoded output will be unknown do you like T end of text in the sunlight Terraces of the unknown so there are actually two unknown words here hello and Palace because Palace is also not in the data set but now by by augmenting the vocabulary with this additional tokens we are able to encode and decode even unknown words successfully awesome so so far we have discussed tokenization and we have also seen some special tokens and we saw the tokens like unknown and end of text one thing to remember is that uh depending on the llm some researchers also consider additional tokens like beginning of sequence end of sequence then a padding token Etc we'll not go into details about these tokens tokens but it's important to remember that this unknown and end of text are not the only special context tokens there are also other tokens with some researchers consider now for GPT uh the tokenizer used for GPT models doesn't use an unknown token uh but it does use an end of text token Okay Okay so until now we have looked at word based uh tokenization you can take a pause here take a break if you want because now we'll be moving to the next section which is types of tokenizers and we'll also look at the tokenizer which is used by GPT and that's called as bite pair encoding so GPT models use a bite pair encoding tokenizer and we'll now learn about that so if you want to take a break at this point feel free to pause and then come back to this second part okay so uh until now we looked at word based tokenization and we actually constructed our own word based tokenizer and we saw that by adding it by adding special context tokens it could also deal with unknown Words which were not present in the vocabulary so then your question would be why not this why this was not used uh for training GPT and why do we even need to study about other tokenization algorithms so word based tokenization as we saw is just breaking down the sentence into individual words uh the main problem with word based tokenization as we saw is that that it gets very difficult to deal with out of vocabulary words we need to add these special context tokens Etc the second major problem is that uh let's say there are two words boy and boys there is ideally a lot of similarity in these words right but both of these words will be assigned separate token IDs and there is no meaning which is captured in the tokenization because the token IDs are different there is nowhere we get some kind of an intuitive meaning that these two are similar they actually contain the same root word so the second major problem with this is that we do not understand the similarity between words based on their root words all of that meaning is lost and the third major problem is that it leads to very large vocabulary sizes because if you are training GPT and if you look at the number of words in the English language let's actually see we we only looked at this data set right but for training GPT you will need to have a vocabulary with a huge number of words so number of words in the English language so let's see so the number of words in the English language are estimated to be around 600,000 to 1 million that's a huge number of words right so to actually make a vocabulary for this we would need a vocabulary size of this much which is really very high so word based tokenization schemes also need a very large vocabulary size and that's a big problem uh then how to solve these problems there is another end of the Spectrum which is essentially character based tokenization what what's what happens in character based tokenization is that when you look at sentences different words are not tokens but instead characters are the tokens so instead of the tokens being my hobby is playing the tokens are now m y h that's it the the tokens are individual characters can you think of the advantages and disadvantages of this you can pause the video here if you want okay so the major advantage of this approach is that it leads to a very small vocabulary remember one of the major problems of Word level word based tokenization is that we would need a large vocabulary but the number of characters in the English language is just around 256 and every language has a fixed number of characters so the vocabulary which we will be using in character based tokenization is actually very small that's one of the big advantage and it actually solves the oov problem which is the out of vocabulary problem but there are some other major issues with character level tokenization similar to the word based tokenization the meaning which words carry is completely lost so for example boy and boys there is no meaning encoded the same root word is used in these two words boy that also is lost why is it lost because because we completely destroy the words down to their individual characters right we get a small vocabulary size but in essence we are removing the soul of Words which is their meaning by breaking it down into characters so the meaning associated with words is completely lost there is another major problem and that is that the tokenized sequence is much longer than the initial raw text so for example hobby right if you look at Hobby in word based tokenization hobby was a separate token but now there will be five tokens for this h o b b y so this one word is actually converted into five tokens and this becomes a big problem when dealing with large data sets the the tokenized sequence is much longer than the initial raw text so these are the problems associated with character level tokenization so essentially the major problems with word based tokenization is that it needs a large vocabulary the major problem with character based tokenization is that the meaning is completely lost can we think of Best of Both Worlds can we think of something which does not need that much vocabulary but it also retains the essence of words that's where subword based tokenization actually comes into the picture and the bite pair encoder which GPT models are using that is actually a subword based tokenization as you must have guessed in subword based tokenization not full Char not full words are tokens and neither full characters are tokens but subwords are tokens uh and there are some rules for subword based tokenization the first rule is that we do not split frequently used words into smaller subwords so those words stay the same but we split the rare words into smaller meaningful subwords now what does this mean so for example if you consider boy and boys boy may occur many times in the data set so that will not be split that will be retained as one word boy but boys boys will actually be split into boy and S so boys is a rare word maybe and it is also derivative of the word boy so it will be split into two words boy and S so now instead of having two separate token for boy and boys we just have one token which is boy so you can see that this tokenization retains the root word so boy and boys both will have the boy token as common just boys will have an extra token which is s So subword based tokenization can also have word entire word as tokens can also have characters as tokens and it can also have subwords as tokens but it follows these two rules so if there is a word which occurs frequently that is retained as a single token but if there is a rare word that split into smaller meaningful subwords which retains the root word uh so the subword splitting ex actually helps the model learn uh that different words with the same root word as token such as tokens and let's say tokenizing are similar in meaning so this is an important point which we did not achieve in word based tokenization and even character based tokenization the subword splitting actually helps the modern learn that different words with the same root words such as token uh such as tokens and tokenizing are similar in meaning that's very important so this meaning is actually encoded in subord based tokenization now how is subword based tokenization implemented there are certain algorithms to implement subword based tokenization and one such algorithm is called as bite pair encoding so this algorithm itself was developed in 1994 and the main purpose of this algorithm was to compress data I'll not spend too much time on it but let me take you through a quick example so let's say if this was the original data what happens in this algorithm is that we actually first look at a bite pair which occurs the most so in this case AA is that bite pair right which occurs the most so we look at the bite pair which which occurs the most and we replace the bite pair with with another variable which does not occur in the data so a a appears the most so we'll replace it with another variable which is called as Zed so now the compressed data becomes Z AB d z a a then we look for the next common bite pair that is AB and that is replaced by another variable which is y so then the further compressed data becomes zyd z y a so only one bite pair is left and after that we do not do the compression further there is one more round which can be done this zedy can be replaced with w and this zedy can be replaced with W so that is the last round of compression this is how the bite pair encoding algorithm works we look at bite pairs which occur the most and we go on replacing them with different variables and we compress the data so you must be thinking how is exactly the bite pair algorithm used for large language models and how does it relate to the subord tokenization rules which we saw so we saw that in uh subord tokenization um the most commonly used words are represented as a single token while rare words are broken down into two or more subword tokens we'll see that this is exactly what the bite pair encoder algorithm achieves when it's applied to sentences or when it's applied to vocabulary so essentially let's take a practical example if you have this data set of words let's say if you have this is your vocabulary let's say old appears seven times older appears three times finest appears nine times lowest appears four times so what you do first is that you add uh another token which is similar to the end of text token which we saw earlier this token indicates that a word has ended so after each word you add this token so now this is our vocabulary now let's see how subord based tokenization or how bite pair encoding is actually done first what's done is that all the words are split into their character based tokens into character level so in this vocabulary we have all of these different character level tokens and we also mention their frequency next what is done is that similar to The Bite pair encoding algorithm we look for the tokens with the most common pairing and then we merge them so let's look at these words it seems that the most common pairing is e and s because it comes in finest nine times and lowest four times so it comes 13 times right so the most common bite which starts with e is es so this is actually the bite which occurs the most so then what we'll do is that we'll merge e and s and then we'll create a separate token so now the new token which is created is es and then we'll apply the same thing then es is one token then we look for the next commonly occurring bite and that is EST so then EST in the next iteration EST will be a separate token in the iteration further than that EST followed by this slw will be a separate token then we'll see that the byes o and L also appear 10 times so then we'll merge o and L into a separate token then we'll see that o l and D is that bite pair which is also occurred 10 times so we'll merge them so finally the list of tokens which you will have will look something like this this is the final list of tokens after you do four to five iterations of the most commonly occurring bites and if you merge them so these will be the list of tokens now if you analyze these list of tokens there are also characters in these tokens but there are also words in these tokens and the beauty of these tokens now is that we are retaining the root words because if you see uh let's look at our words uh in the vocabulary EST occurs in finest and lowest right so EST is a root word which can occur in many words and the world based tokenizer and the character based tokenizer will never learn this but now we have learned this even older if you see older and old these would be two separate tokens in world based tokenizer but now old is a token and then e and r are separate tokens that's exactly how subword based tokenization actually works we retain the root words and uh some kind of meaning is encoded and since it's not exactly word based tokenization also we don't need that much amount of tokens to construct a vocabulary so now in this in this simplified example this list of 11 tokens will serve as our vocabulary and then you must be thinking when when do this iteration stop when do we stop this merging we stop this merging of bytes based on let's say you have a maximum token count or let's say if you have already specified the maximum number of iterations which will happen so this is a gist of how the bite pair encoder algorithm actually works we we have an entire separate lecture devoted to this uh you can have that you can look at that lecture it's a 1 hour lecture which goes into a lot of detail of bite pair encoder but here I just wanted to give you an overview of how the bpe algorithm works and why GPT is using it GPT is using it because it also does not want a very large vocabulary size which would have been needed in world based tokenizer and we also need to retain some kind of root meaning like we know that EST is a root word so we return that and we don't want a separate word for old we don't want a separate token for older instead I'll just have a common token for old so bite pair encoding is the best of both world it's an efficient vocabulary size it also retains meaning awesome so that's why actually uh GPT uses the bite pair encoding what we can do right now in code is we can have a simple implementation of the bite pair encoder algorithm and let's see uh how this actually works so there is a python Library which is called as tick token and uh this is the this is the bite pair encoding tokenizer which is used in open AI models so we are going to use the same tick token so you need to First install the tick token library and then uh you can actually directly create tokenizers so what I'm doing is I've created a tokenizer from tick token and I'm using the same uh encodings which are used in gpt2 uh remember our previous tokenizer class had this encode method and the decode method similarly here also we have the encode and decode so let's see let's see we give this text which we which we had already given in the previous example and you see in word based encoding we have to augment the dictionary with special words right but as you will see over here uh G this bite pair encoding tokenizer deals with this unknown text so I've given this text here hello do you like tea in the sunlit Terra of some unknown Place some unknown place is not even a word so ideally the tokenizer should show an error right let's see if it's indeed the case so now I do tokenizer do encode text and I allow the special character end of text so here you see when we encode there is no error so how do you think the bpe tokenizer dealt with this random word remember the bite pair encoding tokenizer is a subword based tokenizer so it has characters and it has subwords also as tokens so it is very very likely that this word is actually an accumulation of these characters or subwords that's the advantage of that's another advantage of B pair encoder tokenizer we don't need to create special tokens like unknown tokens with the with the same vocabulary of tokens which we have it can actually uh replace any word which is even not present in the vocabulary uh so three advantages of bite pair encoder first the vocabulary size is not too high second uh it retains the meaning of root words and third it automatically deals with unknown words so the vocabulary size of bite pair encoder which was used in gpt2 was around 50,000 you can see that it's much lesser than the total number of words in the English language which is what we might have required if we use the word based tokenizer you can also Google this so number of uh or the vocabulary size uh vocabulary size of DP tokenizer used for gpt2 so let's see so the it has 50,000 tokens yeah so close to around 50,000 tokens so awesome so this encoder method Works without mentioning special unknown tokens also and now we can also do decoder and now you can see hello do you like tea in The sunle Terraces of some unknown place even the decoder works perfectly so instead of writing our own word based tokenizer what actually works the best is using subword based tokenizer which employs bite pair encoding it's already provided to us by The Tick token Library we can use tick token. get encoding to create the tokenizer and then we can test the encode and decode methods there are some notes here so the first note which is important is that the BP tokenizer encodes and decodes unknown work words such as some unknown Place correctly it can handle any unknown word so how does it achieve this that's the important question right so the algorithm underlying BP breaks down words that aren't in the predefined vocabulary into smaller subword units or individual characters as I said and this enables it to handle out of vocabulary words so let's say you are given this and you want to encode it right if you use a word based tokenizer it will failure because these words are not there this is not a word but if you encode this it will print out some token IDs why this is the case so I have a screenshot here so let's look at the screenshot what the BP algorithm is actually doing is that all of these which are individual subwords or sub characters are tokens so whenever this new word is given to us it's usually broken down into these tokens which are either subwords or which are characters so that's why that's how it deals with out of vocabulary words like this which don't even mean anything and of course then the decode method also works and it returns back the same sentence to us so here I've done some exploratory thing like I got the encodings for gpt2 and I got the encodings which are commonly associated with gpt3 and gp4 and I printed out the vocabulary size so you can see that the vocabulary size subsequently increases because as better and better models of GPT are trained we have a larger and larger and larger vocabulary okay so this is how the bite pair encoding algorithm actually works we will not cover character level encoding because that is not generally employed in large scale llm models so now let's go to the start of this lecture and let us see how much have we covered so far okay so yeah right over here so until now we have covered I think tokenization that's the first section which we have covered it took some time but it's very important now let us move on to the next Parts which are token embedding positional embedding and input embedding so uh if you look at this figure this is what we are covering up till now we have finished token tokenization and we have finished token IDs and now we will cover token embeddings and positional embeddings so if you want to take a break over here please pause this video take a break and then we'll start with these sections okay so I hope everyone is with with me until now it has been a long lecture so far but I hope you are energized and excited for the next part and here as I mentioned we have got the token IDs but these token IDs need to be encoded into a higher dimensional Vector space uh that's called token embeddings and then we need to add positional embeddings to these token embeddings to create input embeddings which will be the final input to the large language model so let's begin this embedding Journey uh and this joury Journey Begins with uh token embedding so okay let me scroll down to my lecture notes yeah so before actually coming to token embeddings there is one more uh small point which I want to cover and that point is actually extremely important I could have directly skipped to the embedding portion without covering this point but uh I want this lecture to be as close to real life as possible so I want to show you all the Finer Things which need to be considered um when dealing with the data pre-processing and one such thing is creating input Target pairs so let's see what this actually looks like to start discussing about giving an input to the llm we also need to have a discussion of what the input really looks like so we know that llms predict the next word right that's the main task which llms do so if you have the first word llm it will predict the next word learn if you have llms learn it predicts the next word two if you have llms learn to it predicts the next word predict if you have the llms learn to predict it predicts the next word which is one so there is an input and there is a target but we now need to see how to represent this in numerical format and the way this is done is as follows first we need to determine a variable which is called as context size context size is the maximum length of the input which the llm Sees at one time which basically means how many tokens will the llm process C to predict the next World it can be four tokens it can be 256 tokens in large models the number of tokens which the llm sees is usually pretty large more than 150 more than 200 it can even be more around 500 etc for now here I'm showing a context size of four which means that at one time the llm can see a maximum of four four tokens so what we do is we create input output pairs based on this context size so here is one sample input output pair when the llm receives this input it has to uh predict the next word in the output so for example if one is an input the next word which is predicted by the llm should be word if one and word are the input the next word predicted by the LM should be at if one word at is the input to the llm the next word predicted is a and if one word at a is the input to the llm the next word which is predicted is time so this is how the input output pairs need to be constructed so remember when you look at one input output pair there are actually multiple prediction tasks which are happening so if you look at this input output pair there are four prediction tasks happening right if one is the input word is the output if one word is the input at is the output if one word at is the input o is the output if one word at a is the input time is the output so there are four prediction tasks so keep this in mind so what we now need to do right now is that we have have to if provided a text sample which is a text sample which is like this we have to break this text sample down into input output pairs like these we have to break down the text sample into input output pairs like this based on the context size so let's see what what this will look like so our main aim is to uh create the input output pairs right and the way we will do it is by using something called as data loader so before I started learning LM I have not I had not used data loader before but it's actually a very very useful uh tool I would say which is provided through P torch and it's just it just helps to process data in a much better manner so what we are going to do here is that we are going to create this input output Target pairs through a data loader and we'll use a sliding window approach don't worry about this terminologies I'll explain what it actually means so what we are going to do here is that let's say this is sample text in the Heart of the City stood the old library and let's say we are using a context size of four okay so I will I will create two tensors based on the sample text one will be my input tensor and one will be my output tensor or the target tensor let's look at the input tensor first each row of the input tensor will be uh an input to the llm and each row of the target tensor will be an output of the llm where is similar to the input output pair uh which we just saw over here this is an input output pair so the first row uh so the first row of the tensor X and the first row of the tensor y will be an input output pair the second row of the tensor X and the second row of the tensor y will be another input output pair Etc so using data loader we'll first collect so using first using data loaders we'll collect the inputs in a tensor X where each row represents one input context and we'll also create another tensor y Which con consists of the corresponding prediction targets so if you look at the prediction targets they are just the input which is shifted by one word so remember this is important the prediction so if you look at the Target if you look at the first row of the target it's actually just the first row of the input but shifted by one word because we are going to predict the next word that is important so let's see how these inut output pairs are actually created using the data loader so I'm going to scroll down um right here yeah so we have uh implemented this bite pair encoder tokenizer right and uh using this tokenizer we encoded some sample text but now we'll encode the entire uh data set which we are which we are using in this lecture so the way to do this again is we first read this text and store it in the Raw text and then we use the tokenizer to encode this raw text this tokenizer is this bite pair tokenizer which we have got from tick token okay so we are encoding this raw text now and if you print out the length of the encoding text it is 5145 which are the number of tokens uh in the training set after applying the bite pair encoding tokenizer so now what we'll be doing here is that uh here I have just mentioned what the input output pairs can look like so uh if you have this X as an input pair and Y as an output pair if 290 is the input 4920 is the output if 290a 4920 is the input 2241 is the output so there are basically four prediction tasks happening in one input output pair similar to what we discussed on the white board this can also be printed out in textual format so if and is the input established is the output if and established is the input himself is the output if and EST list himself is the input in is the output this is what we want so we want to create input output pairs such as these so this is an input output pair and for that we'll use the data loader so let's see how we'll use the data loader uh so it's actually very simple we first encode the entire text into token IDs and then what we are going to do is we are going to chunk this book into overlapping sequences so this is one more thing the One More Concept ccept which I want to explain here is that before we learn about chunking you need to understand the concept of stride so uh let's say here in this case my first input is in the heart of right my second input here is the city stood the so if you uh actually let me scroll down below where I have the photo yeah uh so let's look at this second example first my first input is in the heart of my second input is the city stood the so let's mark these two inputs let's mark the first input and the second input so here the stride is four because the input moves by four positions so if this is the first input to construct my second input I have to move 1 2 3 and four then to construct my third input I will move to the this position then my third input will be old library or Relic so this is tried equal to 4 so here what I'm doing is uh I'm not missing out any word but between my different inputs there is no overlap now in this in this first example you see stride equal to one and why is this stride equal to one because the first input here is in the heart of and the second input is the heart of the so see there is an overlap between the first input and the second input because the stride is one if the stride is four it leads to this this second example over here so the stride actually determines how much we move from one input batch to the next input batch so that is another important variable when we construct the input data so what so that's why it's called as a sliding window approach if the stride is equal to one we just slide by one to create the next input but if the stride is equal to four we stride by four to create the next input so that's why it's sliding window approach so what we are doing here is that the final input tensor will be input IDs and the target tensor will be Target IDs so to visualize this you can just think of these two tensors over here yeah the input tensor the input tensor will be X and the target tensor will be the target ID and what we are doing here is that we are appending so we are first looping over all the token IDs based on the context length and the stride which we have and then we are appending the inputs to the input chunk so we are appending the input uh input chunks to the input ID sensor and we're appending the target chunk to the Target ID sensor and what is the input chunk is just the token it's just the four token IDs at one time that's it and what is the target chunk it's just the input chunk shifted by one so let me show this to you in here so when the code is implemented uh the this first row is the input chunk that is appended to the input tensor this first row of the target is the target chunk that is appended in the first iteration in the second iteration the second row is appended second row of the input and the second row of the target similarly we Loop over the entire text and we append the input tensor and we append to the Target tensor so at the end of this data loader when this data loader is created we have the inputs and outputs uh so I hope you are understanding this part this is a bit of a detail but it's very very important because uh what this data loader enables us to do is enables to include the stride and later you will see that it also enables us to to batch processing parallel processing Etc until now just remember that we have created input output pairs like this we have created input pair and we have created output pair and now what we have so this is just the this is the just the data set so we created the input data set and we have created the output data set we have to pass this data set now to the data loader so what the data loader does is that it takes the data set uh and then it also takes some other attributes like we have to specify the batch size this is very important batch size is basically how many input batches do you want to process at one time before updating the parameters this this is the exact same notion as the batch size in other machine learning Frameworks so a batch size of one will mean that only one input output pair is processed at one time before updating a parameter if batch size of four it means four input output pairs are processed before updating the parameters there are some other things here such as number of workers number of workers means how many threads you want to split the code on your CPU let's say for parallel processing you don't need to worry about this right now so data data loader is basically something which can be used to process the data set basically the input output tensors which we described here uh what the data loader will do is that it will go over these input and output tensors and it will generate uh the input output batches so after creating the data loader you can actually print the input batch you can print the output batch and this is what it looks like uh so this is the first batch of input this is the first batch of output this is the second batch of input and this is the second batch of output uh you can also do a batch size of eight and when you use the data loader with a batch size of eight this is the these are the input output tensors so since we are doing a batch size of eight it means the LM parameters which we'll look at later will be updated only after this entire batch is processed so you'll see the input has actually eight eight inputs and the output has eight rows so each row so this is the first input and this is the first output this is a second input and this is a second output what this actually means that is that there are four words in the input and there are four words which need to be predicted in the output like the next word so each input output pair has four prediction tasks which we learned about before okay so uh until now what we have done is that we saw the uh word embedding we saw the token word tokenization we saw the bite pair encoding which is subword tokenization before coming to uh Vector embedding or positional embedding it was very important for me to tell you how the data itself is loaded how the inputs are given and how the outputs are given to the llm and I also wanted to show you the importance of context size and stride if you did not understand this data loader uh data loader part if you did not understand data sets and data loaders I'll also share this link in the uh information section but you can you can now try to proed proceed to the next part we don't strictly need to understand data sets and data loaders before understanding the token embeddings which I'm coming to right now in the next section so again you can take a break here and uh Focus your full energy on what is going to happen next which is token embeddings okay so now we are ready to proceed with the next section which is token embeddings so up till now we have looked at tokenization we have looked at converting tokens to token IDs we have looked at the bite pair encoding we have looked at data data sets and data loaders now let's come to the next step which is token embeddings um so before actually looking at the code for token embeddings I also want to intuitively show you why token embeddings are needed in the first place and uh why can't we just use token ID as inputs so we saw the data sets and data loaders and we saw that inputs have token IDs right why can't we just leave it at that stage why are embeddings needed so to motivate the concept of why embeddings are needed uh let's look at this so computers need numeric numerical information right it requires numerical representation of words so how can we represent words in numbers you might you might be saying that this is already done in token IDs right we can just assign one token ID to each world and that's it we can maybe even assign random numbers to each world so then every word will be a number and computer or the llm rather would understand that information what's the problem with that one of the major problem with that is that let's say if you have two words cat and kitten these words are semantically related but if you randomly assign token IDs or numbers to these words that semantic relation or that meaning between these words is not captured at all because let's say the token ID for cat is 34 let's say or the number associated with kitten is minus 13 we are nowhere encoding this information right that cat and kitten are actually related to each other so then you might think that what about one hot encoding what if I create a dictionary of words and wherever that particular word appears I'll just represented by one all others will be zero so then dog let's say will be 0 00 1 0 0 dot dot dot dot uh puppy would be something like 00 0 1 0 0 dot dot dot so the one will appear at different positions for dog and puppy the same problem exists for even this case one hot encoding fails to capture the semantic relationship between the words so even if we do one hot encoding Noh no where we are saying that dog and puppy are actually related to each other so the semantic meaning between the words is lost if we just used one hot encoding and that's where token embedding actually comes into the picture the main idea is let's say if we have four words dog cat apple and banana and let's say I represent each word in a vectorial format and how to get this Vector let's say each Vector has five Dimensions based on has a tail is eatable has four legs makes sound or is a pet and let's say I fill in the information for the dog so then dog will be represented let's say by five dimensional vector and you can see that has a tail has four legs makes sound and is a pet have higher values now when you encode cat you'll see that has a tail has high value is eatable has low value has four legs has high value make sound has high value and is a pet has high value now when you look at the vectors for dog and cat you can see that the dimensions for which dog is higher is also the same dimensions for which cat is higher so there is some meaning which has been encoded here right you can look at these vectors and you can say that these vectors are actually similar to each other so maybe if you plot these vectors in five dimensional space they will be grouped together that that implies that dog and cat are actually related they are words which which are similar to each other they are both animals they both have four legs they both make a sound they both are pets they both have a tail similarly if you encode apple and banana for has a tail for has four legs for mix sound and is AET they will have lower values but apple and banana will have much higher Valu for is eatable so if you look at Apple and if you look at Banana you'll see that these two vectors are closer to each other and if you look at dog and if you look at Apple you'll see that they are very far apart because apple has a high value for is eatable and dog has a low value for that so if you actually can embed words as vectors then you can capture or retain the semantic relationship or meaning between words this is an extremely important idea and that that's where token embeddings actually come into the picture if we do this if we can successfully embed words as vectors and capture the meaning between them and if you feed these inputs to the llm then those inputs will be will have a lot more meaning and that will lead to much more successful llm models rather than randomly giving token IDs so this is the main idea behind token embedding or vector embedding vectors can essentially capture semantic meaning now you must be thinking that okay uh this is great but how do I generate these vectors how do I get the values uh for these vectors and actually to get the values for these vectors we have to train a neural network and we have to create Vector embeddings so uh if you look at GPT and if you look at the vector embeddings it's a huge High dimensional space and so every word is converted into that higher dimensional space vector and neural network is essentially trained to generate those vectors now let me come back to the Whiteboard again okay up till now hopefully you have understood why token embeddings are needed in the first place and now let us start with the next section that how token embeddings are actually created for large language models so to understand this we we again need to start from the vocabulary so let's say that we have a vocabulary of words so of tokens and there is a token ID right associated with each token so we need to convert every token ID into a vector right so let's look at this Matrix here so this is a matrix which is called as the embedding layer uh this is called as the embedding layer weight Matrix so what is actually happening in this Matrix is that if you have token IDs such as 0 1 2 50257 which are the number of token IDs for training gpt2 for each of these token IDs you will have a vector so let's say if the vector Dimension is 256 for example uh I think a vector dimension of 768 was actually used for training gpt2 but let's consider 256 for now so each token ID will be represented as a 256 Dimension Vector so token ID 0 will be a 256 Dimension Vector token ID 1 will be a 256 dimensional Vector a right up till token ID of 50 257 that will also be a 256 dimensional Vector so this is now a embedding layer weight Matrix which has dimensions of so it has 50257 rows and it has 256 columns so this embedding Matrix needs to be constructed and then whenever we want to fetch an embedding Vector for a particular ID we just look at that particular row so for example if we want the embedding Vector for ID number two we just look at the third row because p as zero indexing and then we retrieve the embedding Vector for that particular ID that's it that's the simple idea behind uh the embedding layer weight Matrix so let us look at that right now uh okay so for the sake of Simplicity right now we are going to look at uh vocabulary size of six and we are going to look at a vector dimension of three so you can think of this table but you can think of the vector Dimension as three so there will be three columns and there will be six rows because the vocabulary size is six so let's say these are the input IDs 2 3 5 and 1 and we want to convert these input IDs into vectors like each of these input ID will be converted into a vector so first we will actually create an embedding layer and I think I should write it down over here uh so first we'll create an embedding layer like this and this embedding layer will actually have uh three columns and it will have one three it will have six rows why will it have six rows because we have six token IDs we have token ID of zero token ID of one token ID of two uh token ID of two token ID of three token ID of four and token ID of five so we have six token IDs and each token ID will be represented as a three D three Di dimensional Vector so this is the embedding layer Matrix how is this created in Python in Python it's created by using tor. nn. embedding and you specify the number of rows which is the vocabulary size you specify the number of columns so let me show you tor. nn. embedding so if you go to tor. nn. embedding you'll see the documentation for this basically uh you can specify the number of embeddings and the embedding dimensions and it creates a matrix uh with randomly assigned values so after you create the embedding layer you can print out the embedding layer weights and you'll see that we have six rows and three columns with randomly assigned values so here's the embedding layer Matrix and random values will be assigned to all of these so for the first ID random three values will be assigned to create uh the vector for the second ID random values will be assigned similarly for ID number five random values will be assigned one very important thing to note here is that these values are random currently right because we don't know the ideal token embedding even this has to be learned when we train large language models so dog and puppy needs to be closer so the vectors for dog and puppy should be aligned we cannot have random values like these and we also have a neural network for this which needs to be trained so even token embeddings need to be trained and uh GPT when it was trained even the embeddings were trained all along with the next word prediction task so these are the if you print out the embedding layer weights these are the all the weights and then as I mentioned you can retrieve the embedding Vector for any ID which you want so if you want the embedding Vector for ID number three you just pass in that particular ID and then you will get the vector because then it it's just the fourth row so if you want to get the embedding Vector for ID number three you just pass in this value and then you will get the vector so one way to think of this embedding layer matrix it's a lookup table and you can actually pass in the ID and you will get the corresponding embedding Vector so here you can see I've passed the ID number three and I've got the token embedding Vector for that ID we can also pass all the input IDs which we want the embedding or the token embedding for at one point and then based on the embedding layer weight Matrix we get the embedding token embedding vectors for all the input IDs so essentially what we are doing in this token embedding section is that every token ID is is converted into a token into a vector into an embedding vector and the best way to do this is to use the embedding layer in py torch so when you create an embedding layer you have to specify two things you have to specify the vocabulary size and you have to specify the vector Dimension why because you have to create a vector for every uh token ID in the vocabulary so the number of rows will be equal to the vocabulary size and the number of columns will be equal to Vector Dimension because for every token ID we will get a vector of those many dimensions and then it's also important to note that this embedding layer weight Matrix is actually optimized during the llm training process that's very important so all of these parameters also need to be optimized and the last point to note in this this token embedding section is that the embedding layer if you look at it it's just a lookup operation basically that retrieves rows from the embedding layer weight Matrix using a token ID now actually what we can do is that before we move to position encoding uh we can we can use the actual value of the token vocabulary size which is 50257 used in chat used in gpt2 and we can use a vector dimension of 256 to construct very real uh real life or practical embedding layer Matrix so that's what we'll be doing in code right now uh so let me go to that particular section in the code uh okay so first yeah this is positional embedding but first I want to create that uh that Matrix and then we'll come to positional embedding yeah okay so now what I will do is that I will create an embedding layer m which has 50257 rows and which has 256 Dimension vectors so let's see how we are going to do that so let's say the vocabulary size is 50257 and the vector Dimension which we want for each token ID is 256 so we'll use the same function tor. nn. embedding this will create an embedding Matrix with a vocabulary size which means number of rows of 50257 and number of columns of 256 now what we'll do is that we have already created the input data from the data loader right uh we will uh we will get that input data initially and then uh this input data has token IDs so we'll pass that those token IDs into the embedding and then get the corresponding embedding vectors for those token IDs it's actually a very simple concept but you just need to pay attention to Dimensions here so remember what we are trying to do here is that we have this uh embedding layer weight Matrix which is essentially a lookup table which means that if you provide a token ID it will give you the uh embedding Vector but to get the token IDs we need to get the input data from the data loader and the data loader we are going to uh get input data in batches of eight and then context length is four which means that at one time we are going to look at eight text samples with four tokens each so if you look at the first batch of this input data it has four tokens if you look at the second uh actually this all is one batch itself I should rather say if you look at the first row of the input data or the first input sequence it has four tokens if you look at the second row of the input data uh which is the second input uh sequence it has four tokens so we have to essentially get all of these token IDs each token is one token ID and we have to convert it into Vector representation and we are going to do this in code it's very simple just keep in mind the dimensions keep in mind this dimension of 50257 rows and uh 256 columns and here keep in mind that we have a context length of four which means four columns and eight rows okay so we are creating a data loader here which takes in the Raw text bat size of eight and we are going to use a maximum length which is the context size of equal to 4 and we are going to generate the inputs and the targets so let's just look at the input right now so if you see these inputs are exactly the same as what I was showing to you in this white board so every row essentially has four tokens and we have eight rows because each batch has essentially eight input sequences now we want to convert each of these token IDs into 256 Dimension Vector so if you see this is currently 8x4 right this is 8x4 Dimensions now we want to convert all of these into a 256 dimensional uh vectors so let's see how the dimensions will play out so we have these inputs right so let's say this is 8x4 Matrix and these are the input IDs so we have to essentially take each input ID here uh go to the lookup table and get the corresponding Vector for that input ID right that is what we are going to do uh so one embedding Vector of length 256 is generated for each token in this input Matrix so if you if you actually see how this looks like it's like this so if you look at the first token ID this will be a 256 Dimension Vector if you look at the second token ID this will be a 256 Dimension Vector if you look at the third token ID it will be a 256 Dimension Vector so now every single token ID here is a 256 Dimension Vector so you can think of this result as 8X 4X 256 so here there are 8x4 entries right and each of these 8x4 entries has now a 256 Dimension Vector associated with it so the result of this embedding is that when we generate this embedding Vector for each of these inputs we'll have an 8X 4X 256 tensor so that's exactly what is happening right here so we we have the token embedding layer and we are passing these inputs which are the input idies basically and the result will be uh the token embeddings which is the result will be an 8x4 by 256 Vector so this is how the token embeddings are obtained in practice for a batch size of eight for a Contex size of four and for a vector Dimension size of 256 now we will add the positional embeddings on top of this these token embeddings for one batch and then that will be the input to the large language model so now we are going to come to positional embedding or positional encoding it's used interchangeably uh again you can pause pause here for some time um because positional encoding is actually going to be the last section of this long lecture so you need to be energetic and motivated in this last part also so I'll just meanwhile scroll to the positional embedding section on the Whiteboard yeah okay so thanks a lot everyone who have stayed until this part which is the last part of this lecture and that's on positional encoding so up till now we have looked at token embedding and we also saw how to convert all the input token IDs into token embedding vectors the problem with token embedding is that it does not take into account the position at all so basically if there are two sentences the cat sat on the mat and the second sentence is on the mat the cat sat so cat the word cat comes in both sentences right but the embedding Vector for both these words will be exactly the same it does not contain any information about the position so the cat word comes in different positions in these two sentences and ideally where the position is really matters for understanding the context of a sentence so it will it will be very advantageous for us if along with encoding the semantic meaning which we did in tokon embeddings if we also encode information about the position of the word um in the sentence so that's where positional encoding actually comes into the picture uh the main uh objective here is that or the main uh drawback of just token embeddings is that it does not include positional information and it is very helpful to inject additional positional information to the large language models there are two types of positional embeddings or encodings first is the absolute positional encoding and second is the relative positional encoding what happens in an absolute positional encoding is that for each position in the input sequence a unique embedding Vector is added to the Token embedding so this figure actually explains it the best so let's say uh we have tokens such as cat uh the second token is let's say set the third token is on and uh the fourth token is the let's say these are the four tokens and for the sake of Simplicity let's say the token embedding vector for all of these four are similar so let's say 11 one one Etc now uh what happens in absolute position embedding is that another embedding Vector is added to the Token embedding and that is the positional embedding vector and since these words have different positions so cat is first word sat is second word on is third word or third token and the is the fourth token they will have different positional embeddings so for example the positional embedding for CAT will be 1.1 1.2 1.3 the positional embedding for set will be 2.1 2.2 2.3 the positional embedding for on will be 3.1 3.2 3.3 the positional embedding for the will be 4.1 4.2 and 4.3 and then you add the token embedding to the positional embedding and then finally you get input embeddings and then this will be 2.1 2.2 2.3 for the word cat this will be 3.1 3.2 3.3 for the word set it will be 4.1 4.2 4.3 for on and it will be 5.1 5.2 5.3 for the word the so these are the input embeddings which will then be finally used by the llm and if you see although the token embeddings are similar the input embeddings are different from each other because now the position is encoded so the input embeddings for cat sat on the mat will be different than the mat the cat sat on let's say so here the position is taken into account so the positional vectors have the same Dimension as the original token embeddings because you have to add the position embedding vectors to the Token embeddings relative positioning or relative positional encoding is a bit different the emphasis here is on the relative position or the distance between tokens so rather than uh focusing on at what exact position the token is the model instead focuses on how far apart the different tokens are so the distance between the token become very important here so uh what are the advantages disadvantages of both these types the absolute positional encoding is very much suited when fixed order of tokens is crucial such as sequence generation so GPT uses absolute positional encoding and it is more commonly used than relative positional encoding even today we are going to learn about absolute positional encoding but even relative positional encoding is actually suitable for tasks like language modeling or long sequences where the same phrase can appear in different parts of the sequence so for that relative position positional encoding can also be used but today we are going to learn about absolute positional encoding uh okay so we have until now we have got this uh token token embeddings right so we have a tensor of 8X 4X 256 the next step is that we need to create another embedding layer for positional em positional encoding or for positional embedding so remember one thing which is very important here is that uh every input essentially just has four tokens right so the context length is important when we create the embedding layer Matrix for positional encoding the vector dimension of course can be 256 which is the same same Dimension as the token embedding and that needs to be the same because we are going to add the positional encoding Vector to the Token encoding Vector but we actually need this these vectors for just four positions because the context length is four the token ID can be either in position number one position number two position number three and position number four and for each of these positions we need a separate Vector which is the positional embedding Vector so the size of this positional embedding Matrix will just be four rows and 256 columns because for each position position 1 2 position 1 2 3 and four we need a separate Vector which is 256 Dimensions so that is what we are going to do right now so yeah when the input batch comes like this uh if we look at first input it will be something like this it will have four token IDs and each will be 256 Dimensions so if our positional encoding has four vectors and each is 256 Dimension then we'll just add it to each input in the batch so that is what we'll be doing so we need only four positional embedding vectors and now what we'll be doing is that we'll generate the positional embedding Matrix and then create the four positional en coding vectors from it as you can see I sometimes use embedding and encoding interchangeably and that's fine so we are creating the positional embedding layer which has context length which has four rows because we only need four vectors and it has 256 columns because each Vector will have size 256 and then what we do is that we create the four positional embeddings from this Matrix that's it so then the positional embedding layer will be 4X 256 why because we have one 256 dimensional Vector for each position and now after we create the positional embedding Vector so we create this four vectors as the last step what we have to do is that we have to just add the token embeddings to the positional embeddings so the token embeddings as we saw is 8x 4X 256 tensor and the positional embedding is 4X 256 so you must be thinking how to actually add these so when you add these two in Python it will do the broadcasting operation which means what it will do is is that it will convert this 4X 256 into an 8X 4X 256 by duplicating these value eight times so what will essentially happen when you do this summation is that you first look at the first input sequence and you add this positional embedding for that input sequence that will give you the the first row of the input embeddings then you look at the second row even for the second row you uh you look at the token embedding second row you again add the same four positional vectors that will give you the the second row of the input embeddings similarly you look at the eighth row which is the final row of the token embedding Matrix and again you add the same four positional embeddings so then you get the final row which is the eighth row of the input embedding Matrix and that is how the input embedding Matrix is actually constructed uh so this is the last step which we'll also be doing in code so input embeddings is token embeddings plus positional embeddings and and then we get the final input embedding one thing to mention here is that similar to token embedding even when you initialize the positional embedding so when you initialize this positional embedding Matrix which I think I showed over here yeah if you initialize this positional encoding Matrix all the values will be initialized randomly as I said the embedding layer in Python just initializes the value randomly uh similar to the Token embedding even the values in the positional embedding layer need to be optimized during the training process itself so actually multiple things are happening to train the large language models of course we have to train it later to predict predict the next word but even in the data processing pipeline we have to train the positional embedding we have to train the token embedding as well which are then fed as input to the llm for the llm training procedure okay so this actually uh brings us to the end of today's lecture this is where we started the lecture with we started with the llm data pre-processing Pipeline and we discussed that will cover all the Four Points in today's lecture we started with tokenization we understood word based subord based character based tokenizer then we actually learned about the bite pair encoding tokenizer which is used by GPT and then we learned about token embeddings which is essentially converting token IDs to vectors we saw that it's not enough to just learn about token embeddings you also Al need to essentially learn about positional embeddings as well because these positional embedding vectors encode information about the position and then finally you add the token embeddings with the positional embeddings to create the input embedding if you were to ask me for one figure to explain today's lecture I would say in today's lecture we have covered the entire data pre-processing pipeline shown here so right from uh tokenization to token IDs to token embeddings to positional embeddings and finally to input embeddings we covered all of this in today's lecture this makes the data ready to be fed as an input to the U llm architecture or to the llm pre-training process um we also have separate lectures on all of these topics but I thought to make one condensed lecture to show you all the things in one glance uh as always I take an approach of the Whiteboard plus coding in Google collab so that your the theoretical foundations are strong plus your coding foundations are also strong I'll be sharing this entire code file with you so that you can play around with it you can run it on your own browser Etc please ask any questions which you might be having in the YouTube comments and I'll be happy to answer all of them"
}