{
  "video": {
    "video_id": "ufrPLpKnapU",
    "title": "Lecture 11: The importance of Positional Embeddings",
    "duration": 2932.0,
    "index": 10
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.84
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.72,
      "duration": 5.52
    },
    {
      "text": "in the build large language models from",
      "start": 8.84,
      "duration": 5.679
    },
    {
      "text": "scratch Series today we are going to be",
      "start": 11.24,
      "duration": 5.959
    },
    {
      "text": "looking at a very important topic and",
      "start": 14.519,
      "duration": 6.281
    },
    {
      "text": "that is called as positional",
      "start": 17.199,
      "duration": 6.361
    },
    {
      "text": "encoding so I've divided today's lecture",
      "start": 20.8,
      "duration": 5.04
    },
    {
      "text": "into two to three modules initially",
      "start": 23.56,
      "duration": 4.2
    },
    {
      "text": "we'll look at what all we have covered",
      "start": 25.84,
      "duration": 4.839
    },
    {
      "text": "so far then we will try to understand",
      "start": 27.76,
      "duration": 5.68
    },
    {
      "text": "why is positional encoding really needed",
      "start": 30.679,
      "duration": 5.441
    },
    {
      "text": "and then finally we will do a Hands-On",
      "start": 33.44,
      "duration": 5.72
    },
    {
      "text": "coding exercise in Python where we'll",
      "start": 36.12,
      "duration": 5.959
    },
    {
      "text": "actually add positional encoding layer",
      "start": 39.16,
      "duration": 4.96
    },
    {
      "text": "along with the token embedding layer",
      "start": 42.079,
      "duration": 5.16
    },
    {
      "text": "which we have already created so far so",
      "start": 44.12,
      "duration": 5.4
    },
    {
      "text": "let's get started if you have not seen",
      "start": 47.239,
      "duration": 4.64
    },
    {
      "text": "the previous lecture on token encoding",
      "start": 49.52,
      "duration": 4.679
    },
    {
      "text": "or token embedding I would highly",
      "start": 51.879,
      "duration": 4.921
    },
    {
      "text": "encourage you to see that lecture",
      "start": 54.199,
      "duration": 4.68
    },
    {
      "text": "because positional encoding Builds on",
      "start": 56.8,
      "duration": 4.559
    },
    {
      "text": "top of token and coding if you are",
      "start": 58.879,
      "duration": 4.241
    },
    {
      "text": "coming to this lecture series for the",
      "start": 61.359,
      "duration": 4.16
    },
    {
      "text": "first time welcome I have designed the",
      "start": 63.12,
      "duration": 4.8
    },
    {
      "text": "lectur so so that they are good to watch",
      "start": 65.519,
      "duration": 4.721
    },
    {
      "text": "even as Standalone lectures but of",
      "start": 67.92,
      "duration": 4.0
    },
    {
      "text": "course if you go through the previous",
      "start": 70.24,
      "duration": 3.44
    },
    {
      "text": "lectures your understanding of the",
      "start": 71.92,
      "duration": 4.559
    },
    {
      "text": "current one will be much better so let's",
      "start": 73.68,
      "duration": 6.479
    },
    {
      "text": "get started until now we have looked at",
      "start": 76.479,
      "duration": 5.201
    },
    {
      "text": "something which is called as token",
      "start": 80.159,
      "duration": 4.441
    },
    {
      "text": "embedding and what is meant by token",
      "start": 81.68,
      "duration": 5.799
    },
    {
      "text": "embedding so token embedding is the step",
      "start": 84.6,
      "duration": 5.68
    },
    {
      "text": "number three in the llm train training",
      "start": 87.479,
      "duration": 5.201
    },
    {
      "text": "process so first we have we start with",
      "start": 90.28,
      "duration": 5.28
    },
    {
      "text": "an input text then we convert that text",
      "start": 92.68,
      "duration": 5.68
    },
    {
      "text": "into tokens so for example if the text",
      "start": 95.56,
      "duration": 6.239
    },
    {
      "text": "is this is an example the tokens can be",
      "start": 98.36,
      "duration": 7.2
    },
    {
      "text": "this is an example this what I'm showing",
      "start": 101.799,
      "duration": 6.92
    },
    {
      "text": "right now is an example of a word based",
      "start": 105.56,
      "duration": 5.72
    },
    {
      "text": "tokenizer but there are other tokenizers",
      "start": 108.719,
      "duration": 5.161
    },
    {
      "text": "such as subword based tokenizer or bite",
      "start": 111.28,
      "duration": 4.759
    },
    {
      "text": "pair encoding tokenizer which actually",
      "start": 113.88,
      "duration": 5.159
    },
    {
      "text": "GPT uses so remember that one word is",
      "start": 116.039,
      "duration": 5.72
    },
    {
      "text": "generally not equal to one token you can",
      "start": 119.039,
      "duration": 4.68
    },
    {
      "text": "even have subwords or characters as",
      "start": 121.759,
      "duration": 4.4
    },
    {
      "text": "tokens for the case of",
      "start": 123.719,
      "duration": 4.68
    },
    {
      "text": "Simplicity I'm just showing here the",
      "start": 126.159,
      "duration": 5.201
    },
    {
      "text": "token X text to be individual words so",
      "start": 128.399,
      "duration": 5.401
    },
    {
      "text": "that's step number one step number two",
      "start": 131.36,
      "duration": 4.159
    },
    {
      "text": "is maintaining a vocabulary of the",
      "start": 133.8,
      "duration": 4.4
    },
    {
      "text": "tokens sorting them in ascending order",
      "start": 135.519,
      "duration": 6.241
    },
    {
      "text": "and assigning a token ID to each token",
      "start": 138.2,
      "duration": 6.2
    },
    {
      "text": "that's step number two but even then we",
      "start": 141.76,
      "duration": 5.4
    },
    {
      "text": "are not ready to give the token IDs as",
      "start": 144.4,
      "duration": 5.76
    },
    {
      "text": "input for the GPT or for the LS M",
      "start": 147.16,
      "duration": 5.32
    },
    {
      "text": "training there is a very crucial step",
      "start": 150.16,
      "duration": 4.159
    },
    {
      "text": "number three which we looked at in the",
      "start": 152.48,
      "duration": 4.119
    },
    {
      "text": "previous lecture and that's called token",
      "start": 154.319,
      "duration": 5.0
    },
    {
      "text": "embeddings basically the token IDs which",
      "start": 156.599,
      "duration": 5.56
    },
    {
      "text": "we obtained are converted into",
      "start": 159.319,
      "duration": 5.681
    },
    {
      "text": "vectors uh so that the semantic meaning",
      "start": 162.159,
      "duration": 5.44
    },
    {
      "text": "between the different words is retained",
      "start": 165.0,
      "duration": 4.959
    },
    {
      "text": "if you directly use the token IDs as",
      "start": 167.599,
      "duration": 5.0
    },
    {
      "text": "inputs to the large language model we do",
      "start": 169.959,
      "duration": 5.0
    },
    {
      "text": "not retain the meaning between words so",
      "start": 172.599,
      "duration": 4.92
    },
    {
      "text": "for example dog and puppy are related to",
      "start": 174.959,
      "duration": 4.881
    },
    {
      "text": "each other cat and kitten are related to",
      "start": 177.519,
      "duration": 4.881
    },
    {
      "text": "to each other if we don't convert token",
      "start": 179.84,
      "duration": 6.039
    },
    {
      "text": "IDs into token embeddings this semantic",
      "start": 182.4,
      "duration": 6.199
    },
    {
      "text": "relationship or this meaning between the",
      "start": 185.879,
      "duration": 3.841
    },
    {
      "text": "words is",
      "start": 188.599,
      "duration": 3.521
    },
    {
      "text": "lost that's why it's very important to",
      "start": 189.72,
      "duration": 4.96
    },
    {
      "text": "have step number three so if you have a",
      "start": 192.12,
      "duration": 5.32
    },
    {
      "text": "vocabulary uh with GPT uses let's say of",
      "start": 194.68,
      "duration": 5.6
    },
    {
      "text": "around 50,000 words gpt2 was trained on",
      "start": 197.44,
      "duration": 5.439
    },
    {
      "text": "a vocabulary of 50,000 Words which means",
      "start": 200.28,
      "duration": 5.2
    },
    {
      "text": "of 50,000 tokens which means we have",
      "start": 202.879,
      "duration": 5.121
    },
    {
      "text": "50,000 token IDs and then there is a",
      "start": 205.48,
      "duration": 4.92
    },
    {
      "text": "vector corresponding to each token",
      "start": 208.0,
      "duration": 6.879
    },
    {
      "text": "ID that is uh called as token embeddings",
      "start": 210.4,
      "duration": 6.72
    },
    {
      "text": "every token is converted into a vector",
      "start": 214.879,
      "duration": 5.121
    },
    {
      "text": "in a higher dimensional space so now you",
      "start": 217.12,
      "duration": 4.92
    },
    {
      "text": "must be thinking okay so now I have",
      "start": 220.0,
      "duration": 4.28
    },
    {
      "text": "these vectors which capture the semantic",
      "start": 222.04,
      "duration": 4.559
    },
    {
      "text": "relationship uh between words and now I",
      "start": 224.28,
      "duration": 4.319
    },
    {
      "text": "can feed these vectors as inputs to the",
      "start": 226.599,
      "duration": 5.161
    },
    {
      "text": "large language model right almost we are",
      "start": 228.599,
      "duration": 5.56
    },
    {
      "text": "almost there I promise there is just one",
      "start": 231.76,
      "duration": 4.28
    },
    {
      "text": "last step remaining in the data",
      "start": 234.159,
      "duration": 5.0
    },
    {
      "text": "pre-processing part and then in the next",
      "start": 236.04,
      "duration": 5.36
    },
    {
      "text": "lecture on will come to training the llm",
      "start": 239.159,
      "duration": 4.841
    },
    {
      "text": "we'll see the attention mechanism Etc",
      "start": 241.4,
      "duration": 4.28
    },
    {
      "text": "and the last step is called as",
      "start": 244.0,
      "duration": 4.36
    },
    {
      "text": "positional embedding so why is this",
      "start": 245.68,
      "duration": 5.639
    },
    {
      "text": "needed so let's say we have a sentence",
      "start": 248.36,
      "duration": 5.519
    },
    {
      "text": "the cat sat on the mat and then another",
      "start": 251.319,
      "duration": 5.6
    },
    {
      "text": "sentence on the mat the cat sat so if",
      "start": 253.879,
      "duration": 4.92
    },
    {
      "text": "you look at both of these sentences the",
      "start": 256.919,
      "duration": 4.241
    },
    {
      "text": "cat cat word appears in both of these",
      "start": 258.799,
      "duration": 4.801
    },
    {
      "text": "sentences right and in both of these",
      "start": 261.16,
      "duration": 4.84
    },
    {
      "text": "sentences cat is a token that will be",
      "start": 263.6,
      "duration": 4.0
    },
    {
      "text": "assigned a token",
      "start": 266.0,
      "duration": 4.96
    },
    {
      "text": "ID and uh and that will be converted",
      "start": 267.6,
      "duration": 7.039
    },
    {
      "text": "into a vector so the vector for CAT will",
      "start": 270.96,
      "duration": 6.079
    },
    {
      "text": "be the same in both of these sentences",
      "start": 274.639,
      "duration": 4.081
    },
    {
      "text": "although the position of the cat in",
      "start": 277.039,
      "duration": 4.281
    },
    {
      "text": "these two sentences is different so",
      "start": 278.72,
      "duration": 4.72
    },
    {
      "text": "until now if we just use the vector",
      "start": 281.32,
      "duration": 4.439
    },
    {
      "text": "embedding we do not have any information",
      "start": 283.44,
      "duration": 5.56
    },
    {
      "text": "about the position of the particular",
      "start": 285.759,
      "duration": 5.761
    },
    {
      "text": "word so in the embedding layer in the",
      "start": 289.0,
      "duration": 4.32
    },
    {
      "text": "token embedding layer whatever we have",
      "start": 291.52,
      "duration": 5.44
    },
    {
      "text": "seen up now the same token ID gets",
      "start": 293.32,
      "duration": 6.879
    },
    {
      "text": "mapped to the same Vector representation",
      "start": 296.96,
      "duration": 6.079
    },
    {
      "text": "so the token ID for CAT will get mapped",
      "start": 300.199,
      "duration": 5.801
    },
    {
      "text": "to the same Vector regardless of where",
      "start": 303.039,
      "duration": 5.681
    },
    {
      "text": "the token ID is positioned in the input",
      "start": 306.0,
      "duration": 5.84
    },
    {
      "text": "sequence so for example cat the position",
      "start": 308.72,
      "duration": 4.64
    },
    {
      "text": "of a cat is different in these two",
      "start": 311.84,
      "duration": 4.72
    },
    {
      "text": "sentences right but if we don't do",
      "start": 313.36,
      "duration": 4.88
    },
    {
      "text": "anything about the position if we don't",
      "start": 316.56,
      "duration": 3.639
    },
    {
      "text": "encode the position and if we just use",
      "start": 318.24,
      "duration": 3.6
    },
    {
      "text": "the embedding layer we learned",
      "start": 320.199,
      "duration": 5.241
    },
    {
      "text": "previously both of these tokens cat in",
      "start": 321.84,
      "duration": 5.24
    },
    {
      "text": "both of the sentences will have the",
      "start": 325.44,
      "duration": 4.64
    },
    {
      "text": "exact same Vector representation",
      "start": 327.08,
      "duration": 4.48
    },
    {
      "text": "so in the embedding layer which we have",
      "start": 330.08,
      "duration": 4.679
    },
    {
      "text": "seen so far the position is not",
      "start": 331.56,
      "duration": 5.12
    },
    {
      "text": "Incorporated there is a figure which",
      "start": 334.759,
      "duration": 5.241
    },
    {
      "text": "demonstrates this pretty nicely so uh",
      "start": 336.68,
      "duration": 5.32
    },
    {
      "text": "let's say this is the embedding layer",
      "start": 340.0,
      "duration": 4.319
    },
    {
      "text": "weight Matrix which is basically we have",
      "start": 342.0,
      "duration": 4.56
    },
    {
      "text": "all of these tokens in our vocabulary",
      "start": 344.319,
      "duration": 4.641
    },
    {
      "text": "those are converted into token IDs and",
      "start": 346.56,
      "duration": 4.479
    },
    {
      "text": "for every token ID there is a vector in",
      "start": 348.96,
      "duration": 4.799
    },
    {
      "text": "a higher dimensional space so each row",
      "start": 351.039,
      "duration": 4.641
    },
    {
      "text": "correspond corresponds to one such",
      "start": 353.759,
      "duration": 4.88
    },
    {
      "text": "Vector for the token ID now if you want",
      "start": 355.68,
      "duration": 5.799
    },
    {
      "text": "to embed these token IDs which means if",
      "start": 358.639,
      "duration": 5.041
    },
    {
      "text": "you want to convert these token IDs into",
      "start": 361.479,
      "duration": 5.321
    },
    {
      "text": "Vector representations you will see that",
      "start": 363.68,
      "duration": 5.68
    },
    {
      "text": "these the five the token five comes here",
      "start": 366.8,
      "duration": 5.08
    },
    {
      "text": "and the token five comes here also and",
      "start": 369.36,
      "duration": 4.76
    },
    {
      "text": "these two tokens clearly appear at",
      "start": 371.88,
      "duration": 4.0
    },
    {
      "text": "different positions in the",
      "start": 374.12,
      "duration": 4.12
    },
    {
      "text": "sentence but if you look at the embedded",
      "start": 375.88,
      "duration": 4.039
    },
    {
      "text": "vectors for these two tokens they are",
      "start": 378.24,
      "duration": 3.16
    },
    {
      "text": "exactly the same if you look at the",
      "start": 379.919,
      "duration": 3.68
    },
    {
      "text": "first row here that's the embedding",
      "start": 381.4,
      "duration": 4.96
    },
    {
      "text": "Vector embedded Vector for the token ID",
      "start": 383.599,
      "duration": 5.561
    },
    {
      "text": "5 the first token ID 5 and for the",
      "start": 386.36,
      "duration": 4.8
    },
    {
      "text": "second token id5 you'll see that the",
      "start": 389.16,
      "duration": 4.28
    },
    {
      "text": "embedded Vector is the third row so if",
      "start": 391.16,
      "duration": 3.96
    },
    {
      "text": "you look at the first row and the third",
      "start": 393.44,
      "duration": 4.36
    },
    {
      "text": "row they are actually exactly the same",
      "start": 395.12,
      "duration": 4.76
    },
    {
      "text": "so the main point here is that the same",
      "start": 397.8,
      "duration": 5.2
    },
    {
      "text": "token ID result in the same embedding",
      "start": 399.88,
      "duration": 5.159
    },
    {
      "text": "vectors which says that we are",
      "start": 403.0,
      "duration": 4.56
    },
    {
      "text": "essentially not exploiting the maximum",
      "start": 405.039,
      "duration": 5.121
    },
    {
      "text": "information present in sentences in",
      "start": 407.56,
      "duration": 4.88
    },
    {
      "text": "sentences there are there is meaning",
      "start": 410.16,
      "duration": 4.12
    },
    {
      "text": "between different words which we already",
      "start": 412.44,
      "duration": 4.36
    },
    {
      "text": "captured through Vector embeddings but",
      "start": 414.28,
      "duration": 5.4
    },
    {
      "text": "the position also matters a lot right",
      "start": 416.8,
      "duration": 4.799
    },
    {
      "text": "cat appears at different positions here",
      "start": 419.68,
      "duration": 4.04
    },
    {
      "text": "and that completely that may change the",
      "start": 421.599,
      "duration": 4.561
    },
    {
      "text": "meaning of the sentence entirely so it's",
      "start": 423.72,
      "duration": 4.52
    },
    {
      "text": "very important to also encode the",
      "start": 426.16,
      "duration": 5.439
    },
    {
      "text": "information related to tokens and the",
      "start": 428.24,
      "duration": 6.04
    },
    {
      "text": "position at which the token appears in",
      "start": 431.599,
      "duration": 4.28
    },
    {
      "text": "the given",
      "start": 434.28,
      "duration": 5.479
    },
    {
      "text": "sentence so it is very helpful to inject",
      "start": 435.879,
      "duration": 6.081
    },
    {
      "text": "additional position information to the",
      "start": 439.759,
      "duration": 4.521
    },
    {
      "text": "large language model along with",
      "start": 441.96,
      "duration": 4.079
    },
    {
      "text": "capturing the semantic meaning it is",
      "start": 444.28,
      "duration": 3.599
    },
    {
      "text": "extremely important to inject this",
      "start": 446.039,
      "duration": 3.521
    },
    {
      "text": "additional information about the",
      "start": 447.879,
      "duration": 5.04
    },
    {
      "text": "position to the llm so now let us talk a",
      "start": 449.56,
      "duration": 5.639
    },
    {
      "text": "bit about how do we encode this",
      "start": 452.919,
      "duration": 4.641
    },
    {
      "text": "information about the position so let's",
      "start": 455.199,
      "duration": 4.481
    },
    {
      "text": "say if I give a word and I convert it",
      "start": 457.56,
      "duration": 4.359
    },
    {
      "text": "into token ID and I convert it into a",
      "start": 459.68,
      "duration": 5.079
    },
    {
      "text": "vector how do I also give information",
      "start": 461.919,
      "duration": 5.201
    },
    {
      "text": "about the position of the world let's",
      "start": 464.759,
      "duration": 4.56
    },
    {
      "text": "look at that so there are essentially",
      "start": 467.12,
      "duration": 4.359
    },
    {
      "text": "two types of positional",
      "start": 469.319,
      "duration": 4.481
    },
    {
      "text": "embeddings the first type of positional",
      "start": 471.479,
      "duration": 4.84
    },
    {
      "text": "embedding is absolute and this is the",
      "start": 473.8,
      "duration": 5.079
    },
    {
      "text": "more commonly used positional embedding",
      "start": 476.319,
      "duration": 4.16
    },
    {
      "text": "and the second type of positional",
      "start": 478.879,
      "duration": 3.841
    },
    {
      "text": "embedding is relative positional",
      "start": 480.479,
      "duration": 4.521
    },
    {
      "text": "embedding so as you must have guessed",
      "start": 482.72,
      "duration": 3.84
    },
    {
      "text": "from the name itself these two",
      "start": 485.0,
      "duration": 3.4
    },
    {
      "text": "positional embeddings are different",
      "start": 486.56,
      "duration": 3.479
    },
    {
      "text": "let's look at Absolute positional",
      "start": 488.4,
      "duration": 3.72
    },
    {
      "text": "embedding initially in absolute",
      "start": 490.039,
      "duration": 4.761
    },
    {
      "text": "positional embedding for each in for",
      "start": 492.12,
      "duration": 5.56
    },
    {
      "text": "each position in the input sequence a",
      "start": 494.8,
      "duration": 6.72
    },
    {
      "text": "unique embedding is added to the tokens",
      "start": 497.68,
      "duration": 7.6
    },
    {
      "text": "embedding to convey its exact location",
      "start": 501.52,
      "duration": 6.6
    },
    {
      "text": "let me repeat that in absolute in",
      "start": 505.28,
      "duration": 5.16
    },
    {
      "text": "absolute positional embed",
      "start": 508.12,
      "duration": 4.599
    },
    {
      "text": "for each position in the input sequence",
      "start": 510.44,
      "duration": 4.32
    },
    {
      "text": "a unique embedding is added to the",
      "start": 512.719,
      "duration": 4.601
    },
    {
      "text": "tokens embedding so for example if you",
      "start": 514.76,
      "duration": 4.68
    },
    {
      "text": "look at these two sentences cat sat on",
      "start": 517.32,
      "duration": 4.399
    },
    {
      "text": "the mat cat sat on the mat the token",
      "start": 519.44,
      "duration": 4.32
    },
    {
      "text": "embedding for cat in both is the same",
      "start": 521.719,
      "duration": 5.041
    },
    {
      "text": "right but as I said for each position",
      "start": 523.76,
      "duration": 4.8
    },
    {
      "text": "there will be a different positional en",
      "start": 526.76,
      "duration": 3.32
    },
    {
      "text": "encoding which will be added to the",
      "start": 528.56,
      "duration": 4.12
    },
    {
      "text": "Token encoding so let's say if the token",
      "start": 530.08,
      "duration": 7.04
    },
    {
      "text": "encoding is X for sentence so let me say",
      "start": 532.68,
      "duration": 7.64
    },
    {
      "text": "so let's say this is sentence number one",
      "start": 537.12,
      "duration": 6.04
    },
    {
      "text": "and let's say this is sentence number",
      "start": 540.32,
      "duration": 5.88
    },
    {
      "text": "two so let's say the token encoding for",
      "start": 543.16,
      "duration": 4.799
    },
    {
      "text": "cat is x in sentence one and for",
      "start": 546.2,
      "duration": 3.6
    },
    {
      "text": "sentence two also it's X because it's",
      "start": 547.959,
      "duration": 4.241
    },
    {
      "text": "the same word but now the positional",
      "start": 549.8,
      "duration": 4.24
    },
    {
      "text": "encoding is different because in the",
      "start": 552.2,
      "duration": 4.079
    },
    {
      "text": "first sentence the cat appears in",
      "start": 554.04,
      "duration": 4.72
    },
    {
      "text": "position number two so there will be",
      "start": 556.279,
      "duration": 4.761
    },
    {
      "text": "some different positional encoding for",
      "start": 558.76,
      "duration": 5.8
    },
    {
      "text": "this position and for uh for the second",
      "start": 561.04,
      "duration": 5.239
    },
    {
      "text": "sentence the cat appears in position",
      "start": 564.56,
      "duration": 3.08
    },
    {
      "text": "number five so there will be some",
      "start": 566.279,
      "duration": 3.441
    },
    {
      "text": "different positional encoding so the",
      "start": 567.64,
      "duration": 3.6
    },
    {
      "text": "final",
      "start": 569.72,
      "duration": 4.52
    },
    {
      "text": "embedding uh will be the addition of the",
      "start": 571.24,
      "duration": 5.0
    },
    {
      "text": "token embedding plus the positional",
      "start": 574.24,
      "duration": 4.64
    },
    {
      "text": "embedding so for sentence one the final",
      "start": 576.24,
      "duration": 5.36
    },
    {
      "text": "Vector embedding for CAT will be x + y",
      "start": 578.88,
      "duration": 4.6
    },
    {
      "text": "for sentence two the final Vector",
      "start": 581.6,
      "duration": 4.44
    },
    {
      "text": "embedding for CAT will be X Plus Zed so",
      "start": 583.48,
      "duration": 4.799
    },
    {
      "text": "they are different now see they are not",
      "start": 586.04,
      "duration": 4.72
    },
    {
      "text": "the same so cat won't be embedded in the",
      "start": 588.279,
      "duration": 5.201
    },
    {
      "text": "same manner in both the sentences and so",
      "start": 590.76,
      "duration": 4.56
    },
    {
      "text": "some information about its Position will",
      "start": 593.48,
      "duration": 4.56
    },
    {
      "text": "be incorporated that's exactly what",
      "start": 595.32,
      "duration": 4.959
    },
    {
      "text": "happens in absolute position embedding",
      "start": 598.04,
      "duration": 4.08
    },
    {
      "text": "and there is a nice figure to Showcase",
      "start": 600.279,
      "duration": 6.161
    },
    {
      "text": "this so let's say uh we have four tokens",
      "start": 602.12,
      "duration": 6.68
    },
    {
      "text": "in the input sentence so let's say the",
      "start": 606.44,
      "duration": 4.12
    },
    {
      "text": "first token is",
      "start": 608.8,
      "duration": 5.2
    },
    {
      "text": "the the second is",
      "start": 610.56,
      "duration": 6.32
    },
    {
      "text": "cat uh the third is",
      "start": 614.0,
      "duration": 5.2
    },
    {
      "text": "sat and the fourth is",
      "start": 616.88,
      "duration": 4.92
    },
    {
      "text": "mat so what I've have shown over here is",
      "start": 619.2,
      "duration": 4.72
    },
    {
      "text": "that these are the token embeddings so",
      "start": 621.8,
      "duration": 3.68
    },
    {
      "text": "for the word the this is a",
      "start": 623.92,
      "duration": 3.72
    },
    {
      "text": "three-dimensional token embedding for",
      "start": 625.48,
      "duration": 4.24
    },
    {
      "text": "cat it's the same threedimensional token",
      "start": 627.64,
      "duration": 4.12
    },
    {
      "text": "embedding for sat it's the same and for",
      "start": 629.72,
      "duration": 4.32
    },
    {
      "text": "mat it's the same ideally the vector",
      "start": 631.76,
      "duration": 3.96
    },
    {
      "text": "embeddings for every word will be",
      "start": 634.04,
      "duration": 3.88
    },
    {
      "text": "different but I'm just showing this for",
      "start": 635.72,
      "duration": 4.28
    },
    {
      "text": "the sake of Simplicity remember the",
      "start": 637.92,
      "duration": 3.84
    },
    {
      "text": "vector embeddings for each word are",
      "start": 640.0,
      "duration": 4.32
    },
    {
      "text": "ideally are actually different but I",
      "start": 641.76,
      "duration": 4.8
    },
    {
      "text": "just want to uh show a visual",
      "start": 644.32,
      "duration": 4.12
    },
    {
      "text": "demonstration here so these are the",
      "start": 646.56,
      "duration": 4.32
    },
    {
      "text": "vector embeddings now what we will do is",
      "start": 648.44,
      "duration": 4.48
    },
    {
      "text": "that for each Vector embedding We'll add",
      "start": 650.88,
      "duration": 4.079
    },
    {
      "text": "a positional embedding and these",
      "start": 652.92,
      "duration": 3.84
    },
    {
      "text": "positional embeddings are different",
      "start": 654.959,
      "duration": 4.641
    },
    {
      "text": "based on the position so for example the",
      "start": 656.76,
      "duration": 4.92
    },
    {
      "text": "comes in position number one right so",
      "start": 659.6,
      "duration": 4.919
    },
    {
      "text": "the positional embeddings are 1.1 1.2",
      "start": 661.68,
      "duration": 5.92
    },
    {
      "text": "and 1.3 cat comes in the second position",
      "start": 664.519,
      "duration": 5.601
    },
    {
      "text": "so the positional embeddings are 2.1 2.2",
      "start": 667.6,
      "duration": 5.919
    },
    {
      "text": "2.3 sat comes in the third position so",
      "start": 670.12,
      "duration": 5.839
    },
    {
      "text": "the positional embeddings are 3.1 3.2",
      "start": 673.519,
      "duration": 5.56
    },
    {
      "text": "3.3 Matt comes in the fourth position so",
      "start": 675.959,
      "duration": 5.641
    },
    {
      "text": "the positional embeddings are 4.1 4.2",
      "start": 679.079,
      "duration": 5.44
    },
    {
      "text": "and 4.3 again these numbers I'm just",
      "start": 681.6,
      "duration": 5.679
    },
    {
      "text": "showing for representative purpose now",
      "start": 684.519,
      "duration": 4.401
    },
    {
      "text": "if you add the token embedding to the",
      "start": 687.279,
      "duration": 3.68
    },
    {
      "text": "position embedding you'll see that",
      "start": 688.92,
      "duration": 3.64
    },
    {
      "text": "although the token embeddings are the",
      "start": 690.959,
      "duration": 3.961
    },
    {
      "text": "same for these words the final",
      "start": 692.56,
      "duration": 3.8
    },
    {
      "text": "embeddings which are also called as the",
      "start": 694.92,
      "duration": 3.2
    },
    {
      "text": "input embeddings which will be fed as",
      "start": 696.36,
      "duration": 4.24
    },
    {
      "text": "input to the llm they are different",
      "start": 698.12,
      "duration": 4.0
    },
    {
      "text": "because the positional embeddings which",
      "start": 700.6,
      "duration": 3.56
    },
    {
      "text": "are added to each token embedding is",
      "start": 702.12,
      "duration": 4.48
    },
    {
      "text": "different so for the word the the final",
      "start": 704.16,
      "duration": 5.96
    },
    {
      "text": "input embedding is 2.1 2.2 2.3 for the",
      "start": 706.6,
      "duration": 5.84
    },
    {
      "text": "word Cat the final input em embedding is",
      "start": 710.12,
      "duration": 6.24
    },
    {
      "text": "3.1 3.2 3.3 and similarly for ma the",
      "start": 712.44,
      "duration": 7.92
    },
    {
      "text": "final input embedding is 5.1 5.2 and 5.3",
      "start": 716.36,
      "duration": 6.719
    },
    {
      "text": "this is the intuitive idea behind",
      "start": 720.36,
      "duration": 5.719
    },
    {
      "text": "absolute positional embeddings we have",
      "start": 723.079,
      "duration": 5.041
    },
    {
      "text": "token embeddings and then we add",
      "start": 726.079,
      "duration": 4.0
    },
    {
      "text": "positional embeddings for different",
      "start": 728.12,
      "duration": 4.519
    },
    {
      "text": "positions so finally we get an input",
      "start": 730.079,
      "duration": 4.681
    },
    {
      "text": "embedding which encodes positional",
      "start": 732.639,
      "duration": 4.56
    },
    {
      "text": "information that's",
      "start": 734.76,
      "duration": 5.04
    },
    {
      "text": "it uh one more key thing to remember",
      "start": 737.199,
      "duration": 4.721
    },
    {
      "text": "here is that the positional vectors have",
      "start": 739.8,
      "duration": 4.76
    },
    {
      "text": "the same Dimension as the original token",
      "start": 741.92,
      "duration": 4.96
    },
    {
      "text": "embeddings can you think why this is the",
      "start": 744.56,
      "duration": 3.959
    },
    {
      "text": "case why should the positional",
      "start": 746.88,
      "duration": 3.48
    },
    {
      "text": "embeddings have the same Dimension as",
      "start": 748.519,
      "duration": 4.68
    },
    {
      "text": "the token embeddings pause for a moment",
      "start": 750.36,
      "duration": 4.52
    },
    {
      "text": "to think about",
      "start": 753.199,
      "duration": 4.281
    },
    {
      "text": "this okay so the reason the positional",
      "start": 754.88,
      "duration": 4.519
    },
    {
      "text": "embeddings should have the same",
      "start": 757.48,
      "duration": 3.359
    },
    {
      "text": "Dimension as the original token",
      "start": 759.399,
      "duration": 3.081
    },
    {
      "text": "embeddings is because we want to add",
      "start": 760.839,
      "duration": 3.8
    },
    {
      "text": "them together right and this addition",
      "start": 762.48,
      "duration": 3.799
    },
    {
      "text": "would be difficult if the dimension of",
      "start": 764.639,
      "duration": 3.601
    },
    {
      "text": "the positional embedding is different so",
      "start": 766.279,
      "duration": 3.641
    },
    {
      "text": "for example here if this positional",
      "start": 768.24,
      "duration": 3.76
    },
    {
      "text": "embedding was a four dimension thing",
      "start": 769.92,
      "duration": 3.96
    },
    {
      "text": "adding this four dimensional Vector to a",
      "start": 772.0,
      "duration": 3.48
    },
    {
      "text": "three-dimensional Vector would not be",
      "start": 773.88,
      "duration": 4.28
    },
    {
      "text": "possible so the in absolute positional",
      "start": 775.48,
      "duration": 4.719
    },
    {
      "text": "embedding the position vectors have the",
      "start": 778.16,
      "duration": 5.44
    },
    {
      "text": "same Dimension as the original token",
      "start": 780.199,
      "duration": 7.121
    },
    {
      "text": "embeddings uh so this is uh absolute",
      "start": 783.6,
      "duration": 6.359
    },
    {
      "text": "positional embedding the second type of",
      "start": 787.32,
      "duration": 4.16
    },
    {
      "text": "positional embedding is called as",
      "start": 789.959,
      "duration": 3.921
    },
    {
      "text": "relative positional embedding and this",
      "start": 791.48,
      "duration": 4.719
    },
    {
      "text": "is also a very interesting way of",
      "start": 793.88,
      "duration": 4.72
    },
    {
      "text": "encoding positional information so in",
      "start": 796.199,
      "duration": 5.281
    },
    {
      "text": "this type of embedding the emphasis is",
      "start": 798.6,
      "duration": 5.12
    },
    {
      "text": "on the relative position or the distance",
      "start": 801.48,
      "duration": 5.159
    },
    {
      "text": "between the tokens so the model",
      "start": 803.72,
      "duration": 5.44
    },
    {
      "text": "essentially learns the relationships in",
      "start": 806.639,
      "duration": 5.161
    },
    {
      "text": "terms of how far apart rather than at",
      "start": 809.16,
      "duration": 5.16
    },
    {
      "text": "which exact position so that's a bit",
      "start": 811.8,
      "duration": 4.399
    },
    {
      "text": "different than the absolute embedding",
      "start": 814.32,
      "duration": 3.879
    },
    {
      "text": "right for each position in absolute",
      "start": 816.199,
      "duration": 4.041
    },
    {
      "text": "embedding there is a separate positional",
      "start": 818.199,
      "duration": 4.64
    },
    {
      "text": "embedding but in relative positional",
      "start": 820.24,
      "duration": 5.76
    },
    {
      "text": "embedding what we care about is how far",
      "start": 822.839,
      "duration": 5.961
    },
    {
      "text": "apart different words are rather than",
      "start": 826.0,
      "duration": 4.16
    },
    {
      "text": "their exact",
      "start": 828.8,
      "duration": 3.76
    },
    {
      "text": "position now you must be thinking where",
      "start": 830.16,
      "duration": 5.2
    },
    {
      "text": "exactly is relative uh positional",
      "start": 832.56,
      "duration": 4.959
    },
    {
      "text": "encoding important so relative",
      "start": 835.36,
      "duration": 3.839
    },
    {
      "text": "positional encoding is actually very",
      "start": 837.519,
      "duration": 3.841
    },
    {
      "text": "important because the model in this case",
      "start": 839.199,
      "duration": 4.161
    },
    {
      "text": "can generalize better to sequence of",
      "start": 841.36,
      "duration": 5.56
    },
    {
      "text": "varing lens even if it has not seen such",
      "start": 843.36,
      "duration": 6.36
    },
    {
      "text": "lenss during training so for example in",
      "start": 846.92,
      "duration": 4.719
    },
    {
      "text": "in absolute positional in coding if you",
      "start": 849.72,
      "duration": 4.44
    },
    {
      "text": "train with a sequence length of five and",
      "start": 851.639,
      "duration": 4.56
    },
    {
      "text": "if in the test you have a sequence",
      "start": 854.16,
      "duration": 4.0
    },
    {
      "text": "length of six it's very difficult for",
      "start": 856.199,
      "duration": 3.721
    },
    {
      "text": "the absolute position encoding to know",
      "start": 858.16,
      "duration": 3.919
    },
    {
      "text": "what to do because it has trained for",
      "start": 859.92,
      "duration": 4.12
    },
    {
      "text": "five positions and five positional",
      "start": 862.079,
      "duration": 5.401
    },
    {
      "text": "embeddings but in this case of relative",
      "start": 864.04,
      "duration": 6.239
    },
    {
      "text": "positional embedding uh the model can",
      "start": 867.48,
      "duration": 4.76
    },
    {
      "text": "generalize better to sequence of varing",
      "start": 870.279,
      "duration": 4.401
    },
    {
      "text": "lens because even if it gets some random",
      "start": 872.24,
      "duration": 6.36
    },
    {
      "text": "length during uh the testing phase the",
      "start": 874.68,
      "duration": 5.519
    },
    {
      "text": "absolute position does not matter",
      "start": 878.6,
      "duration": 3.599
    },
    {
      "text": "anyways all that matters is the relative",
      "start": 880.199,
      "duration": 3.64
    },
    {
      "text": "position between different",
      "start": 882.199,
      "duration": 4.32
    },
    {
      "text": "words can you try to think of an example",
      "start": 883.839,
      "duration": 4.56
    },
    {
      "text": "where relative positioning might be more",
      "start": 886.519,
      "duration": 5.041
    },
    {
      "text": "important than absolute",
      "start": 888.399,
      "duration": 3.161
    },
    {
      "text": "positioning okay so it turns out that",
      "start": 895.12,
      "duration": 4.32
    },
    {
      "text": "relative positionings are actually",
      "start": 897.639,
      "duration": 4.32
    },
    {
      "text": "better if the sequences are very long",
      "start": 899.44,
      "duration": 4.92
    },
    {
      "text": "and if longer paragraphs or longer input",
      "start": 901.959,
      "duration": 4.521
    },
    {
      "text": "sequences need to be analyzed because",
      "start": 904.36,
      "duration": 3.839
    },
    {
      "text": "then we need to know the relationship",
      "start": 906.48,
      "duration": 3.12
    },
    {
      "text": "between how different words are",
      "start": 908.199,
      "duration": 4.921
    },
    {
      "text": "connected rather than the exact specific",
      "start": 909.6,
      "duration": 6.96
    },
    {
      "text": "position I'll come to the uh advantages",
      "start": 913.12,
      "duration": 6.56
    },
    {
      "text": "and disadvantages of both of these in a",
      "start": 916.56,
      "duration": 6.76
    },
    {
      "text": "moment okay so in the next uh in the",
      "start": 919.68,
      "duration": 5.279
    },
    {
      "text": "next section which is section number",
      "start": 923.32,
      "duration": 4.72
    },
    {
      "text": "five I want to discuss about uh just",
      "start": 924.959,
      "duration": 5.521
    },
    {
      "text": "these two types of of encodings and",
      "start": 928.04,
      "duration": 4.2
    },
    {
      "text": "which one to use in practice which one",
      "start": 930.48,
      "duration": 5.039
    },
    {
      "text": "does GPT uses Etc so both of these type",
      "start": 932.24,
      "duration": 4.8
    },
    {
      "text": "of positional embeddings which are",
      "start": 935.519,
      "duration": 3.68
    },
    {
      "text": "absolute embedding and Rel relative",
      "start": 937.04,
      "duration": 4.799
    },
    {
      "text": "positional embedding are very good",
      "start": 939.199,
      "duration": 5.88
    },
    {
      "text": "because they enable the llms let me uh",
      "start": 941.839,
      "duration": 6.161
    },
    {
      "text": "switch color they enable the large",
      "start": 945.079,
      "duration": 5.641
    },
    {
      "text": "language models to understand the order",
      "start": 948.0,
      "duration": 5.56
    },
    {
      "text": "and relationship between the tokens and",
      "start": 950.72,
      "duration": 5.359
    },
    {
      "text": "this actually ensures more accurate and",
      "start": 953.56,
      "duration": 5.04
    },
    {
      "text": "context aware predictions so whichever",
      "start": 956.079,
      "duration": 5.56
    },
    {
      "text": "position encoding you use it is actually",
      "start": 958.6,
      "duration": 4.64
    },
    {
      "text": "much better than not using any",
      "start": 961.639,
      "duration": 4.921
    },
    {
      "text": "positional encoding because uh it makes",
      "start": 963.24,
      "duration": 5.399
    },
    {
      "text": "the llm more aware of the order and",
      "start": 966.56,
      "duration": 3.92
    },
    {
      "text": "relationship between tokens and that",
      "start": 968.639,
      "duration": 3.961
    },
    {
      "text": "actually leads to better",
      "start": 970.48,
      "duration": 4.4
    },
    {
      "text": "predictions um now let's come to point",
      "start": 972.6,
      "duration": 5.08
    },
    {
      "text": "number six so the choice between the two",
      "start": 974.88,
      "duration": 6.84
    },
    {
      "text": "types of uh positional embedding really",
      "start": 977.68,
      "duration": 6.04
    },
    {
      "text": "depends on the specific application and",
      "start": 981.72,
      "duration": 4.4
    },
    {
      "text": "the nature of the data being processed",
      "start": 983.72,
      "duration": 5.72
    },
    {
      "text": "so for example generally uh abs abolute",
      "start": 986.12,
      "duration": 5.48
    },
    {
      "text": "positional encoding is preferred when",
      "start": 989.44,
      "duration": 4.839
    },
    {
      "text": "the fixed order of tokens is crucial",
      "start": 991.6,
      "duration": 5.64
    },
    {
      "text": "such as for sequence generation so GPT",
      "start": 994.279,
      "duration": 6.081
    },
    {
      "text": "was trained using an absolute positional",
      "start": 997.24,
      "duration": 5.32
    },
    {
      "text": "encoding and the original Transformer",
      "start": 1000.36,
      "duration": 4.2
    },
    {
      "text": "paper was also trained using absolute",
      "start": 1002.56,
      "duration": 3.16
    },
    {
      "text": "positional",
      "start": 1004.56,
      "duration": 3.68
    },
    {
      "text": "encoding relative positional encoding on",
      "start": 1005.72,
      "duration": 4.359
    },
    {
      "text": "the other hand is suitable for tasks",
      "start": 1008.24,
      "duration": 4.839
    },
    {
      "text": "like language modeling or long sequences",
      "start": 1010.079,
      "duration": 5.081
    },
    {
      "text": "where the same phrase can appear in",
      "start": 1013.079,
      "duration": 4.601
    },
    {
      "text": "different parts of the sequence so",
      "start": 1015.16,
      "duration": 4.88
    },
    {
      "text": "generally relative positional encoding",
      "start": 1017.68,
      "duration": 4.639
    },
    {
      "text": "is useful if you are analyzing long",
      "start": 1020.04,
      "duration": 4.24
    },
    {
      "text": "sequences and where the same phrase can",
      "start": 1022.319,
      "duration": 3.561
    },
    {
      "text": "repeat over and over",
      "start": 1024.28,
      "duration": 4.36
    },
    {
      "text": "again for all practical purposes I would",
      "start": 1025.88,
      "duration": 4.76
    },
    {
      "text": "say absolute positional encoding is the",
      "start": 1028.64,
      "duration": 4.84
    },
    {
      "text": "one which is used more commonly in fact",
      "start": 1030.64,
      "duration": 7.64
    },
    {
      "text": "as I mentioned uh open a GPT models so",
      "start": 1033.48,
      "duration": 8.76
    },
    {
      "text": "gpt3 GPT 4 Etc use absolute positional",
      "start": 1038.28,
      "duration": 6.36
    },
    {
      "text": "embeddings that are optimized during the",
      "start": 1042.24,
      "duration": 5.04
    },
    {
      "text": "training process so one thing to",
      "start": 1044.64,
      "duration": 4.919
    },
    {
      "text": "remember here is that similar to the",
      "start": 1047.28,
      "duration": 4.6
    },
    {
      "text": "embedding vectors we do not know what",
      "start": 1049.559,
      "duration": 4.401
    },
    {
      "text": "values of the positional embedding to be",
      "start": 1051.88,
      "duration": 4.48
    },
    {
      "text": "used so for example here I randomly",
      "start": 1053.96,
      "duration": 5.36
    },
    {
      "text": "showed these values 1.1 1.2 and 1.3",
      "start": 1056.36,
      "duration": 5.8
    },
    {
      "text": "right ideally as I mentioned in the",
      "start": 1059.32,
      "duration": 4.64
    },
    {
      "text": "previous lecture the vector embeddings",
      "start": 1062.16,
      "duration": 3.519
    },
    {
      "text": "need to be optimized right we need to",
      "start": 1063.96,
      "duration": 4.599
    },
    {
      "text": "know the weights for each Vector which",
      "start": 1065.679,
      "duration": 5.201
    },
    {
      "text": "what's the value for each Vector",
      "start": 1068.559,
      "duration": 6.161
    },
    {
      "text": "similarly uh when GPT was developed even",
      "start": 1070.88,
      "duration": 6.88
    },
    {
      "text": "the values of the positional embedding",
      "start": 1074.72,
      "duration": 5.4
    },
    {
      "text": "vectors need to be op optimized and",
      "start": 1077.76,
      "duration": 5.08
    },
    {
      "text": "these positional embedding vectors",
      "start": 1080.12,
      "duration": 4.4
    },
    {
      "text": "Vector values are actually optimized",
      "start": 1082.84,
      "duration": 3.52
    },
    {
      "text": "during the training process this",
      "start": 1084.52,
      "duration": 3.88
    },
    {
      "text": "optimization is actually a part of the",
      "start": 1086.36,
      "duration": 4.48
    },
    {
      "text": "training process itself so when you look",
      "start": 1088.4,
      "duration": 4.92
    },
    {
      "text": "at the model training for GPT we also",
      "start": 1090.84,
      "duration": 4.76
    },
    {
      "text": "have to optimize for the token",
      "start": 1093.32,
      "duration": 4.32
    },
    {
      "text": "embeddings and we also have to optimize",
      "start": 1095.6,
      "duration": 3.88
    },
    {
      "text": "for the positional embeddings we do not",
      "start": 1097.64,
      "duration": 4.56
    },
    {
      "text": "know about these values",
      "start": 1099.48,
      "duration": 5.319
    },
    {
      "text": "before uh remember I mentioned that",
      "start": 1102.2,
      "duration": 4.959
    },
    {
      "text": "along with GPT the original Transformer",
      "start": 1104.799,
      "duration": 4.641
    },
    {
      "text": "paper which is called as attention is",
      "start": 1107.159,
      "duration": 4.041
    },
    {
      "text": "all you need so let me show you this",
      "start": 1109.44,
      "duration": 4.479
    },
    {
      "text": "paper yeah this paper so they also used",
      "start": 1111.2,
      "duration": 5.32
    },
    {
      "text": "an absolute positional encoding and in",
      "start": 1113.919,
      "duration": 4.321
    },
    {
      "text": "this case they actually propos some",
      "start": 1116.52,
      "duration": 4.08
    },
    {
      "text": "formula for how to encode the different",
      "start": 1118.24,
      "duration": 4.679
    },
    {
      "text": "positions so they use sinusoidal and",
      "start": 1120.6,
      "duration": 6.16
    },
    {
      "text": "cosine formula over here uh so you can",
      "start": 1122.919,
      "duration": 5.481
    },
    {
      "text": "read a bit about what they have written",
      "start": 1126.76,
      "duration": 3.68
    },
    {
      "text": "here since our model contains no",
      "start": 1128.4,
      "duration": 4.24
    },
    {
      "text": "recurrence in order for the model to",
      "start": 1130.44,
      "duration": 4.599
    },
    {
      "text": "make use of the order of the sequence we",
      "start": 1132.64,
      "duration": 4.32
    },
    {
      "text": "must inject some information about the",
      "start": 1135.039,
      "duration": 3.76
    },
    {
      "text": "relative or absolute position of the to",
      "start": 1136.96,
      "duration": 4.199
    },
    {
      "text": "tokens so they have added absolute",
      "start": 1138.799,
      "duration": 3.841
    },
    {
      "text": "oppositional embedding and they have",
      "start": 1141.159,
      "duration": 4.921
    },
    {
      "text": "used some formula for how to actually",
      "start": 1142.64,
      "duration": 5.76
    },
    {
      "text": "calculate the positional embedding for",
      "start": 1146.08,
      "duration": 3.079
    },
    {
      "text": "each",
      "start": 1148.4,
      "duration": 3.92
    },
    {
      "text": "token uh on the contrary when GPT was",
      "start": 1149.159,
      "duration": 7.081
    },
    {
      "text": "trained no such formula was used and uh",
      "start": 1152.32,
      "duration": 6.719
    },
    {
      "text": "the positional embedding Vector values",
      "start": 1156.24,
      "duration": 4.72
    },
    {
      "text": "were actually optimized during the",
      "start": 1159.039,
      "duration": 5.041
    },
    {
      "text": "training process itself I hope everyone",
      "start": 1160.96,
      "duration": 5.76
    },
    {
      "text": "is with me until this point because now",
      "start": 1164.08,
      "duration": 5.04
    },
    {
      "text": "we are going to jump into a handson",
      "start": 1166.72,
      "duration": 7.079
    },
    {
      "text": "demonstration of uh looking at a very uh",
      "start": 1169.12,
      "duration": 8.52
    },
    {
      "text": "real life example such as gpt2 looking",
      "start": 1173.799,
      "duration": 6.081
    },
    {
      "text": "at the vocabulary transforming that",
      "start": 1177.64,
      "duration": 4.919
    },
    {
      "text": "vocabulary into token embeddings adding",
      "start": 1179.88,
      "duration": 4.32
    },
    {
      "text": "positional embeddings to those token",
      "start": 1182.559,
      "duration": 4.0
    },
    {
      "text": "embeddings and then generating the input",
      "start": 1184.2,
      "duration": 4.56
    },
    {
      "text": "embeddings which are the final input to",
      "start": 1186.559,
      "duration": 4.441
    },
    {
      "text": "the llm training so you might be",
      "start": 1188.76,
      "duration": 4.0
    },
    {
      "text": "noticing that I'm using the words",
      "start": 1191.0,
      "duration": 4.36
    },
    {
      "text": "encoding and embedding interchangeably",
      "start": 1192.76,
      "duration": 4.76
    },
    {
      "text": "so sometimes positional encodings are",
      "start": 1195.36,
      "duration": 4.36
    },
    {
      "text": "also called positional embed edings so",
      "start": 1197.52,
      "duration": 4.519
    },
    {
      "text": "these words are used interchangeably all",
      "start": 1199.72,
      "duration": 3.839
    },
    {
      "text": "you need to remember is that when",
      "start": 1202.039,
      "duration": 3.441
    },
    {
      "text": "someone says encoding or embedding it's",
      "start": 1203.559,
      "duration": 5.12
    },
    {
      "text": "usually a vector in higher dimensional",
      "start": 1205.48,
      "duration": 6.12
    },
    {
      "text": "space okay so now let's come to the next",
      "start": 1208.679,
      "duration": 5.12
    },
    {
      "text": "part in the next part what we are going",
      "start": 1211.6,
      "duration": 3.959
    },
    {
      "text": "to do is we are going to implement",
      "start": 1213.799,
      "duration": 3.481
    },
    {
      "text": "positional embeddings in a Hands-On",
      "start": 1215.559,
      "duration": 4.041
    },
    {
      "text": "manner so I'm going to take you to the",
      "start": 1217.28,
      "duration": 4.6
    },
    {
      "text": "Jupiter notebook now very similar to how",
      "start": 1219.6,
      "duration": 5.04
    },
    {
      "text": "we have been doing in the previous",
      "start": 1221.88,
      "duration": 5.84
    },
    {
      "text": "lectures okay so previously uh",
      "start": 1224.64,
      "duration": 4.56
    },
    {
      "text": "especially in the last lecture we",
      "start": 1227.72,
      "duration": 3.76
    },
    {
      "text": "focused on very small embedding sizes in",
      "start": 1229.2,
      "duration": 4.719
    },
    {
      "text": "this chapter for illustration purposes",
      "start": 1231.48,
      "duration": 5.0
    },
    {
      "text": "now we are going to consider much more",
      "start": 1233.919,
      "duration": 5.361
    },
    {
      "text": "realistic and useful embedding sizes and",
      "start": 1236.48,
      "duration": 5.52
    },
    {
      "text": "encode the input tokens into a 256",
      "start": 1239.28,
      "duration": 5.12
    },
    {
      "text": "dimensional Vector space so remember I",
      "start": 1242.0,
      "duration": 4.6
    },
    {
      "text": "mentioned that in token embedding you",
      "start": 1244.4,
      "duration": 3.96
    },
    {
      "text": "have every word and that word is",
      "start": 1246.6,
      "duration": 3.68
    },
    {
      "text": "projected into higher dimensional Vector",
      "start": 1248.36,
      "duration": 4.28
    },
    {
      "text": "space usually that Dimension is very",
      "start": 1250.28,
      "duration": 5.639
    },
    {
      "text": "high so gpt3 I think was trained on a",
      "start": 1252.64,
      "duration": 6.399
    },
    {
      "text": "vector space of around 256 or even more",
      "start": 1255.919,
      "duration": 5.041
    },
    {
      "text": "Dimensions uh so we are going to",
      "start": 1259.039,
      "duration": 3.801
    },
    {
      "text": "consider a vector space of that size",
      "start": 1260.96,
      "duration": 3.76
    },
    {
      "text": "right now for",
      "start": 1262.84,
      "duration": 4.079
    },
    {
      "text": "demonstration uh as is written here this",
      "start": 1264.72,
      "duration": 4.48
    },
    {
      "text": "is smaller than what the original gpt3",
      "start": 1266.919,
      "duration": 5.601
    },
    {
      "text": "model used so in gpt3 the embedding size",
      "start": 1269.2,
      "duration": 6.88
    },
    {
      "text": "is actually 1 12288 Dimensions so it's",
      "start": 1272.52,
      "duration": 6.24
    },
    {
      "text": "nowhere close to 256 but it's fine let",
      "start": 1276.08,
      "duration": 6.0
    },
    {
      "text": "me actually ask G uh Chad GPT what is",
      "start": 1278.76,
      "duration": 7.64
    },
    {
      "text": "the um Vector embedding size for gpt2",
      "start": 1282.08,
      "duration": 6.839
    },
    {
      "text": "for gpt2 for one of their smallest model",
      "start": 1286.4,
      "duration": 5.72
    },
    {
      "text": "I think it is around 256 so let me ask",
      "start": 1288.919,
      "duration": 6.0
    },
    {
      "text": "chat GPT what is",
      "start": 1292.12,
      "duration": 9.24
    },
    {
      "text": "the vector embedding size for",
      "start": 1294.919,
      "duration": 6.441
    },
    {
      "text": "gpt2 so if I ask this to chat GPT you",
      "start": 1301.4,
      "duration": 6.0
    },
    {
      "text": "will see that uh uh for the for all",
      "start": 1304.2,
      "duration": 5.56
    },
    {
      "text": "their models it actually starts from 768",
      "start": 1307.4,
      "duration": 4.36
    },
    {
      "text": "so the value of 256 we are using is",
      "start": 1309.76,
      "duration": 4.08
    },
    {
      "text": "three times smaller but it's fine it's",
      "start": 1311.76,
      "duration": 4.84
    },
    {
      "text": "at least the same order of magnitude so",
      "start": 1313.84,
      "duration": 4.719
    },
    {
      "text": "okay we are going to encode the input",
      "start": 1316.6,
      "duration": 4.52
    },
    {
      "text": "tokens into 256 dimensional Vector",
      "start": 1318.559,
      "duration": 5.201
    },
    {
      "text": "representation and then we are going to",
      "start": 1321.12,
      "duration": 4.6
    },
    {
      "text": "assume that the token IDs were created",
      "start": 1323.76,
      "duration": 4.84
    },
    {
      "text": "by the bite pair encoder tokenizer which",
      "start": 1325.72,
      "duration": 4.72
    },
    {
      "text": "has a vocabulary size of",
      "start": 1328.6,
      "duration": 5.64
    },
    {
      "text": "50257 let's check this",
      "start": 1330.44,
      "duration": 8.96
    },
    {
      "text": "uh what's the",
      "start": 1334.24,
      "duration": 5.16
    },
    {
      "text": "vocabulary size of",
      "start": 1339.559,
      "duration": 6.72
    },
    {
      "text": "gpt2 pre trining so let's see the",
      "start": 1342.679,
      "duration": 6.48
    },
    {
      "text": "vocabulary size is 502 25 7 using the",
      "start": 1346.279,
      "duration": 5.361
    },
    {
      "text": "bite pair encoder this is exactly what",
      "start": 1349.159,
      "duration": 4.321
    },
    {
      "text": "we are going to use right now in today's",
      "start": 1351.64,
      "duration": 4.56
    },
    {
      "text": "Hands-On example so as I mentioned",
      "start": 1353.48,
      "duration": 5.199
    },
    {
      "text": "before to actually create the embedding",
      "start": 1356.2,
      "duration": 5.88
    },
    {
      "text": "layer uh or this is also called as the",
      "start": 1358.679,
      "duration": 6.36
    },
    {
      "text": "uh token embedding Matrix you need two",
      "start": 1362.08,
      "duration": 5.599
    },
    {
      "text": "you need two quantities or two variables",
      "start": 1365.039,
      "duration": 4.801
    },
    {
      "text": "the first you need the vector Dimension",
      "start": 1367.679,
      "duration": 4.281
    },
    {
      "text": "so basically every token ID in the",
      "start": 1369.84,
      "duration": 4.12
    },
    {
      "text": "vocabulary will be converted to a vector",
      "start": 1371.96,
      "duration": 4.28
    },
    {
      "text": "of these many dimensions and right now",
      "start": 1373.96,
      "duration": 7.16
    },
    {
      "text": "we are going to use uh 25 6 over",
      "start": 1376.24,
      "duration": 4.88
    },
    {
      "text": "here uh",
      "start": 1381.6,
      "duration": 5.319
    },
    {
      "text": "256 and the second thing which we need",
      "start": 1384.44,
      "duration": 4.56
    },
    {
      "text": "is the vocabulary size and the",
      "start": 1386.919,
      "duration": 4.36
    },
    {
      "text": "vocabulary size in our case is as I've",
      "start": 1389.0,
      "duration": 4.12
    },
    {
      "text": "already mentioned here it's",
      "start": 1391.279,
      "duration": 4.961
    },
    {
      "text": "50257 this means that there are 50257",
      "start": 1393.12,
      "duration": 5.72
    },
    {
      "text": "token IDs and each of these token IDs",
      "start": 1396.24,
      "duration": 5.039
    },
    {
      "text": "will be transformed into a 256",
      "start": 1398.84,
      "duration": 6.24
    },
    {
      "text": "dimensional Vector awesome so now what",
      "start": 1401.279,
      "duration": 5.52
    },
    {
      "text": "we are going to do is that we have",
      "start": 1405.08,
      "duration": 3.92
    },
    {
      "text": "defined the vocabulary size to be",
      "start": 1406.799,
      "duration": 4.441
    },
    {
      "text": "0257 we have defined the output",
      "start": 1409.0,
      "duration": 4.32
    },
    {
      "text": "Dimension which is the vector size as",
      "start": 1411.24,
      "duration": 4.4
    },
    {
      "text": "256 and then we are going to create a",
      "start": 1413.32,
      "duration": 5.0
    },
    {
      "text": "token embedding layer using torch. nn.",
      "start": 1415.64,
      "duration": 5.519
    },
    {
      "text": "embedding so torch. nn. embedding",
      "start": 1418.32,
      "duration": 4.56
    },
    {
      "text": "actually creates this kind of an",
      "start": 1421.159,
      "duration": 4.0
    },
    {
      "text": "embedding layer provide provided we have",
      "start": 1422.88,
      "duration": 5.2
    },
    {
      "text": "those two inputs which is the vocabulary",
      "start": 1425.159,
      "duration": 5.361
    },
    {
      "text": "size and second is the vector length",
      "start": 1428.08,
      "duration": 5.24
    },
    {
      "text": "which we want so I I'll put the link of",
      "start": 1430.52,
      "duration": 5.48
    },
    {
      "text": "this in the chat or in the information",
      "start": 1433.32,
      "duration": 5.12
    },
    {
      "text": "section of the YouTube video as well so",
      "start": 1436.0,
      "duration": 3.84
    },
    {
      "text": "when you run this you will see that the",
      "start": 1438.44,
      "duration": 3.4
    },
    {
      "text": "token embedding layer has been created",
      "start": 1439.84,
      "duration": 3.839
    },
    {
      "text": "and it takes two inputs the vocabulary",
      "start": 1441.84,
      "duration": 4.4
    },
    {
      "text": "size and the output Dimension",
      "start": 1443.679,
      "duration": 5.161
    },
    {
      "text": "awesome so now once the token embedding",
      "start": 1446.24,
      "duration": 5.88
    },
    {
      "text": "layer is created we need to uh remember",
      "start": 1448.84,
      "duration": 5.28
    },
    {
      "text": "the token embedding layer is is",
      "start": 1452.12,
      "duration": 5.039
    },
    {
      "text": "essentially a lookup Matrix where if you",
      "start": 1454.12,
      "duration": 7.36
    },
    {
      "text": "give it uh the IDS so if you have a",
      "start": 1457.159,
      "duration": 6.201
    },
    {
      "text": "token embedding layer and if you provide",
      "start": 1461.48,
      "duration": 3.96
    },
    {
      "text": "the token embedding layer with input IDs",
      "start": 1463.36,
      "duration": 4.28
    },
    {
      "text": "which you want to look look for it will",
      "start": 1465.44,
      "duration": 4.92
    },
    {
      "text": "give you the responding embedding Vector",
      "start": 1467.64,
      "duration": 5.159
    },
    {
      "text": "so for example uh this is the token",
      "start": 1470.36,
      "duration": 4.76
    },
    {
      "text": "embedding Matrix right if you and this",
      "start": 1472.799,
      "duration": 4.36
    },
    {
      "text": "is a lookup table why is it a lookup",
      "start": 1475.12,
      "duration": 3.76
    },
    {
      "text": "table because if you pass in the input",
      "start": 1477.159,
      "duration": 4.4
    },
    {
      "text": "ID it looks up that particular row and",
      "start": 1478.88,
      "duration": 5.24
    },
    {
      "text": "it gives you the vector associated with",
      "start": 1481.559,
      "duration": 5.081
    },
    {
      "text": "that so now we need to create the input",
      "start": 1484.12,
      "duration": 6.32
    },
    {
      "text": "IDs so that we can generate the vector",
      "start": 1486.64,
      "duration": 6.0
    },
    {
      "text": "embeddings or the token embeddings for",
      "start": 1490.44,
      "duration": 5.16
    },
    {
      "text": "those inputs and to create the input IDs",
      "start": 1492.64,
      "duration": 4.56
    },
    {
      "text": "we are going to use something called as",
      "start": 1495.6,
      "duration": 4.28
    },
    {
      "text": "data loader so we looked at data loader",
      "start": 1497.2,
      "duration": 4.959
    },
    {
      "text": "in the in one of the previous lectures",
      "start": 1499.88,
      "duration": 4.679
    },
    {
      "text": "so what we are going to do here is that",
      "start": 1502.159,
      "duration": 4.4
    },
    {
      "text": "uh we are going to have a batch and that",
      "start": 1504.559,
      "duration": 4.48
    },
    {
      "text": "batch size will be equal to 8 and we are",
      "start": 1506.559,
      "duration": 4.881
    },
    {
      "text": "going to use a context size of four",
      "start": 1509.039,
      "duration": 4.12
    },
    {
      "text": "which means that the maximum input",
      "start": 1511.44,
      "duration": 4.08
    },
    {
      "text": "length is four which means four tokens",
      "start": 1513.159,
      "duration": 4.64
    },
    {
      "text": "at a time can be passed as inputs that's",
      "start": 1515.52,
      "duration": 4.279
    },
    {
      "text": "also called as the context",
      "start": 1517.799,
      "duration": 5.441
    },
    {
      "text": "size so uh let me actually show this to",
      "start": 1519.799,
      "duration": 6.24
    },
    {
      "text": "you in pictorial format so that it's",
      "start": 1523.24,
      "duration": 5.48
    },
    {
      "text": "easier for you to visualize okay okay so",
      "start": 1526.039,
      "duration": 4.681
    },
    {
      "text": "this is the input which we are going to",
      "start": 1528.72,
      "duration": 4.36
    },
    {
      "text": "create uh we are going to divide the",
      "start": 1530.72,
      "duration": 4.6
    },
    {
      "text": "data into batches and the batch size",
      "start": 1533.08,
      "duration": 4.719
    },
    {
      "text": "will be equal to 8 so the parameters",
      "start": 1535.32,
      "duration": 5.359
    },
    {
      "text": "will be updated after processing every",
      "start": 1537.799,
      "duration": 4.6
    },
    {
      "text": "eight batches right now we are not",
      "start": 1540.679,
      "duration": 3.681
    },
    {
      "text": "looking at parameter updation at all I",
      "start": 1542.399,
      "duration": 4.241
    },
    {
      "text": "just want you to be aware of the",
      "start": 1544.36,
      "duration": 4.64
    },
    {
      "text": "dimensions so the input which we will be",
      "start": 1546.64,
      "duration": 4.84
    },
    {
      "text": "looking at one time is a batch of eight",
      "start": 1549.0,
      "duration": 4.84
    },
    {
      "text": "so there are eight rows over here and",
      "start": 1551.48,
      "duration": 5.36
    },
    {
      "text": "each row corresponds to one input",
      "start": 1553.84,
      "duration": 5.559
    },
    {
      "text": "sequence so if you look at the first row",
      "start": 1556.84,
      "duration": 4.64
    },
    {
      "text": "these will be four tokens if you look at",
      "start": 1559.399,
      "duration": 4.16
    },
    {
      "text": "the second row these will be four tokens",
      "start": 1561.48,
      "duration": 4.28
    },
    {
      "text": "so the first row when I say four tokens",
      "start": 1563.559,
      "duration": 5.081
    },
    {
      "text": "these are four token IDs so remember the",
      "start": 1565.76,
      "duration": 5.12
    },
    {
      "text": "goal is to look at these four token IDs",
      "start": 1568.64,
      "duration": 5.32
    },
    {
      "text": "and to predict the next word so you can",
      "start": 1570.88,
      "duration": 6.2
    },
    {
      "text": "think of each row as an input to the llm",
      "start": 1573.96,
      "duration": 6.439
    },
    {
      "text": "and each row consists of four token IDs",
      "start": 1577.08,
      "duration": 6.04
    },
    {
      "text": "now our goal is to transform each of",
      "start": 1580.399,
      "duration": 7.081
    },
    {
      "text": "these token IDs into a uh 256",
      "start": 1583.12,
      "duration": 5.679
    },
    {
      "text": "dimensional vector",
      "start": 1587.48,
      "duration": 3.799
    },
    {
      "text": "right so if you look at the first row",
      "start": 1588.799,
      "duration": 6.161
    },
    {
      "text": "the first row has uh four token IDs and",
      "start": 1591.279,
      "duration": 6.081
    },
    {
      "text": "we want to transform each into a 256",
      "start": 1594.96,
      "duration": 3.839
    },
    {
      "text": "dimensional",
      "start": 1597.36,
      "duration": 4.039
    },
    {
      "text": "Vector so before coming to that let me",
      "start": 1598.799,
      "duration": 4.961
    },
    {
      "text": "show you how the inputs are initialized",
      "start": 1601.399,
      "duration": 5.321
    },
    {
      "text": "and how from the data loader so you so I",
      "start": 1603.76,
      "duration": 4.76
    },
    {
      "text": "have already defined a data loader in",
      "start": 1606.72,
      "duration": 3.559
    },
    {
      "text": "this jupyter notebook and you will have",
      "start": 1608.52,
      "duration": 3.32
    },
    {
      "text": "access to that when I share the code",
      "start": 1610.279,
      "duration": 3.841
    },
    {
      "text": "file with you if you have not seen the",
      "start": 1611.84,
      "duration": 4.28
    },
    {
      "text": "data loader lecture I highly encourage",
      "start": 1614.12,
      "duration": 4.2
    },
    {
      "text": "you to go to that but if not it's fine",
      "start": 1616.12,
      "duration": 3.72
    },
    {
      "text": "I'll try to give you an intuition of",
      "start": 1618.32,
      "duration": 3.479
    },
    {
      "text": "what's Happening Here essentially what",
      "start": 1619.84,
      "duration": 3.559
    },
    {
      "text": "we are doing here is that we are looking",
      "start": 1621.799,
      "duration": 4.321
    },
    {
      "text": "at the raw text and this raw text is",
      "start": 1623.399,
      "duration": 4.801
    },
    {
      "text": "actually the",
      "start": 1626.12,
      "duration": 4.679
    },
    {
      "text": "verdict so this is a book called The",
      "start": 1628.2,
      "duration": 6.0
    },
    {
      "text": "Verdict and uh let me actually refresh",
      "start": 1630.799,
      "duration": 5.0
    },
    {
      "text": "this so that it appears in a better",
      "start": 1634.2,
      "duration": 3.56
    },
    {
      "text": "format yeah so this is actually a book",
      "start": 1635.799,
      "duration": 3.801
    },
    {
      "text": "called The Verdict and this is the main",
      "start": 1637.76,
      "duration": 4.24
    },
    {
      "text": "text which we are using as sample text",
      "start": 1639.6,
      "duration": 5.799
    },
    {
      "text": "for uh for these set of lectures so",
      "start": 1642.0,
      "duration": 4.919
    },
    {
      "text": "what's happening is that we are taking",
      "start": 1645.399,
      "duration": 4.4
    },
    {
      "text": "this raw text and uh we are chunking it",
      "start": 1646.919,
      "duration": 5.321
    },
    {
      "text": "into batches so each each batch is of",
      "start": 1649.799,
      "duration": 7.201
    },
    {
      "text": "size eight and uh for each batch we are",
      "start": 1652.24,
      "duration": 6.84
    },
    {
      "text": "looking at a max length of four so only",
      "start": 1657.0,
      "duration": 4.24
    },
    {
      "text": "four input tokens will be used to",
      "start": 1659.08,
      "duration": 4.599
    },
    {
      "text": "predict the next World so think of this",
      "start": 1661.24,
      "duration": 4.4
    },
    {
      "text": "Matrix which I showed you over",
      "start": 1663.679,
      "duration": 5.041
    },
    {
      "text": "here so we are going to create a data",
      "start": 1665.64,
      "duration": 4.96
    },
    {
      "text": "loader which takes in the Raw text which",
      "start": 1668.72,
      "duration": 4.24
    },
    {
      "text": "is a batch size of eight which is a max",
      "start": 1670.6,
      "duration": 3.799
    },
    {
      "text": "length which are the number of",
      "start": 1672.96,
      "duration": 3.88
    },
    {
      "text": "essentially columns which is the context",
      "start": 1674.399,
      "duration": 4.441
    },
    {
      "text": "length which is equal to four",
      "start": 1676.84,
      "duration": 4.319
    },
    {
      "text": "and then a stride of four and Shuffle",
      "start": 1678.84,
      "duration": 4.959
    },
    {
      "text": "equal to false so stride basically means",
      "start": 1681.159,
      "duration": 4.921
    },
    {
      "text": "that let's say if you look at this text",
      "start": 1683.799,
      "duration": 5.961
    },
    {
      "text": "right and if you want to uh create",
      "start": 1686.08,
      "duration": 6.319
    },
    {
      "text": "inputs so the first input will be I had",
      "start": 1689.76,
      "duration": 5.0
    },
    {
      "text": "always thought because the context size",
      "start": 1692.399,
      "duration": 4.841
    },
    {
      "text": "is four now since stride is equal to",
      "start": 1694.76,
      "duration": 4.84
    },
    {
      "text": "four the next input will be Jack gisburn",
      "start": 1697.24,
      "duration": 5.36
    },
    {
      "text": "rather a if the stride was equal to one",
      "start": 1699.6,
      "duration": 4.76
    },
    {
      "text": "the next input would be had always",
      "start": 1702.6,
      "duration": 4.199
    },
    {
      "text": "thought Jack but now the stride is equal",
      "start": 1704.36,
      "duration": 4.559
    },
    {
      "text": "to four so after one in input we'll Skip",
      "start": 1706.799,
      "duration": 4.36
    },
    {
      "text": "One 2 3 four times and then give the",
      "start": 1708.919,
      "duration": 4.681
    },
    {
      "text": "next input so what we are essentially",
      "start": 1711.159,
      "duration": 5.0
    },
    {
      "text": "doing is creating inputs so the batch",
      "start": 1713.6,
      "duration": 4.16
    },
    {
      "text": "size is eight right so the first batch",
      "start": 1716.159,
      "duration": 4.4
    },
    {
      "text": "will have this as the first row of input",
      "start": 1717.76,
      "duration": 4.84
    },
    {
      "text": "this as the second row of input this as",
      "start": 1720.559,
      "duration": 4.0
    },
    {
      "text": "the third row of input and so on up till",
      "start": 1722.6,
      "duration": 4.76
    },
    {
      "text": "eight eight batches and each of these",
      "start": 1724.559,
      "duration": 6.0
    },
    {
      "text": "tokens uh are converted into token",
      "start": 1727.36,
      "duration": 5.88
    },
    {
      "text": "IDs and then what we want to do is map",
      "start": 1730.559,
      "duration": 6.081
    },
    {
      "text": "each of those token IDs into vectors",
      "start": 1733.24,
      "duration": 5.08
    },
    {
      "text": "that's what we are doing exactly through",
      "start": 1736.64,
      "duration": 4.2
    },
    {
      "text": "this data loader data loader just helps",
      "start": 1738.32,
      "duration": 6.479
    },
    {
      "text": "us to uh manage the task of inputting",
      "start": 1740.84,
      "duration": 6.319
    },
    {
      "text": "the data batching the data creating",
      "start": 1744.799,
      "duration": 4.24
    },
    {
      "text": "different batches parallel processing",
      "start": 1747.159,
      "duration": 4.52
    },
    {
      "text": "much easier so it's highly recommended",
      "start": 1749.039,
      "duration": 4.841
    },
    {
      "text": "to use data loader I'll actually just",
      "start": 1751.679,
      "duration": 5.72
    },
    {
      "text": "show you the data loader so it's this",
      "start": 1753.88,
      "duration": 6.0
    },
    {
      "text": "link data sets and data loaders in",
      "start": 1757.399,
      "duration": 5.081
    },
    {
      "text": "Python uh it's actually highly useful",
      "start": 1759.88,
      "duration": 4.88
    },
    {
      "text": "when dealing with large language",
      "start": 1762.48,
      "duration": 4.96
    },
    {
      "text": "models okay so once we Define a data",
      "start": 1764.76,
      "duration": 4.6
    },
    {
      "text": "loader like this we just iterate through",
      "start": 1767.44,
      "duration": 3.88
    },
    {
      "text": "the data loader and we can get inputs",
      "start": 1769.36,
      "duration": 5.039
    },
    {
      "text": "and targets so if you print out the",
      "start": 1771.32,
      "duration": 5.44
    },
    {
      "text": "token IDs so this is the input of a",
      "start": 1774.399,
      "duration": 4.441
    },
    {
      "text": "batch and if you see this exactly",
      "start": 1776.76,
      "duration": 3.72
    },
    {
      "text": "similar to what we had written on the",
      "start": 1778.84,
      "duration": 4.319
    },
    {
      "text": "Whiteboard so if you look at one batch",
      "start": 1780.48,
      "duration": 4.079
    },
    {
      "text": "it will have",
      "start": 1783.159,
      "duration": 5.081
    },
    {
      "text": "uh uh eight input sequences and each",
      "start": 1784.559,
      "duration": 6.041
    },
    {
      "text": "input sequence has four token IDs or",
      "start": 1788.24,
      "duration": 5.279
    },
    {
      "text": "four tokens and using these we want to",
      "start": 1790.6,
      "duration": 5.439
    },
    {
      "text": "predict the next word for each input",
      "start": 1793.519,
      "duration": 5.16
    },
    {
      "text": "sequence so this is the batch of inputs",
      "start": 1796.039,
      "duration": 4.801
    },
    {
      "text": "which we have received and now what we",
      "start": 1798.679,
      "duration": 4.681
    },
    {
      "text": "actually want to do is for each of these",
      "start": 1800.84,
      "duration": 4.8
    },
    {
      "text": "input token IDs we want to convert each",
      "start": 1803.36,
      "duration": 6.28
    },
    {
      "text": "of these into a 256 uh dimensional",
      "start": 1805.64,
      "duration": 5.36
    },
    {
      "text": "Vector",
      "start": 1809.64,
      "duration": 3.879
    },
    {
      "text": "representation but first let's look at",
      "start": 1811.0,
      "duration": 5.84
    },
    {
      "text": "this token ID tensor and it's a 8x4",
      "start": 1813.519,
      "duration": 5.04
    },
    {
      "text": "tensor because it has eight rows and",
      "start": 1816.84,
      "duration": 4.36
    },
    {
      "text": "four columns uh the data badge consists",
      "start": 1818.559,
      "duration": 4.96
    },
    {
      "text": "of eight text samples with four tokens",
      "start": 1821.2,
      "duration": 5.079
    },
    {
      "text": "each awesome now what we are going to do",
      "start": 1823.519,
      "duration": 5.081
    },
    {
      "text": "is that we are going to uh convert each",
      "start": 1826.279,
      "duration": 5.88
    },
    {
      "text": "of these token IDs into a 256",
      "start": 1828.6,
      "duration": 6.16
    },
    {
      "text": "dimensional uh Vector using this",
      "start": 1832.159,
      "duration": 3.721
    },
    {
      "text": "embedding",
      "start": 1834.76,
      "duration": 3.519
    },
    {
      "text": "layer so as I told you what this",
      "start": 1835.88,
      "duration": 4.639
    },
    {
      "text": "embedding layer is is it's actually a",
      "start": 1838.279,
      "duration": 4.961
    },
    {
      "text": "lookup table right so if you look at",
      "start": 1840.519,
      "duration": 5.28
    },
    {
      "text": "this embedding layer and if you give the",
      "start": 1843.24,
      "duration": 4.96
    },
    {
      "text": "token ID it will generate the or it will",
      "start": 1845.799,
      "duration": 4.0
    },
    {
      "text": "fetch the corresponding Vector",
      "start": 1848.2,
      "duration": 3.76
    },
    {
      "text": "representation for you that's exactly",
      "start": 1849.799,
      "duration": 4.841
    },
    {
      "text": "what we are going to do over here so",
      "start": 1851.96,
      "duration": 4.36
    },
    {
      "text": "these are the inputs right now let's say",
      "start": 1854.64,
      "duration": 4.399
    },
    {
      "text": "I'm looking at one batch uh which has",
      "start": 1856.32,
      "duration": 5.28
    },
    {
      "text": "eight rows and which has four columns",
      "start": 1859.039,
      "duration": 4.401
    },
    {
      "text": "and I have given the input IDs as",
      "start": 1861.6,
      "duration": 4.16
    },
    {
      "text": "randomly assigned over here right",
      "start": 1863.44,
      "duration": 4.719
    },
    {
      "text": "now but let's say these are the input",
      "start": 1865.76,
      "duration": 4.799
    },
    {
      "text": "IDs so for the first first batch the",
      "start": 1868.159,
      "duration": 7.76
    },
    {
      "text": "input IDs are 10 8 uh 20 and uh",
      "start": 1870.559,
      "duration": 8.761
    },
    {
      "text": "21 now what is done is that these input",
      "start": 1875.919,
      "duration": 6.6
    },
    {
      "text": "IDs are then mapped to the embedding",
      "start": 1879.32,
      "duration": 5.199
    },
    {
      "text": "Vector Matrix so we we know the",
      "start": 1882.519,
      "duration": 4.0
    },
    {
      "text": "corresponding row so if you look at the",
      "start": 1884.519,
      "duration": 5.081
    },
    {
      "text": "first row it's 10 8 10 and 20 these are",
      "start": 1886.519,
      "duration": 4.921
    },
    {
      "text": "the input IDs right so then here you",
      "start": 1889.6,
      "duration": 5.28
    },
    {
      "text": "look for the input ID of 10 and then if",
      "start": 1891.44,
      "duration": 5.239
    },
    {
      "text": "you find then you find the corresponding",
      "start": 1894.88,
      "duration": 4.08
    },
    {
      "text": "Vector for it then you look for the",
      "start": 1896.679,
      "duration": 4.72
    },
    {
      "text": "input ID of8 and then you find the",
      "start": 1898.96,
      "duration": 5.079
    },
    {
      "text": "corresponding Vector for it then you",
      "start": 1901.399,
      "duration": 4.601
    },
    {
      "text": "look at the input ID of",
      "start": 1904.039,
      "duration": 4.52
    },
    {
      "text": "20 and then you find the corresponding",
      "start": 1906.0,
      "duration": 5.039
    },
    {
      "text": "Vector for it and then you look at the",
      "start": 1908.559,
      "duration": 5.281
    },
    {
      "text": "input ID of 21 and then you find the",
      "start": 1911.039,
      "duration": 6.561
    },
    {
      "text": "corresponding Vector for it so basically",
      "start": 1913.84,
      "duration": 7.04
    },
    {
      "text": "for each token in this input uh input",
      "start": 1917.6,
      "duration": 6.319
    },
    {
      "text": "batch one embedding Vector of size 256",
      "start": 1920.88,
      "duration": 6.039
    },
    {
      "text": "is generated for each token in the input",
      "start": 1923.919,
      "duration": 4.48
    },
    {
      "text": "so I just constructed another",
      "start": 1926.919,
      "duration": 5.041
    },
    {
      "text": "visualization for you all to see it",
      "start": 1928.399,
      "duration": 6.361
    },
    {
      "text": "further so if this is the input batch",
      "start": 1931.96,
      "duration": 5.88
    },
    {
      "text": "for each token ID here it's converted",
      "start": 1934.76,
      "duration": 6.56
    },
    {
      "text": "into a 256 dimensional Vector so if the",
      "start": 1937.84,
      "duration": 6.16
    },
    {
      "text": "size of this original input batch was",
      "start": 1941.32,
      "duration": 5.359
    },
    {
      "text": "8x4 when we generate the embedding",
      "start": 1944.0,
      "duration": 6.36
    },
    {
      "text": "vectors for each of this the we will get",
      "start": 1946.679,
      "duration": 6.96
    },
    {
      "text": "a tensor which is 8x 4 by 256 so it's a",
      "start": 1950.36,
      "duration": 6.12
    },
    {
      "text": "threedimensional tensor now why 256",
      "start": 1953.639,
      "duration": 5.52
    },
    {
      "text": "because for each of these 8x4 32 values",
      "start": 1956.48,
      "duration": 4.48
    },
    {
      "text": "for each of these 32 values we have a",
      "start": 1959.159,
      "duration": 5.281
    },
    {
      "text": "256 dimensional tensor so for each of",
      "start": 1960.96,
      "duration": 7.28
    },
    {
      "text": "these you can think of like a vector",
      "start": 1964.44,
      "duration": 6.599
    },
    {
      "text": "which has basically 256 Dimensions so",
      "start": 1968.24,
      "duration": 4.399
    },
    {
      "text": "there is a vector here Vector here",
      "start": 1971.039,
      "duration": 4.041
    },
    {
      "text": "Vector here Vector here Etc I cannot",
      "start": 1972.639,
      "duration": 4.201
    },
    {
      "text": "show the three-dimensional structure",
      "start": 1975.08,
      "duration": 5.839
    },
    {
      "text": "right now but it's basically Ally 8X 4X",
      "start": 1976.84,
      "duration": 7.319
    },
    {
      "text": "256 uh okay so that's what we are going",
      "start": 1980.919,
      "duration": 5.961
    },
    {
      "text": "to do next so we have this inputs right",
      "start": 1984.159,
      "duration": 5.36
    },
    {
      "text": "which is the batch of inputs generated",
      "start": 1986.88,
      "duration": 4.639
    },
    {
      "text": "from the data loader we are just going",
      "start": 1989.519,
      "duration": 4.0
    },
    {
      "text": "to pass it as an argument to the lookup",
      "start": 1991.519,
      "duration": 4.28
    },
    {
      "text": "table to the embedding layer and then we",
      "start": 1993.519,
      "duration": 4.241
    },
    {
      "text": "are going to get this tensor which is 8x",
      "start": 1995.799,
      "duration": 5.801
    },
    {
      "text": "4X 256 which is exactly the tensor shape",
      "start": 1997.76,
      "duration": 5.639
    },
    {
      "text": "which I was showing to you over here",
      "start": 2001.6,
      "duration": 5.12
    },
    {
      "text": "that's the 8x4 by 256 tensor after we",
      "start": 2003.399,
      "duration": 4.921
    },
    {
      "text": "pass the input ID",
      "start": 2006.72,
      "duration": 4.52
    },
    {
      "text": "to this uh lookup table of the embedding",
      "start": 2008.32,
      "duration": 5.079
    },
    {
      "text": "layer basically what this lookup table",
      "start": 2011.24,
      "duration": 4.76
    },
    {
      "text": "does is that it generates the 256",
      "start": 2013.399,
      "duration": 4.441
    },
    {
      "text": "dimensional Vector for each of these",
      "start": 2016.0,
      "duration": 4.88
    },
    {
      "text": "token IDs up till now what we have done",
      "start": 2017.84,
      "duration": 5.16
    },
    {
      "text": "is that we have broken down the input",
      "start": 2020.88,
      "duration": 4.56
    },
    {
      "text": "text into batches so let's say we so",
      "start": 2023.0,
      "duration": 4.96
    },
    {
      "text": "this is the first first batch this this",
      "start": 2025.44,
      "duration": 5.76
    },
    {
      "text": "is the first this is the uh first input",
      "start": 2027.96,
      "duration": 5.319
    },
    {
      "text": "of the first batch remember each batch",
      "start": 2031.2,
      "duration": 4.319
    },
    {
      "text": "has eight inputs so this is the first",
      "start": 2033.279,
      "duration": 4.76
    },
    {
      "text": "input we converted into token IDs and",
      "start": 2035.519,
      "duration": 5.04
    },
    {
      "text": "then each of the token ID is have has a",
      "start": 2038.039,
      "duration": 7.36
    },
    {
      "text": "256 or 256 Vector length uh embedding",
      "start": 2040.559,
      "duration": 6.561
    },
    {
      "text": "that's what we have done until now it's",
      "start": 2045.399,
      "duration": 4.0
    },
    {
      "text": "as simple as that I hope everyone is",
      "start": 2047.12,
      "duration": 4.36
    },
    {
      "text": "following until now I'm trying to",
      "start": 2049.399,
      "duration": 4.361
    },
    {
      "text": "explain this vectorial and tensorial",
      "start": 2051.48,
      "duration": 5.919
    },
    {
      "text": "notation as uh in detail as possible but",
      "start": 2053.76,
      "duration": 5.68
    },
    {
      "text": "of course sometimes it gets a bit",
      "start": 2057.399,
      "duration": 3.321
    },
    {
      "text": "difficult so if you have any questions",
      "start": 2059.44,
      "duration": 4.36
    },
    {
      "text": "you can ask it in the comment",
      "start": 2060.72,
      "duration": 6.159
    },
    {
      "text": "section so uh here I have written that",
      "start": 2063.8,
      "duration": 6.079
    },
    {
      "text": "as we can tell based on the 8X 4X 256",
      "start": 2066.879,
      "duration": 5.48
    },
    {
      "text": "dimensional tensor output each token ID",
      "start": 2069.879,
      "duration": 4.76
    },
    {
      "text": "is now embedded as a 256 dimensional",
      "start": 2072.359,
      "duration": 5.0
    },
    {
      "text": "Vector awesome and now we have what we",
      "start": 2074.639,
      "duration": 4.321
    },
    {
      "text": "have to do is that we have to add a",
      "start": 2077.359,
      "duration": 4.361
    },
    {
      "text": "positional embedding uh positional",
      "start": 2078.96,
      "duration": 4.32
    },
    {
      "text": "embedding Vector to each of these",
      "start": 2081.72,
      "duration": 4.6
    },
    {
      "text": "vectors similar to the uh token",
      "start": 2083.28,
      "duration": 5.44
    },
    {
      "text": "embedding we also have to create another",
      "start": 2086.32,
      "duration": 4.599
    },
    {
      "text": "embedding layer for the positional",
      "start": 2088.72,
      "duration": 5.359
    },
    {
      "text": "encoding so remember that at each time",
      "start": 2090.919,
      "duration": 5.92
    },
    {
      "text": "only four vectors need to be processed",
      "start": 2094.079,
      "duration": 5.681
    },
    {
      "text": "right because if you look at the context",
      "start": 2096.839,
      "duration": 4.921
    },
    {
      "text": "size that is equal to four at one time",
      "start": 2099.76,
      "duration": 4.28
    },
    {
      "text": "in the input only four tokens are going",
      "start": 2101.76,
      "duration": 5.44
    },
    {
      "text": "to be given so at one time only the",
      "start": 2104.04,
      "duration": 5.72
    },
    {
      "text": "input the maximum input size is four",
      "start": 2107.2,
      "duration": 4.32
    },
    {
      "text": "which means the llm is going to predict",
      "start": 2109.76,
      "duration": 3.72
    },
    {
      "text": "the next word based on a maximum of four",
      "start": 2111.52,
      "duration": 4.599
    },
    {
      "text": "tokens so actually we need to encode",
      "start": 2113.48,
      "duration": 5.76
    },
    {
      "text": "only four positions in this case so the",
      "start": 2116.119,
      "duration": 6.841
    },
    {
      "text": "embedding layer size will have one 2 3 4",
      "start": 2119.24,
      "duration": 6.0
    },
    {
      "text": "rows and the number of columns will be",
      "start": 2122.96,
      "duration": 4.879
    },
    {
      "text": "the vector Dimension which is 256 that",
      "start": 2125.24,
      "duration": 4.879
    },
    {
      "text": "is fine because for every position we",
      "start": 2127.839,
      "duration": 4.24
    },
    {
      "text": "are going to have a 256 dimensional",
      "start": 2130.119,
      "duration": 4.121
    },
    {
      "text": "Vector remember we have to add this",
      "start": 2132.079,
      "duration": 4.961
    },
    {
      "text": "Vector to the Token embedding Vector so",
      "start": 2134.24,
      "duration": 4.96
    },
    {
      "text": "the size should be same the size here",
      "start": 2137.04,
      "duration": 5.72
    },
    {
      "text": "was 256 right every uh token which was",
      "start": 2139.2,
      "duration": 6.84
    },
    {
      "text": "embedded had a size 256 so here also for",
      "start": 2142.76,
      "duration": 5.64
    },
    {
      "text": "every position we need a 256 dimensional",
      "start": 2146.04,
      "duration": 4.76
    },
    {
      "text": "Vector but there are only four positions",
      "start": 2148.4,
      "duration": 4.16
    },
    {
      "text": "right it can either be the first token",
      "start": 2150.8,
      "duration": 3.6
    },
    {
      "text": "the second token third token or fourth",
      "start": 2152.56,
      "duration": 4.279
    },
    {
      "text": "token so the number of rows when we",
      "start": 2154.4,
      "duration": 3.84
    },
    {
      "text": "create the embedding layer for",
      "start": 2156.839,
      "duration": 3.321
    },
    {
      "text": "positional encoding is going to be four",
      "start": 2158.24,
      "duration": 4.68
    },
    {
      "text": "which is the context length so now let",
      "start": 2160.16,
      "duration": 5.04
    },
    {
      "text": "us write that in the code so remember",
      "start": 2162.92,
      "duration": 4.6
    },
    {
      "text": "when we created the token embedding the",
      "start": 2165.2,
      "duration": 5.08
    },
    {
      "text": "number of rows was the vocabulary size",
      "start": 2167.52,
      "duration": 4.88
    },
    {
      "text": "but here the number of rows is going to",
      "start": 2170.28,
      "duration": 6.0
    },
    {
      "text": "be equal to the context length so now we",
      "start": 2172.4,
      "duration": 6.0
    },
    {
      "text": "are creating a embedding layer for the",
      "start": 2176.28,
      "duration": 4.0
    },
    {
      "text": "positional embedding number of rows is",
      "start": 2178.4,
      "duration": 3.88
    },
    {
      "text": "equal to the context length and the",
      "start": 2180.28,
      "duration": 4.12
    },
    {
      "text": "number of columns is output Dimension",
      "start": 2182.28,
      "duration": 4.68
    },
    {
      "text": "which is 256 because each Vector needs",
      "start": 2184.4,
      "duration": 5.4
    },
    {
      "text": "to have a size of 256 great so we have",
      "start": 2186.96,
      "duration": 4.6
    },
    {
      "text": "created the positional embedding layer",
      "start": 2189.8,
      "duration": 4.799
    },
    {
      "text": "right now and now I'm going to uh",
      "start": 2191.56,
      "duration": 5.72
    },
    {
      "text": "visually try to explain how we are going",
      "start": 2194.599,
      "duration": 4.441
    },
    {
      "text": "to add or how we are going to",
      "start": 2197.28,
      "duration": 3.24
    },
    {
      "text": "essentially create the positional",
      "start": 2199.04,
      "duration": 4.52
    },
    {
      "text": "embedding vectors so let's look at our",
      "start": 2200.52,
      "duration": 5.44
    },
    {
      "text": "input Matrix again here the batch size",
      "start": 2203.56,
      "duration": 4.36
    },
    {
      "text": "is eight so we have eight rows and the",
      "start": 2205.96,
      "duration": 3.879
    },
    {
      "text": "context length is four which that's why",
      "start": 2207.92,
      "duration": 4.12
    },
    {
      "text": "we have four columns so if you look at",
      "start": 2209.839,
      "duration": 3.961
    },
    {
      "text": "each input sequence let's look at the",
      "start": 2212.04,
      "duration": 3.559
    },
    {
      "text": "first row if you look at the first row",
      "start": 2213.8,
      "duration": 4.4
    },
    {
      "text": "it has four token IDs let's say those",
      "start": 2215.599,
      "duration": 6.961
    },
    {
      "text": "token IDs are 10 8 20 and 21 each of",
      "start": 2218.2,
      "duration": 6.24
    },
    {
      "text": "these token ID is now a",
      "start": 2222.56,
      "duration": 5.12
    },
    {
      "text": "256 uh Vector length Vector because we",
      "start": 2224.44,
      "duration": 5.08
    },
    {
      "text": "have done the embedding we have done the",
      "start": 2227.68,
      "duration": 3.76
    },
    {
      "text": "token embedding so each token ID is a",
      "start": 2229.52,
      "duration": 5.16
    },
    {
      "text": "256 dimensional Vector so this is the",
      "start": 2231.44,
      "duration": 4.879
    },
    {
      "text": "first batch of input you can see that",
      "start": 2234.68,
      "duration": 3.72
    },
    {
      "text": "there are four positions here maybe",
      "start": 2236.319,
      "duration": 5.28
    },
    {
      "text": "these words are the cat sat on now what",
      "start": 2238.4,
      "duration": 6.08
    },
    {
      "text": "we need to do is that we need to add one",
      "start": 2241.599,
      "duration": 5.601
    },
    {
      "text": "one uh positional encoding Vector for",
      "start": 2244.48,
      "duration": 6.0
    },
    {
      "text": "each of these to this uh to the Token",
      "start": 2247.2,
      "duration": 6.04
    },
    {
      "text": "embedding for the 10 for the token ID 10",
      "start": 2250.48,
      "duration": 5.72
    },
    {
      "text": "we have to add a positional uh embedding",
      "start": 2253.24,
      "duration": 5.68
    },
    {
      "text": "Vector to a token ID of 8 we have to add",
      "start": 2256.2,
      "duration": 5.28
    },
    {
      "text": "another Vector to a token ID of 20 we",
      "start": 2258.92,
      "duration": 4.56
    },
    {
      "text": "need to add positional Vector to a token",
      "start": 2261.48,
      "duration": 3.68
    },
    {
      "text": "ID of 21 we need to add another",
      "start": 2263.48,
      "duration": 3.48
    },
    {
      "text": "positional embedding",
      "start": 2265.16,
      "duration": 5.64
    },
    {
      "text": "Vector so uh we need to add one position",
      "start": 2266.96,
      "duration": 6.399
    },
    {
      "text": "Vector to each of these four token",
      "start": 2270.8,
      "duration": 5.24
    },
    {
      "text": "embeddings and remember that the same",
      "start": 2273.359,
      "duration": 5.441
    },
    {
      "text": "positional embeddings are applied",
      "start": 2276.04,
      "duration": 5.36
    },
    {
      "text": "uh because there are only four positions",
      "start": 2278.8,
      "duration": 6.2
    },
    {
      "text": "right so uh the positional",
      "start": 2281.4,
      "duration": 7.48
    },
    {
      "text": "embedding we just need to do it uh once",
      "start": 2285.0,
      "duration": 6.76
    },
    {
      "text": "and then for every token or for every",
      "start": 2288.88,
      "duration": 5.36
    },
    {
      "text": "input sequence the same four positional",
      "start": 2291.76,
      "duration": 4.359
    },
    {
      "text": "embeddings can be applied so for example",
      "start": 2294.24,
      "duration": 3.839
    },
    {
      "text": "this is the batch one right if you look",
      "start": 2296.119,
      "duration": 3.361
    },
    {
      "text": "at this",
      "start": 2298.079,
      "duration": 4.441
    },
    {
      "text": "batch uh which is row number three let's",
      "start": 2299.48,
      "duration": 7.119
    },
    {
      "text": "say there are some input IDs like one um",
      "start": 2302.52,
      "duration": 5.44
    },
    {
      "text": "two",
      "start": 2306.599,
      "duration": 3.441
    },
    {
      "text": "five and six let's say these are the",
      "start": 2307.96,
      "duration": 5.52
    },
    {
      "text": "token IDs now whatever positional",
      "start": 2310.04,
      "duration": 6.279
    },
    {
      "text": "positional embedding we added to the",
      "start": 2313.48,
      "duration": 5.52
    },
    {
      "text": "first input the same positional",
      "start": 2316.319,
      "duration": 4.441
    },
    {
      "text": "embedding can be added to this input",
      "start": 2319.0,
      "duration": 3.24
    },
    {
      "text": "because the positions are the same",
      "start": 2320.76,
      "duration": 5.0
    },
    {
      "text": "either it's position one 2 3 or four we",
      "start": 2322.24,
      "duration": 4.92
    },
    {
      "text": "just need to encode the different",
      "start": 2325.76,
      "duration": 4.8
    },
    {
      "text": "positions right so that's why uh the",
      "start": 2327.16,
      "duration": 5.8
    },
    {
      "text": "positional encoding size or the",
      "start": 2330.56,
      "duration": 6.0
    },
    {
      "text": "positional embedding size uh has to be",
      "start": 2332.96,
      "duration": 6.639
    },
    {
      "text": "4X 250 56 we only need four positional",
      "start": 2336.56,
      "duration": 5.08
    },
    {
      "text": "vectors four positional embedding",
      "start": 2339.599,
      "duration": 4.441
    },
    {
      "text": "vectors and then each Vector will of",
      "start": 2341.64,
      "duration": 3.64
    },
    {
      "text": "course have size",
      "start": 2344.04,
      "duration": 4.48
    },
    {
      "text": "256 so then the positional embedding",
      "start": 2345.28,
      "duration": 5.44
    },
    {
      "text": "Vector Matrix which we have will be a 4X",
      "start": 2348.52,
      "duration": 6.0
    },
    {
      "text": "256 Matrix why do we only need four why",
      "start": 2350.72,
      "duration": 6.72
    },
    {
      "text": "not have a separate positional uh",
      "start": 2354.52,
      "duration": 5.44
    },
    {
      "text": "embedding for each of the inputs here",
      "start": 2357.44,
      "duration": 4.76
    },
    {
      "text": "because for each of these inputs we only",
      "start": 2359.96,
      "duration": 4.96
    },
    {
      "text": "want to encode whether the token ID is",
      "start": 2362.2,
      "duration": 4.56
    },
    {
      "text": "in the first position second third or",
      "start": 2364.92,
      "duration": 5.88
    },
    {
      "text": "fourth so we only need the positional uh",
      "start": 2366.76,
      "duration": 7.48
    },
    {
      "text": "we only need four positional embedding",
      "start": 2370.8,
      "duration": 5.88
    },
    {
      "text": "vectors one one for the first position",
      "start": 2374.24,
      "duration": 4.16
    },
    {
      "text": "one for the second one for the third one",
      "start": 2376.68,
      "duration": 3.679
    },
    {
      "text": "for the fourth and then we can add the",
      "start": 2378.4,
      "duration": 4.56
    },
    {
      "text": "same four to basically all the input",
      "start": 2380.359,
      "duration": 5.201
    },
    {
      "text": "sequences in a given batch that's what",
      "start": 2382.96,
      "duration": 4.84
    },
    {
      "text": "we are going to do so in the next step",
      "start": 2385.56,
      "duration": 4.08
    },
    {
      "text": "which is the step number 13 we are going",
      "start": 2387.8,
      "duration": 3.48
    },
    {
      "text": "to generate the four positional",
      "start": 2389.64,
      "duration": 3.479
    },
    {
      "text": "embedding vectors from the positional",
      "start": 2391.28,
      "duration": 4.92
    },
    {
      "text": "embedding Matrix so as I told you all",
      "start": 2393.119,
      "duration": 5.081
    },
    {
      "text": "embedding Matrix are essentially lookup",
      "start": 2396.2,
      "duration": 4.08
    },
    {
      "text": "tables so to generate the embedding",
      "start": 2398.2,
      "duration": 3.6
    },
    {
      "text": "vectors we just need to pass these",
      "start": 2400.28,
      "duration": 5.6
    },
    {
      "text": "positions 0 1 2 and 3 and then it will",
      "start": 2401.8,
      "duration": 6.279
    },
    {
      "text": "generate the corresponding vectors",
      "start": 2405.88,
      "duration": 5.199
    },
    {
      "text": "according to that so how to pass the",
      "start": 2408.079,
      "duration": 5.361
    },
    {
      "text": "positions you just use tor. arranged max",
      "start": 2411.079,
      "duration": 4.921
    },
    {
      "text": "length what torch. arrange max length",
      "start": 2413.44,
      "duration": 5.919
    },
    {
      "text": "will do is that it will create 0 1 and 1",
      "start": 2416.0,
      "duration": 5.56
    },
    {
      "text": "2 and three so max length is equal to",
      "start": 2419.359,
      "duration": 4.48
    },
    {
      "text": "four so torch. arrange will create a",
      "start": 2421.56,
      "duration": 5.48
    },
    {
      "text": "sequence of number 0 1 up to Max input",
      "start": 2423.839,
      "duration": 5.801
    },
    {
      "text": "length minus 1 so this will be 0 1 2 3",
      "start": 2427.04,
      "duration": 4.76
    },
    {
      "text": "so essentially it will create uh the",
      "start": 2429.64,
      "duration": 5.0
    },
    {
      "text": "token ID 0 1 2 and 3 and then we can",
      "start": 2431.8,
      "duration": 4.72
    },
    {
      "text": "just look up the positional embedding",
      "start": 2434.64,
      "duration": 3.92
    },
    {
      "text": "table and generate these four positional",
      "start": 2436.52,
      "duration": 3.4
    },
    {
      "text": "embedding",
      "start": 2438.56,
      "duration": 4.039
    },
    {
      "text": "vectors so then what we do is that as I",
      "start": 2439.92,
      "duration": 4.199
    },
    {
      "text": "said we just look up the positional",
      "start": 2442.599,
      "duration": 2.961
    },
    {
      "text": "embedding layer",
      "start": 2444.119,
      "duration": 4.041
    },
    {
      "text": "Matrix uh which is a lookup table and",
      "start": 2445.56,
      "duration": 5.24
    },
    {
      "text": "just pass in these four arguments 0 1 2",
      "start": 2448.16,
      "duration": 5.56
    },
    {
      "text": "3 so then it generates four vectors each",
      "start": 2450.8,
      "duration": 5.36
    },
    {
      "text": "of size 256 these are the four",
      "start": 2453.72,
      "duration": 4.0
    },
    {
      "text": "positional embedding vectors which we",
      "start": 2456.16,
      "duration": 4.04
    },
    {
      "text": "need one is for position number one",
      "start": 2457.72,
      "duration": 4.56
    },
    {
      "text": "second is for position number two third",
      "start": 2460.2,
      "duration": 3.879
    },
    {
      "text": "is for position number three and fourth",
      "start": 2462.28,
      "duration": 3.6
    },
    {
      "text": "is for position number",
      "start": 2464.079,
      "duration": 4.52
    },
    {
      "text": "four so as we can see the positional",
      "start": 2465.88,
      "duration": 5.52
    },
    {
      "text": "embedding tensor consists of four 256",
      "start": 2468.599,
      "duration": 5.48
    },
    {
      "text": "dimensional vectors we can now add this",
      "start": 2471.4,
      "duration": 4.52
    },
    {
      "text": "directly to the Token",
      "start": 2474.079,
      "duration": 4.161
    },
    {
      "text": "embedding uh so let's see how that is",
      "start": 2475.92,
      "duration": 6.0
    },
    {
      "text": "done uh let's look at one batch for now",
      "start": 2478.24,
      "duration": 6.28
    },
    {
      "text": "and then uh let's see how to add the",
      "start": 2481.92,
      "duration": 4.08
    },
    {
      "text": "token embeddings with the position",
      "start": 2484.52,
      "duration": 3.079
    },
    {
      "text": "embeddings",
      "start": 2486.0,
      "duration": 3.119
    },
    {
      "text": "so we have generated these four",
      "start": 2487.599,
      "duration": 3.161
    },
    {
      "text": "positional embedding vectors from the",
      "start": 2489.119,
      "duration": 3.401
    },
    {
      "text": "positional embedding Matrix great so we",
      "start": 2490.76,
      "duration": 4.28
    },
    {
      "text": "have completed step number 13 and so",
      "start": 2492.52,
      "duration": 4.52
    },
    {
      "text": "finally we come to step number 14 this",
      "start": 2495.04,
      "duration": 3.92
    },
    {
      "text": "is the last step where we have to add",
      "start": 2497.04,
      "duration": 3.84
    },
    {
      "text": "the position embeddings to the Token",
      "start": 2498.96,
      "duration": 4.119
    },
    {
      "text": "embeddings so if you look at the token",
      "start": 2500.88,
      "duration": 4.68
    },
    {
      "text": "embedding matrix it's 8X 4X",
      "start": 2503.079,
      "duration": 5.76
    },
    {
      "text": "256 as we had already seen before so for",
      "start": 2505.56,
      "duration": 6.279
    },
    {
      "text": "each token ID there is a 256 dimensional",
      "start": 2508.839,
      "duration": 5.561
    },
    {
      "text": "vector and how many token IDs are there",
      "start": 2511.839,
      "duration": 4.28
    },
    {
      "text": "there are eight batches and each batch",
      "start": 2514.4,
      "duration": 4.56
    },
    {
      "text": "has four token IDs so 8x4 that's why the",
      "start": 2516.119,
      "duration": 5.921
    },
    {
      "text": "size is 8x4 by 256 and then the",
      "start": 2518.96,
      "duration": 4.8
    },
    {
      "text": "positional embedding we just have four",
      "start": 2522.04,
      "duration": 5.88
    },
    {
      "text": "256 256 256 256 so this is just 4X",
      "start": 2523.76,
      "duration": 6.72
    },
    {
      "text": "256 so for the first position we have a",
      "start": 2527.92,
      "duration": 5.12
    },
    {
      "text": "256 Dimension Vector for the second",
      "start": 2530.48,
      "duration": 5.08
    },
    {
      "text": "position we have a 256 Dimension Vector",
      "start": 2533.04,
      "duration": 4.52
    },
    {
      "text": "for the third position we have a 256",
      "start": 2535.56,
      "duration": 3.759
    },
    {
      "text": "Dimension vector and for the fourth",
      "start": 2537.56,
      "duration": 4.24
    },
    {
      "text": "position we have a 256 Dimension",
      "start": 2539.319,
      "duration": 4.921
    },
    {
      "text": "Vector so now we are adding the token",
      "start": 2541.8,
      "duration": 3.68
    },
    {
      "text": "embeddings with the positional",
      "start": 2544.24,
      "duration": 3.4
    },
    {
      "text": "embeddings so you must be thinking this",
      "start": 2545.48,
      "duration": 6.599
    },
    {
      "text": "is 8x 4X 256 this is 4X 256 how does",
      "start": 2547.64,
      "duration": 7.28
    },
    {
      "text": "python really add them so when you add",
      "start": 2552.079,
      "duration": 5.321
    },
    {
      "text": "such matrixes matrices what happens is a",
      "start": 2554.92,
      "duration": 4.84
    },
    {
      "text": "broadcasting operation so what python",
      "start": 2557.4,
      "duration": 5.36
    },
    {
      "text": "does is that it converts this 4X 256 to",
      "start": 2559.76,
      "duration": 6.52
    },
    {
      "text": "8x4 by 256 by duplicating these same",
      "start": 2562.76,
      "duration": 6.64
    },
    {
      "text": "values eight times so then what is",
      "start": 2566.28,
      "duration": 4.68
    },
    {
      "text": "essentially happening is that to the",
      "start": 2569.4,
      "duration": 5.12
    },
    {
      "text": "first row these four values are added to",
      "start": 2570.96,
      "duration": 5.599
    },
    {
      "text": "to the second row these same four values",
      "start": 2574.52,
      "duration": 4.319
    },
    {
      "text": "are added to the third row these same",
      "start": 2576.559,
      "duration": 4.56
    },
    {
      "text": "four values are added similarly to the",
      "start": 2578.839,
      "duration": 4.361
    },
    {
      "text": "eighth Row the same four values are",
      "start": 2581.119,
      "duration": 5.161
    },
    {
      "text": "added so finally the input embeddings",
      "start": 2583.2,
      "duration": 4.8
    },
    {
      "text": "which are the result of the token",
      "start": 2586.28,
      "duration": 3.12
    },
    {
      "text": "embeddings plus the positional",
      "start": 2588.0,
      "duration": 5.4
    },
    {
      "text": "embeddings have the size of 8X 4X 256",
      "start": 2589.4,
      "duration": 6.0
    },
    {
      "text": "and these are the input embeddings which",
      "start": 2593.4,
      "duration": 4.88
    },
    {
      "text": "then will be the final training input to",
      "start": 2595.4,
      "duration": 3.64
    },
    {
      "text": "the",
      "start": 2598.28,
      "duration": 3.4
    },
    {
      "text": "llms so we did so many things to reach",
      "start": 2599.04,
      "duration": 4.96
    },
    {
      "text": "this stage right but I hope you have",
      "start": 2601.68,
      "duration": 4.04
    },
    {
      "text": "understood this part I just wanted to",
      "start": 2604.0,
      "duration": 4.119
    },
    {
      "text": "show you for gpt2 we had a vocabulary",
      "start": 2605.72,
      "duration": 5.04
    },
    {
      "text": "size of around 50,000 and I showed you a",
      "start": 2608.119,
      "duration": 5.881
    },
    {
      "text": "vector length of 256 so first I showed",
      "start": 2610.76,
      "duration": 6.64
    },
    {
      "text": "you how to create the 8X 4X 256 token",
      "start": 2614.0,
      "duration": 6.16
    },
    {
      "text": "embedding uh Matrix in the first place",
      "start": 2617.4,
      "duration": 5.159
    },
    {
      "text": "so our initial task was for you to",
      "start": 2620.16,
      "duration": 4.919
    },
    {
      "text": "understand uh how is this token",
      "start": 2622.559,
      "duration": 4.481
    },
    {
      "text": "embedding itself created so every token",
      "start": 2625.079,
      "duration": 4.361
    },
    {
      "text": "ID in a batch is converted into a vector",
      "start": 2627.04,
      "duration": 6.4
    },
    {
      "text": "of size 256 and then for each position",
      "start": 2629.44,
      "duration": 7.2
    },
    {
      "text": "we add uh a positional uh positional",
      "start": 2633.44,
      "duration": 5.24
    },
    {
      "text": "embedding so first we need to see how",
      "start": 2636.64,
      "duration": 4.24
    },
    {
      "text": "many positions are there and for that",
      "start": 2638.68,
      "duration": 4.08
    },
    {
      "text": "the parameter which becomes important is",
      "start": 2640.88,
      "duration": 4.4
    },
    {
      "text": "the context length because that's the",
      "start": 2642.76,
      "duration": 4.68
    },
    {
      "text": "maximum length of the input to be fed at",
      "start": 2645.28,
      "duration": 4.4
    },
    {
      "text": "any time to the llm so only those many",
      "start": 2647.44,
      "duration": 4.52
    },
    {
      "text": "positions are important so the context",
      "start": 2649.68,
      "duration": 5.24
    },
    {
      "text": "length is four so we then create so we",
      "start": 2651.96,
      "duration": 4.879
    },
    {
      "text": "only need four positional embedding",
      "start": 2654.92,
      "duration": 4.88
    },
    {
      "text": "vectors one for position one second for",
      "start": 2656.839,
      "duration": 4.961
    },
    {
      "text": "position two third for position three",
      "start": 2659.8,
      "duration": 4.12
    },
    {
      "text": "and fourth for the position number four",
      "start": 2661.8,
      "duration": 4.0
    },
    {
      "text": "and then we add it to the toer embedding",
      "start": 2663.92,
      "duration": 5.36
    },
    {
      "text": "Matrix how do we add it python does a",
      "start": 2665.8,
      "duration": 5.519
    },
    {
      "text": "broadcasting so even if the token",
      "start": 2669.28,
      "duration": 5.279
    },
    {
      "text": "embedding Matrix is 8x 4X 256 and even",
      "start": 2671.319,
      "duration": 5.481
    },
    {
      "text": "if the positional embedding is just 4X",
      "start": 2674.559,
      "duration": 6.121
    },
    {
      "text": "256 it just copies these uh these four",
      "start": 2676.8,
      "duration": 7.0
    },
    {
      "text": "values eight times uh so essentially",
      "start": 2680.68,
      "duration": 5.28
    },
    {
      "text": "what happens is that to each row of the",
      "start": 2683.8,
      "duration": 3.92
    },
    {
      "text": "token embedding the same positional",
      "start": 2685.96,
      "duration": 4.639
    },
    {
      "text": "embedding four vectors are added and",
      "start": 2687.72,
      "duration": 4.68
    },
    {
      "text": "that's how we get the input embeddings",
      "start": 2690.599,
      "duration": 4.76
    },
    {
      "text": "8X 4X 256 I hope you have understood",
      "start": 2692.4,
      "duration": 5.64
    },
    {
      "text": "these Dimensions so eight because in",
      "start": 2695.359,
      "duration": 5.641
    },
    {
      "text": "each batch we have eight input sequences",
      "start": 2698.04,
      "duration": 5.0
    },
    {
      "text": "and in each input sequence we have four",
      "start": 2701.0,
      "duration": 4.96
    },
    {
      "text": "tokens that's why four and why 256",
      "start": 2703.04,
      "duration": 5.64
    },
    {
      "text": "because each token ID or each token is",
      "start": 2705.96,
      "duration": 5.28
    },
    {
      "text": "essentially a 256 length",
      "start": 2708.68,
      "duration": 5.04
    },
    {
      "text": "Vector I hope everyone has understood",
      "start": 2711.24,
      "duration": 4.879
    },
    {
      "text": "this lecture on positional positional",
      "start": 2713.72,
      "duration": 4.48
    },
    {
      "text": "embedding now let me go back to the",
      "start": 2716.119,
      "duration": 4.44
    },
    {
      "text": "start where we looked at what all we",
      "start": 2718.2,
      "duration": 4.28
    },
    {
      "text": "have covered and what needs to be the",
      "start": 2720.559,
      "duration": 3.04
    },
    {
      "text": "input to the",
      "start": 2722.48,
      "duration": 6.48
    },
    {
      "text": "llm so look at this diagram um",
      "start": 2723.599,
      "duration": 5.361
    },
    {
      "text": "so in today's lecture we actually looked",
      "start": 2729.16,
      "duration": 5.399
    },
    {
      "text": "at one more step which is I would say",
      "start": 2731.96,
      "duration": 5.04
    },
    {
      "text": "maybe step 3.5 in that case and that is",
      "start": 2734.559,
      "duration": 4.481
    },
    {
      "text": "adding positional embeddings to the",
      "start": 2737.0,
      "duration": 4.839
    },
    {
      "text": "token embeddings and that finally uh",
      "start": 2739.04,
      "duration": 3.76
    },
    {
      "text": "leads",
      "start": 2741.839,
      "duration": 3.881
    },
    {
      "text": "to uh input embeddings so these input",
      "start": 2742.8,
      "duration": 5.279
    },
    {
      "text": "embeddings which we obtained so actually",
      "start": 2745.72,
      "duration": 4.16
    },
    {
      "text": "let me add write here positional",
      "start": 2748.079,
      "duration": 3.52
    },
    {
      "text": "embedding so what we actually added to",
      "start": 2749.88,
      "duration": 4.76
    },
    {
      "text": "the Token embeddings was positional",
      "start": 2751.599,
      "duration": 5.52
    },
    {
      "text": "embeddings",
      "start": 2754.64,
      "duration": 5.439
    },
    {
      "text": "so",
      "start": 2757.119,
      "duration": 5.801
    },
    {
      "text": "uh so what we added here",
      "start": 2760.079,
      "duration": 6.681
    },
    {
      "text": "was yeah positional embeddings and then",
      "start": 2762.92,
      "duration": 7.439
    },
    {
      "text": "this resulted into input embeddings so",
      "start": 2766.76,
      "duration": 5.16
    },
    {
      "text": "token embeddings plus positional",
      "start": 2770.359,
      "duration": 3.401
    },
    {
      "text": "embeddings is input embeddings and then",
      "start": 2771.92,
      "duration": 3.32
    },
    {
      "text": "these are the ones which are actually",
      "start": 2773.76,
      "duration": 4.44
    },
    {
      "text": "used as input to the GPT so essentially",
      "start": 2775.24,
      "duration": 4.56
    },
    {
      "text": "what we did in token embedding and",
      "start": 2778.2,
      "duration": 3.2
    },
    {
      "text": "positional embedding is we try to",
      "start": 2779.8,
      "duration": 4.08
    },
    {
      "text": "exploit as many things in textual",
      "start": 2781.4,
      "duration": 4.08
    },
    {
      "text": "language as possible the first thing",
      "start": 2783.88,
      "duration": 4.04
    },
    {
      "text": "which we exploited semantic meaning when",
      "start": 2785.48,
      "duration": 4.48
    },
    {
      "text": "we did token embeddings when we did",
      "start": 2787.92,
      "duration": 4.399
    },
    {
      "text": "positional embeddings we exploited the",
      "start": 2789.96,
      "duration": 4.04
    },
    {
      "text": "fact that the different positions also",
      "start": 2792.319,
      "duration": 4.441
    },
    {
      "text": "mean something until now we have not",
      "start": 2794.0,
      "duration": 5.04
    },
    {
      "text": "seen how exactly to obtain the values in",
      "start": 2796.76,
      "duration": 3.839
    },
    {
      "text": "the positional embedding see even in",
      "start": 2799.04,
      "duration": 3.72
    },
    {
      "text": "today's lecture we just randomly",
      "start": 2800.599,
      "duration": 4.48
    },
    {
      "text": "initialized uh the embedding Matrix",
      "start": 2802.76,
      "duration": 4.799
    },
    {
      "text": "right like if you see in the code this",
      "start": 2805.079,
      "duration": 4.201
    },
    {
      "text": "positional embedding layer is randomly",
      "start": 2807.559,
      "duration": 4.04
    },
    {
      "text": "initialized so what what tor. nn.",
      "start": 2809.28,
      "duration": 4.88
    },
    {
      "text": "embedding does is that it initializes a",
      "start": 2811.599,
      "duration": 5.321
    },
    {
      "text": "matrix with number of rows as context",
      "start": 2814.16,
      "duration": 4.48
    },
    {
      "text": "length number of columns as output",
      "start": 2816.92,
      "duration": 3.76
    },
    {
      "text": "Dimension and it puts random values in",
      "start": 2818.64,
      "duration": 4.6
    },
    {
      "text": "this so then how do we know the actual",
      "start": 2820.68,
      "duration": 4.679
    },
    {
      "text": "values so that is actually a part of the",
      "start": 2823.24,
      "duration": 4.56
    },
    {
      "text": "llm training process and it's exactly",
      "start": 2825.359,
      "duration": 4.2
    },
    {
      "text": "similar for",
      "start": 2827.8,
      "duration": 4.6
    },
    {
      "text": "the uh token embeddings as well so we",
      "start": 2829.559,
      "duration": 4.8
    },
    {
      "text": "need to optimize the values in the token",
      "start": 2832.4,
      "duration": 4.24
    },
    {
      "text": "embedding layer and we need to optimize",
      "start": 2834.359,
      "duration": 3.881
    },
    {
      "text": "the value in the positional embedding",
      "start": 2836.64,
      "duration": 3.88
    },
    {
      "text": "layer however for us to reach the",
      "start": 2838.24,
      "duration": 4.64
    },
    {
      "text": "optimization stage stage first it's very",
      "start": 2840.52,
      "duration": 4.52
    },
    {
      "text": "important for you to understand what",
      "start": 2842.88,
      "duration": 4.28
    },
    {
      "text": "exactly is positional embedding what",
      "start": 2845.04,
      "duration": 4.799
    },
    {
      "text": "exactly is token embedding and that was",
      "start": 2847.16,
      "duration": 5.08
    },
    {
      "text": "the whole purpose of today's",
      "start": 2849.839,
      "duration": 5.321
    },
    {
      "text": "lecture uh this brings us to the end of",
      "start": 2852.24,
      "duration": 4.76
    },
    {
      "text": "today's lecture thank you so much",
      "start": 2855.16,
      "duration": 3.88
    },
    {
      "text": "everyone I hope you understood a lot I",
      "start": 2857.0,
      "duration": 3.44
    },
    {
      "text": "hope you understood the difference",
      "start": 2859.04,
      "duration": 5.84
    },
    {
      "text": "between absolute um absolute and",
      "start": 2860.44,
      "duration": 5.919
    },
    {
      "text": "positional",
      "start": 2864.88,
      "duration": 3.76
    },
    {
      "text": "embedding yeah absolute and relative",
      "start": 2866.359,
      "duration": 4.161
    },
    {
      "text": "sorry absolute and relative positional",
      "start": 2868.64,
      "duration": 5.36
    },
    {
      "text": "embedding I hope you understood uh why",
      "start": 2870.52,
      "duration": 5.72
    },
    {
      "text": "positional embeddings are added and most",
      "start": 2874.0,
      "duration": 3.839
    },
    {
      "text": "importantly I hope you understood the",
      "start": 2876.24,
      "duration": 3.8
    },
    {
      "text": "dimensions ultimately I feel it all",
      "start": 2877.839,
      "duration": 4.121
    },
    {
      "text": "comes down to Dimensions people who",
      "start": 2880.04,
      "duration": 4.12
    },
    {
      "text": "understand the dimensions really don't",
      "start": 2881.96,
      "duration": 3.96
    },
    {
      "text": "feel scared or intimidated by the",
      "start": 2884.16,
      "duration": 3.8
    },
    {
      "text": "subject so if someone understands where",
      "start": 2885.92,
      "duration": 5.48
    },
    {
      "text": "this 8X 4X 256 is actually coming from",
      "start": 2887.96,
      "duration": 6.32
    },
    {
      "text": "right this 8x4 by 256 then I feel they",
      "start": 2891.4,
      "duration": 4.56
    },
    {
      "text": "will have a much stronger grasp on the",
      "start": 2894.28,
      "duration": 3.839
    },
    {
      "text": "subject so that's why today I spent a",
      "start": 2895.96,
      "duration": 5.2
    },
    {
      "text": "lot of time on explaining these",
      "start": 2898.119,
      "duration": 5.72
    },
    {
      "text": "Dimensions um thank you so much everyone",
      "start": 2901.16,
      "duration": 4.24
    },
    {
      "text": "and if you have any doubts or questions",
      "start": 2903.839,
      "duration": 3.321
    },
    {
      "text": "please put it in the comment section the",
      "start": 2905.4,
      "duration": 3.52
    },
    {
      "text": "lectures are getting bit more involved",
      "start": 2907.16,
      "duration": 4.12
    },
    {
      "text": "and detailed now so I'll be happy to",
      "start": 2908.92,
      "duration": 3.88
    },
    {
      "text": "interact in the comment section and",
      "start": 2911.28,
      "duration": 3.76
    },
    {
      "text": "solve any doubts or questions also let",
      "start": 2912.8,
      "duration": 3.72
    },
    {
      "text": "me know if you're liking this teaching",
      "start": 2915.04,
      "duration": 3.48
    },
    {
      "text": "style which is a mix of the Whiteboard",
      "start": 2916.52,
      "duration": 4.88
    },
    {
      "text": "lectures uh plus the Hands-On coding",
      "start": 2918.52,
      "duration": 4.599
    },
    {
      "text": "I'll of course be sharing the code file",
      "start": 2921.4,
      "duration": 3.84
    },
    {
      "text": "with all of you thanks everyone and I'll",
      "start": 2923.119,
      "duration": 6.0
    },
    {
      "text": "see you in the next lecture",
      "start": 2925.24,
      "duration": 3.879
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch Series today we are going to be looking at a very important topic and that is called as positional encoding so I've divided today's lecture into two to three modules initially we'll look at what all we have covered so far then we will try to understand why is positional encoding really needed and then finally we will do a Hands-On coding exercise in Python where we'll actually add positional encoding layer along with the token embedding layer which we have already created so far so let's get started if you have not seen the previous lecture on token encoding or token embedding I would highly encourage you to see that lecture because positional encoding Builds on top of token and coding if you are coming to this lecture series for the first time welcome I have designed the lectur so so that they are good to watch even as Standalone lectures but of course if you go through the previous lectures your understanding of the current one will be much better so let's get started until now we have looked at something which is called as token embedding and what is meant by token embedding so token embedding is the step number three in the llm train training process so first we have we start with an input text then we convert that text into tokens so for example if the text is this is an example the tokens can be this is an example this what I'm showing right now is an example of a word based tokenizer but there are other tokenizers such as subword based tokenizer or bite pair encoding tokenizer which actually GPT uses so remember that one word is generally not equal to one token you can even have subwords or characters as tokens for the case of Simplicity I'm just showing here the token X text to be individual words so that's step number one step number two is maintaining a vocabulary of the tokens sorting them in ascending order and assigning a token ID to each token that's step number two but even then we are not ready to give the token IDs as input for the GPT or for the LS M training there is a very crucial step number three which we looked at in the previous lecture and that's called token embeddings basically the token IDs which we obtained are converted into vectors uh so that the semantic meaning between the different words is retained if you directly use the token IDs as inputs to the large language model we do not retain the meaning between words so for example dog and puppy are related to each other cat and kitten are related to to each other if we don't convert token IDs into token embeddings this semantic relationship or this meaning between the words is lost that's why it's very important to have step number three so if you have a vocabulary uh with GPT uses let's say of around 50,000 words gpt2 was trained on a vocabulary of 50,000 Words which means of 50,000 tokens which means we have 50,000 token IDs and then there is a vector corresponding to each token ID that is uh called as token embeddings every token is converted into a vector in a higher dimensional space so now you must be thinking okay so now I have these vectors which capture the semantic relationship uh between words and now I can feed these vectors as inputs to the large language model right almost we are almost there I promise there is just one last step remaining in the data pre-processing part and then in the next lecture on will come to training the llm we'll see the attention mechanism Etc and the last step is called as positional embedding so why is this needed so let's say we have a sentence the cat sat on the mat and then another sentence on the mat the cat sat so if you look at both of these sentences the cat cat word appears in both of these sentences right and in both of these sentences cat is a token that will be assigned a token ID and uh and that will be converted into a vector so the vector for CAT will be the same in both of these sentences although the position of the cat in these two sentences is different so until now if we just use the vector embedding we do not have any information about the position of the particular word so in the embedding layer in the token embedding layer whatever we have seen up now the same token ID gets mapped to the same Vector representation so the token ID for CAT will get mapped to the same Vector regardless of where the token ID is positioned in the input sequence so for example cat the position of a cat is different in these two sentences right but if we don't do anything about the position if we don't encode the position and if we just use the embedding layer we learned previously both of these tokens cat in both of the sentences will have the exact same Vector representation so in the embedding layer which we have seen so far the position is not Incorporated there is a figure which demonstrates this pretty nicely so uh let's say this is the embedding layer weight Matrix which is basically we have all of these tokens in our vocabulary those are converted into token IDs and for every token ID there is a vector in a higher dimensional space so each row correspond corresponds to one such Vector for the token ID now if you want to embed these token IDs which means if you want to convert these token IDs into Vector representations you will see that these the five the token five comes here and the token five comes here also and these two tokens clearly appear at different positions in the sentence but if you look at the embedded vectors for these two tokens they are exactly the same if you look at the first row here that's the embedding Vector embedded Vector for the token ID 5 the first token ID 5 and for the second token id5 you'll see that the embedded Vector is the third row so if you look at the first row and the third row they are actually exactly the same so the main point here is that the same token ID result in the same embedding vectors which says that we are essentially not exploiting the maximum information present in sentences in sentences there are there is meaning between different words which we already captured through Vector embeddings but the position also matters a lot right cat appears at different positions here and that completely that may change the meaning of the sentence entirely so it's very important to also encode the information related to tokens and the position at which the token appears in the given sentence so it is very helpful to inject additional position information to the large language model along with capturing the semantic meaning it is extremely important to inject this additional information about the position to the llm so now let us talk a bit about how do we encode this information about the position so let's say if I give a word and I convert it into token ID and I convert it into a vector how do I also give information about the position of the world let's look at that so there are essentially two types of positional embeddings the first type of positional embedding is absolute and this is the more commonly used positional embedding and the second type of positional embedding is relative positional embedding so as you must have guessed from the name itself these two positional embeddings are different let's look at Absolute positional embedding initially in absolute positional embedding for each in for each position in the input sequence a unique embedding is added to the tokens embedding to convey its exact location let me repeat that in absolute in absolute positional embed for each position in the input sequence a unique embedding is added to the tokens embedding so for example if you look at these two sentences cat sat on the mat cat sat on the mat the token embedding for cat in both is the same right but as I said for each position there will be a different positional en encoding which will be added to the Token encoding so let's say if the token encoding is X for sentence so let me say so let's say this is sentence number one and let's say this is sentence number two so let's say the token encoding for cat is x in sentence one and for sentence two also it's X because it's the same word but now the positional encoding is different because in the first sentence the cat appears in position number two so there will be some different positional encoding for this position and for uh for the second sentence the cat appears in position number five so there will be some different positional encoding so the final embedding uh will be the addition of the token embedding plus the positional embedding so for sentence one the final Vector embedding for CAT will be x + y for sentence two the final Vector embedding for CAT will be X Plus Zed so they are different now see they are not the same so cat won't be embedded in the same manner in both the sentences and so some information about its Position will be incorporated that's exactly what happens in absolute position embedding and there is a nice figure to Showcase this so let's say uh we have four tokens in the input sentence so let's say the first token is the the second is cat uh the third is sat and the fourth is mat so what I've have shown over here is that these are the token embeddings so for the word the this is a three-dimensional token embedding for cat it's the same threedimensional token embedding for sat it's the same and for mat it's the same ideally the vector embeddings for every word will be different but I'm just showing this for the sake of Simplicity remember the vector embeddings for each word are ideally are actually different but I just want to uh show a visual demonstration here so these are the vector embeddings now what we will do is that for each Vector embedding We'll add a positional embedding and these positional embeddings are different based on the position so for example the comes in position number one right so the positional embeddings are 1.1 1.2 and 1.3 cat comes in the second position so the positional embeddings are 2.1 2.2 2.3 sat comes in the third position so the positional embeddings are 3.1 3.2 3.3 Matt comes in the fourth position so the positional embeddings are 4.1 4.2 and 4.3 again these numbers I'm just showing for representative purpose now if you add the token embedding to the position embedding you'll see that although the token embeddings are the same for these words the final embeddings which are also called as the input embeddings which will be fed as input to the llm they are different because the positional embeddings which are added to each token embedding is different so for the word the the final input embedding is 2.1 2.2 2.3 for the word Cat the final input em embedding is 3.1 3.2 3.3 and similarly for ma the final input embedding is 5.1 5.2 and 5.3 this is the intuitive idea behind absolute positional embeddings we have token embeddings and then we add positional embeddings for different positions so finally we get an input embedding which encodes positional information that's it uh one more key thing to remember here is that the positional vectors have the same Dimension as the original token embeddings can you think why this is the case why should the positional embeddings have the same Dimension as the token embeddings pause for a moment to think about this okay so the reason the positional embeddings should have the same Dimension as the original token embeddings is because we want to add them together right and this addition would be difficult if the dimension of the positional embedding is different so for example here if this positional embedding was a four dimension thing adding this four dimensional Vector to a three-dimensional Vector would not be possible so the in absolute positional embedding the position vectors have the same Dimension as the original token embeddings uh so this is uh absolute positional embedding the second type of positional embedding is called as relative positional embedding and this is also a very interesting way of encoding positional information so in this type of embedding the emphasis is on the relative position or the distance between the tokens so the model essentially learns the relationships in terms of how far apart rather than at which exact position so that's a bit different than the absolute embedding right for each position in absolute embedding there is a separate positional embedding but in relative positional embedding what we care about is how far apart different words are rather than their exact position now you must be thinking where exactly is relative uh positional encoding important so relative positional encoding is actually very important because the model in this case can generalize better to sequence of varing lens even if it has not seen such lenss during training so for example in in absolute positional in coding if you train with a sequence length of five and if in the test you have a sequence length of six it's very difficult for the absolute position encoding to know what to do because it has trained for five positions and five positional embeddings but in this case of relative positional embedding uh the model can generalize better to sequence of varing lens because even if it gets some random length during uh the testing phase the absolute position does not matter anyways all that matters is the relative position between different words can you try to think of an example where relative positioning might be more important than absolute positioning okay so it turns out that relative positionings are actually better if the sequences are very long and if longer paragraphs or longer input sequences need to be analyzed because then we need to know the relationship between how different words are connected rather than the exact specific position I'll come to the uh advantages and disadvantages of both of these in a moment okay so in the next uh in the next section which is section number five I want to discuss about uh just these two types of of encodings and which one to use in practice which one does GPT uses Etc so both of these type of positional embeddings which are absolute embedding and Rel relative positional embedding are very good because they enable the llms let me uh switch color they enable the large language models to understand the order and relationship between the tokens and this actually ensures more accurate and context aware predictions so whichever position encoding you use it is actually much better than not using any positional encoding because uh it makes the llm more aware of the order and relationship between tokens and that actually leads to better predictions um now let's come to point number six so the choice between the two types of uh positional embedding really depends on the specific application and the nature of the data being processed so for example generally uh abs abolute positional encoding is preferred when the fixed order of tokens is crucial such as for sequence generation so GPT was trained using an absolute positional encoding and the original Transformer paper was also trained using absolute positional encoding relative positional encoding on the other hand is suitable for tasks like language modeling or long sequences where the same phrase can appear in different parts of the sequence so generally relative positional encoding is useful if you are analyzing long sequences and where the same phrase can repeat over and over again for all practical purposes I would say absolute positional encoding is the one which is used more commonly in fact as I mentioned uh open a GPT models so gpt3 GPT 4 Etc use absolute positional embeddings that are optimized during the training process so one thing to remember here is that similar to the embedding vectors we do not know what values of the positional embedding to be used so for example here I randomly showed these values 1.1 1.2 and 1.3 right ideally as I mentioned in the previous lecture the vector embeddings need to be optimized right we need to know the weights for each Vector which what's the value for each Vector similarly uh when GPT was developed even the values of the positional embedding vectors need to be op optimized and these positional embedding vectors Vector values are actually optimized during the training process this optimization is actually a part of the training process itself so when you look at the model training for GPT we also have to optimize for the token embeddings and we also have to optimize for the positional embeddings we do not know about these values before uh remember I mentioned that along with GPT the original Transformer paper which is called as attention is all you need so let me show you this paper yeah this paper so they also used an absolute positional encoding and in this case they actually propos some formula for how to encode the different positions so they use sinusoidal and cosine formula over here uh so you can read a bit about what they have written here since our model contains no recurrence in order for the model to make use of the order of the sequence we must inject some information about the relative or absolute position of the to tokens so they have added absolute oppositional embedding and they have used some formula for how to actually calculate the positional embedding for each token uh on the contrary when GPT was trained no such formula was used and uh the positional embedding Vector values were actually optimized during the training process itself I hope everyone is with me until this point because now we are going to jump into a handson demonstration of uh looking at a very uh real life example such as gpt2 looking at the vocabulary transforming that vocabulary into token embeddings adding positional embeddings to those token embeddings and then generating the input embeddings which are the final input to the llm training so you might be noticing that I'm using the words encoding and embedding interchangeably so sometimes positional encodings are also called positional embed edings so these words are used interchangeably all you need to remember is that when someone says encoding or embedding it's usually a vector in higher dimensional space okay so now let's come to the next part in the next part what we are going to do is we are going to implement positional embeddings in a Hands-On manner so I'm going to take you to the Jupiter notebook now very similar to how we have been doing in the previous lectures okay so previously uh especially in the last lecture we focused on very small embedding sizes in this chapter for illustration purposes now we are going to consider much more realistic and useful embedding sizes and encode the input tokens into a 256 dimensional Vector space so remember I mentioned that in token embedding you have every word and that word is projected into higher dimensional Vector space usually that Dimension is very high so gpt3 I think was trained on a vector space of around 256 or even more Dimensions uh so we are going to consider a vector space of that size right now for demonstration uh as is written here this is smaller than what the original gpt3 model used so in gpt3 the embedding size is actually 1 12288 Dimensions so it's nowhere close to 256 but it's fine let me actually ask G uh Chad GPT what is the um Vector embedding size for gpt2 for gpt2 for one of their smallest model I think it is around 256 so let me ask chat GPT what is the vector embedding size for gpt2 so if I ask this to chat GPT you will see that uh uh for the for all their models it actually starts from 768 so the value of 256 we are using is three times smaller but it's fine it's at least the same order of magnitude so okay we are going to encode the input tokens into 256 dimensional Vector representation and then we are going to assume that the token IDs were created by the bite pair encoder tokenizer which has a vocabulary size of 50257 let's check this uh what's the vocabulary size of gpt2 pre trining so let's see the vocabulary size is 502 25 7 using the bite pair encoder this is exactly what we are going to use right now in today's Hands-On example so as I mentioned before to actually create the embedding layer uh or this is also called as the uh token embedding Matrix you need two you need two quantities or two variables the first you need the vector Dimension so basically every token ID in the vocabulary will be converted to a vector of these many dimensions and right now we are going to use uh 25 6 over here uh 256 and the second thing which we need is the vocabulary size and the vocabulary size in our case is as I've already mentioned here it's 50257 this means that there are 50257 token IDs and each of these token IDs will be transformed into a 256 dimensional Vector awesome so now what we are going to do is that we have defined the vocabulary size to be 0257 we have defined the output Dimension which is the vector size as 256 and then we are going to create a token embedding layer using torch. nn. embedding so torch. nn. embedding actually creates this kind of an embedding layer provide provided we have those two inputs which is the vocabulary size and second is the vector length which we want so I I'll put the link of this in the chat or in the information section of the YouTube video as well so when you run this you will see that the token embedding layer has been created and it takes two inputs the vocabulary size and the output Dimension awesome so now once the token embedding layer is created we need to uh remember the token embedding layer is is essentially a lookup Matrix where if you give it uh the IDS so if you have a token embedding layer and if you provide the token embedding layer with input IDs which you want to look look for it will give you the responding embedding Vector so for example uh this is the token embedding Matrix right if you and this is a lookup table why is it a lookup table because if you pass in the input ID it looks up that particular row and it gives you the vector associated with that so now we need to create the input IDs so that we can generate the vector embeddings or the token embeddings for those inputs and to create the input IDs we are going to use something called as data loader so we looked at data loader in the in one of the previous lectures so what we are going to do here is that uh we are going to have a batch and that batch size will be equal to 8 and we are going to use a context size of four which means that the maximum input length is four which means four tokens at a time can be passed as inputs that's also called as the context size so uh let me actually show this to you in pictorial format so that it's easier for you to visualize okay okay so this is the input which we are going to create uh we are going to divide the data into batches and the batch size will be equal to 8 so the parameters will be updated after processing every eight batches right now we are not looking at parameter updation at all I just want you to be aware of the dimensions so the input which we will be looking at one time is a batch of eight so there are eight rows over here and each row corresponds to one input sequence so if you look at the first row these will be four tokens if you look at the second row these will be four tokens so the first row when I say four tokens these are four token IDs so remember the goal is to look at these four token IDs and to predict the next word so you can think of each row as an input to the llm and each row consists of four token IDs now our goal is to transform each of these token IDs into a uh 256 dimensional vector right so if you look at the first row the first row has uh four token IDs and we want to transform each into a 256 dimensional Vector so before coming to that let me show you how the inputs are initialized and how from the data loader so you so I have already defined a data loader in this jupyter notebook and you will have access to that when I share the code file with you if you have not seen the data loader lecture I highly encourage you to go to that but if not it's fine I'll try to give you an intuition of what's Happening Here essentially what we are doing here is that we are looking at the raw text and this raw text is actually the verdict so this is a book called The Verdict and uh let me actually refresh this so that it appears in a better format yeah so this is actually a book called The Verdict and this is the main text which we are using as sample text for uh for these set of lectures so what's happening is that we are taking this raw text and uh we are chunking it into batches so each each batch is of size eight and uh for each batch we are looking at a max length of four so only four input tokens will be used to predict the next World so think of this Matrix which I showed you over here so we are going to create a data loader which takes in the Raw text which is a batch size of eight which is a max length which are the number of essentially columns which is the context length which is equal to four and then a stride of four and Shuffle equal to false so stride basically means that let's say if you look at this text right and if you want to uh create inputs so the first input will be I had always thought because the context size is four now since stride is equal to four the next input will be Jack gisburn rather a if the stride was equal to one the next input would be had always thought Jack but now the stride is equal to four so after one in input we'll Skip One 2 3 four times and then give the next input so what we are essentially doing is creating inputs so the batch size is eight right so the first batch will have this as the first row of input this as the second row of input this as the third row of input and so on up till eight eight batches and each of these tokens uh are converted into token IDs and then what we want to do is map each of those token IDs into vectors that's what we are doing exactly through this data loader data loader just helps us to uh manage the task of inputting the data batching the data creating different batches parallel processing much easier so it's highly recommended to use data loader I'll actually just show you the data loader so it's this link data sets and data loaders in Python uh it's actually highly useful when dealing with large language models okay so once we Define a data loader like this we just iterate through the data loader and we can get inputs and targets so if you print out the token IDs so this is the input of a batch and if you see this exactly similar to what we had written on the Whiteboard so if you look at one batch it will have uh uh eight input sequences and each input sequence has four token IDs or four tokens and using these we want to predict the next word for each input sequence so this is the batch of inputs which we have received and now what we actually want to do is for each of these input token IDs we want to convert each of these into a 256 uh dimensional Vector representation but first let's look at this token ID tensor and it's a 8x4 tensor because it has eight rows and four columns uh the data badge consists of eight text samples with four tokens each awesome now what we are going to do is that we are going to uh convert each of these token IDs into a 256 dimensional uh Vector using this embedding layer so as I told you what this embedding layer is is it's actually a lookup table right so if you look at this embedding layer and if you give the token ID it will generate the or it will fetch the corresponding Vector representation for you that's exactly what we are going to do over here so these are the inputs right now let's say I'm looking at one batch uh which has eight rows and which has four columns and I have given the input IDs as randomly assigned over here right now but let's say these are the input IDs so for the first first batch the input IDs are 10 8 uh 20 and uh 21 now what is done is that these input IDs are then mapped to the embedding Vector Matrix so we we know the corresponding row so if you look at the first row it's 10 8 10 and 20 these are the input IDs right so then here you look for the input ID of 10 and then if you find then you find the corresponding Vector for it then you look for the input ID of8 and then you find the corresponding Vector for it then you look at the input ID of 20 and then you find the corresponding Vector for it and then you look at the input ID of 21 and then you find the corresponding Vector for it so basically for each token in this input uh input batch one embedding Vector of size 256 is generated for each token in the input so I just constructed another visualization for you all to see it further so if this is the input batch for each token ID here it's converted into a 256 dimensional Vector so if the size of this original input batch was 8x4 when we generate the embedding vectors for each of this the we will get a tensor which is 8x 4 by 256 so it's a threedimensional tensor now why 256 because for each of these 8x4 32 values for each of these 32 values we have a 256 dimensional tensor so for each of these you can think of like a vector which has basically 256 Dimensions so there is a vector here Vector here Vector here Vector here Etc I cannot show the three-dimensional structure right now but it's basically Ally 8X 4X 256 uh okay so that's what we are going to do next so we have this inputs right which is the batch of inputs generated from the data loader we are just going to pass it as an argument to the lookup table to the embedding layer and then we are going to get this tensor which is 8x 4X 256 which is exactly the tensor shape which I was showing to you over here that's the 8x4 by 256 tensor after we pass the input ID to this uh lookup table of the embedding layer basically what this lookup table does is that it generates the 256 dimensional Vector for each of these token IDs up till now what we have done is that we have broken down the input text into batches so let's say we so this is the first first batch this this is the first this is the uh first input of the first batch remember each batch has eight inputs so this is the first input we converted into token IDs and then each of the token ID is have has a 256 or 256 Vector length uh embedding that's what we have done until now it's as simple as that I hope everyone is following until now I'm trying to explain this vectorial and tensorial notation as uh in detail as possible but of course sometimes it gets a bit difficult so if you have any questions you can ask it in the comment section so uh here I have written that as we can tell based on the 8X 4X 256 dimensional tensor output each token ID is now embedded as a 256 dimensional Vector awesome and now we have what we have to do is that we have to add a positional embedding uh positional embedding Vector to each of these vectors similar to the uh token embedding we also have to create another embedding layer for the positional encoding so remember that at each time only four vectors need to be processed right because if you look at the context size that is equal to four at one time in the input only four tokens are going to be given so at one time only the input the maximum input size is four which means the llm is going to predict the next word based on a maximum of four tokens so actually we need to encode only four positions in this case so the embedding layer size will have one 2 3 4 rows and the number of columns will be the vector Dimension which is 256 that is fine because for every position we are going to have a 256 dimensional Vector remember we have to add this Vector to the Token embedding Vector so the size should be same the size here was 256 right every uh token which was embedded had a size 256 so here also for every position we need a 256 dimensional Vector but there are only four positions right it can either be the first token the second token third token or fourth token so the number of rows when we create the embedding layer for positional encoding is going to be four which is the context length so now let us write that in the code so remember when we created the token embedding the number of rows was the vocabulary size but here the number of rows is going to be equal to the context length so now we are creating a embedding layer for the positional embedding number of rows is equal to the context length and the number of columns is output Dimension which is 256 because each Vector needs to have a size of 256 great so we have created the positional embedding layer right now and now I'm going to uh visually try to explain how we are going to add or how we are going to essentially create the positional embedding vectors so let's look at our input Matrix again here the batch size is eight so we have eight rows and the context length is four which that's why we have four columns so if you look at each input sequence let's look at the first row if you look at the first row it has four token IDs let's say those token IDs are 10 8 20 and 21 each of these token ID is now a 256 uh Vector length Vector because we have done the embedding we have done the token embedding so each token ID is a 256 dimensional Vector so this is the first batch of input you can see that there are four positions here maybe these words are the cat sat on now what we need to do is that we need to add one one uh positional encoding Vector for each of these to this uh to the Token embedding for the 10 for the token ID 10 we have to add a positional uh embedding Vector to a token ID of 8 we have to add another Vector to a token ID of 20 we need to add positional Vector to a token ID of 21 we need to add another positional embedding Vector so uh we need to add one position Vector to each of these four token embeddings and remember that the same positional embeddings are applied uh because there are only four positions right so uh the positional embedding we just need to do it uh once and then for every token or for every input sequence the same four positional embeddings can be applied so for example this is the batch one right if you look at this batch uh which is row number three let's say there are some input IDs like one um two five and six let's say these are the token IDs now whatever positional positional embedding we added to the first input the same positional embedding can be added to this input because the positions are the same either it's position one 2 3 or four we just need to encode the different positions right so that's why uh the positional encoding size or the positional embedding size uh has to be 4X 250 56 we only need four positional vectors four positional embedding vectors and then each Vector will of course have size 256 so then the positional embedding Vector Matrix which we have will be a 4X 256 Matrix why do we only need four why not have a separate positional uh embedding for each of the inputs here because for each of these inputs we only want to encode whether the token ID is in the first position second third or fourth so we only need the positional uh we only need four positional embedding vectors one one for the first position one for the second one for the third one for the fourth and then we can add the same four to basically all the input sequences in a given batch that's what we are going to do so in the next step which is the step number 13 we are going to generate the four positional embedding vectors from the positional embedding Matrix so as I told you all embedding Matrix are essentially lookup tables so to generate the embedding vectors we just need to pass these positions 0 1 2 and 3 and then it will generate the corresponding vectors according to that so how to pass the positions you just use tor. arranged max length what torch. arrange max length will do is that it will create 0 1 and 1 2 and three so max length is equal to four so torch. arrange will create a sequence of number 0 1 up to Max input length minus 1 so this will be 0 1 2 3 so essentially it will create uh the token ID 0 1 2 and 3 and then we can just look up the positional embedding table and generate these four positional embedding vectors so then what we do is that as I said we just look up the positional embedding layer Matrix uh which is a lookup table and just pass in these four arguments 0 1 2 3 so then it generates four vectors each of size 256 these are the four positional embedding vectors which we need one is for position number one second is for position number two third is for position number three and fourth is for position number four so as we can see the positional embedding tensor consists of four 256 dimensional vectors we can now add this directly to the Token embedding uh so let's see how that is done uh let's look at one batch for now and then uh let's see how to add the token embeddings with the position embeddings so we have generated these four positional embedding vectors from the positional embedding Matrix great so we have completed step number 13 and so finally we come to step number 14 this is the last step where we have to add the position embeddings to the Token embeddings so if you look at the token embedding matrix it's 8X 4X 256 as we had already seen before so for each token ID there is a 256 dimensional vector and how many token IDs are there there are eight batches and each batch has four token IDs so 8x4 that's why the size is 8x4 by 256 and then the positional embedding we just have four 256 256 256 256 so this is just 4X 256 so for the first position we have a 256 Dimension Vector for the second position we have a 256 Dimension Vector for the third position we have a 256 Dimension vector and for the fourth position we have a 256 Dimension Vector so now we are adding the token embeddings with the positional embeddings so you must be thinking this is 8x 4X 256 this is 4X 256 how does python really add them so when you add such matrixes matrices what happens is a broadcasting operation so what python does is that it converts this 4X 256 to 8x4 by 256 by duplicating these same values eight times so then what is essentially happening is that to the first row these four values are added to to the second row these same four values are added to the third row these same four values are added similarly to the eighth Row the same four values are added so finally the input embeddings which are the result of the token embeddings plus the positional embeddings have the size of 8X 4X 256 and these are the input embeddings which then will be the final training input to the llms so we did so many things to reach this stage right but I hope you have understood this part I just wanted to show you for gpt2 we had a vocabulary size of around 50,000 and I showed you a vector length of 256 so first I showed you how to create the 8X 4X 256 token embedding uh Matrix in the first place so our initial task was for you to understand uh how is this token embedding itself created so every token ID in a batch is converted into a vector of size 256 and then for each position we add uh a positional uh positional embedding so first we need to see how many positions are there and for that the parameter which becomes important is the context length because that's the maximum length of the input to be fed at any time to the llm so only those many positions are important so the context length is four so we then create so we only need four positional embedding vectors one for position one second for position two third for position three and fourth for the position number four and then we add it to the toer embedding Matrix how do we add it python does a broadcasting so even if the token embedding Matrix is 8x 4X 256 and even if the positional embedding is just 4X 256 it just copies these uh these four values eight times uh so essentially what happens is that to each row of the token embedding the same positional embedding four vectors are added and that's how we get the input embeddings 8X 4X 256 I hope you have understood these Dimensions so eight because in each batch we have eight input sequences and in each input sequence we have four tokens that's why four and why 256 because each token ID or each token is essentially a 256 length Vector I hope everyone has understood this lecture on positional positional embedding now let me go back to the start where we looked at what all we have covered and what needs to be the input to the llm so look at this diagram um so in today's lecture we actually looked at one more step which is I would say maybe step 3.5 in that case and that is adding positional embeddings to the token embeddings and that finally uh leads to uh input embeddings so these input embeddings which we obtained so actually let me add write here positional embedding so what we actually added to the Token embeddings was positional embeddings so uh so what we added here was yeah positional embeddings and then this resulted into input embeddings so token embeddings plus positional embeddings is input embeddings and then these are the ones which are actually used as input to the GPT so essentially what we did in token embedding and positional embedding is we try to exploit as many things in textual language as possible the first thing which we exploited semantic meaning when we did token embeddings when we did positional embeddings we exploited the fact that the different positions also mean something until now we have not seen how exactly to obtain the values in the positional embedding see even in today's lecture we just randomly initialized uh the embedding Matrix right like if you see in the code this positional embedding layer is randomly initialized so what what tor. nn. embedding does is that it initializes a matrix with number of rows as context length number of columns as output Dimension and it puts random values in this so then how do we know the actual values so that is actually a part of the llm training process and it's exactly similar for the uh token embeddings as well so we need to optimize the values in the token embedding layer and we need to optimize the value in the positional embedding layer however for us to reach the optimization stage stage first it's very important for you to understand what exactly is positional embedding what exactly is token embedding and that was the whole purpose of today's lecture uh this brings us to the end of today's lecture thank you so much everyone I hope you understood a lot I hope you understood the difference between absolute um absolute and positional embedding yeah absolute and relative sorry absolute and relative positional embedding I hope you understood uh why positional embeddings are added and most importantly I hope you understood the dimensions ultimately I feel it all comes down to Dimensions people who understand the dimensions really don't feel scared or intimidated by the subject so if someone understands where this 8X 4X 256 is actually coming from right this 8x4 by 256 then I feel they will have a much stronger grasp on the subject so that's why today I spent a lot of time on explaining these Dimensions um thank you so much everyone and if you have any doubts or questions please put it in the comment section the lectures are getting bit more involved and detailed now so I'll be happy to interact in the comment section and solve any doubts or questions also let me know if you're liking this teaching style which is a mix of the Whiteboard lectures uh plus the Hands-On coding I'll of course be sharing the code file with all of you thanks everyone and I'll see you in the next lecture"
}