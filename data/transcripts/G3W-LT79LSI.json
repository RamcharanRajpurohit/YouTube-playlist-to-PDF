{
  "video": {
    "video_id": "G3W-LT79LSI",
    "title": "Lecture 20: Layer Normalization in the LLM Architecture",
    "duration": 2337.0,
    "index": 19
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.04
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.24,
      "duration": 4.96
    },
    {
      "text": "in the build large language models from",
      "start": 8.04,
      "duration": 5.559
    },
    {
      "text": "scratch Series in today's lecture we are",
      "start": 10.2,
      "duration": 5.88
    },
    {
      "text": "going to look at one specific component",
      "start": 13.599,
      "duration": 4.881
    },
    {
      "text": "of the large language model architecture",
      "start": 16.08,
      "duration": 4.279
    },
    {
      "text": "and that is layer",
      "start": 18.48,
      "duration": 4.16
    },
    {
      "text": "normalization before that let me quickly",
      "start": 20.359,
      "duration": 3.92
    },
    {
      "text": "recap what we covered in the last",
      "start": 22.64,
      "duration": 3.44
    },
    {
      "text": "lecture in the last lecture we",
      "start": 24.279,
      "duration": 4.361
    },
    {
      "text": "essentially had a bird ey view of how",
      "start": 26.08,
      "duration": 6.159
    },
    {
      "text": "the llm architecture looks like so what",
      "start": 28.64,
      "duration": 5.32
    },
    {
      "text": "we are going to do is that the llm",
      "start": 32.239,
      "duration": 3.601
    },
    {
      "text": "architecture consists of several",
      "start": 33.96,
      "duration": 4.64
    },
    {
      "text": "components which are arranged together",
      "start": 35.84,
      "duration": 4.68
    },
    {
      "text": "uh yes uh in the previous lecture we",
      "start": 38.6,
      "duration": 4.279
    },
    {
      "text": "constructed this GPT backbone which is",
      "start": 40.52,
      "duration": 4.28
    },
    {
      "text": "like a dummy class which will hold all",
      "start": 42.879,
      "duration": 4.401
    },
    {
      "text": "these components the first component is",
      "start": 44.8,
      "duration": 5.72
    },
    {
      "text": "the layer normalization Jou activation",
      "start": 47.28,
      "duration": 5.279
    },
    {
      "text": "then the feed forward neural network and",
      "start": 50.52,
      "duration": 4.44
    },
    {
      "text": "then shortcut connections these four",
      "start": 52.559,
      "duration": 5.16
    },
    {
      "text": "components make up the Transformer block",
      "start": 54.96,
      "duration": 4.559
    },
    {
      "text": "which is one of the key components of",
      "start": 57.719,
      "duration": 3.64
    },
    {
      "text": "the entire GPT",
      "start": 59.519,
      "duration": 4.321
    },
    {
      "text": "architecture so let me show you how this",
      "start": 61.359,
      "duration": 4.961
    },
    {
      "text": "looks like so if you zoom into the",
      "start": 63.84,
      "duration": 4.4
    },
    {
      "text": "Transformer block you'll see that the",
      "start": 66.32,
      "duration": 4.119
    },
    {
      "text": "Transformer block has a number of layers",
      "start": 68.24,
      "duration": 4.48
    },
    {
      "text": "which are stacked together you have a",
      "start": 70.439,
      "duration": 5.081
    },
    {
      "text": "layer normalization followed by The Mask",
      "start": 72.72,
      "duration": 4.92
    },
    {
      "text": "multi-head attention layer followed by",
      "start": 75.52,
      "duration": 4.599
    },
    {
      "text": "the Dropout layer you have the shortcut",
      "start": 77.64,
      "duration": 4.159
    },
    {
      "text": "connections then another layer",
      "start": 80.119,
      "duration": 3.761
    },
    {
      "text": "normalization a feed forward neural",
      "start": 81.799,
      "duration": 4.481
    },
    {
      "text": "network Dropout layer and then another",
      "start": 83.88,
      "duration": 5.04
    },
    {
      "text": "shortcut connections Etc if you zoom",
      "start": 86.28,
      "duration": 4.799
    },
    {
      "text": "into the feed for neural network you'll",
      "start": 88.92,
      "duration": 5.04
    },
    {
      "text": "see that it it consists of linear neural",
      "start": 91.079,
      "duration": 4.761
    },
    {
      "text": "network layers along with the Jou",
      "start": 93.96,
      "duration": 3.479
    },
    {
      "text": "activation",
      "start": 95.84,
      "duration": 4.279
    },
    {
      "text": "function uh all of these are stacked",
      "start": 97.439,
      "duration": 4.241
    },
    {
      "text": "together in a format which looks",
      "start": 100.119,
      "duration": 3.801
    },
    {
      "text": "something like this so let me take you",
      "start": 101.68,
      "duration": 4.88
    },
    {
      "text": "to the broadest level view looks",
      "start": 103.92,
      "duration": 4.6
    },
    {
      "text": "something like this so you have the",
      "start": 106.56,
      "duration": 4.599
    },
    {
      "text": "input text which you tokenize then you",
      "start": 108.52,
      "duration": 5.76
    },
    {
      "text": "pass it into the GPT model in the GPT",
      "start": 111.159,
      "duration": 5.28
    },
    {
      "text": "model we first do the token embedding",
      "start": 114.28,
      "duration": 3.64
    },
    {
      "text": "then add the positional embedding",
      "start": 116.439,
      "duration": 4.64
    },
    {
      "text": "vectors pass these input embeddings to",
      "start": 117.92,
      "duration": 5.0
    },
    {
      "text": "the Transformer block within the",
      "start": 121.079,
      "duration": 3.481
    },
    {
      "text": "Transformer block it goes through all",
      "start": 122.92,
      "duration": 3.76
    },
    {
      "text": "the layers which I just showed you and",
      "start": 124.56,
      "duration": 4.52
    },
    {
      "text": "then we have the output layers which",
      "start": 126.68,
      "duration": 4.559
    },
    {
      "text": "essentially decode the output from the",
      "start": 129.08,
      "duration": 3.92
    },
    {
      "text": "Transformer block and we predict the",
      "start": 131.239,
      "duration": 4.481
    },
    {
      "text": "next word from the input this is what",
      "start": 133.0,
      "duration": 6.72
    },
    {
      "text": "happens inside the GPT architecture and",
      "start": 135.72,
      "duration": 5.68
    },
    {
      "text": "uh we are going to start looking at",
      "start": 139.72,
      "duration": 3.76
    },
    {
      "text": "every single component of this GPT",
      "start": 141.4,
      "duration": 4.16
    },
    {
      "text": "architecture today we are going to look",
      "start": 143.48,
      "duration": 3.72
    },
    {
      "text": "at this component which is called as",
      "start": 145.56,
      "duration": 2.84
    },
    {
      "text": "layer",
      "start": 147.2,
      "duration": 3.6
    },
    {
      "text": "normalization so if you look at layer",
      "start": 148.4,
      "duration": 4.44
    },
    {
      "text": "normalization it actually comes up at",
      "start": 150.8,
      "duration": 4.6
    },
    {
      "text": "multiple places within the Transformer",
      "start": 152.84,
      "duration": 5.039
    },
    {
      "text": "block itself if you see before the",
      "start": 155.4,
      "duration": 4.479
    },
    {
      "text": "multi-head attention we have a layer",
      "start": 157.879,
      "duration": 3.961
    },
    {
      "text": "normalization after the multi-head",
      "start": 159.879,
      "duration": 3.72
    },
    {
      "text": "attention also we have a layer",
      "start": 161.84,
      "duration": 4.84
    },
    {
      "text": "normalization so the layer normalization",
      "start": 163.599,
      "duration": 4.961
    },
    {
      "text": "comes within the Transformer block at",
      "start": 166.68,
      "duration": 4.279
    },
    {
      "text": "multiple places and actually it also",
      "start": 168.56,
      "duration": 5.52
    },
    {
      "text": "comes outside of the Transformer block",
      "start": 170.959,
      "duration": 5.041
    },
    {
      "text": "um that's why it's important to define a",
      "start": 174.08,
      "duration": 3.4
    },
    {
      "text": "separate class for the layer",
      "start": 176.0,
      "duration": 3.44
    },
    {
      "text": "normalization when we code the GPT",
      "start": 177.48,
      "duration": 4.759
    },
    {
      "text": "architecture lecture in the last lecture",
      "start": 179.44,
      "duration": 7.04
    },
    {
      "text": "we looked at this dummy GPT model class",
      "start": 182.239,
      "duration": 6.961
    },
    {
      "text": "and the goal here was to put all of the",
      "start": 186.48,
      "duration": 4.399
    },
    {
      "text": "different components together so that",
      "start": 189.2,
      "duration": 3.44
    },
    {
      "text": "you get a bird's eye view Without Really",
      "start": 190.879,
      "duration": 4.601
    },
    {
      "text": "coding too many of these components so",
      "start": 192.64,
      "duration": 4.959
    },
    {
      "text": "what happens in this GPT model class is",
      "start": 195.48,
      "duration": 4.16
    },
    {
      "text": "that there is this forward method which",
      "start": 197.599,
      "duration": 5.72
    },
    {
      "text": "takes in the input and then we uh",
      "start": 199.64,
      "duration": 5.959
    },
    {
      "text": "convert the input into token embeddings",
      "start": 203.319,
      "duration": 4.161
    },
    {
      "text": "we add the positional embeddings to the",
      "start": 205.599,
      "duration": 4.401
    },
    {
      "text": "Token embeddings the resulting embedding",
      "start": 207.48,
      "duration": 4.8
    },
    {
      "text": "Vector we add the Dropout layer to it we",
      "start": 210.0,
      "duration": 4.799
    },
    {
      "text": "pass it through the Transformer blocks",
      "start": 212.28,
      "duration": 5.039
    },
    {
      "text": "then even after it comes out from the",
      "start": 214.799,
      "duration": 4.321
    },
    {
      "text": "Transformer we have a normalization",
      "start": 217.319,
      "duration": 3.721
    },
    {
      "text": "layer as I showed you within the",
      "start": 219.12,
      "duration": 3.64
    },
    {
      "text": "Transformer block itself there is a",
      "start": 221.04,
      "duration": 3.64
    },
    {
      "text": "normalization layer before the attention",
      "start": 222.76,
      "duration": 4.199
    },
    {
      "text": "head and after the attention head and",
      "start": 224.68,
      "duration": 4.119
    },
    {
      "text": "after we come out of the Transformer",
      "start": 226.959,
      "duration": 3.48
    },
    {
      "text": "block there is another normalization",
      "start": 228.799,
      "duration": 3.601
    },
    {
      "text": "layer and then there are output",
      "start": 230.439,
      "duration": 4.121
    },
    {
      "text": "processing steps and the output is",
      "start": 232.4,
      "duration": 4.96
    },
    {
      "text": "returned right so to execute this",
      "start": 234.56,
      "duration": 4.92
    },
    {
      "text": "forward method we need to Define several",
      "start": 237.36,
      "duration": 2.92
    },
    {
      "text": "things",
      "start": 239.48,
      "duration": 3.2
    },
    {
      "text": "so when an instance of the dummy GPT",
      "start": 240.28,
      "duration": 5.799
    },
    {
      "text": "model class is created we uh we Define",
      "start": 242.68,
      "duration": 5.279
    },
    {
      "text": "these Transformer blocks and for that",
      "start": 246.079,
      "duration": 3.841
    },
    {
      "text": "we'll need to create a separate class",
      "start": 247.959,
      "duration": 4.12
    },
    {
      "text": "called Demi Transformer block which will",
      "start": 249.92,
      "duration": 4.239
    },
    {
      "text": "ultimately become just Transformer block",
      "start": 252.079,
      "duration": 4.041
    },
    {
      "text": "when we learn about what happens in the",
      "start": 254.159,
      "duration": 4.6
    },
    {
      "text": "Transformer and then we also create a",
      "start": 256.12,
      "duration": 5.28
    },
    {
      "text": "separate class for layer normalization",
      "start": 258.759,
      "duration": 4.361
    },
    {
      "text": "why do we create a separate class",
      "start": 261.4,
      "duration": 3.72
    },
    {
      "text": "because layer normalization not just",
      "start": 263.12,
      "duration": 4.079
    },
    {
      "text": "happens in the Transformer otherwise we",
      "start": 265.12,
      "duration": 4.079
    },
    {
      "text": "would have just written this procedure",
      "start": 267.199,
      "duration": 4.681
    },
    {
      "text": "here but as you can see even after the",
      "start": 269.199,
      "duration": 4.681
    },
    {
      "text": "input comes from the Transformer we pass",
      "start": 271.88,
      "duration": 3.879
    },
    {
      "text": "it through another layer of",
      "start": 273.88,
      "duration": 3.879
    },
    {
      "text": "normalization so that's why it makes",
      "start": 275.759,
      "duration": 3.681
    },
    {
      "text": "sense to define a separate class of",
      "start": 277.759,
      "duration": 4.28
    },
    {
      "text": "layer normalization in today's lecture",
      "start": 279.44,
      "duration": 4.52
    },
    {
      "text": "we are going to dive deep into this",
      "start": 282.039,
      "duration": 4.121
    },
    {
      "text": "class how to create this class what",
      "start": 283.96,
      "duration": 4.72
    },
    {
      "text": "exactly does layer normalization do and",
      "start": 286.16,
      "duration": 4.599
    },
    {
      "text": "why do we need it in the first",
      "start": 288.68,
      "duration": 5.28
    },
    {
      "text": "place so let's get started diving deep",
      "start": 290.759,
      "duration": 5.28
    },
    {
      "text": "into layer",
      "start": 293.96,
      "duration": 4.6
    },
    {
      "text": "normalization okay so the reason layer",
      "start": 296.039,
      "duration": 4.401
    },
    {
      "text": "normalization comes into to the picture",
      "start": 298.56,
      "duration": 4.68
    },
    {
      "text": "is that when we look at the Transformer",
      "start": 300.44,
      "duration": 5.16
    },
    {
      "text": "block as you can see number of these",
      "start": 303.24,
      "duration": 4.32
    },
    {
      "text": "layers are stacked on top of each other",
      "start": 305.6,
      "duration": 3.96
    },
    {
      "text": "right so there are huge number of",
      "start": 307.56,
      "duration": 4.079
    },
    {
      "text": "parameters coming from each of these",
      "start": 309.56,
      "duration": 3.8
    },
    {
      "text": "layers and we have to train these",
      "start": 311.639,
      "duration": 4.241
    },
    {
      "text": "parameters initially we'll train these",
      "start": 313.36,
      "duration": 4.36
    },
    {
      "text": "initialize these parameters to random",
      "start": 315.88,
      "duration": 4.039
    },
    {
      "text": "values but through back propagation",
      "start": 317.72,
      "duration": 4.599
    },
    {
      "text": "we'll train these parameters so that the",
      "start": 319.919,
      "duration": 4.28
    },
    {
      "text": "next word is predicted",
      "start": 322.319,
      "duration": 3.961
    },
    {
      "text": "correctly so we need to make the",
      "start": 324.199,
      "duration": 4.321
    },
    {
      "text": "training process more efficient and",
      "start": 326.28,
      "duration": 3.759
    },
    {
      "text": "that's where layer normaliz ation",
      "start": 328.52,
      "duration": 4.519
    },
    {
      "text": "becomes very important it turns out that",
      "start": 330.039,
      "duration": 5.0
    },
    {
      "text": "training deep neural networks with many",
      "start": 333.039,
      "duration": 4.88
    },
    {
      "text": "layers can be challenging predominant",
      "start": 335.039,
      "duration": 5.72
    },
    {
      "text": "due to two things it can either lead to",
      "start": 337.919,
      "duration": 5.241
    },
    {
      "text": "a Vanishing gradient problem or it can",
      "start": 340.759,
      "duration": 5.16
    },
    {
      "text": "lead to an exploding gradient problem",
      "start": 343.16,
      "duration": 6.319
    },
    {
      "text": "and this leads to unstable training",
      "start": 345.919,
      "duration": 6.361
    },
    {
      "text": "Dynamics so the first advantage of layer",
      "start": 349.479,
      "duration": 5.801
    },
    {
      "text": "normalization is that it improves really",
      "start": 352.28,
      "duration": 5.96
    },
    {
      "text": "the stability and the efficiency of",
      "start": 355.28,
      "duration": 5.639
    },
    {
      "text": "neural network training",
      "start": 358.24,
      "duration": 4.92
    },
    {
      "text": "so let me zoom into this further and",
      "start": 360.919,
      "duration": 4.241
    },
    {
      "text": "explain this if you consider a deep",
      "start": 363.16,
      "duration": 3.96
    },
    {
      "text": "neural network like this you have an",
      "start": 365.16,
      "duration": 4.36
    },
    {
      "text": "input layer and then you have multiple",
      "start": 367.12,
      "duration": 4.28
    },
    {
      "text": "hidden layers which are stacked together",
      "start": 369.52,
      "duration": 4.16
    },
    {
      "text": "after the input layer right and you have",
      "start": 371.4,
      "duration": 5.48
    },
    {
      "text": "the output layer so what happens is that",
      "start": 373.68,
      "duration": 5.6
    },
    {
      "text": "when you do a forward pass so initially",
      "start": 376.88,
      "duration": 4.08
    },
    {
      "text": "when the weights are initialized",
      "start": 379.28,
      "duration": 3.44
    },
    {
      "text": "randomly you do a forward pass and you",
      "start": 380.96,
      "duration": 4.2
    },
    {
      "text": "get the outputs and then what you do is",
      "start": 382.72,
      "duration": 5.0
    },
    {
      "text": "that based on the gradients you do a",
      "start": 385.16,
      "duration": 4.56
    },
    {
      "text": "backward pass and you try to update",
      "start": 387.72,
      "duration": 5.12
    },
    {
      "text": "these parameters So eventually for every",
      "start": 389.72,
      "duration": 4.479
    },
    {
      "text": "of these hidden layers you will have",
      "start": 392.84,
      "duration": 3.28
    },
    {
      "text": "gradients of the loss with respect to",
      "start": 394.199,
      "duration": 4.56
    },
    {
      "text": "the parameters so when you look at every",
      "start": 396.12,
      "duration": 4.84
    },
    {
      "text": "layer think of layers as accumulating",
      "start": 398.759,
      "duration": 4.0
    },
    {
      "text": "gradients every layer will have a",
      "start": 400.96,
      "duration": 4.0
    },
    {
      "text": "certain set of gradient values which",
      "start": 402.759,
      "duration": 4.601
    },
    {
      "text": "will get updated after every iteration",
      "start": 404.96,
      "duration": 4.92
    },
    {
      "text": "right now since we are doing the",
      "start": 407.36,
      "duration": 4.2
    },
    {
      "text": "backward pass let's say if we want to",
      "start": 409.88,
      "duration": 5.08
    },
    {
      "text": "find the gradients of this layer um the",
      "start": 411.56,
      "duration": 5.319
    },
    {
      "text": "gradients of this layer will depend a",
      "start": 414.96,
      "duration": 4.359
    },
    {
      "text": "lot on the output of this layer because",
      "start": 416.879,
      "duration": 4.201
    },
    {
      "text": "we are doing backward pass let me show",
      "start": 419.319,
      "duration": 4.6
    },
    {
      "text": "it with a different color uh yeah so the",
      "start": 421.08,
      "duration": 4.519
    },
    {
      "text": "output of this layers I'm showing with",
      "start": 423.919,
      "duration": 3.921
    },
    {
      "text": "the red color and if you want to find",
      "start": 425.599,
      "duration": 3.88
    },
    {
      "text": "the gradients of the first layer it",
      "start": 427.84,
      "duration": 3.6
    },
    {
      "text": "depends on this red colored outputs",
      "start": 429.479,
      "duration": 4.28
    },
    {
      "text": "since we are doing the backward pass now",
      "start": 431.44,
      "duration": 4.599
    },
    {
      "text": "it turns out that if the layer output if",
      "start": 433.759,
      "duration": 3.641
    },
    {
      "text": "this layer output which which I'm",
      "start": 436.039,
      "duration": 3.401
    },
    {
      "text": "showing by red right now if this is too",
      "start": 437.4,
      "duration": 4.28
    },
    {
      "text": "large or this is too small that affects",
      "start": 439.44,
      "duration": 3.159
    },
    {
      "text": "the",
      "start": 441.68,
      "duration": 3.799
    },
    {
      "text": "gradients so if the layer output is too",
      "start": 442.599,
      "duration": 4.6
    },
    {
      "text": "large or too small then gradient",
      "start": 445.479,
      "duration": 3.761
    },
    {
      "text": "magnitudes can become too large or too",
      "start": 447.199,
      "duration": 2.921
    },
    {
      "text": "small",
      "start": 449.24,
      "duration": 2.44
    },
    {
      "text": "now think about what will happen if",
      "start": 450.12,
      "duration": 3.4
    },
    {
      "text": "gradient magnitudes become too large",
      "start": 451.68,
      "duration": 4.0
    },
    {
      "text": "let's say so if we are back propagating",
      "start": 453.52,
      "duration": 3.799
    },
    {
      "text": "and the gradient magnitude in this layer",
      "start": 455.68,
      "duration": 4.4
    },
    {
      "text": "becomes too large what will happen when",
      "start": 457.319,
      "duration": 4.961
    },
    {
      "text": "you back propagate essentially you are",
      "start": 460.08,
      "duration": 4.119
    },
    {
      "text": "multiplying different gradients together",
      "start": 462.28,
      "duration": 4.16
    },
    {
      "text": "right so if the gradient here becomes",
      "start": 464.199,
      "duration": 4.161
    },
    {
      "text": "too large by the time we reach the first",
      "start": 466.44,
      "duration": 3.719
    },
    {
      "text": "layer the gradient would have exploded",
      "start": 468.36,
      "duration": 4.16
    },
    {
      "text": "to a very large value that's called as",
      "start": 470.159,
      "duration": 3.641
    },
    {
      "text": "the gradient",
      "start": 472.52,
      "duration": 3.44
    },
    {
      "text": "explosion on the other hand if the",
      "start": 473.8,
      "duration": 4.399
    },
    {
      "text": "gradient of the last layer or one of the",
      "start": 475.96,
      "duration": 4.32
    },
    {
      "text": "intermediate layers is very small when",
      "start": 478.199,
      "duration": 4.081
    },
    {
      "text": "you're propagating backwards till the",
      "start": 480.28,
      "duration": 3.56
    },
    {
      "text": "time you reach the first layer or the",
      "start": 482.28,
      "duration": 3.24
    },
    {
      "text": "second layer the gradient will become",
      "start": 483.84,
      "duration": 3.799
    },
    {
      "text": "very small what will happen when the",
      "start": 485.52,
      "duration": 4.28
    },
    {
      "text": "gradient becomes very small we will not",
      "start": 487.639,
      "duration": 4.201
    },
    {
      "text": "update the parameters because the",
      "start": 489.8,
      "duration": 4.32
    },
    {
      "text": "parameter updates depend on the gradient",
      "start": 491.84,
      "duration": 4.68
    },
    {
      "text": "magnitude if the gradient is too small",
      "start": 494.12,
      "duration": 4.799
    },
    {
      "text": "learning will stagnate if the gradient",
      "start": 496.52,
      "duration": 4.519
    },
    {
      "text": "is too large will lead to that will lead",
      "start": 498.919,
      "duration": 4.321
    },
    {
      "text": "to an unstable learning procedure so",
      "start": 501.039,
      "duration": 4.201
    },
    {
      "text": "both small gradients and large gradients",
      "start": 503.24,
      "duration": 5.079
    },
    {
      "text": "lead to unstable training Dynamics we do",
      "start": 505.24,
      "duration": 5.799
    },
    {
      "text": "not want unstable training Dynamics and",
      "start": 508.319,
      "duration": 4.441
    },
    {
      "text": "one reason for the unstable training",
      "start": 511.039,
      "duration": 4.0
    },
    {
      "text": "Dynamics is that if layer outputs",
      "start": 512.76,
      "duration": 4.399
    },
    {
      "text": "themselves are very large or small as I",
      "start": 515.039,
      "duration": 3.841
    },
    {
      "text": "mentioned layer outputs affect the",
      "start": 517.159,
      "duration": 4.281
    },
    {
      "text": "gradient values so if we control the",
      "start": 518.88,
      "duration": 4.519
    },
    {
      "text": "magnitude of the layer outputs we can",
      "start": 521.44,
      "duration": 4.079
    },
    {
      "text": "ensure that the gradient magnitudes",
      "start": 523.399,
      "duration": 3.921
    },
    {
      "text": "themselves do not become too large or",
      "start": 525.519,
      "duration": 5.041
    },
    {
      "text": "too small batch normalization helps this",
      "start": 527.32,
      "duration": 6.44
    },
    {
      "text": "batch normalization helps uh keep the",
      "start": 530.56,
      "duration": 4.8
    },
    {
      "text": "outputs of the layers to certain",
      "start": 533.76,
      "duration": 3.4
    },
    {
      "text": "specific values and prevents the",
      "start": 535.36,
      "duration": 3.599
    },
    {
      "text": "magnitude of the output from being too",
      "start": 537.16,
      "duration": 3.16
    },
    {
      "text": "large or too",
      "start": 538.959,
      "duration": 3.88
    },
    {
      "text": "small and that's what keeps the gradient",
      "start": 540.32,
      "duration": 4.4
    },
    {
      "text": "stable which leads to stable training",
      "start": 542.839,
      "duration": 4.24
    },
    {
      "text": "Dynamics that's one of the first reasons",
      "start": 544.72,
      "duration": 4.52
    },
    {
      "text": "why batch normalization or I should call",
      "start": 547.079,
      "duration": 3.641
    },
    {
      "text": "it layer normalization there is a",
      "start": 549.24,
      "duration": 3.92
    },
    {
      "text": "difference right so I should actually",
      "start": 550.72,
      "duration": 4.799
    },
    {
      "text": "call this layer normalization batch",
      "start": 553.16,
      "duration": 4.04
    },
    {
      "text": "normalization is something different and",
      "start": 555.519,
      "duration": 4.041
    },
    {
      "text": "we'll come to that later today we are",
      "start": 557.2,
      "duration": 3.56
    },
    {
      "text": "only going to look at layer",
      "start": 559.56,
      "duration": 3.64
    },
    {
      "text": "normalization in which the outputs",
      "start": 560.76,
      "duration": 4.44
    },
    {
      "text": "coming from every layer are",
      "start": 563.2,
      "duration": 4.44
    },
    {
      "text": "normalized the second advantage of layer",
      "start": 565.2,
      "duration": 4.44
    },
    {
      "text": "normalization is that it prevents this",
      "start": 567.64,
      "duration": 3.52
    },
    {
      "text": "problem which is called as internal",
      "start": 569.64,
      "duration": 4.24
    },
    {
      "text": "coate shift so what happens is that as",
      "start": 571.16,
      "duration": 4.76
    },
    {
      "text": "training proceeds the inputs to every",
      "start": 573.88,
      "duration": 3.84
    },
    {
      "text": "layer can change let's say we look at",
      "start": 575.92,
      "duration": 4.159
    },
    {
      "text": "the second layer right in in first",
      "start": 577.72,
      "duration": 4.28
    },
    {
      "text": "training iteration the inputs can have a",
      "start": 580.079,
      "duration": 3.561
    },
    {
      "text": "distribution like this in the second",
      "start": 582.0,
      "duration": 3.48
    },
    {
      "text": "training iteration maybe the inputs are",
      "start": 583.64,
      "duration": 4.4
    },
    {
      "text": "skewed so the input distribution which",
      "start": 585.48,
      "duration": 4.88
    },
    {
      "text": "every layer receives can change",
      "start": 588.04,
      "duration": 4.799
    },
    {
      "text": "according to the iterations and what",
      "start": 590.36,
      "duration": 4.28
    },
    {
      "text": "that leads to is that it makes training",
      "start": 592.839,
      "duration": 3.841
    },
    {
      "text": "very difficult so if the input",
      "start": 594.64,
      "duration": 4.08
    },
    {
      "text": "distribution to every layer is changing",
      "start": 596.68,
      "duration": 4.0
    },
    {
      "text": "the weights the updating the weights",
      "start": 598.72,
      "duration": 4.48
    },
    {
      "text": "becomes very hard and that delays the",
      "start": 600.68,
      "duration": 5.04
    },
    {
      "text": "convergence of the parameters and that",
      "start": 603.2,
      "duration": 4.879
    },
    {
      "text": "delays the overall solution reaching an",
      "start": 605.72,
      "duration": 5.28
    },
    {
      "text": "optimal value we don't want that layer",
      "start": 608.079,
      "duration": 4.921
    },
    {
      "text": "normalization really helps to prevent",
      "start": 611.0,
      "duration": 3.72
    },
    {
      "text": "this layer",
      "start": 613.0,
      "duration": 4.92
    },
    {
      "text": "normalization make sure that um since we",
      "start": 614.72,
      "duration": 4.799
    },
    {
      "text": "are normalizing which means that as",
      "start": 617.92,
      "duration": 3.479
    },
    {
      "text": "we'll see the variance of the standard",
      "start": 619.519,
      "duration": 4.241
    },
    {
      "text": "deviation is kept to one we'll make sure",
      "start": 621.399,
      "duration": 4.481
    },
    {
      "text": "that the mean and standard deviation of",
      "start": 623.76,
      "duration": 5.639
    },
    {
      "text": "the output from every layer is fixed and",
      "start": 625.88,
      "duration": 5.48
    },
    {
      "text": "this reduces the problem of internal",
      "start": 629.399,
      "duration": 4.44
    },
    {
      "text": "coari shift which accelerates",
      "start": 631.36,
      "duration": 4.56
    },
    {
      "text": "convergence so there are two main",
      "start": 633.839,
      "duration": 3.761
    },
    {
      "text": "reasons why layer normalization is",
      "start": 635.92,
      "duration": 3.52
    },
    {
      "text": "employed the first reason is that it",
      "start": 637.6,
      "duration": 4.52
    },
    {
      "text": "keeps the training procedure stable by",
      "start": 639.44,
      "duration": 4.56
    },
    {
      "text": "preventing the vanishing gradient or the",
      "start": 642.12,
      "duration": 4.159
    },
    {
      "text": "exploding gradient problem and the",
      "start": 644.0,
      "duration": 5.04
    },
    {
      "text": "second uh the second major reason for",
      "start": 646.279,
      "duration": 5.281
    },
    {
      "text": "using layer normalization is that it",
      "start": 649.04,
      "duration": 4.96
    },
    {
      "text": "prevents or reduces the problem of",
      "start": 651.56,
      "duration": 5.04
    },
    {
      "text": "internal covariate shift and that",
      "start": 654.0,
      "duration": 4.76
    },
    {
      "text": "accelerates convergence and we get to a",
      "start": 656.6,
      "duration": 4.799
    },
    {
      "text": "result for faster so that's why layer",
      "start": 658.76,
      "duration": 4.68
    },
    {
      "text": "normalization is employed not just in",
      "start": 661.399,
      "duration": 4.68
    },
    {
      "text": "the GPT or the llm architecture which we",
      "start": 663.44,
      "duration": 4.48
    },
    {
      "text": "are going to see right now but in fact",
      "start": 666.079,
      "duration": 3.681
    },
    {
      "text": "in many deep learning architectures",
      "start": 667.92,
      "duration": 4.8
    },
    {
      "text": "layer normalization is very frequently",
      "start": 669.76,
      "duration": 7.199
    },
    {
      "text": "used okay uh so what exactly is the main",
      "start": 672.72,
      "duration": 7.6
    },
    {
      "text": "idea of layer normalization it's very",
      "start": 676.959,
      "duration": 6.641
    },
    {
      "text": "simple so we look at a specific layer",
      "start": 680.32,
      "duration": 5.0
    },
    {
      "text": "and we'll look at the outputs of that",
      "start": 683.6,
      "duration": 4.08
    },
    {
      "text": "specific layer and what we'll do is that",
      "start": 685.32,
      "duration": 4.24
    },
    {
      "text": "we'll adjust those outputs so that they",
      "start": 687.68,
      "duration": 3.8
    },
    {
      "text": "have a mean of zero and they have a",
      "start": 689.56,
      "duration": 4.32
    },
    {
      "text": "variance of one let me illustrate this",
      "start": 691.48,
      "duration": 4.0
    },
    {
      "text": "through a simple example let's say you",
      "start": 693.88,
      "duration": 3.72
    },
    {
      "text": "are looking at a neural network and",
      "start": 695.48,
      "duration": 3.88
    },
    {
      "text": "these four are the outputs from one",
      "start": 697.6,
      "duration": 3.679
    },
    {
      "text": "specific layer of the neural network",
      "start": 699.36,
      "duration": 6.24
    },
    {
      "text": "right uh so the four outputs are X1 X2",
      "start": 701.279,
      "duration": 9.841
    },
    {
      "text": "X3 and X4 and X1 is equal to 1.1 X2 is8",
      "start": 705.6,
      "duration": 9.64
    },
    {
      "text": "X3 is 2.3 and X4 is 4.4 in layer",
      "start": 711.12,
      "duration": 5.959
    },
    {
      "text": "normalization what you do is you find",
      "start": 715.24,
      "duration": 4.44
    },
    {
      "text": "two quantities first you find find mean",
      "start": 717.079,
      "duration": 5.521
    },
    {
      "text": "of this so the mean will just be X1 + X2",
      "start": 719.68,
      "duration": 6.719
    },
    {
      "text": "+ X3 + X4 / 4 so in this case it will be",
      "start": 722.6,
      "duration": 6.16
    },
    {
      "text": "2.15 and the second thing which you do",
      "start": 726.399,
      "duration": 4.801
    },
    {
      "text": "is you find the variance so the variance",
      "start": 728.76,
      "duration": 5.4
    },
    {
      "text": "will be 1X 4 because there are four um",
      "start": 731.2,
      "duration": 5.8
    },
    {
      "text": "quantities here and then we'll sum up X1",
      "start": 734.16,
      "duration": 5.88
    },
    {
      "text": "minus the mean whole square + X2 - the",
      "start": 737.0,
      "duration": 6.36
    },
    {
      "text": "mean square + x3 - the mean whole square",
      "start": 740.04,
      "duration": 6.2
    },
    {
      "text": "+ X4 minus the mean whole Square so that",
      "start": 743.36,
      "duration": 5.08
    },
    {
      "text": "gives us the variance value now what we",
      "start": 746.24,
      "duration": 4.599
    },
    {
      "text": "do is we perform the normalization",
      "start": 748.44,
      "duration": 4.959
    },
    {
      "text": "procedure which means that for every",
      "start": 750.839,
      "duration": 4.761
    },
    {
      "text": "variable we subtract the mean and we",
      "start": 753.399,
      "duration": 3.68
    },
    {
      "text": "divide by the square root of the",
      "start": 755.6,
      "duration": 3.2
    },
    {
      "text": "variance which is the stand standard",
      "start": 757.079,
      "duration": 4.921
    },
    {
      "text": "deviation so X1 will be replaced by X1",
      "start": 758.8,
      "duration": 6.479
    },
    {
      "text": "minus mu / square root of variance X2",
      "start": 762.0,
      "duration": 6.04
    },
    {
      "text": "will be replaced by X2 - mu / square",
      "start": 765.279,
      "duration": 5.68
    },
    {
      "text": "root of variance X3 will be replaced by",
      "start": 768.04,
      "duration": 6.4
    },
    {
      "text": "x3 - mu / square root of variance and X4",
      "start": 770.959,
      "duration": 6.56
    },
    {
      "text": "will be replaced by X4 minus mu / square",
      "start": 774.44,
      "duration": 5.519
    },
    {
      "text": "root of variance so when you do this",
      "start": 777.519,
      "duration": 4.481
    },
    {
      "text": "normalization it leads to these four",
      "start": 779.959,
      "duration": 4.401
    },
    {
      "text": "values do you notice something about",
      "start": 782.0,
      "duration": 5.32
    },
    {
      "text": "these four normalized values if you add",
      "start": 784.36,
      "duration": 5.2
    },
    {
      "text": "these together in the numerator you will",
      "start": 787.32,
      "duration": 6.879
    },
    {
      "text": "have X1 + X2 + X3 + X4 - 4 * mu so that",
      "start": 789.56,
      "duration": 7.44
    },
    {
      "text": "will be zero which means that the mean",
      "start": 794.199,
      "duration": 5.281
    },
    {
      "text": "of this these normalized values is equal",
      "start": 797.0,
      "duration": 4.72
    },
    {
      "text": "to zero and if you compute the variance",
      "start": 799.48,
      "duration": 4.0
    },
    {
      "text": "of these normalized values through this",
      "start": 801.72,
      "duration": 5.0
    },
    {
      "text": "formula you'll see that the variance",
      "start": 803.48,
      "duration": 5.76
    },
    {
      "text": "of these normalized values is is",
      "start": 806.72,
      "duration": 5.96
    },
    {
      "text": "actually equal to 1 that's the most",
      "start": 809.24,
      "duration": 6.32
    },
    {
      "text": "important thing which uh which is which",
      "start": 812.68,
      "duration": 5.12
    },
    {
      "text": "you should realize or you you should",
      "start": 815.56,
      "duration": 4.279
    },
    {
      "text": "understand is that after performing the",
      "start": 817.8,
      "duration": 4.8
    },
    {
      "text": "normalization procedure uh the values",
      "start": 819.839,
      "duration": 4.521
    },
    {
      "text": "which you get these four values their",
      "start": 822.6,
      "duration": 3.84
    },
    {
      "text": "mean is equal to zero and their variance",
      "start": 824.36,
      "duration": 5.24
    },
    {
      "text": "is equal to one that's the whole idea",
      "start": 826.44,
      "duration": 4.199
    },
    {
      "text": "behind",
      "start": 829.6,
      "duration": 3.479
    },
    {
      "text": "normalization the in normalization we",
      "start": 830.639,
      "duration": 4.281
    },
    {
      "text": "adjust the outputs of every layer of",
      "start": 833.079,
      "duration": 4.281
    },
    {
      "text": "neural network to have mean of zero and",
      "start": 834.92,
      "duration": 4.599
    },
    {
      "text": "variance of one and it turns out that",
      "start": 837.36,
      "duration": 4.8
    },
    {
      "text": "this simple procedure helps us in the",
      "start": 839.519,
      "duration": 4.841
    },
    {
      "text": "stability neural network training and it",
      "start": 842.16,
      "duration": 4.16
    },
    {
      "text": "also helps us reduce the problem of the",
      "start": 844.36,
      "duration": 4.279
    },
    {
      "text": "internal coari",
      "start": 846.32,
      "duration": 5.72
    },
    {
      "text": "shift um so let us actually uh see this",
      "start": 848.639,
      "duration": 5.721
    },
    {
      "text": "in code but before that I want to tell",
      "start": 852.04,
      "duration": 4.239
    },
    {
      "text": "you where the layer normalization is",
      "start": 854.36,
      "duration": 4.44
    },
    {
      "text": "used and uh we discussed this at the",
      "start": 856.279,
      "duration": 5.841
    },
    {
      "text": "start of the lecture but um the input is",
      "start": 858.8,
      "duration": 5.039
    },
    {
      "text": "converted into an input embedding then",
      "start": 862.12,
      "duration": 3.6
    },
    {
      "text": "we add the token embeddings and then",
      "start": 863.839,
      "duration": 4.881
    },
    {
      "text": "here right we feed it before going into",
      "start": 865.72,
      "duration": 5.0
    },
    {
      "text": "the multi head attention we have a layer",
      "start": 868.72,
      "duration": 4.08
    },
    {
      "text": "normalization layer so that the inputs",
      "start": 870.72,
      "duration": 4.559
    },
    {
      "text": "to the multi-ad attention are normalized",
      "start": 872.8,
      "duration": 5.2
    },
    {
      "text": "even after the multi attention there is",
      "start": 875.279,
      "duration": 4.721
    },
    {
      "text": "a layer normalization layer before",
      "start": 878.0,
      "duration": 4.279
    },
    {
      "text": "feeding into the neural network module",
      "start": 880.0,
      "duration": 4.399
    },
    {
      "text": "within the Transformer block remember",
      "start": 882.279,
      "duration": 3.961
    },
    {
      "text": "this Blue Block here is the Transformer",
      "start": 884.399,
      "duration": 4.521
    },
    {
      "text": "block so the layer normalization layer",
      "start": 886.24,
      "duration": 4.32
    },
    {
      "text": "appears two times here and then it",
      "start": 888.92,
      "duration": 5.32
    },
    {
      "text": "appears once more again outside the",
      "start": 890.56,
      "duration": 6.719
    },
    {
      "text": "Transformer uh so in GPT and modern",
      "start": 894.24,
      "duration": 4.88
    },
    {
      "text": "Transformer architecture layer",
      "start": 897.279,
      "duration": 3.56
    },
    {
      "text": "normalization is typically applied",
      "start": 899.12,
      "duration": 3.24
    },
    {
      "text": "before and after the multi-head",
      "start": 900.839,
      "duration": 3.44
    },
    {
      "text": "attention module like what we have seen",
      "start": 902.36,
      "duration": 4.919
    },
    {
      "text": "over here and it also appears once",
      "start": 904.279,
      "duration": 5.601
    },
    {
      "text": "before the final output layer and we saw",
      "start": 907.279,
      "duration": 4.641
    },
    {
      "text": "this when we coded in the last lecture",
      "start": 909.88,
      "duration": 4.44
    },
    {
      "text": "here if you see within the Transformer",
      "start": 911.92,
      "duration": 4.12
    },
    {
      "text": "block layer normalization appears two",
      "start": 914.32,
      "duration": 3.319
    },
    {
      "text": "times before and after the multi-ad",
      "start": 916.04,
      "duration": 4.68
    },
    {
      "text": "attention but even before the output it",
      "start": 917.639,
      "duration": 5.521
    },
    {
      "text": "we employ it once so overall it appears",
      "start": 920.72,
      "duration": 4.52
    },
    {
      "text": "three times that's why we need to define",
      "start": 923.16,
      "duration": 4.479
    },
    {
      "text": "a separate class of the layer",
      "start": 925.24,
      "duration": 4.76
    },
    {
      "text": "normalization this one figure which I'm",
      "start": 927.639,
      "duration": 4.041
    },
    {
      "text": "which I've shown over here actually",
      "start": 930.0,
      "duration": 4.519
    },
    {
      "text": "illustrates the procedure of layer",
      "start": 931.68,
      "duration": 5.639
    },
    {
      "text": "normalization um let's say we have a",
      "start": 934.519,
      "duration": 4.841
    },
    {
      "text": "neural network layer these are the five",
      "start": 937.319,
      "duration": 4.44
    },
    {
      "text": "inputs uh to the neural network right",
      "start": 939.36,
      "duration": 4.039
    },
    {
      "text": "and these are the six outputs of the",
      "start": 941.759,
      "duration": 4.281
    },
    {
      "text": "neural network without the normalization",
      "start": 943.399,
      "duration": 4.36
    },
    {
      "text": "you'll see that their mean is not equal",
      "start": 946.04,
      "duration": 3.68
    },
    {
      "text": "to zero and their variance is not equal",
      "start": 947.759,
      "duration": 4.44
    },
    {
      "text": "to one but after we perform",
      "start": 949.72,
      "duration": 4.32
    },
    {
      "text": "normalization on these layer outputs",
      "start": 952.199,
      "duration": 4.56
    },
    {
      "text": "which means that for from every output",
      "start": 954.04,
      "duration": 4.68
    },
    {
      "text": "here we are going to subtract the mean",
      "start": 956.759,
      "duration": 3.681
    },
    {
      "text": "and divide by the square root of the",
      "start": 958.72,
      "duration": 4.2
    },
    {
      "text": "variance so you'll get these as the",
      "start": 960.44,
      "duration": 4.639
    },
    {
      "text": "resultant values after applying layer",
      "start": 962.92,
      "duration": 4.32
    },
    {
      "text": "normalization and if you take a mean of",
      "start": 965.079,
      "duration": 3.76
    },
    {
      "text": "these values you'll get that mean is",
      "start": 967.24,
      "duration": 3.519
    },
    {
      "text": "equal to zero and the variance of these",
      "start": 968.839,
      "duration": 3.36
    },
    {
      "text": "values is equal to",
      "start": 970.759,
      "duration": 4.0
    },
    {
      "text": "one this is the simple",
      "start": 972.199,
      "duration": 6.361
    },
    {
      "text": "illustration which describes uh what",
      "start": 974.759,
      "duration": 6.241
    },
    {
      "text": "happens underneath the hood for layer",
      "start": 978.56,
      "duration": 4.399
    },
    {
      "text": "normalization now what we are going to",
      "start": 981.0,
      "duration": 3.72
    },
    {
      "text": "do is that I'm going to take you through",
      "start": 982.959,
      "duration": 3.961
    },
    {
      "text": "code and we are going to implement layer",
      "start": 984.72,
      "duration": 4.239
    },
    {
      "text": "normalization first on a neural network",
      "start": 986.92,
      "duration": 3.96
    },
    {
      "text": "which looks like this and then we are",
      "start": 988.959,
      "duration": 4.0
    },
    {
      "text": "going to create a separate class of",
      "start": 990.88,
      "duration": 3.68
    },
    {
      "text": "layer normalization which we can",
      "start": 992.959,
      "duration": 4.36
    },
    {
      "text": "integrate within our GPT architecture so",
      "start": 994.56,
      "duration": 6.199
    },
    {
      "text": "let's jump into code right now all right",
      "start": 997.319,
      "duration": 6.921
    },
    {
      "text": "so here's the code file for layer",
      "start": 1000.759,
      "duration": 5.44
    },
    {
      "text": "normalization the first thing which we",
      "start": 1004.24,
      "duration": 3.92
    },
    {
      "text": "are going to do is start out with a",
      "start": 1006.199,
      "duration": 5.12
    },
    {
      "text": "simple example to illustrate how layer",
      "start": 1008.16,
      "duration": 6.239
    },
    {
      "text": "normalization is implemented in practice",
      "start": 1011.319,
      "duration": 5.281
    },
    {
      "text": "and then we will actually fill out this",
      "start": 1014.399,
      "duration": 4.841
    },
    {
      "text": "layer normalization class which we had",
      "start": 1016.6,
      "duration": 5.479
    },
    {
      "text": "created in the previous lecture so let's",
      "start": 1019.24,
      "duration": 5.199
    },
    {
      "text": "get started what we are doing here is",
      "start": 1022.079,
      "duration": 4.6
    },
    {
      "text": "that we are essentially let me take you",
      "start": 1024.439,
      "duration": 4.081
    },
    {
      "text": "to the white board to demonstrate what",
      "start": 1026.679,
      "duration": 3.681
    },
    {
      "text": "we are doing",
      "start": 1028.52,
      "duration": 4.76
    },
    {
      "text": "here we'll have a simple neural network",
      "start": 1030.36,
      "duration": 4.88
    },
    {
      "text": "layer and the neural network is",
      "start": 1033.28,
      "duration": 4.6
    },
    {
      "text": "constructed such that we have two",
      "start": 1035.24,
      "duration": 5.64
    },
    {
      "text": "batches of inputs so here is batch",
      "start": 1037.88,
      "duration": 6.12
    },
    {
      "text": "number one and here is batch number two",
      "start": 1040.88,
      "duration": 5.52
    },
    {
      "text": "and each batch has five inputs so batch",
      "start": 1044.0,
      "duration": 6.32
    },
    {
      "text": "number one has X1 X2 X3 X for X5 and",
      "start": 1046.4,
      "duration": 7.84
    },
    {
      "text": "batch number two has X1 X2 X3 X4 H5",
      "start": 1050.32,
      "duration": 7.08
    },
    {
      "text": "X5 now here we are looking at one layer",
      "start": 1054.24,
      "duration": 5.48
    },
    {
      "text": "of neurons and there are six neurons",
      "start": 1057.4,
      "duration": 5.76
    },
    {
      "text": "here so when these inputs essentially",
      "start": 1059.72,
      "duration": 5.28
    },
    {
      "text": "pass through this first layer of neurons",
      "start": 1063.16,
      "duration": 4.84
    },
    {
      "text": "we have the output which is produced and",
      "start": 1065.0,
      "duration": 5.72
    },
    {
      "text": "uh there will be six outputs for batch",
      "start": 1068.0,
      "duration": 6.6
    },
    {
      "text": "number one which is y1 Y2 Y3 y4 y5 Y6",
      "start": 1070.72,
      "duration": 5.68
    },
    {
      "text": "and there will be six outputs for batch",
      "start": 1074.6,
      "duration": 6.199
    },
    {
      "text": "number two y1 Y2 Y3 y4 y5 Y6 so if you",
      "start": 1076.4,
      "duration": 5.8
    },
    {
      "text": "look at the",
      "start": 1080.799,
      "duration": 3.801
    },
    {
      "text": "inputs the shape of the inputs will be",
      "start": 1082.2,
      "duration": 4.959
    },
    {
      "text": "two rows and five columns because we",
      "start": 1084.6,
      "duration": 4.64
    },
    {
      "text": "have two batches and each batch will",
      "start": 1087.159,
      "duration": 4.601
    },
    {
      "text": "have five inputs right then we have a",
      "start": 1089.24,
      "duration": 4.919
    },
    {
      "text": "sequential layer which essentially takes",
      "start": 1091.76,
      "duration": 5.56
    },
    {
      "text": "in five inputs and it has six outputs",
      "start": 1094.159,
      "duration": 6.281
    },
    {
      "text": "this sequential layer is this uh the",
      "start": 1097.32,
      "duration": 4.76
    },
    {
      "text": "second layer which I've shown you over",
      "start": 1100.44,
      "duration": 5.08
    },
    {
      "text": "here this layer of six neurons and after",
      "start": 1102.08,
      "duration": 5.079
    },
    {
      "text": "every neuron here we essentially have",
      "start": 1105.52,
      "duration": 4.36
    },
    {
      "text": "The Rao activation function",
      "start": 1107.159,
      "duration": 4.64
    },
    {
      "text": "so which has been mentioned over here if",
      "start": 1109.88,
      "duration": 3.76
    },
    {
      "text": "you don't know what Rao activation",
      "start": 1111.799,
      "duration": 4.401
    },
    {
      "text": "function is it's fine for this lecture",
      "start": 1113.64,
      "duration": 4.919
    },
    {
      "text": "we don't need to understand",
      "start": 1116.2,
      "duration": 5.32
    },
    {
      "text": "this uh the output of the layer is that",
      "start": 1118.559,
      "duration": 5.12
    },
    {
      "text": "the layer is then applied on this input",
      "start": 1121.52,
      "duration": 5.08
    },
    {
      "text": "batch and we get the output can you try",
      "start": 1123.679,
      "duration": 4.48
    },
    {
      "text": "to understand why the shape of the",
      "start": 1126.6,
      "duration": 3.959
    },
    {
      "text": "output is like this so here you can see",
      "start": 1128.159,
      "duration": 5.321
    },
    {
      "text": "that we have two rows and we have six",
      "start": 1130.559,
      "duration": 5.441
    },
    {
      "text": "columns if you look at the first row",
      "start": 1133.48,
      "duration": 4.88
    },
    {
      "text": "this represents the six outputs from the",
      "start": 1136.0,
      "duration": 3.88
    },
    {
      "text": "first",
      "start": 1138.36,
      "duration": 3.679
    },
    {
      "text": "um batch and if you look at the second",
      "start": 1139.88,
      "duration": 5.48
    },
    {
      "text": "row this represents the six outputs from",
      "start": 1142.039,
      "duration": 7.321
    },
    {
      "text": "the uh second batch and that's what",
      "start": 1145.36,
      "duration": 6.92
    },
    {
      "text": "exactly being shown here y1 Y2 Y6 and",
      "start": 1149.36,
      "duration": 6.36
    },
    {
      "text": "batch two has y1 Y2 up till Y6 so this",
      "start": 1152.28,
      "duration": 6.56
    },
    {
      "text": "can be y1 this is Y2 this is Y3 this is",
      "start": 1155.72,
      "duration": 6.36
    },
    {
      "text": "Y6 for batch one and this is y1 this is",
      "start": 1158.84,
      "duration": 7.04
    },
    {
      "text": "Y2 and this is Y6 for batch number two",
      "start": 1162.08,
      "duration": 5.92
    },
    {
      "text": "okay so this is the layer which we have",
      "start": 1165.88,
      "duration": 3.56
    },
    {
      "text": "and now what we are we are going to do",
      "start": 1168.0,
      "duration": 4.44
    },
    {
      "text": "is that after this layer we are going to",
      "start": 1169.44,
      "duration": 5.52
    },
    {
      "text": "uh apply the batch normalization uh",
      "start": 1172.44,
      "duration": 6.0
    },
    {
      "text": "sorry we are going to apply the layer",
      "start": 1174.96,
      "duration": 7.48
    },
    {
      "text": "normalization so uh here I have simply",
      "start": 1178.44,
      "duration": 6.0
    },
    {
      "text": "explained that we have a neural network",
      "start": 1182.44,
      "duration": 4.04
    },
    {
      "text": "which consists of a linear layer",
      "start": 1184.44,
      "duration": 4.44
    },
    {
      "text": "followed by The Rao activation layer to",
      "start": 1186.48,
      "duration": 4.0
    },
    {
      "text": "quickly illustrate what The Rao",
      "start": 1188.88,
      "duration": 3.919
    },
    {
      "text": "activation function actually looks like",
      "start": 1190.48,
      "duration": 5.6
    },
    {
      "text": "take a look at this this image over here",
      "start": 1192.799,
      "duration": 6.561
    },
    {
      "text": "so if x is positive The Rao is just y =",
      "start": 1196.08,
      "duration": 5.599
    },
    {
      "text": "to X but if x is equal to negative The",
      "start": 1199.36,
      "duration": 5.12
    },
    {
      "text": "Rao is zero so there is a nonlinearity",
      "start": 1201.679,
      "duration": 5.641
    },
    {
      "text": "here that's The Rao activation",
      "start": 1204.48,
      "duration": 5.12
    },
    {
      "text": "function now what we are going to do as",
      "start": 1207.32,
      "duration": 3.8
    },
    {
      "text": "I mentioned is we are going to apply",
      "start": 1209.6,
      "duration": 3.84
    },
    {
      "text": "layer normalization so the way it is",
      "start": 1211.12,
      "duration": 4.24
    },
    {
      "text": "applied is very similar to the Hands-On",
      "start": 1213.44,
      "duration": 3.84
    },
    {
      "text": "example which we saw on the white board",
      "start": 1215.36,
      "duration": 3.36
    },
    {
      "text": "over here this",
      "start": 1217.28,
      "duration": 3.639
    },
    {
      "text": "example uh the same thing will be",
      "start": 1218.72,
      "duration": 4.4
    },
    {
      "text": "applying to the first",
      "start": 1220.919,
      "duration": 4.321
    },
    {
      "text": "batch and the same thing will be",
      "start": 1223.12,
      "duration": 4.52
    },
    {
      "text": "applying to the second batch so what",
      "start": 1225.24,
      "duration": 4.0
    },
    {
      "text": "we'll be doing is that when you look at",
      "start": 1227.64,
      "duration": 5.96
    },
    {
      "text": "the first batch we will do y1",
      "start": 1229.24,
      "duration": 8.12
    },
    {
      "text": "minus so y1 will be replaced by y1 - mu",
      "start": 1233.6,
      "duration": 8.319
    },
    {
      "text": "divided by uh square root of we write",
      "start": 1237.36,
      "duration": 7.52
    },
    {
      "text": "this again divided by square root of",
      "start": 1241.919,
      "duration": 8.681
    },
    {
      "text": "variance Y2 will be replaced by Y2 minus",
      "start": 1244.88,
      "duration": 12.32
    },
    {
      "text": "mu uh divided square root of variance",
      "start": 1250.6,
      "duration": 6.6
    },
    {
      "text": "and like this similarly the last output",
      "start": 1260.28,
      "duration": 6.519
    },
    {
      "text": "which is Y6 here Y6 will also be",
      "start": 1263.6,
      "duration": 4.4
    },
    {
      "text": "replaced",
      "start": 1266.799,
      "duration": 3.041
    },
    {
      "text": "with",
      "start": 1268.0,
      "duration": 4.159
    },
    {
      "text": "Y6",
      "start": 1269.84,
      "duration": 4.0
    },
    {
      "text": "minus",
      "start": 1272.159,
      "duration": 7.0
    },
    {
      "text": "mu divided by square root of",
      "start": 1273.84,
      "duration": 8.12
    },
    {
      "text": "variance and first we process the first",
      "start": 1279.159,
      "duration": 4.4
    },
    {
      "text": "batch and then we process the second",
      "start": 1281.96,
      "duration": 4.12
    },
    {
      "text": "batch in a very similar manner so now",
      "start": 1283.559,
      "duration": 6.24
    },
    {
      "text": "let's go to code to see how this is done",
      "start": 1286.08,
      "duration": 5.959
    },
    {
      "text": "so now what we are going to do is that",
      "start": 1289.799,
      "duration": 4.36
    },
    {
      "text": "we we have the output which is this",
      "start": 1292.039,
      "duration": 4.52
    },
    {
      "text": "tensor right and then we are doing",
      "start": 1294.159,
      "duration": 5.801
    },
    {
      "text": "output do mean Dimension equal to minus1",
      "start": 1296.559,
      "duration": 5.36
    },
    {
      "text": "why Dimension equal to minus1 because we",
      "start": 1299.96,
      "duration": 4.44
    },
    {
      "text": "have to take the mean along the columns",
      "start": 1301.919,
      "duration": 4.561
    },
    {
      "text": "so first we look at the first batch",
      "start": 1304.4,
      "duration": 3.84
    },
    {
      "text": "outputs and we want to take the mean of",
      "start": 1306.48,
      "duration": 4.64
    },
    {
      "text": "this so we do output do mean Dimension",
      "start": 1308.24,
      "duration": 5.319
    },
    {
      "text": "equal to minus1 and this keep Dimension",
      "start": 1311.12,
      "duration": 4.439
    },
    {
      "text": "equal to true that is very important",
      "start": 1313.559,
      "duration": 4.12
    },
    {
      "text": "because if we don't include keep",
      "start": 1315.559,
      "duration": 4.041
    },
    {
      "text": "Dimension equal to true The Returned",
      "start": 1317.679,
      "duration": 5.201
    },
    {
      "text": "mean would be a two dimensional Vector",
      "start": 1319.6,
      "duration": 5.24
    },
    {
      "text": "instead of a two into one dimensional",
      "start": 1322.88,
      "duration": 6.6
    },
    {
      "text": "Matrix so essentially uh if you use keep",
      "start": 1324.84,
      "duration": 6.8
    },
    {
      "text": "dim equal to true the output which you",
      "start": 1329.48,
      "duration": 5.88
    },
    {
      "text": "get for the mean is this so the first",
      "start": 1331.64,
      "duration": 5.44
    },
    {
      "text": "value here corresponds to the mean of",
      "start": 1335.36,
      "duration": 4.0
    },
    {
      "text": "the first batch the second value here",
      "start": 1337.08,
      "duration": 3.8
    },
    {
      "text": "corresponds to the mean of the second",
      "start": 1339.36,
      "duration": 4.319
    },
    {
      "text": "batch since we used keyd equal to true",
      "start": 1340.88,
      "duration": 4.52
    },
    {
      "text": "the shape of this output is that it's a",
      "start": 1343.679,
      "duration": 5.12
    },
    {
      "text": "matrix uh or rather uh yeah it's a a two",
      "start": 1345.4,
      "duration": 5.36
    },
    {
      "text": "into one dimensional Matrix over here",
      "start": 1348.799,
      "duration": 4.161
    },
    {
      "text": "right now if we did not use keep dim",
      "start": 1350.76,
      "duration": 4.76
    },
    {
      "text": "equal to true this would not be a matrix",
      "start": 1352.96,
      "duration": 4.16
    },
    {
      "text": "in fact it would just be a two-",
      "start": 1355.52,
      "duration": 4.08
    },
    {
      "text": "dimensional vector and that's generally",
      "start": 1357.12,
      "duration": 4.919
    },
    {
      "text": "not good because it's good for the",
      "start": 1359.6,
      "duration": 4.559
    },
    {
      "text": "dimensions to be preserved as we are",
      "start": 1362.039,
      "duration": 4.52
    },
    {
      "text": "doing all of these",
      "start": 1364.159,
      "duration": 4.801
    },
    {
      "text": "calculations similarly for the variance",
      "start": 1366.559,
      "duration": 3.881
    },
    {
      "text": "what we are doing is that we are taking",
      "start": 1368.96,
      "duration": 3.24
    },
    {
      "text": "the variance across the column for both",
      "start": 1370.44,
      "duration": 3.599
    },
    {
      "text": "the batches and we use keep them equal",
      "start": 1372.2,
      "duration": 4.16
    },
    {
      "text": "to true and then you print out the mean",
      "start": 1374.039,
      "duration": 3.921
    },
    {
      "text": "and then you print out the variance for",
      "start": 1376.36,
      "duration": 2.72
    },
    {
      "text": "every batch",
      "start": 1377.96,
      "duration": 3.199
    },
    {
      "text": "so for the first batch of data the mean",
      "start": 1379.08,
      "duration": 5.32
    },
    {
      "text": "is. 1324 for the second for the first",
      "start": 1381.159,
      "duration": 6.561
    },
    {
      "text": "batch of data the variance is 0.02 31",
      "start": 1384.4,
      "duration": 5.759
    },
    {
      "text": "for the second batch of data the mean is",
      "start": 1387.72,
      "duration": 5.88
    },
    {
      "text": "216 2170 and for the second batch of",
      "start": 1390.159,
      "duration": 5.801
    },
    {
      "text": "data the variance is",
      "start": 1393.6,
      "duration": 5.6
    },
    {
      "text": "0.398 so remember the two",
      "start": 1395.96,
      "duration": 5.719
    },
    {
      "text": "uh uh two commands which we have used",
      "start": 1399.2,
      "duration": 4.959
    },
    {
      "text": "here dim equal to minus1 because we have",
      "start": 1401.679,
      "duration": 4.321
    },
    {
      "text": "to perform that operation along the",
      "start": 1404.159,
      "duration": 4.041
    },
    {
      "text": "columns and keep dim equal to true",
      "start": 1406.0,
      "duration": 4.64
    },
    {
      "text": "because we have to retain um the",
      "start": 1408.2,
      "duration": 6.079
    },
    {
      "text": "dimension of the final mean and the",
      "start": 1410.64,
      "duration": 5.8
    },
    {
      "text": "variance Matrix which we have if we did",
      "start": 1414.279,
      "duration": 4.081
    },
    {
      "text": "not use keep D equal to True later when",
      "start": 1416.44,
      "duration": 3.92
    },
    {
      "text": "we subtract this mean from every",
      "start": 1418.36,
      "duration": 4.0
    },
    {
      "text": "individual element it will lead to some",
      "start": 1420.36,
      "duration": 5.319
    },
    {
      "text": "problems so we want to avoid that so in",
      "start": 1422.36,
      "duration": 4.64
    },
    {
      "text": "this text over here I have just",
      "start": 1425.679,
      "duration": 3.321
    },
    {
      "text": "explained why we used keep dim equal to",
      "start": 1427.0,
      "duration": 4.2
    },
    {
      "text": "true and why we used Dimension equal to",
      "start": 1429.0,
      "duration": 4.48
    },
    {
      "text": "minus1 so if you have some confusion",
      "start": 1431.2,
      "duration": 4.64
    },
    {
      "text": "along those lines please read this text",
      "start": 1433.48,
      "duration": 4.76
    },
    {
      "text": "when I share this Google or when I share",
      "start": 1435.84,
      "duration": 5.64
    },
    {
      "text": "this Jupiter notebook with you great and",
      "start": 1438.24,
      "duration": 4.88
    },
    {
      "text": "now what we are going to do is that we",
      "start": 1441.48,
      "duration": 4.079
    },
    {
      "text": "are going to uh",
      "start": 1443.12,
      "duration": 5.159
    },
    {
      "text": "subtract the mean so like over here we",
      "start": 1445.559,
      "duration": 4.281
    },
    {
      "text": "are going to subtract the mean and",
      "start": 1448.279,
      "duration": 4.721
    },
    {
      "text": "divide by the square root of variance so",
      "start": 1449.84,
      "duration": 5.36
    },
    {
      "text": "we have the output Matrix which is there",
      "start": 1453.0,
      "duration": 4.159
    },
    {
      "text": "we are going to subtract the mean which",
      "start": 1455.2,
      "duration": 4.599
    },
    {
      "text": "is now again you can see the mean is",
      "start": 1457.159,
      "duration": 5.601
    },
    {
      "text": "also a tensor which has two rows and one",
      "start": 1459.799,
      "duration": 5.201
    },
    {
      "text": "column and we are going to subtract the",
      "start": 1462.76,
      "duration": 4.039
    },
    {
      "text": "mean from the output and we are going to",
      "start": 1465.0,
      "duration": 3.88
    },
    {
      "text": "divide by the square root of variable",
      "start": 1466.799,
      "duration": 5.041
    },
    {
      "text": "this is the main normalization",
      "start": 1468.88,
      "duration": 5.96
    },
    {
      "text": "step so this is my output now and the",
      "start": 1471.84,
      "duration": 5.959
    },
    {
      "text": "normalized layer outputs are given like",
      "start": 1474.84,
      "duration": 5.52
    },
    {
      "text": "this uh the first row again corresponds",
      "start": 1477.799,
      "duration": 4.721
    },
    {
      "text": "to the normalized outputs of batch one",
      "start": 1480.36,
      "duration": 3.64
    },
    {
      "text": "the second row corresponds to the",
      "start": 1482.52,
      "duration": 4.32
    },
    {
      "text": "normalized outputs of batch number two",
      "start": 1484.0,
      "duration": 4.64
    },
    {
      "text": "so here I'm just printing out the mean",
      "start": 1486.84,
      "duration": 4.16
    },
    {
      "text": "and variance of the batch one and batch",
      "start": 1488.64,
      "duration": 4.399
    },
    {
      "text": "two so if you look batch one and batch",
      "start": 1491.0,
      "duration": 4.12
    },
    {
      "text": "two so if you look at the mean you'll",
      "start": 1493.039,
      "duration": 3.721
    },
    {
      "text": "see that the mean of the first batch is",
      "start": 1495.12,
      "duration": 5.24
    },
    {
      "text": "almost close to zero this is is 10us 8",
      "start": 1496.76,
      "duration": 5.68
    },
    {
      "text": "which is really very close to zero we",
      "start": 1500.36,
      "duration": 5.039
    },
    {
      "text": "can approximate it to zero for the",
      "start": 1502.44,
      "duration": 4.599
    },
    {
      "text": "second batch the mean is again very",
      "start": 1505.399,
      "duration": 4.121
    },
    {
      "text": "close to 10us 8 again that's almost",
      "start": 1507.039,
      "duration": 4.801
    },
    {
      "text": "equal to zero and if you look at the",
      "start": 1509.52,
      "duration": 4.039
    },
    {
      "text": "variance for both the batches you'll see",
      "start": 1511.84,
      "duration": 3.28
    },
    {
      "text": "that the variance is equal to one",
      "start": 1513.559,
      "duration": 3.36
    },
    {
      "text": "awesome this is exactly what we wanted",
      "start": 1515.12,
      "duration": 3.279
    },
    {
      "text": "right which means that the layers have",
      "start": 1516.919,
      "duration": 4.161
    },
    {
      "text": "been normalized now so note that the",
      "start": 1518.399,
      "duration": 5.921
    },
    {
      "text": "value 2.9 into 10- 8 is the scientific",
      "start": 1521.08,
      "duration": 7.64
    },
    {
      "text": "notation for 2.9 * 10us 8 this value is",
      "start": 1524.32,
      "duration": 7.079
    },
    {
      "text": "very close to zero but not exactly zero",
      "start": 1528.72,
      "duration": 5.559
    },
    {
      "text": "due to small numerical errors in Python",
      "start": 1531.399,
      "duration": 5.801
    },
    {
      "text": "what we have is this uh we can turn on",
      "start": 1534.279,
      "duration": 5.12
    },
    {
      "text": "the turn off the scientific mode So",
      "start": 1537.2,
      "duration": 4.68
    },
    {
      "text": "currently the scientific notation is",
      "start": 1539.399,
      "duration": 6.52
    },
    {
      "text": "on that's why we are getting these uh um",
      "start": 1541.88,
      "duration": 5.72
    },
    {
      "text": "values which have been represented in",
      "start": 1545.919,
      "duration": 3.64
    },
    {
      "text": "the scientific notation we can turn off",
      "start": 1547.6,
      "duration": 4.079
    },
    {
      "text": "the scientific notation and then let's",
      "start": 1549.559,
      "duration": 4.161
    },
    {
      "text": "print out the mean and the variance so",
      "start": 1551.679,
      "duration": 3.761
    },
    {
      "text": "you'll see that the mean for both the",
      "start": 1553.72,
      "duration": 4.0
    },
    {
      "text": "batches is equal to zero and the",
      "start": 1555.44,
      "duration": 4.16
    },
    {
      "text": "variance for both the batches is equal",
      "start": 1557.72,
      "duration": 5.04
    },
    {
      "text": "to one great and now we'll achieve the",
      "start": 1559.6,
      "duration": 4.799
    },
    {
      "text": "goal which we started out this lecture",
      "start": 1562.76,
      "duration": 4.48
    },
    {
      "text": "with we want to create a class for layer",
      "start": 1564.399,
      "duration": 7.081
    },
    {
      "text": "normalization um what would be the",
      "start": 1567.24,
      "duration": 6.28
    },
    {
      "text": "output of this class basically this",
      "start": 1571.48,
      "duration": 5.199
    },
    {
      "text": "class will take in the um the output of",
      "start": 1573.52,
      "duration": 6.56
    },
    {
      "text": "a layer and it will apply the um",
      "start": 1576.679,
      "duration": 6.961
    },
    {
      "text": "normalization to that so let's look at",
      "start": 1580.08,
      "duration": 5.56
    },
    {
      "text": "where this layer normalization step is",
      "start": 1583.64,
      "duration": 4.24
    },
    {
      "text": "implemented so the layer normalization",
      "start": 1585.64,
      "duration": 5.039
    },
    {
      "text": "step is is implemented here the layer",
      "start": 1587.88,
      "duration": 5.36
    },
    {
      "text": "normalization step is implemented here",
      "start": 1590.679,
      "duration": 5.441
    },
    {
      "text": "so at both of these places when we get",
      "start": 1593.24,
      "duration": 5.24
    },
    {
      "text": "uh when the inputs are received to this",
      "start": 1596.12,
      "duration": 4.159
    },
    {
      "text": "block and when the input is received to",
      "start": 1598.48,
      "duration": 6.24
    },
    {
      "text": "this block um we have certain number of",
      "start": 1600.279,
      "duration": 7.201
    },
    {
      "text": "tokens uh which is let's say let's say",
      "start": 1604.72,
      "duration": 4.52
    },
    {
      "text": "we are looking at",
      "start": 1607.48,
      "duration": 5.559
    },
    {
      "text": "the uh Contex size for the number of",
      "start": 1609.24,
      "duration": 6.0
    },
    {
      "text": "embedding vectors which we have but the",
      "start": 1613.039,
      "duration": 4.24
    },
    {
      "text": "main thing which I want to point out is",
      "start": 1615.24,
      "duration": 4.6
    },
    {
      "text": "that the the number of columns which we",
      "start": 1617.279,
      "duration": 5.081
    },
    {
      "text": "have is equal to the embedding",
      "start": 1619.84,
      "duration": 5.0
    },
    {
      "text": "size and this is the embedding Vector",
      "start": 1622.36,
      "duration": 7.0
    },
    {
      "text": "Dimension which we are using so for gpt2",
      "start": 1624.84,
      "duration": 7.92
    },
    {
      "text": "this embedding size is equal to",
      "start": 1629.36,
      "duration": 3.4
    },
    {
      "text": "768 so when we look at the so when we",
      "start": 1635.08,
      "duration": 6.0
    },
    {
      "text": "look at let's say the first row over",
      "start": 1639.32,
      "duration": 3.719
    },
    {
      "text": "here the first row corresponds to the",
      "start": 1641.08,
      "duration": 3.68
    },
    {
      "text": "embedding for the first token which is",
      "start": 1643.039,
      "duration": 4.161
    },
    {
      "text": "an input to this layer normalization",
      "start": 1644.76,
      "duration": 5.6
    },
    {
      "text": "right so so we will take the mean and",
      "start": 1647.2,
      "duration": 5.199
    },
    {
      "text": "we'll take the variance along the column",
      "start": 1650.36,
      "duration": 4.72
    },
    {
      "text": "Dimension which is 768 so we'll take the",
      "start": 1652.399,
      "duration": 4.64
    },
    {
      "text": "mean of all of this and take the",
      "start": 1655.08,
      "duration": 4.04
    },
    {
      "text": "variance and then do the normalization",
      "start": 1657.039,
      "duration": 4.48
    },
    {
      "text": "similarly we'll do this for every single",
      "start": 1659.12,
      "duration": 5.159
    },
    {
      "text": "row um for the input to this layer",
      "start": 1661.519,
      "duration": 4.601
    },
    {
      "text": "normalization as well as the input to",
      "start": 1664.279,
      "duration": 4.12
    },
    {
      "text": "this layer normalization so if you see",
      "start": 1666.12,
      "duration": 4.88
    },
    {
      "text": "the dimension when an instance of this",
      "start": 1668.399,
      "duration": 4.64
    },
    {
      "text": "class is created we have to pass in the",
      "start": 1671.0,
      "duration": 4.44
    },
    {
      "text": "embedding Dimension why do we have to",
      "start": 1673.039,
      "duration": 4.201
    },
    {
      "text": "pass in the embedding Dimension because",
      "start": 1675.44,
      "duration": 3.64
    },
    {
      "text": "we'll see that we are going to implement",
      "start": 1677.24,
      "duration": 4.52
    },
    {
      "text": "something like scale and shift these are",
      "start": 1679.08,
      "duration": 5.24
    },
    {
      "text": "trainable parameters but the size of the",
      "start": 1681.76,
      "duration": 4.399
    },
    {
      "text": "scale and shift will be governed by the",
      "start": 1684.32,
      "duration": 4.28
    },
    {
      "text": "embedding Dimension and that's the same",
      "start": 1686.159,
      "duration": 5.0
    },
    {
      "text": "as the input Vector to the layer Norm",
      "start": 1688.6,
      "duration": 4.24
    },
    {
      "text": "module and that will be the same as the",
      "start": 1691.159,
      "duration": 2.76
    },
    {
      "text": "output",
      "start": 1692.84,
      "duration": 3.76
    },
    {
      "text": "Vector so the input to the layer Norm",
      "start": 1693.919,
      "duration": 4.441
    },
    {
      "text": "module will have certain number of rows",
      "start": 1696.6,
      "duration": 4.04
    },
    {
      "text": "but it will have embedding columns the",
      "start": 1698.36,
      "duration": 4.08
    },
    {
      "text": "output of the layer normalization which",
      "start": 1700.64,
      "duration": 3.759
    },
    {
      "text": "is Norm X will have the same dimensions",
      "start": 1702.44,
      "duration": 4.16
    },
    {
      "text": "as the input because normalization does",
      "start": 1704.399,
      "duration": 4.321
    },
    {
      "text": "not change dimensions and we are going",
      "start": 1706.6,
      "duration": 4.16
    },
    {
      "text": "to scale the output with the scale and",
      "start": 1708.72,
      "duration": 5.0
    },
    {
      "text": "shift I'll come to that in a moment so",
      "start": 1710.76,
      "duration": 4.24
    },
    {
      "text": "the main part of this layer",
      "start": 1713.72,
      "duration": 3.079
    },
    {
      "text": "normalization class is the forward",
      "start": 1715.0,
      "duration": 3.76
    },
    {
      "text": "method which takes in the input which I",
      "start": 1716.799,
      "duration": 3.921
    },
    {
      "text": "described so when you think of the input",
      "start": 1718.76,
      "duration": 3.519
    },
    {
      "text": "think of the input as having certain",
      "start": 1720.72,
      "duration": 4.28
    },
    {
      "text": "number of rows but mostly focus on the",
      "start": 1722.279,
      "duration": 4.041
    },
    {
      "text": "number of columns which will be the",
      "start": 1725.0,
      "duration": 3.519
    },
    {
      "text": "embedding dimensions in each row let's",
      "start": 1726.32,
      "duration": 5.0
    },
    {
      "text": "say we have 768 embedding Dimension so",
      "start": 1728.519,
      "duration": 4.481
    },
    {
      "text": "in the first step what we do is that",
      "start": 1731.32,
      "duration": 3.92
    },
    {
      "text": "along the column we take the",
      "start": 1733.0,
      "duration": 5.32
    },
    {
      "text": "mean ex exactly similar to what we",
      "start": 1735.24,
      "duration": 4.72
    },
    {
      "text": "actually did over here just keep this",
      "start": 1738.32,
      "duration": 3.12
    },
    {
      "text": "example in",
      "start": 1739.96,
      "duration": 4.16
    },
    {
      "text": "mind um then what we do is along the",
      "start": 1741.44,
      "duration": 5.479
    },
    {
      "text": "column we take the variance and then we",
      "start": 1744.12,
      "duration": 5.279
    },
    {
      "text": "subtract the mean and then we divide by",
      "start": 1746.919,
      "duration": 4.76
    },
    {
      "text": "the square root of variance note that we",
      "start": 1749.399,
      "duration": 4.4
    },
    {
      "text": "have added uh this small variable",
      "start": 1751.679,
      "duration": 4.641
    },
    {
      "text": "Epsilon so this is a small constant",
      "start": 1753.799,
      "duration": 3.88
    },
    {
      "text": "which is added to the variance to",
      "start": 1756.32,
      "duration": 3.28
    },
    {
      "text": "prevent division by zero during",
      "start": 1757.679,
      "duration": 4.041
    },
    {
      "text": "normalization so we don't want to divide",
      "start": 1759.6,
      "duration": 3.84
    },
    {
      "text": "by the square root of zero right so we",
      "start": 1761.72,
      "duration": 4.959
    },
    {
      "text": "add a small uh variable here which is",
      "start": 1763.44,
      "duration": 6.599
    },
    {
      "text": "called self. Epsilon or Epsilon so this",
      "start": 1766.679,
      "duration": 5.441
    },
    {
      "text": "is the output of the normaliz layer",
      "start": 1770.039,
      "duration": 4.441
    },
    {
      "text": "normalization but we do one more step",
      "start": 1772.12,
      "duration": 4.76
    },
    {
      "text": "here which is we multiply by the scale",
      "start": 1774.48,
      "duration": 5.0
    },
    {
      "text": "and we multiply by the shift so let me",
      "start": 1776.88,
      "duration": 5.44
    },
    {
      "text": "explain what the scale and shift are uh",
      "start": 1779.48,
      "duration": 4.919
    },
    {
      "text": "just like we are training we are going",
      "start": 1782.32,
      "duration": 3.8
    },
    {
      "text": "to train the embedding parameters the",
      "start": 1784.399,
      "duration": 3.201
    },
    {
      "text": "positional embedding parameters the",
      "start": 1786.12,
      "duration": 3.76
    },
    {
      "text": "neural network parameters in the GPT",
      "start": 1787.6,
      "duration": 5.04
    },
    {
      "text": "architecture the scale and shelf uh the",
      "start": 1789.88,
      "duration": 4.36
    },
    {
      "text": "scale and",
      "start": 1792.64,
      "duration": 4.72
    },
    {
      "text": "shift are two trainable parameters which",
      "start": 1794.24,
      "duration": 5.36
    },
    {
      "text": "have the same Dimension as the input",
      "start": 1797.36,
      "duration": 4.199
    },
    {
      "text": "that the llm automatically adjusts",
      "start": 1799.6,
      "duration": 4.72
    },
    {
      "text": "during training if it is determined that",
      "start": 1801.559,
      "duration": 4.36
    },
    {
      "text": "doing so would improve the model's",
      "start": 1804.32,
      "duration": 4.28
    },
    {
      "text": "performance on the training task so this",
      "start": 1805.919,
      "duration": 4.441
    },
    {
      "text": "allows the model to learn appropriate",
      "start": 1808.6,
      "duration": 3.88
    },
    {
      "text": "scaling and shifting that best suits the",
      "start": 1810.36,
      "duration": 5.039
    },
    {
      "text": "data uh it is processing so you can",
      "start": 1812.48,
      "duration": 5.4
    },
    {
      "text": "think of it as knobs turnable knobs or",
      "start": 1815.399,
      "duration": 4.201
    },
    {
      "text": "fine-tuning parameters which we have",
      "start": 1817.88,
      "duration": 5.039
    },
    {
      "text": "added just to make sure that",
      "start": 1819.6,
      "duration": 5.959
    },
    {
      "text": "uh the layer normalization proceeds",
      "start": 1822.919,
      "duration": 4.401
    },
    {
      "text": "smoothly or if we want to tweak the",
      "start": 1825.559,
      "duration": 3.281
    },
    {
      "text": "layer normaliz ation of bit we can",
      "start": 1827.32,
      "duration": 3.199
    },
    {
      "text": "always do it with the scale and with the",
      "start": 1828.84,
      "duration": 4.0
    },
    {
      "text": "shift these are trainable parameters",
      "start": 1830.519,
      "duration": 4.081
    },
    {
      "text": "which means we don't specify the values",
      "start": 1832.84,
      "duration": 4.6
    },
    {
      "text": "of these parameters at the start and",
      "start": 1834.6,
      "duration": 6.6
    },
    {
      "text": "remember that the these scale and shift",
      "start": 1837.44,
      "duration": 6.28
    },
    {
      "text": "have the embedding Dimension so that we",
      "start": 1841.2,
      "duration": 5.88
    },
    {
      "text": "can multiply the U the scale and the",
      "start": 1843.72,
      "duration": 5.6
    },
    {
      "text": "output of the normalization layer",
      "start": 1847.08,
      "duration": 4.439
    },
    {
      "text": "together so remember when you look at",
      "start": 1849.32,
      "duration": 6.04
    },
    {
      "text": "each row each row has 768 Dimensions",
      "start": 1851.519,
      "duration": 6.961
    },
    {
      "text": "right um or the embedding dimensions and",
      "start": 1855.36,
      "duration": 5.88
    },
    {
      "text": "this self do scale is again a vector",
      "start": 1858.48,
      "duration": 4.96
    },
    {
      "text": "which has embedding Dimensions so you",
      "start": 1861.24,
      "duration": 4.559
    },
    {
      "text": "can do element wise multiplication here",
      "start": 1863.44,
      "duration": 3.839
    },
    {
      "text": "and then you can do an element twise",
      "start": 1865.799,
      "duration": 4.76
    },
    {
      "text": "addition here and the Epsilon parameter",
      "start": 1867.279,
      "duration": 6.561
    },
    {
      "text": "is set to a small value such as 10us 5",
      "start": 1870.559,
      "duration": 4.801
    },
    {
      "text": "but the main step which is being",
      "start": 1873.84,
      "duration": 3.36
    },
    {
      "text": "performed in this normalization layer is",
      "start": 1875.36,
      "duration": 3.96
    },
    {
      "text": "that we take the input and then every",
      "start": 1877.2,
      "duration": 3.959
    },
    {
      "text": "row of the input we subtract with the",
      "start": 1879.32,
      "duration": 3.719
    },
    {
      "text": "mean subtract the mean and divide by the",
      "start": 1881.159,
      "duration": 4.12
    },
    {
      "text": "square root of variance so that the mean",
      "start": 1883.039,
      "duration": 5.52
    },
    {
      "text": "of the resultant row is zero and the",
      "start": 1885.279,
      "duration": 5.041
    },
    {
      "text": "standard deviation of the variance is",
      "start": 1888.559,
      "duration": 3.761
    },
    {
      "text": "equal to",
      "start": 1890.32,
      "duration": 4.079
    },
    {
      "text": "1 that's it this is the layer",
      "start": 1892.32,
      "duration": 5.76
    },
    {
      "text": "normalization class and uh when we bring",
      "start": 1894.399,
      "duration": 6.28
    },
    {
      "text": "all the elements of the GPT architecture",
      "start": 1898.08,
      "duration": 5.0
    },
    {
      "text": "together we'll see that we'll replace",
      "start": 1900.679,
      "duration": 4.0
    },
    {
      "text": "this dummy class now with the layer",
      "start": 1903.08,
      "duration": 4.479
    },
    {
      "text": "normalization class which we just",
      "start": 1904.679,
      "duration": 5.24
    },
    {
      "text": "defined I just have one small note",
      "start": 1907.559,
      "duration": 4.321
    },
    {
      "text": "towards the end which is regarding",
      "start": 1909.919,
      "duration": 3.0
    },
    {
      "text": "biased",
      "start": 1911.88,
      "duration": 3.6
    },
    {
      "text": "variance so uh in our variance",
      "start": 1912.919,
      "duration": 4.561
    },
    {
      "text": "calculation method we have opted for an",
      "start": 1915.48,
      "duration": 3.799
    },
    {
      "text": "implement mation detail by setting",
      "start": 1917.48,
      "duration": 4.76
    },
    {
      "text": "unbiased equal to false so here so you",
      "start": 1919.279,
      "duration": 4.721
    },
    {
      "text": "might be thinking what happens if",
      "start": 1922.24,
      "duration": 4.72
    },
    {
      "text": "unbiased equal to True right if unbiased",
      "start": 1924.0,
      "duration": 4.639
    },
    {
      "text": "equal to True we'll apply something",
      "start": 1926.96,
      "duration": 4.36
    },
    {
      "text": "which is known as besels correction and",
      "start": 1928.639,
      "duration": 4.76
    },
    {
      "text": "in besels correction when we do the",
      "start": 1931.32,
      "duration": 4.04
    },
    {
      "text": "variance calculation we divide by n",
      "start": 1933.399,
      "duration": 4.201
    },
    {
      "text": "minus one instead of dividing by n like",
      "start": 1935.36,
      "duration": 4.88
    },
    {
      "text": "what we did right now so if you look at",
      "start": 1937.6,
      "duration": 4.559
    },
    {
      "text": "this uh",
      "start": 1940.24,
      "duration": 4.36
    },
    {
      "text": "this implementation which I did on the",
      "start": 1942.159,
      "duration": 4.64
    },
    {
      "text": "Whiteboard to calculate the variance I'm",
      "start": 1944.6,
      "duration": 4.319
    },
    {
      "text": "dividing by n right which is the number",
      "start": 1946.799,
      "duration": 6.801
    },
    {
      "text": "of um number of inputs if we do the",
      "start": 1948.919,
      "duration": 7.48
    },
    {
      "text": "besels correction we divide by nus one",
      "start": 1953.6,
      "duration": 5.52
    },
    {
      "text": "so this four is replaced by three this",
      "start": 1956.399,
      "duration": 4.4
    },
    {
      "text": "does not matter too much in the case of",
      "start": 1959.12,
      "duration": 3.0
    },
    {
      "text": "large language models where the",
      "start": 1960.799,
      "duration": 3.321
    },
    {
      "text": "embedding Dimension n is so large that",
      "start": 1962.12,
      "duration": 3.72
    },
    {
      "text": "the difference between n and n minus one",
      "start": 1964.12,
      "duration": 4.039
    },
    {
      "text": "is practically negligible if you are",
      "start": 1965.84,
      "duration": 5.719
    },
    {
      "text": "interested to learn about bessels",
      "start": 1968.159,
      "duration": 5.88
    },
    {
      "text": "correction um there are many good",
      "start": 1971.559,
      "duration": 4.84
    },
    {
      "text": "articles on this and you will see that",
      "start": 1974.039,
      "duration": 4.841
    },
    {
      "text": "it's the use of uh n minus one instead",
      "start": 1976.399,
      "duration": 5.041
    },
    {
      "text": "of n in the formula for sample variance",
      "start": 1978.88,
      "duration": 5.759
    },
    {
      "text": "and that leads to an unbiased uh",
      "start": 1981.44,
      "duration": 6.04
    },
    {
      "text": "variance but for now it totally works",
      "start": 1984.639,
      "duration": 4.721
    },
    {
      "text": "for us if we set the unbiased equal to",
      "start": 1987.48,
      "duration": 5.079
    },
    {
      "text": "false it does not lead to too much of a",
      "start": 1989.36,
      "duration": 5.6
    },
    {
      "text": "difference so the reason we choose this",
      "start": 1992.559,
      "duration": 4.201
    },
    {
      "text": "approach is to ensure compatibility with",
      "start": 1994.96,
      "duration": 5.559
    },
    {
      "text": "gp2 Mod gpt2 models normalization layers",
      "start": 1996.76,
      "duration": 6.24
    },
    {
      "text": "and uh it reflects tensor flows default",
      "start": 2000.519,
      "duration": 4.441
    },
    {
      "text": "Behavior which is to set unbiased equal",
      "start": 2003.0,
      "duration": 4.559
    },
    {
      "text": "to false this was actually what was used",
      "start": 2004.96,
      "duration": 5.079
    },
    {
      "text": "to implement the original gpt2 model and",
      "start": 2007.559,
      "duration": 4.201
    },
    {
      "text": "that's why we are also using unbiased",
      "start": 2010.039,
      "duration": 4.52
    },
    {
      "text": "equal to false over here so now that we",
      "start": 2011.76,
      "duration": 4.639
    },
    {
      "text": "have defined a class for layer",
      "start": 2014.559,
      "duration": 3.881
    },
    {
      "text": "normalization let's try the layer",
      "start": 2016.399,
      "duration": 4.201
    },
    {
      "text": "normalization in practice and apply it",
      "start": 2018.44,
      "duration": 4.359
    },
    {
      "text": "to the batch input right so we have",
      "start": 2020.6,
      "duration": 4.36
    },
    {
      "text": "defined this batch over",
      "start": 2022.799,
      "duration": 5.72
    },
    {
      "text": "here this batch over here let's try to",
      "start": 2024.96,
      "duration": 5.199
    },
    {
      "text": "uh pass in this batch through the layer",
      "start": 2028.519,
      "duration": 4.361
    },
    {
      "text": "normalization layer so I'm defining a",
      "start": 2030.159,
      "duration": 4.801
    },
    {
      "text": "layer normalization class I'm creating",
      "start": 2032.88,
      "duration": 5.08
    },
    {
      "text": "an instance of it and as I told you over",
      "start": 2034.96,
      "duration": 4.4
    },
    {
      "text": "here we need to pass in the embedding",
      "start": 2037.96,
      "duration": 3.76
    },
    {
      "text": "Dimension right so I'm saying that the",
      "start": 2039.36,
      "duration": 4.72
    },
    {
      "text": "EM embedding Dimension is equal to five",
      "start": 2041.72,
      "duration": 5.16
    },
    {
      "text": "so I'm taking in batch example so let's",
      "start": 2044.08,
      "duration": 6.44
    },
    {
      "text": "see where batch example has been",
      "start": 2046.88,
      "duration": 3.64
    },
    {
      "text": "defined yeah so this is the batch",
      "start": 2051.079,
      "duration": 5.32
    },
    {
      "text": "example I'm taking in this example and",
      "start": 2053.639,
      "duration": 5.601
    },
    {
      "text": "then what I'm doing is that I am",
      "start": 2056.399,
      "duration": 4.841
    },
    {
      "text": "applying the layer normalization layer",
      "start": 2059.24,
      "duration": 3.32
    },
    {
      "text": "to this",
      "start": 2061.24,
      "duration": 3.639
    },
    {
      "text": "batch and then it will do all of the",
      "start": 2062.56,
      "duration": 4.64
    },
    {
      "text": "steps which I have mentioned over here",
      "start": 2064.879,
      "duration": 4.641
    },
    {
      "text": "first it will will find the mean over",
      "start": 2067.2,
      "duration": 4.12
    },
    {
      "text": "all the dimensions and then it will",
      "start": 2069.52,
      "duration": 3.8
    },
    {
      "text": "essentially find the variance it will",
      "start": 2071.32,
      "duration": 4.079
    },
    {
      "text": "subtract the mean and then divide by",
      "start": 2073.32,
      "duration": 4.64
    },
    {
      "text": "square root of variance so let me go to",
      "start": 2075.399,
      "duration": 4.28
    },
    {
      "text": "the batch example again and tell you",
      "start": 2077.96,
      "duration": 5.08
    },
    {
      "text": "what it's going to do um let me print",
      "start": 2079.679,
      "duration": 5.081
    },
    {
      "text": "out the batch example actually right",
      "start": 2083.04,
      "duration": 2.599
    },
    {
      "text": "over",
      "start": 2084.76,
      "duration": 5.079
    },
    {
      "text": "here so if you print",
      "start": 2085.639,
      "duration": 4.2
    },
    {
      "text": "out print out the batch example you'll",
      "start": 2092.72,
      "duration": 6.32
    },
    {
      "text": "see that it's this right so so what this",
      "start": 2095.399,
      "duration": 6.281
    },
    {
      "text": "layer normalization will do is that it",
      "start": 2099.04,
      "duration": 4.28
    },
    {
      "text": "will first take the mean of this",
      "start": 2101.68,
      "duration": 3.76
    },
    {
      "text": "subtract mean from all of these elements",
      "start": 2103.32,
      "duration": 4.44
    },
    {
      "text": "and divide all of these elements by",
      "start": 2105.44,
      "duration": 4.919
    },
    {
      "text": "square root of the variance of this",
      "start": 2107.76,
      "duration": 4.96
    },
    {
      "text": "first batch then it will do the same for",
      "start": 2110.359,
      "duration": 4.201
    },
    {
      "text": "the second batch and then when you get",
      "start": 2112.72,
      "duration": 3.879
    },
    {
      "text": "the mean and the variance of the",
      "start": 2114.56,
      "duration": 3.519
    },
    {
      "text": "resulting",
      "start": 2116.599,
      "duration": 5.161
    },
    {
      "text": "output so output Ln is my resulting",
      "start": 2118.079,
      "duration": 5.201
    },
    {
      "text": "output and if you get the mean and the",
      "start": 2121.76,
      "duration": 3.079
    },
    {
      "text": "variance you'll see that the mean for",
      "start": 2123.28,
      "duration": 4.44
    },
    {
      "text": "both the batches is zero and the",
      "start": 2124.839,
      "duration": 4.361
    },
    {
      "text": "standard deviation or the variance for",
      "start": 2127.72,
      "duration": 3.96
    },
    {
      "text": "both the batches is equal to one so as",
      "start": 2129.2,
      "duration": 4.72
    },
    {
      "text": "we can clearly see based on the results",
      "start": 2131.68,
      "duration": 4.919
    },
    {
      "text": "the layer normalization code works as",
      "start": 2133.92,
      "duration": 5.32
    },
    {
      "text": "expected and normalizes the value of",
      "start": 2136.599,
      "duration": 4.76
    },
    {
      "text": "each of the two inputs such that they",
      "start": 2139.24,
      "duration": 4.68
    },
    {
      "text": "have a mean of zero and variance of",
      "start": 2141.359,
      "duration": 4.921
    },
    {
      "text": "one uh so I hope you have understood",
      "start": 2143.92,
      "duration": 4.48
    },
    {
      "text": "this concept I tried to explain this",
      "start": 2146.28,
      "duration": 3.92
    },
    {
      "text": "through through whiteboard as well as",
      "start": 2148.4,
      "duration": 3.679
    },
    {
      "text": "through code and I hope that you have",
      "start": 2150.2,
      "duration": 4.08
    },
    {
      "text": "understood the basics as well as the",
      "start": 2152.079,
      "duration": 4.881
    },
    {
      "text": "coding aspect of it now let's see what",
      "start": 2154.28,
      "duration": 4.36
    },
    {
      "text": "all we have learned in this lecture so",
      "start": 2156.96,
      "duration": 4.0
    },
    {
      "text": "far so in this lecture what we have",
      "start": 2158.64,
      "duration": 5.4
    },
    {
      "text": "learned is that we learned about layer",
      "start": 2160.96,
      "duration": 5.0
    },
    {
      "text": "normalization and let's see how this",
      "start": 2164.04,
      "duration": 4.16
    },
    {
      "text": "fits into the context of the entire GPT",
      "start": 2165.96,
      "duration": 4.44
    },
    {
      "text": "architecture so to master the GPT",
      "start": 2168.2,
      "duration": 3.72
    },
    {
      "text": "architecture we need to learn all of",
      "start": 2170.4,
      "duration": 3.6
    },
    {
      "text": "these building blocks in the previous",
      "start": 2171.92,
      "duration": 4.76
    },
    {
      "text": "lecture we learned about the GPT",
      "start": 2174.0,
      "duration": 4.92
    },
    {
      "text": "Backbone in this lecture we looked at",
      "start": 2176.68,
      "duration": 4.72
    },
    {
      "text": "layer normalization in the next lecture",
      "start": 2178.92,
      "duration": 3.96
    },
    {
      "text": "we are going to look at this thing",
      "start": 2181.4,
      "duration": 3.88
    },
    {
      "text": "called G activation then we'll look at",
      "start": 2182.88,
      "duration": 4.12
    },
    {
      "text": "feed forward Network and then we'll look",
      "start": 2185.28,
      "duration": 4.12
    },
    {
      "text": "at shortcut connections so in separate",
      "start": 2187.0,
      "duration": 3.96
    },
    {
      "text": "lectures we're going to cover all of",
      "start": 2189.4,
      "duration": 3.24
    },
    {
      "text": "this and then you will see that all of",
      "start": 2190.96,
      "duration": 4.0
    },
    {
      "text": "this essentially comes together to teach",
      "start": 2192.64,
      "duration": 4.8
    },
    {
      "text": "us about the Transformer",
      "start": 2194.96,
      "duration": 4.8
    },
    {
      "text": "block and only then we'll be fully",
      "start": 2197.44,
      "duration": 4.2
    },
    {
      "text": "equipped to understand the GPT",
      "start": 2199.76,
      "duration": 4.04
    },
    {
      "text": "architecture I don't think there are any",
      "start": 2201.64,
      "duration": 3.8
    },
    {
      "text": "other YouTube videos out there which",
      "start": 2203.8,
      "duration": 4.319
    },
    {
      "text": "cover the GPT architecture from scratch",
      "start": 2205.44,
      "duration": 5.32
    },
    {
      "text": "in so much depth but I believe this much",
      "start": 2208.119,
      "duration": 4.0
    },
    {
      "text": "understanding is necessary for you to",
      "start": 2210.76,
      "duration": 3.28
    },
    {
      "text": "truly understand how the llm",
      "start": 2212.119,
      "duration": 4.561
    },
    {
      "text": "architecture works one last point which",
      "start": 2214.04,
      "duration": 4.72
    },
    {
      "text": "I need to mention is is that in this",
      "start": 2216.68,
      "duration": 5.159
    },
    {
      "text": "lecture U sometimes I said batch",
      "start": 2218.76,
      "duration": 4.72
    },
    {
      "text": "normalization and then I corrected to",
      "start": 2221.839,
      "duration": 4.041
    },
    {
      "text": "layer normalization right if you also",
      "start": 2223.48,
      "duration": 5.119
    },
    {
      "text": "have this confusion remember that layer",
      "start": 2225.88,
      "duration": 4.32
    },
    {
      "text": "and batch normalization are very",
      "start": 2228.599,
      "duration": 3.601
    },
    {
      "text": "different from each other in layer",
      "start": 2230.2,
      "duration": 4.08
    },
    {
      "text": "normalization we usually normalize along",
      "start": 2232.2,
      "duration": 3.48
    },
    {
      "text": "the feature Dimension which is the",
      "start": 2234.28,
      "duration": 4.28
    },
    {
      "text": "columns and layer normalization does not",
      "start": 2235.68,
      "duration": 5.96
    },
    {
      "text": "depend on the batch size at all no",
      "start": 2238.56,
      "duration": 5.08
    },
    {
      "text": "matter what the batch size is we just",
      "start": 2241.64,
      "duration": 3.56
    },
    {
      "text": "take the output of every layer and",
      "start": 2243.64,
      "duration": 3.52
    },
    {
      "text": "normalize it based on the mean and the",
      "start": 2245.2,
      "duration": 3.96
    },
    {
      "text": "standard standard deviation batch",
      "start": 2247.16,
      "duration": 3.64
    },
    {
      "text": "normalization on the other hand we do",
      "start": 2249.16,
      "duration": 4.0
    },
    {
      "text": "the normalization for an entire batch so",
      "start": 2250.8,
      "duration": 5.44
    },
    {
      "text": "it definitely depends on the batch size",
      "start": 2253.16,
      "duration": 5.56
    },
    {
      "text": "now U the main issue is that the",
      "start": 2256.24,
      "duration": 4.359
    },
    {
      "text": "available Hardware typically dictates",
      "start": 2258.72,
      "duration": 4.16
    },
    {
      "text": "the batch size if you if you don't have",
      "start": 2260.599,
      "duration": 4.601
    },
    {
      "text": "too powerful of a hardware you might",
      "start": 2262.88,
      "duration": 4.239
    },
    {
      "text": "need to use a lower batch",
      "start": 2265.2,
      "duration": 4.68
    },
    {
      "text": "size so batch normalization is not that",
      "start": 2267.119,
      "duration": 4.561
    },
    {
      "text": "flexible because it depends on the batch",
      "start": 2269.88,
      "duration": 4.12
    },
    {
      "text": "size which depends which depends on the",
      "start": 2271.68,
      "duration": 4.48
    },
    {
      "text": "available Hardware on the other hand",
      "start": 2274.0,
      "duration": 4.92
    },
    {
      "text": "layer normalization is pretty flexible",
      "start": 2276.16,
      "duration": 4.76
    },
    {
      "text": "it leads to more flexibility and",
      "start": 2278.92,
      "duration": 3.76
    },
    {
      "text": "training for it leads to more",
      "start": 2280.92,
      "duration": 3.72
    },
    {
      "text": "flexibility and stability for",
      "start": 2282.68,
      "duration": 5.04
    },
    {
      "text": "distributed training so if you have if",
      "start": 2284.64,
      "duration": 4.52
    },
    {
      "text": "you are in environments which lack",
      "start": 2287.72,
      "duration": 3.44
    },
    {
      "text": "resources and Hardware capabilities are",
      "start": 2289.16,
      "duration": 5.08
    },
    {
      "text": "not there which leads to low batches Etc",
      "start": 2291.16,
      "duration": 4.8
    },
    {
      "text": "or if you don't want to care about batch",
      "start": 2294.24,
      "duration": 4.24
    },
    {
      "text": "size you want the normalization to be",
      "start": 2295.96,
      "duration": 4.6
    },
    {
      "text": "independent of the batch size layer",
      "start": 2298.48,
      "duration": 4.4
    },
    {
      "text": "normalization is much better in terms of",
      "start": 2300.56,
      "duration": 5.039
    },
    {
      "text": "flexibility so don't make the error or",
      "start": 2302.88,
      "duration": 4.64
    },
    {
      "text": "don't make the confusion between layer",
      "start": 2305.599,
      "duration": 4.72
    },
    {
      "text": "normalization versus batch",
      "start": 2307.52,
      "duration": 4.839
    },
    {
      "text": "normalization this brings us to the end",
      "start": 2310.319,
      "duration": 4.161
    },
    {
      "text": "of today's lecture as I mentioned in the",
      "start": 2312.359,
      "duration": 4.681
    },
    {
      "text": "subsequent lectures we'll talk about Jou",
      "start": 2314.48,
      "duration": 4.359
    },
    {
      "text": "we'll talk about feed forward Network",
      "start": 2317.04,
      "duration": 3.36
    },
    {
      "text": "and we'll also talk about shortcut",
      "start": 2318.839,
      "duration": 3.801
    },
    {
      "text": "connections and then later we'll see how",
      "start": 2320.4,
      "duration": 5.08
    },
    {
      "text": "all of it comes together to make the GPT",
      "start": 2322.64,
      "duration": 4.92
    },
    {
      "text": "architecture thank you so much everyone",
      "start": 2325.48,
      "duration": 3.72
    },
    {
      "text": "and I look forward to seeing you in the",
      "start": 2327.56,
      "duration": 4.92
    },
    {
      "text": "next lecture",
      "start": 2329.2,
      "duration": 3.28
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch Series in today's lecture we are going to look at one specific component of the large language model architecture and that is layer normalization before that let me quickly recap what we covered in the last lecture in the last lecture we essentially had a bird ey view of how the llm architecture looks like so what we are going to do is that the llm architecture consists of several components which are arranged together uh yes uh in the previous lecture we constructed this GPT backbone which is like a dummy class which will hold all these components the first component is the layer normalization Jou activation then the feed forward neural network and then shortcut connections these four components make up the Transformer block which is one of the key components of the entire GPT architecture so let me show you how this looks like so if you zoom into the Transformer block you'll see that the Transformer block has a number of layers which are stacked together you have a layer normalization followed by The Mask multi-head attention layer followed by the Dropout layer you have the shortcut connections then another layer normalization a feed forward neural network Dropout layer and then another shortcut connections Etc if you zoom into the feed for neural network you'll see that it it consists of linear neural network layers along with the Jou activation function uh all of these are stacked together in a format which looks something like this so let me take you to the broadest level view looks something like this so you have the input text which you tokenize then you pass it into the GPT model in the GPT model we first do the token embedding then add the positional embedding vectors pass these input embeddings to the Transformer block within the Transformer block it goes through all the layers which I just showed you and then we have the output layers which essentially decode the output from the Transformer block and we predict the next word from the input this is what happens inside the GPT architecture and uh we are going to start looking at every single component of this GPT architecture today we are going to look at this component which is called as layer normalization so if you look at layer normalization it actually comes up at multiple places within the Transformer block itself if you see before the multi-head attention we have a layer normalization after the multi-head attention also we have a layer normalization so the layer normalization comes within the Transformer block at multiple places and actually it also comes outside of the Transformer block um that's why it's important to define a separate class for the layer normalization when we code the GPT architecture lecture in the last lecture we looked at this dummy GPT model class and the goal here was to put all of the different components together so that you get a bird's eye view Without Really coding too many of these components so what happens in this GPT model class is that there is this forward method which takes in the input and then we uh convert the input into token embeddings we add the positional embeddings to the Token embeddings the resulting embedding Vector we add the Dropout layer to it we pass it through the Transformer blocks then even after it comes out from the Transformer we have a normalization layer as I showed you within the Transformer block itself there is a normalization layer before the attention head and after the attention head and after we come out of the Transformer block there is another normalization layer and then there are output processing steps and the output is returned right so to execute this forward method we need to Define several things so when an instance of the dummy GPT model class is created we uh we Define these Transformer blocks and for that we'll need to create a separate class called Demi Transformer block which will ultimately become just Transformer block when we learn about what happens in the Transformer and then we also create a separate class for layer normalization why do we create a separate class because layer normalization not just happens in the Transformer otherwise we would have just written this procedure here but as you can see even after the input comes from the Transformer we pass it through another layer of normalization so that's why it makes sense to define a separate class of layer normalization in today's lecture we are going to dive deep into this class how to create this class what exactly does layer normalization do and why do we need it in the first place so let's get started diving deep into layer normalization okay so the reason layer normalization comes into to the picture is that when we look at the Transformer block as you can see number of these layers are stacked on top of each other right so there are huge number of parameters coming from each of these layers and we have to train these parameters initially we'll train these initialize these parameters to random values but through back propagation we'll train these parameters so that the next word is predicted correctly so we need to make the training process more efficient and that's where layer normaliz ation becomes very important it turns out that training deep neural networks with many layers can be challenging predominant due to two things it can either lead to a Vanishing gradient problem or it can lead to an exploding gradient problem and this leads to unstable training Dynamics so the first advantage of layer normalization is that it improves really the stability and the efficiency of neural network training so let me zoom into this further and explain this if you consider a deep neural network like this you have an input layer and then you have multiple hidden layers which are stacked together after the input layer right and you have the output layer so what happens is that when you do a forward pass so initially when the weights are initialized randomly you do a forward pass and you get the outputs and then what you do is that based on the gradients you do a backward pass and you try to update these parameters So eventually for every of these hidden layers you will have gradients of the loss with respect to the parameters so when you look at every layer think of layers as accumulating gradients every layer will have a certain set of gradient values which will get updated after every iteration right now since we are doing the backward pass let's say if we want to find the gradients of this layer um the gradients of this layer will depend a lot on the output of this layer because we are doing backward pass let me show it with a different color uh yeah so the output of this layers I'm showing with the red color and if you want to find the gradients of the first layer it depends on this red colored outputs since we are doing the backward pass now it turns out that if the layer output if this layer output which which I'm showing by red right now if this is too large or this is too small that affects the gradients so if the layer output is too large or too small then gradient magnitudes can become too large or too small now think about what will happen if gradient magnitudes become too large let's say so if we are back propagating and the gradient magnitude in this layer becomes too large what will happen when you back propagate essentially you are multiplying different gradients together right so if the gradient here becomes too large by the time we reach the first layer the gradient would have exploded to a very large value that's called as the gradient explosion on the other hand if the gradient of the last layer or one of the intermediate layers is very small when you're propagating backwards till the time you reach the first layer or the second layer the gradient will become very small what will happen when the gradient becomes very small we will not update the parameters because the parameter updates depend on the gradient magnitude if the gradient is too small learning will stagnate if the gradient is too large will lead to that will lead to an unstable learning procedure so both small gradients and large gradients lead to unstable training Dynamics we do not want unstable training Dynamics and one reason for the unstable training Dynamics is that if layer outputs themselves are very large or small as I mentioned layer outputs affect the gradient values so if we control the magnitude of the layer outputs we can ensure that the gradient magnitudes themselves do not become too large or too small batch normalization helps this batch normalization helps uh keep the outputs of the layers to certain specific values and prevents the magnitude of the output from being too large or too small and that's what keeps the gradient stable which leads to stable training Dynamics that's one of the first reasons why batch normalization or I should call it layer normalization there is a difference right so I should actually call this layer normalization batch normalization is something different and we'll come to that later today we are only going to look at layer normalization in which the outputs coming from every layer are normalized the second advantage of layer normalization is that it prevents this problem which is called as internal coate shift so what happens is that as training proceeds the inputs to every layer can change let's say we look at the second layer right in in first training iteration the inputs can have a distribution like this in the second training iteration maybe the inputs are skewed so the input distribution which every layer receives can change according to the iterations and what that leads to is that it makes training very difficult so if the input distribution to every layer is changing the weights the updating the weights becomes very hard and that delays the convergence of the parameters and that delays the overall solution reaching an optimal value we don't want that layer normalization really helps to prevent this layer normalization make sure that um since we are normalizing which means that as we'll see the variance of the standard deviation is kept to one we'll make sure that the mean and standard deviation of the output from every layer is fixed and this reduces the problem of internal coari shift which accelerates convergence so there are two main reasons why layer normalization is employed the first reason is that it keeps the training procedure stable by preventing the vanishing gradient or the exploding gradient problem and the second uh the second major reason for using layer normalization is that it prevents or reduces the problem of internal covariate shift and that accelerates convergence and we get to a result for faster so that's why layer normalization is employed not just in the GPT or the llm architecture which we are going to see right now but in fact in many deep learning architectures layer normalization is very frequently used okay uh so what exactly is the main idea of layer normalization it's very simple so we look at a specific layer and we'll look at the outputs of that specific layer and what we'll do is that we'll adjust those outputs so that they have a mean of zero and they have a variance of one let me illustrate this through a simple example let's say you are looking at a neural network and these four are the outputs from one specific layer of the neural network right uh so the four outputs are X1 X2 X3 and X4 and X1 is equal to 1.1 X2 is8 X3 is 2.3 and X4 is 4.4 in layer normalization what you do is you find two quantities first you find find mean of this so the mean will just be X1 + X2 + X3 + X4 / 4 so in this case it will be 2.15 and the second thing which you do is you find the variance so the variance will be 1X 4 because there are four um quantities here and then we'll sum up X1 minus the mean whole square + X2 - the mean square + x3 - the mean whole square + X4 minus the mean whole Square so that gives us the variance value now what we do is we perform the normalization procedure which means that for every variable we subtract the mean and we divide by the square root of the variance which is the stand standard deviation so X1 will be replaced by X1 minus mu / square root of variance X2 will be replaced by X2 - mu / square root of variance X3 will be replaced by x3 - mu / square root of variance and X4 will be replaced by X4 minus mu / square root of variance so when you do this normalization it leads to these four values do you notice something about these four normalized values if you add these together in the numerator you will have X1 + X2 + X3 + X4 - 4 * mu so that will be zero which means that the mean of this these normalized values is equal to zero and if you compute the variance of these normalized values through this formula you'll see that the variance of these normalized values is is actually equal to 1 that's the most important thing which uh which is which you should realize or you you should understand is that after performing the normalization procedure uh the values which you get these four values their mean is equal to zero and their variance is equal to one that's the whole idea behind normalization the in normalization we adjust the outputs of every layer of neural network to have mean of zero and variance of one and it turns out that this simple procedure helps us in the stability neural network training and it also helps us reduce the problem of the internal coari shift um so let us actually uh see this in code but before that I want to tell you where the layer normalization is used and uh we discussed this at the start of the lecture but um the input is converted into an input embedding then we add the token embeddings and then here right we feed it before going into the multi head attention we have a layer normalization layer so that the inputs to the multi-ad attention are normalized even after the multi attention there is a layer normalization layer before feeding into the neural network module within the Transformer block remember this Blue Block here is the Transformer block so the layer normalization layer appears two times here and then it appears once more again outside the Transformer uh so in GPT and modern Transformer architecture layer normalization is typically applied before and after the multi-head attention module like what we have seen over here and it also appears once before the final output layer and we saw this when we coded in the last lecture here if you see within the Transformer block layer normalization appears two times before and after the multi-ad attention but even before the output it we employ it once so overall it appears three times that's why we need to define a separate class of the layer normalization this one figure which I'm which I've shown over here actually illustrates the procedure of layer normalization um let's say we have a neural network layer these are the five inputs uh to the neural network right and these are the six outputs of the neural network without the normalization you'll see that their mean is not equal to zero and their variance is not equal to one but after we perform normalization on these layer outputs which means that for from every output here we are going to subtract the mean and divide by the square root of the variance so you'll get these as the resultant values after applying layer normalization and if you take a mean of these values you'll get that mean is equal to zero and the variance of these values is equal to one this is the simple illustration which describes uh what happens underneath the hood for layer normalization now what we are going to do is that I'm going to take you through code and we are going to implement layer normalization first on a neural network which looks like this and then we are going to create a separate class of layer normalization which we can integrate within our GPT architecture so let's jump into code right now all right so here's the code file for layer normalization the first thing which we are going to do is start out with a simple example to illustrate how layer normalization is implemented in practice and then we will actually fill out this layer normalization class which we had created in the previous lecture so let's get started what we are doing here is that we are essentially let me take you to the white board to demonstrate what we are doing here we'll have a simple neural network layer and the neural network is constructed such that we have two batches of inputs so here is batch number one and here is batch number two and each batch has five inputs so batch number one has X1 X2 X3 X for X5 and batch number two has X1 X2 X3 X4 H5 X5 now here we are looking at one layer of neurons and there are six neurons here so when these inputs essentially pass through this first layer of neurons we have the output which is produced and uh there will be six outputs for batch number one which is y1 Y2 Y3 y4 y5 Y6 and there will be six outputs for batch number two y1 Y2 Y3 y4 y5 Y6 so if you look at the inputs the shape of the inputs will be two rows and five columns because we have two batches and each batch will have five inputs right then we have a sequential layer which essentially takes in five inputs and it has six outputs this sequential layer is this uh the second layer which I've shown you over here this layer of six neurons and after every neuron here we essentially have The Rao activation function so which has been mentioned over here if you don't know what Rao activation function is it's fine for this lecture we don't need to understand this uh the output of the layer is that the layer is then applied on this input batch and we get the output can you try to understand why the shape of the output is like this so here you can see that we have two rows and we have six columns if you look at the first row this represents the six outputs from the first um batch and if you look at the second row this represents the six outputs from the uh second batch and that's what exactly being shown here y1 Y2 Y6 and batch two has y1 Y2 up till Y6 so this can be y1 this is Y2 this is Y3 this is Y6 for batch one and this is y1 this is Y2 and this is Y6 for batch number two okay so this is the layer which we have and now what we are we are going to do is that after this layer we are going to uh apply the batch normalization uh sorry we are going to apply the layer normalization so uh here I have simply explained that we have a neural network which consists of a linear layer followed by The Rao activation layer to quickly illustrate what The Rao activation function actually looks like take a look at this this image over here so if x is positive The Rao is just y = to X but if x is equal to negative The Rao is zero so there is a nonlinearity here that's The Rao activation function now what we are going to do as I mentioned is we are going to apply layer normalization so the way it is applied is very similar to the Hands-On example which we saw on the white board over here this example uh the same thing will be applying to the first batch and the same thing will be applying to the second batch so what we'll be doing is that when you look at the first batch we will do y1 minus so y1 will be replaced by y1 - mu divided by uh square root of we write this again divided by square root of variance Y2 will be replaced by Y2 minus mu uh divided square root of variance and like this similarly the last output which is Y6 here Y6 will also be replaced with Y6 minus mu divided by square root of variance and first we process the first batch and then we process the second batch in a very similar manner so now let's go to code to see how this is done so now what we are going to do is that we we have the output which is this tensor right and then we are doing output do mean Dimension equal to minus1 why Dimension equal to minus1 because we have to take the mean along the columns so first we look at the first batch outputs and we want to take the mean of this so we do output do mean Dimension equal to minus1 and this keep Dimension equal to true that is very important because if we don't include keep Dimension equal to true The Returned mean would be a two dimensional Vector instead of a two into one dimensional Matrix so essentially uh if you use keep dim equal to true the output which you get for the mean is this so the first value here corresponds to the mean of the first batch the second value here corresponds to the mean of the second batch since we used keyd equal to true the shape of this output is that it's a matrix uh or rather uh yeah it's a a two into one dimensional Matrix over here right now if we did not use keep dim equal to true this would not be a matrix in fact it would just be a two- dimensional vector and that's generally not good because it's good for the dimensions to be preserved as we are doing all of these calculations similarly for the variance what we are doing is that we are taking the variance across the column for both the batches and we use keep them equal to true and then you print out the mean and then you print out the variance for every batch so for the first batch of data the mean is. 1324 for the second for the first batch of data the variance is 0.02 31 for the second batch of data the mean is 216 2170 and for the second batch of data the variance is 0.398 so remember the two uh uh two commands which we have used here dim equal to minus1 because we have to perform that operation along the columns and keep dim equal to true because we have to retain um the dimension of the final mean and the variance Matrix which we have if we did not use keep D equal to True later when we subtract this mean from every individual element it will lead to some problems so we want to avoid that so in this text over here I have just explained why we used keep dim equal to true and why we used Dimension equal to minus1 so if you have some confusion along those lines please read this text when I share this Google or when I share this Jupiter notebook with you great and now what we are going to do is that we are going to uh subtract the mean so like over here we are going to subtract the mean and divide by the square root of variance so we have the output Matrix which is there we are going to subtract the mean which is now again you can see the mean is also a tensor which has two rows and one column and we are going to subtract the mean from the output and we are going to divide by the square root of variable this is the main normalization step so this is my output now and the normalized layer outputs are given like this uh the first row again corresponds to the normalized outputs of batch one the second row corresponds to the normalized outputs of batch number two so here I'm just printing out the mean and variance of the batch one and batch two so if you look batch one and batch two so if you look at the mean you'll see that the mean of the first batch is almost close to zero this is is 10us 8 which is really very close to zero we can approximate it to zero for the second batch the mean is again very close to 10us 8 again that's almost equal to zero and if you look at the variance for both the batches you'll see that the variance is equal to one awesome this is exactly what we wanted right which means that the layers have been normalized now so note that the value 2.9 into 10- 8 is the scientific notation for 2.9 * 10us 8 this value is very close to zero but not exactly zero due to small numerical errors in Python what we have is this uh we can turn on the turn off the scientific mode So currently the scientific notation is on that's why we are getting these uh um values which have been represented in the scientific notation we can turn off the scientific notation and then let's print out the mean and the variance so you'll see that the mean for both the batches is equal to zero and the variance for both the batches is equal to one great and now we'll achieve the goal which we started out this lecture with we want to create a class for layer normalization um what would be the output of this class basically this class will take in the um the output of a layer and it will apply the um normalization to that so let's look at where this layer normalization step is implemented so the layer normalization step is is implemented here the layer normalization step is implemented here so at both of these places when we get uh when the inputs are received to this block and when the input is received to this block um we have certain number of tokens uh which is let's say let's say we are looking at the uh Contex size for the number of embedding vectors which we have but the main thing which I want to point out is that the the number of columns which we have is equal to the embedding size and this is the embedding Vector Dimension which we are using so for gpt2 this embedding size is equal to 768 so when we look at the so when we look at let's say the first row over here the first row corresponds to the embedding for the first token which is an input to this layer normalization right so so we will take the mean and we'll take the variance along the column Dimension which is 768 so we'll take the mean of all of this and take the variance and then do the normalization similarly we'll do this for every single row um for the input to this layer normalization as well as the input to this layer normalization so if you see the dimension when an instance of this class is created we have to pass in the embedding Dimension why do we have to pass in the embedding Dimension because we'll see that we are going to implement something like scale and shift these are trainable parameters but the size of the scale and shift will be governed by the embedding Dimension and that's the same as the input Vector to the layer Norm module and that will be the same as the output Vector so the input to the layer Norm module will have certain number of rows but it will have embedding columns the output of the layer normalization which is Norm X will have the same dimensions as the input because normalization does not change dimensions and we are going to scale the output with the scale and shift I'll come to that in a moment so the main part of this layer normalization class is the forward method which takes in the input which I described so when you think of the input think of the input as having certain number of rows but mostly focus on the number of columns which will be the embedding dimensions in each row let's say we have 768 embedding Dimension so in the first step what we do is that along the column we take the mean ex exactly similar to what we actually did over here just keep this example in mind um then what we do is along the column we take the variance and then we subtract the mean and then we divide by the square root of variance note that we have added uh this small variable Epsilon so this is a small constant which is added to the variance to prevent division by zero during normalization so we don't want to divide by the square root of zero right so we add a small uh variable here which is called self. Epsilon or Epsilon so this is the output of the normaliz layer normalization but we do one more step here which is we multiply by the scale and we multiply by the shift so let me explain what the scale and shift are uh just like we are training we are going to train the embedding parameters the positional embedding parameters the neural network parameters in the GPT architecture the scale and shelf uh the scale and shift are two trainable parameters which have the same Dimension as the input that the llm automatically adjusts during training if it is determined that doing so would improve the model's performance on the training task so this allows the model to learn appropriate scaling and shifting that best suits the data uh it is processing so you can think of it as knobs turnable knobs or fine-tuning parameters which we have added just to make sure that uh the layer normalization proceeds smoothly or if we want to tweak the layer normaliz ation of bit we can always do it with the scale and with the shift these are trainable parameters which means we don't specify the values of these parameters at the start and remember that the these scale and shift have the embedding Dimension so that we can multiply the U the scale and the output of the normalization layer together so remember when you look at each row each row has 768 Dimensions right um or the embedding dimensions and this self do scale is again a vector which has embedding Dimensions so you can do element wise multiplication here and then you can do an element twise addition here and the Epsilon parameter is set to a small value such as 10us 5 but the main step which is being performed in this normalization layer is that we take the input and then every row of the input we subtract with the mean subtract the mean and divide by the square root of variance so that the mean of the resultant row is zero and the standard deviation of the variance is equal to 1 that's it this is the layer normalization class and uh when we bring all the elements of the GPT architecture together we'll see that we'll replace this dummy class now with the layer normalization class which we just defined I just have one small note towards the end which is regarding biased variance so uh in our variance calculation method we have opted for an implement mation detail by setting unbiased equal to false so here so you might be thinking what happens if unbiased equal to True right if unbiased equal to True we'll apply something which is known as besels correction and in besels correction when we do the variance calculation we divide by n minus one instead of dividing by n like what we did right now so if you look at this uh this implementation which I did on the Whiteboard to calculate the variance I'm dividing by n right which is the number of um number of inputs if we do the besels correction we divide by nus one so this four is replaced by three this does not matter too much in the case of large language models where the embedding Dimension n is so large that the difference between n and n minus one is practically negligible if you are interested to learn about bessels correction um there are many good articles on this and you will see that it's the use of uh n minus one instead of n in the formula for sample variance and that leads to an unbiased uh variance but for now it totally works for us if we set the unbiased equal to false it does not lead to too much of a difference so the reason we choose this approach is to ensure compatibility with gp2 Mod gpt2 models normalization layers and uh it reflects tensor flows default Behavior which is to set unbiased equal to false this was actually what was used to implement the original gpt2 model and that's why we are also using unbiased equal to false over here so now that we have defined a class for layer normalization let's try the layer normalization in practice and apply it to the batch input right so we have defined this batch over here this batch over here let's try to uh pass in this batch through the layer normalization layer so I'm defining a layer normalization class I'm creating an instance of it and as I told you over here we need to pass in the embedding Dimension right so I'm saying that the EM embedding Dimension is equal to five so I'm taking in batch example so let's see where batch example has been defined yeah so this is the batch example I'm taking in this example and then what I'm doing is that I am applying the layer normalization layer to this batch and then it will do all of the steps which I have mentioned over here first it will will find the mean over all the dimensions and then it will essentially find the variance it will subtract the mean and then divide by square root of variance so let me go to the batch example again and tell you what it's going to do um let me print out the batch example actually right over here so if you print out print out the batch example you'll see that it's this right so so what this layer normalization will do is that it will first take the mean of this subtract mean from all of these elements and divide all of these elements by square root of the variance of this first batch then it will do the same for the second batch and then when you get the mean and the variance of the resulting output so output Ln is my resulting output and if you get the mean and the variance you'll see that the mean for both the batches is zero and the standard deviation or the variance for both the batches is equal to one so as we can clearly see based on the results the layer normalization code works as expected and normalizes the value of each of the two inputs such that they have a mean of zero and variance of one uh so I hope you have understood this concept I tried to explain this through through whiteboard as well as through code and I hope that you have understood the basics as well as the coding aspect of it now let's see what all we have learned in this lecture so far so in this lecture what we have learned is that we learned about layer normalization and let's see how this fits into the context of the entire GPT architecture so to master the GPT architecture we need to learn all of these building blocks in the previous lecture we learned about the GPT Backbone in this lecture we looked at layer normalization in the next lecture we are going to look at this thing called G activation then we'll look at feed forward Network and then we'll look at shortcut connections so in separate lectures we're going to cover all of this and then you will see that all of this essentially comes together to teach us about the Transformer block and only then we'll be fully equipped to understand the GPT architecture I don't think there are any other YouTube videos out there which cover the GPT architecture from scratch in so much depth but I believe this much understanding is necessary for you to truly understand how the llm architecture works one last point which I need to mention is is that in this lecture U sometimes I said batch normalization and then I corrected to layer normalization right if you also have this confusion remember that layer and batch normalization are very different from each other in layer normalization we usually normalize along the feature Dimension which is the columns and layer normalization does not depend on the batch size at all no matter what the batch size is we just take the output of every layer and normalize it based on the mean and the standard standard deviation batch normalization on the other hand we do the normalization for an entire batch so it definitely depends on the batch size now U the main issue is that the available Hardware typically dictates the batch size if you if you don't have too powerful of a hardware you might need to use a lower batch size so batch normalization is not that flexible because it depends on the batch size which depends which depends on the available Hardware on the other hand layer normalization is pretty flexible it leads to more flexibility and training for it leads to more flexibility and stability for distributed training so if you have if you are in environments which lack resources and Hardware capabilities are not there which leads to low batches Etc or if you don't want to care about batch size you want the normalization to be independent of the batch size layer normalization is much better in terms of flexibility so don't make the error or don't make the confusion between layer normalization versus batch normalization this brings us to the end of today's lecture as I mentioned in the subsequent lectures we'll talk about Jou we'll talk about feed forward Network and we'll also talk about shortcut connections and then later we'll see how all of it comes together to make the GPT architecture thank you so much everyone and I look forward to seeing you in the next lecture"
}