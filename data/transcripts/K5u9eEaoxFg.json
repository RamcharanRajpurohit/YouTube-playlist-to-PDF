{
  "video": {
    "video_id": "K5u9eEaoxFg",
    "title": "Lecture 18: Multi Head Attention Part 2 - Entire mathematics explained",
    "duration": 3673.0,
    "index": 17
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.559
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.72,
      "duration": 5.28
    },
    {
      "text": "in the build large language models from",
      "start": 8.559,
      "duration": 5.641
    },
    {
      "text": "scratch Series this is the second part",
      "start": 11.0,
      "duration": 6.199
    },
    {
      "text": "of the multihead attention lectures in",
      "start": 14.2,
      "duration": 4.919
    },
    {
      "text": "the previous part we looked at",
      "start": 17.199,
      "duration": 3.92
    },
    {
      "text": "implementing multi-head attention in the",
      "start": 19.119,
      "duration": 3.641
    },
    {
      "text": "following",
      "start": 21.119,
      "duration": 6.441
    },
    {
      "text": "way what we did is that we had the input",
      "start": 22.76,
      "duration": 7.519
    },
    {
      "text": "tokens so let me show you this figure",
      "start": 27.56,
      "duration": 5.719
    },
    {
      "text": "which summarizes",
      "start": 30.279,
      "duration": 3.0
    },
    {
      "text": "everything yeah so this was the input",
      "start": 34.399,
      "duration": 5.401
    },
    {
      "text": "Matrix which we had essentially the",
      "start": 37.399,
      "duration": 4.401
    },
    {
      "text": "number of rows here represent the number",
      "start": 39.8,
      "duration": 5.079
    },
    {
      "text": "of tokens which we have and each token",
      "start": 41.8,
      "duration": 5.8
    },
    {
      "text": "was encoded as a three-dimensional input",
      "start": 44.879,
      "duration": 3.761
    },
    {
      "text": "embedding",
      "start": 47.6,
      "duration": 3.72
    },
    {
      "text": "Vector right in the first part of the",
      "start": 48.64,
      "duration": 4.48
    },
    {
      "text": "multi-ad attention what we essentially",
      "start": 51.32,
      "duration": 4.44
    },
    {
      "text": "did was we created multiple weight",
      "start": 53.12,
      "duration": 6.2
    },
    {
      "text": "matrices for the query key and the value",
      "start": 55.76,
      "duration": 5.959
    },
    {
      "text": "so if we have two attention heads we'll",
      "start": 59.32,
      "duration": 5.199
    },
    {
      "text": "create two weight matrices for the query",
      "start": 61.719,
      "duration": 5.641
    },
    {
      "text": "two weight matrices for the keys and two",
      "start": 64.519,
      "duration": 5.681
    },
    {
      "text": "weight matrices for the values and then",
      "start": 67.36,
      "duration": 5.04
    },
    {
      "text": "we will multiply the inputs with these",
      "start": 70.2,
      "duration": 6.04
    },
    {
      "text": "weight matrices to get two queries two",
      "start": 72.4,
      "duration": 6.399
    },
    {
      "text": "keys and two",
      "start": 76.24,
      "duration": 5.04
    },
    {
      "text": "values so the main problem with this",
      "start": 78.799,
      "duration": 4.241
    },
    {
      "text": "approach is that here you can see that",
      "start": 81.28,
      "duration": 4.519
    },
    {
      "text": "there are two Matrix multiplications",
      "start": 83.04,
      "duration": 5.6
    },
    {
      "text": "which are needed as we saw earlier gpt3",
      "start": 85.799,
      "duration": 5.64
    },
    {
      "text": "used 996 attention attention heads so if",
      "start": 88.64,
      "duration": 5.32
    },
    {
      "text": "you have 96 attention heads you'll need",
      "start": 91.439,
      "duration": 4.96
    },
    {
      "text": "96 multiplications to get the queries",
      "start": 93.96,
      "duration": 5.199
    },
    {
      "text": "Matrix 96 multiplications to get the",
      "start": 96.399,
      "duration": 5.36
    },
    {
      "text": "keys Matrix and 96 multiplications to",
      "start": 99.159,
      "duration": 5.721
    },
    {
      "text": "get the values Matrix that's not very",
      "start": 101.759,
      "duration": 5.921
    },
    {
      "text": "efficient right so today what we are",
      "start": 104.88,
      "duration": 5.08
    },
    {
      "text": "going to see is that how can we make",
      "start": 107.68,
      "duration": 4.6
    },
    {
      "text": "sure that the number of multiplications",
      "start": 109.96,
      "duration": 4.799
    },
    {
      "text": "are reduced in particular what if we",
      "start": 112.28,
      "duration": 5.08
    },
    {
      "text": "just need to do one multiplication for",
      "start": 114.759,
      "duration": 5.201
    },
    {
      "text": "quick Keys one one multiplication for",
      "start": 117.36,
      "duration": 4.52
    },
    {
      "text": "queries and one multiplication for",
      "start": 119.96,
      "duration": 4.159
    },
    {
      "text": "values and then once we do one",
      "start": 121.88,
      "duration": 3.919
    },
    {
      "text": "multiplication then we can split the",
      "start": 124.119,
      "duration": 4.081
    },
    {
      "text": "queries again into two parts we can",
      "start": 125.799,
      "duration": 4.721
    },
    {
      "text": "split the keys into two parts and we can",
      "start": 128.2,
      "duration": 4.64
    },
    {
      "text": "split the values also into two parts for",
      "start": 130.52,
      "duration": 4.88
    },
    {
      "text": "two heads and then we can perform the",
      "start": 132.84,
      "duration": 4.96
    },
    {
      "text": "rest so once we get the copies or the",
      "start": 135.4,
      "duration": 5.08
    },
    {
      "text": "multiple matrices for queries keys and",
      "start": 137.8,
      "duration": 4.76
    },
    {
      "text": "values what we can do is multiply",
      "start": 140.48,
      "duration": 4.0
    },
    {
      "text": "queries with keys transposed to get the",
      "start": 142.56,
      "duration": 4.0
    },
    {
      "text": "attention scores then we can get the",
      "start": 144.48,
      "duration": 3.92
    },
    {
      "text": "attention weights and we can multiply",
      "start": 146.56,
      "duration": 5.36
    },
    {
      "text": "them with the values to get the context",
      "start": 148.4,
      "duration": 6.04
    },
    {
      "text": "vectors so the rest of the procedure can",
      "start": 151.92,
      "duration": 5.12
    },
    {
      "text": "be bit similar but what if we can reduce",
      "start": 154.44,
      "duration": 4.48
    },
    {
      "text": "the number of Matrix multiplications at",
      "start": 157.04,
      "duration": 3.919
    },
    {
      "text": "the start this is what we are going to",
      "start": 158.92,
      "duration": 4.8
    },
    {
      "text": "look at in today's lecture so let's get",
      "start": 160.959,
      "duration": 4.56
    },
    {
      "text": "started this procedure is called",
      "start": 163.72,
      "duration": 3.56
    },
    {
      "text": "implementing multi-head attention with",
      "start": 165.519,
      "duration": 4.161
    },
    {
      "text": "weight splits and it's definitely much",
      "start": 167.28,
      "duration": 4.36
    },
    {
      "text": "more efficient than the multi-head",
      "start": 169.68,
      "duration": 4.8
    },
    {
      "text": "attention which we saw previously so in",
      "start": 171.64,
      "duration": 4.36
    },
    {
      "text": "the previous lecture the way we",
      "start": 174.48,
      "duration": 3.28
    },
    {
      "text": "implemented multi-head attention was",
      "start": 176.0,
      "duration": 4.04
    },
    {
      "text": "something like this we had the the",
      "start": 177.76,
      "duration": 4.32
    },
    {
      "text": "causal attention module and we",
      "start": 180.04,
      "duration": 3.919
    },
    {
      "text": "calculated the causal attention module",
      "start": 182.08,
      "duration": 4.079
    },
    {
      "text": "result which is the context Vector the",
      "start": 183.959,
      "duration": 3.92
    },
    {
      "text": "context Vector for every single",
      "start": 186.159,
      "duration": 4.241
    },
    {
      "text": "attention head and then we concatenated",
      "start": 187.879,
      "duration": 6.08
    },
    {
      "text": "the results from the different uh",
      "start": 190.4,
      "duration": 5.72
    },
    {
      "text": "context vectors together and that led to",
      "start": 193.959,
      "duration": 4.961
    },
    {
      "text": "a large Matrix this was the process now",
      "start": 196.12,
      "duration": 4.32
    },
    {
      "text": "what we are going to do is we are going",
      "start": 198.92,
      "duration": 3.879
    },
    {
      "text": "to follow a slightly different procedure",
      "start": 200.44,
      "duration": 4.159
    },
    {
      "text": "and that's called multihead attention",
      "start": 202.799,
      "duration": 3.881
    },
    {
      "text": "with weight splits and it's much more",
      "start": 204.599,
      "duration": 4.681
    },
    {
      "text": "computationally efficient so the main",
      "start": 206.68,
      "duration": 4.36
    },
    {
      "text": "idea is that in the previous code we had",
      "start": 209.28,
      "duration": 3.879
    },
    {
      "text": "maintained two separate classes we had",
      "start": 211.04,
      "duration": 3.759
    },
    {
      "text": "maintained a class for the multi-head",
      "start": 213.159,
      "duration": 4.201
    },
    {
      "text": "attention rapper and we had maintained a",
      "start": 214.799,
      "duration": 5.08
    },
    {
      "text": "class for the causal attention and then",
      "start": 217.36,
      "duration": 4.799
    },
    {
      "text": "we combine both of them into a single",
      "start": 219.879,
      "duration": 5.08
    },
    {
      "text": "multi-head attention class so here in",
      "start": 222.159,
      "duration": 4.841
    },
    {
      "text": "the in the top what you can see over",
      "start": 224.959,
      "duration": 4.441
    },
    {
      "text": "here is what we did previously We",
      "start": 227.0,
      "duration": 5.159
    },
    {
      "text": "performed two Matrix multiplications to",
      "start": 229.4,
      "duration": 5.72
    },
    {
      "text": "obtain the two query matrices q1 and Q2",
      "start": 232.159,
      "duration": 5.64
    },
    {
      "text": "Q2 what we are going to do right now is",
      "start": 235.12,
      "duration": 4.679
    },
    {
      "text": "what if the weight Matrix which we start",
      "start": 237.799,
      "duration": 4.201
    },
    {
      "text": "start out initially itself was a larger",
      "start": 239.799,
      "duration": 3.481
    },
    {
      "text": "weight",
      "start": 242.0,
      "duration": 5.079
    },
    {
      "text": "Matrix and then uh we multiply the",
      "start": 243.28,
      "duration": 6.4
    },
    {
      "text": "inputs x with the query Matrix to get",
      "start": 247.079,
      "duration": 5.8
    },
    {
      "text": "the queries and then after that we split",
      "start": 249.68,
      "duration": 4.88
    },
    {
      "text": "the queries into two",
      "start": 252.879,
      "duration": 3.681
    },
    {
      "text": "components so here you see the",
      "start": 254.56,
      "duration": 3.6
    },
    {
      "text": "difference in the previous case we",
      "start": 256.56,
      "duration": 4.32
    },
    {
      "text": "multiplied x with wq1 and we multiplied",
      "start": 258.16,
      "duration": 3.96
    },
    {
      "text": "x with",
      "start": 260.88,
      "duration": 5.08
    },
    {
      "text": "wq2 but what if we multiply x with a WQ",
      "start": 262.12,
      "duration": 5.76
    },
    {
      "text": "which is already a large Matrix which",
      "start": 265.96,
      "duration": 4.92
    },
    {
      "text": "consists of D out so here you see the",
      "start": 267.88,
      "duration": 5.0
    },
    {
      "text": "dimensions of this initial weight Matrix",
      "start": 270.88,
      "duration": 5.12
    },
    {
      "text": "are larger and that these Dimensions",
      "start": 272.88,
      "duration": 5.68
    },
    {
      "text": "already include the number of heads so",
      "start": 276.0,
      "duration": 5.6
    },
    {
      "text": "this Dimension 4 is the D out which is",
      "start": 278.56,
      "duration": 6.079
    },
    {
      "text": "two multiplied by the number of heads so",
      "start": 281.6,
      "duration": 5.56
    },
    {
      "text": "this D out is already specified before",
      "start": 284.639,
      "duration": 4.28
    },
    {
      "text": "so the weight Matrix for the queries",
      "start": 287.16,
      "duration": 4.08
    },
    {
      "text": "keys and the values which we specify",
      "start": 288.919,
      "duration": 4.84
    },
    {
      "text": "already will kind of include the head",
      "start": 291.24,
      "duration": 4.92
    },
    {
      "text": "Dimension and then we when we get the",
      "start": 293.759,
      "duration": 4.321
    },
    {
      "text": "queries keys and values we'll split them",
      "start": 296.16,
      "duration": 4.52
    },
    {
      "text": "based on the number of heads so here we",
      "start": 298.08,
      "duration": 4.24
    },
    {
      "text": "can see where we split the queries",
      "start": 300.68,
      "duration": 3.799
    },
    {
      "text": "Matrix into two q1 and Q2 because there",
      "start": 302.32,
      "duration": 4.92
    },
    {
      "text": "are two heads so ultimately the rest of",
      "start": 304.479,
      "duration": 4.361
    },
    {
      "text": "the procedure will remain the same but",
      "start": 307.24,
      "duration": 3.36
    },
    {
      "text": "we are just reducing the number of",
      "start": 308.84,
      "duration": 3.16
    },
    {
      "text": "Matrix",
      "start": 310.6,
      "duration": 3.599
    },
    {
      "text": "multiplications so if you look at this",
      "start": 312.0,
      "duration": 3.84
    },
    {
      "text": "weight Matrix right now the number of",
      "start": 314.199,
      "duration": 4.041
    },
    {
      "text": "attention head is specified",
      "start": 315.84,
      "duration": 6.12
    },
    {
      "text": "right uh so how is it specified so D out",
      "start": 318.24,
      "duration": 6.84
    },
    {
      "text": "is equal to 4 and D out is equal to head",
      "start": 321.96,
      "duration": 4.959
    },
    {
      "text": "Dimension multiplied by the number of",
      "start": 325.08,
      "duration": 4.28
    },
    {
      "text": "heads so the head Dimension is equal to",
      "start": 326.919,
      "duration": 3.521
    },
    {
      "text": "2",
      "start": 329.36,
      "duration": 3.0
    },
    {
      "text": "because each head has a dimension of two",
      "start": 330.44,
      "duration": 4.199
    },
    {
      "text": "this is what we had done here each head",
      "start": 332.36,
      "duration": 4.92
    },
    {
      "text": "had a dimension of two each attention",
      "start": 334.639,
      "duration": 4.721
    },
    {
      "text": "head here you see D out equal to two",
      "start": 337.28,
      "duration": 3.52
    },
    {
      "text": "which was implemented in the previous",
      "start": 339.36,
      "duration": 3.88
    },
    {
      "text": "case so each head had a dimension of Two",
      "start": 340.8,
      "duration": 4.72
    },
    {
      "text": "And there are two heads so the D out in",
      "start": 343.24,
      "duration": 5.2
    },
    {
      "text": "this larger trainable Q Matrix already",
      "start": 345.52,
      "duration": 4.84
    },
    {
      "text": "includes number of",
      "start": 348.44,
      "duration": 4.759
    },
    {
      "text": "heads I'll explain to you in detail what",
      "start": 350.36,
      "duration": 5.119
    },
    {
      "text": "this means right now if you just get an",
      "start": 353.199,
      "duration": 4.161
    },
    {
      "text": "intuitive idea of what we are trying to",
      "start": 355.479,
      "duration": 5.321
    },
    {
      "text": "do that will be very helpful",
      "start": 357.36,
      "duration": 6.08
    },
    {
      "text": "okay so now uh let us get started with",
      "start": 360.8,
      "duration": 4.76
    },
    {
      "text": "the code so we are going to implement",
      "start": 363.44,
      "duration": 4.159
    },
    {
      "text": "multi-head attention with weight splits",
      "start": 365.56,
      "duration": 4.479
    },
    {
      "text": "right so instead of maintaining two",
      "start": 367.599,
      "duration": 4.201
    },
    {
      "text": "separate classes so here you can see",
      "start": 370.039,
      "duration": 5.28
    },
    {
      "text": "earlier in our code we had the we had a",
      "start": 371.8,
      "duration": 5.92
    },
    {
      "text": "causal attention class which did all the",
      "start": 375.319,
      "duration": 4.121
    },
    {
      "text": "computations of attention scores",
      "start": 377.72,
      "duration": 4.16
    },
    {
      "text": "attention weights Etc and then we",
      "start": 379.44,
      "duration": 4.319
    },
    {
      "text": "integrated this causal attention class",
      "start": 381.88,
      "duration": 4.08
    },
    {
      "text": "with the multi-head attention rapper so",
      "start": 383.759,
      "duration": 4.121
    },
    {
      "text": "we had a multi-head attention rapper and",
      "start": 385.96,
      "duration": 3.76
    },
    {
      "text": "we created multiple instances or",
      "start": 387.88,
      "duration": 4.439
    },
    {
      "text": "multiple causal attention objects within",
      "start": 389.72,
      "duration": 3.599
    },
    {
      "text": "this",
      "start": 392.319,
      "duration": 3.401
    },
    {
      "text": "rapper now the idea is instead of",
      "start": 393.319,
      "duration": 4.641
    },
    {
      "text": "maintaining two separate classes why",
      "start": 395.72,
      "duration": 4.24
    },
    {
      "text": "don't we combine both of these Concepts",
      "start": 397.96,
      "duration": 4.799
    },
    {
      "text": "into a single multi-head attention",
      "start": 399.96,
      "duration": 5.6
    },
    {
      "text": "class also in addition to just merging",
      "start": 402.759,
      "duration": 4.401
    },
    {
      "text": "the multi-head attention rapper with the",
      "start": 405.56,
      "duration": 4.12
    },
    {
      "text": "causal attention code let's make some",
      "start": 407.16,
      "duration": 4.4
    },
    {
      "text": "other modifications to implement",
      "start": 409.68,
      "duration": 3.76
    },
    {
      "text": "multi-head attention more",
      "start": 411.56,
      "duration": 4.24
    },
    {
      "text": "effectively so as I told you earlier in",
      "start": 413.44,
      "duration": 4.08
    },
    {
      "text": "the multi-ad attention rapper which we",
      "start": 415.8,
      "duration": 3.839
    },
    {
      "text": "had earlier multiple heads are",
      "start": 417.52,
      "duration": 4.56
    },
    {
      "text": "implemented by creating causal attention",
      "start": 419.639,
      "duration": 5.12
    },
    {
      "text": "objects and uh the causal attention",
      "start": 422.08,
      "duration": 4.399
    },
    {
      "text": "class independently performed the",
      "start": 424.759,
      "duration": 3.961
    },
    {
      "text": "attention mechanism earlier and then the",
      "start": 426.479,
      "duration": 4.0
    },
    {
      "text": "results from each attention head were",
      "start": 428.72,
      "duration": 4.199
    },
    {
      "text": "effectively concatenated now we are",
      "start": 430.479,
      "duration": 4.0
    },
    {
      "text": "going to implement a class which is",
      "start": 432.919,
      "duration": 4.0
    },
    {
      "text": "called as multi-head attention class and",
      "start": 434.479,
      "duration": 4.241
    },
    {
      "text": "we are going to integrate the multi-head",
      "start": 436.919,
      "duration": 3.521
    },
    {
      "text": "functionality as well as the causal",
      "start": 438.72,
      "duration": 3.4
    },
    {
      "text": "attention functionality everything",
      "start": 440.44,
      "duration": 3.36
    },
    {
      "text": "within a single",
      "start": 442.12,
      "duration": 4.44
    },
    {
      "text": "class the way we are going to do do this",
      "start": 443.8,
      "duration": 6.04
    },
    {
      "text": "is that the this class splits the input",
      "start": 446.56,
      "duration": 5.68
    },
    {
      "text": "into multiple heads by reshaping the",
      "start": 449.84,
      "duration": 5.0
    },
    {
      "text": "query key and value tensors let's see",
      "start": 452.24,
      "duration": 4.639
    },
    {
      "text": "what this means don't worry about this",
      "start": 454.84,
      "duration": 3.96
    },
    {
      "text": "sentence in this lecture I have",
      "start": 456.879,
      "duration": 3.88
    },
    {
      "text": "constructed a Hands-On example so that",
      "start": 458.8,
      "duration": 3.44
    },
    {
      "text": "you understand the code which we are",
      "start": 460.759,
      "duration": 3.681
    },
    {
      "text": "about to write so first let's look at",
      "start": 462.24,
      "duration": 3.959
    },
    {
      "text": "the multi-ad attention class and how we",
      "start": 464.44,
      "duration": 3.879
    },
    {
      "text": "are going to Define it this code right",
      "start": 466.199,
      "duration": 3.641
    },
    {
      "text": "here which I'm showing on the screen is",
      "start": 468.319,
      "duration": 3.041
    },
    {
      "text": "at the heart of the Transformer",
      "start": 469.84,
      "duration": 4.799
    },
    {
      "text": "mechanism so you see we have the in init",
      "start": 471.36,
      "duration": 5.799
    },
    {
      "text": "Constructor which is invoked by default",
      "start": 474.639,
      "duration": 4.641
    },
    {
      "text": "and then there is the forward method at",
      "start": 477.159,
      "duration": 4.04
    },
    {
      "text": "the end of the forward Method All We are",
      "start": 479.28,
      "duration": 3.639
    },
    {
      "text": "going to do is calc calculate the",
      "start": 481.199,
      "duration": 4.201
    },
    {
      "text": "context Vector for each of the input",
      "start": 482.919,
      "duration": 4.761
    },
    {
      "text": "embedding vectors but what happens in",
      "start": 485.4,
      "duration": 5.479
    },
    {
      "text": "the middle that is the main key which",
      "start": 487.68,
      "duration": 6.239
    },
    {
      "text": "you really need to understand okay so I",
      "start": 490.879,
      "duration": 4.401
    },
    {
      "text": "could have just taken you through this",
      "start": 493.919,
      "duration": 3.68
    },
    {
      "text": "code but I have seen that if I take",
      "start": 495.28,
      "duration": 4.039
    },
    {
      "text": "students through this code it becomes",
      "start": 497.599,
      "duration": 3.401
    },
    {
      "text": "very difficult for them to wrap their",
      "start": 499.319,
      "duration": 3.921
    },
    {
      "text": "heads around what exactly is going on",
      "start": 501.0,
      "duration": 3.759
    },
    {
      "text": "especially because if you see the",
      "start": 503.24,
      "duration": 3.2
    },
    {
      "text": "dimensions there are four dimensional",
      "start": 504.759,
      "duration": 3.681
    },
    {
      "text": "tensors which are involved in this code",
      "start": 506.44,
      "duration": 3.96
    },
    {
      "text": "and there is a very good reason for why",
      "start": 508.44,
      "duration": 4.76
    },
    {
      "text": "we need four dimensional tensors so if",
      "start": 510.4,
      "duration": 4.8
    },
    {
      "text": "you if you just go through the code you",
      "start": 513.2,
      "duration": 3.12
    },
    {
      "text": "will you will think that you have",
      "start": 515.2,
      "duration": 2.839
    },
    {
      "text": "understood it but you would not have",
      "start": 516.32,
      "duration": 3.48
    },
    {
      "text": "because there are lot of subtleties with",
      "start": 518.039,
      "duration": 4.0
    },
    {
      "text": "respect to the dimensions so what we are",
      "start": 519.8,
      "duration": 4.119
    },
    {
      "text": "going to do is that we are going to go",
      "start": 522.039,
      "duration": 4.201
    },
    {
      "text": "to the Whiteboard and I constructed this",
      "start": 523.919,
      "duration": 4.241
    },
    {
      "text": "example completely from",
      "start": 526.24,
      "duration": 4.52
    },
    {
      "text": "scratch so we are going to take a simple",
      "start": 528.16,
      "duration": 4.359
    },
    {
      "text": "example we are directly going to start",
      "start": 530.76,
      "duration": 4.16
    },
    {
      "text": "from the input and we are going to do",
      "start": 532.519,
      "duration": 5.44
    },
    {
      "text": "all the steps on the Whiteboard which",
      "start": 534.92,
      "duration": 5.039
    },
    {
      "text": "are implemented in this code then you",
      "start": 537.959,
      "duration": 3.841
    },
    {
      "text": "will find that understanding the code is",
      "start": 539.959,
      "duration": 4.961
    },
    {
      "text": "extremely easy at every step of the code",
      "start": 541.8,
      "duration": 4.76
    },
    {
      "text": "I'm going to take you to the Whiteboard",
      "start": 544.92,
      "duration": 3.28
    },
    {
      "text": "and I'm going to explain to you what",
      "start": 546.56,
      "duration": 4.16
    },
    {
      "text": "exactly is going on okay so let's get",
      "start": 548.2,
      "duration": 4.36
    },
    {
      "text": "started I've tried to distill this down",
      "start": 550.72,
      "duration": 5.72
    },
    {
      "text": "to 11 steps and uh I I will explain",
      "start": 552.56,
      "duration": 5.48
    },
    {
      "text": "everything related to matrices",
      "start": 556.44,
      "duration": 4.12
    },
    {
      "text": "Dimensions extremely clearly I will not",
      "start": 558.04,
      "duration": 4.2
    },
    {
      "text": "assume anything everything is written",
      "start": 560.56,
      "duration": 4.399
    },
    {
      "text": "down on the Whiteboard so that you will",
      "start": 562.24,
      "duration": 5.279
    },
    {
      "text": "not be afraid of this code I have seen",
      "start": 564.959,
      "duration": 4.961
    },
    {
      "text": "several other YouTube videos and",
      "start": 567.519,
      "duration": 5.601
    },
    {
      "text": "even lectures where uh people just",
      "start": 569.92,
      "duration": 5.359
    },
    {
      "text": "explain this as if it's very easy to",
      "start": 573.12,
      "duration": 4.32
    },
    {
      "text": "understand but you need to decompose it",
      "start": 575.279,
      "duration": 4.24
    },
    {
      "text": "into individual layers and explain every",
      "start": 577.44,
      "duration": 4.76
    },
    {
      "text": "single one of them okay so I'm going to",
      "start": 579.519,
      "duration": 5.121
    },
    {
      "text": "start with the forward method and I will",
      "start": 582.2,
      "duration": 4.44
    },
    {
      "text": "explain every single line here step by",
      "start": 584.64,
      "duration": 4.6
    },
    {
      "text": "step so first the forward method takes",
      "start": 586.64,
      "duration": 5.12
    },
    {
      "text": "the input X right let's see what that",
      "start": 589.24,
      "duration": 5.52
    },
    {
      "text": "input looks like and what it means so",
      "start": 591.76,
      "duration": 5.48
    },
    {
      "text": "the first step to the attention make the",
      "start": 594.76,
      "duration": 5.519
    },
    {
      "text": "multi-ad attention with weight splits",
      "start": 597.24,
      "duration": 4.76
    },
    {
      "text": "that code or the multihead attention",
      "start": 600.279,
      "duration": 4.361
    },
    {
      "text": "class is that we have to start with the",
      "start": 602.0,
      "duration": 5.399
    },
    {
      "text": "input the way we will specify the input",
      "start": 604.64,
      "duration": 4.639
    },
    {
      "text": "is that the input has three dimensions",
      "start": 607.399,
      "duration": 4.201
    },
    {
      "text": "the First Dimension is the batch the",
      "start": 609.279,
      "duration": 4.321
    },
    {
      "text": "second is the number of tokens and the",
      "start": 611.6,
      "duration": 4.84
    },
    {
      "text": "third is the input Dimension right what",
      "start": 613.6,
      "duration": 5.84
    },
    {
      "text": "is this D in the D in is basically every",
      "start": 616.44,
      "duration": 6.16
    },
    {
      "text": "token is represented by a vector",
      "start": 619.44,
      "duration": 5.56
    },
    {
      "text": "embedding so this D in is the dimension",
      "start": 622.6,
      "duration": 5.679
    },
    {
      "text": "of that Vector embedding so 1A 3 comma 6",
      "start": 625.0,
      "duration": 6.279
    },
    {
      "text": "means that I have three tokens you can",
      "start": 628.279,
      "duration": 4.761
    },
    {
      "text": "think of one token as one word for",
      "start": 631.279,
      "duration": 3.921
    },
    {
      "text": "Simplicity so let's",
      "start": 633.04,
      "duration": 5.56
    },
    {
      "text": "say the three tokens are",
      "start": 635.2,
      "duration": 5.439
    },
    {
      "text": "the",
      "start": 638.6,
      "duration": 4.16
    },
    {
      "text": "cat",
      "start": 640.639,
      "duration": 5.0
    },
    {
      "text": "sleeps let's say these are my let's say",
      "start": 642.76,
      "duration": 6.56
    },
    {
      "text": "these are my three words then what we",
      "start": 645.639,
      "duration": 5.561
    },
    {
      "text": "are essentially doing here is that we",
      "start": 649.32,
      "duration": 3.84
    },
    {
      "text": "are converting each of these words we",
      "start": 651.2,
      "duration": 4.24
    },
    {
      "text": "are converting each of these words into",
      "start": 653.16,
      "duration": 4.359
    },
    {
      "text": "six dimensional vectors so the will be a",
      "start": 655.44,
      "duration": 4.16
    },
    {
      "text": "six dimensional Vector which is the",
      "start": 657.519,
      "duration": 5.44
    },
    {
      "text": "first row so which is the first row over",
      "start": 659.6,
      "duration": 5.96
    },
    {
      "text": "here so look at the first row over here",
      "start": 662.959,
      "duration": 4.361
    },
    {
      "text": "this is the six dimensional Vector for",
      "start": 665.56,
      "duration": 4.0
    },
    {
      "text": "the this is the six dimensional Vector",
      "start": 667.32,
      "duration": 4.639
    },
    {
      "text": "for cat and this is the six dimensional",
      "start": 669.56,
      "duration": 4.88
    },
    {
      "text": "Vector for sleep so you'll see that this",
      "start": 671.959,
      "duration": 5.041
    },
    {
      "text": "is a 1x 3x 6 for Simplicity I have taken",
      "start": 674.44,
      "duration": 5.04
    },
    {
      "text": "the batch size equal to",
      "start": 677.0,
      "duration": 6.399
    },
    {
      "text": "1 okay so this is a 1x 3x 6 tensor why",
      "start": 679.48,
      "duration": 6.0
    },
    {
      "text": "3x 6 because we have three rows here and",
      "start": 683.399,
      "duration": 3.161
    },
    {
      "text": "we have six",
      "start": 685.48,
      "duration": 4.919
    },
    {
      "text": "columns uh each row consists of six six",
      "start": 686.56,
      "duration": 5.719
    },
    {
      "text": "dimensional vectors so I hope you have",
      "start": 690.399,
      "duration": 3.88
    },
    {
      "text": "understood how the input has been",
      "start": 692.279,
      "duration": 4.081
    },
    {
      "text": "defined so here you can see the input",
      "start": 694.279,
      "duration": 4.36
    },
    {
      "text": "shape is B comma number of tokens comma",
      "start": 696.36,
      "duration": 4.279
    },
    {
      "text": "input Dimensions so I hope you you have",
      "start": 698.639,
      "duration": 4.721
    },
    {
      "text": "understood this okay now let's come to",
      "start": 700.639,
      "duration": 4.841
    },
    {
      "text": "the next step the next step is what we",
      "start": 703.36,
      "duration": 3.68
    },
    {
      "text": "have to do is we have to essentially",
      "start": 705.48,
      "duration": 4.08
    },
    {
      "text": "decide two things we have to decide what",
      "start": 707.04,
      "duration": 5.599
    },
    {
      "text": "our output Dimension is going to be and",
      "start": 709.56,
      "duration": 5.76
    },
    {
      "text": "we have to determine the number of",
      "start": 712.639,
      "duration": 6.161
    },
    {
      "text": "Heads This output Dimension is basically",
      "start": 715.32,
      "duration": 5.959
    },
    {
      "text": "we have the input input embedding Vector",
      "start": 718.8,
      "duration": 4.96
    },
    {
      "text": "for each token right ultimately we will",
      "start": 721.279,
      "duration": 5.401
    },
    {
      "text": "get a context Vector for every token so",
      "start": 723.76,
      "duration": 4.879
    },
    {
      "text": "ultimately similar to this input",
      "start": 726.68,
      "duration": 4.839
    },
    {
      "text": "embedding Matrix which is 3x which is 3x",
      "start": 728.639,
      "duration": 5.0
    },
    {
      "text": "6 we will have a context embedding",
      "start": 731.519,
      "duration": 4.281
    },
    {
      "text": "Vector which is three because we have",
      "start": 733.639,
      "duration": 6.44
    },
    {
      "text": "three tokens multiplied by D",
      "start": 735.8,
      "duration": 4.279
    },
    {
      "text": "out so now we have to also decide what",
      "start": 741.199,
      "duration": 6.801
    },
    {
      "text": "is D out which is the so each each token",
      "start": 744.279,
      "duration": 5.521
    },
    {
      "text": "will have a context vector what is the",
      "start": 748.0,
      "duration": 3.36
    },
    {
      "text": "dimension of that Vector we have to",
      "start": 749.8,
      "duration": 4.56
    },
    {
      "text": "decide so now I am deciding that D out",
      "start": 751.36,
      "duration": 5.2
    },
    {
      "text": "will be equal to 6 which is same as D in",
      "start": 754.36,
      "duration": 4.08
    },
    {
      "text": "this is typically done in GPT based",
      "start": 756.56,
      "duration": 4.12
    },
    {
      "text": "models the D in and the D out are the",
      "start": 758.44,
      "duration": 4.839
    },
    {
      "text": "same second thing we also have to decide",
      "start": 760.68,
      "duration": 4.64
    },
    {
      "text": "is how many attention heads do we want",
      "start": 763.279,
      "duration": 5.321
    },
    {
      "text": "to have so I have decided that we are",
      "start": 765.32,
      "duration": 5.44
    },
    {
      "text": "having two attention heads right now in",
      "start": 768.6,
      "duration": 4.64
    },
    {
      "text": "GPT the number of attention heads are 96",
      "start": 770.76,
      "duration": 5.12
    },
    {
      "text": "and the D out is also pretty large but",
      "start": 773.24,
      "duration": 4.36
    },
    {
      "text": "exactly what we are doing right now can",
      "start": 775.88,
      "duration": 4.199
    },
    {
      "text": "be scaled to a larger D out and larger",
      "start": 777.6,
      "duration": 5.52
    },
    {
      "text": "number of heads okay so I'm using D out",
      "start": 780.079,
      "duration": 4.921
    },
    {
      "text": "is equal to 6 and number of heads equal",
      "start": 783.12,
      "duration": 4.719
    },
    {
      "text": "to three so then each head will have a",
      "start": 785.0,
      "duration": 5.76
    },
    {
      "text": "dimension which is called as head dim",
      "start": 787.839,
      "duration": 5.921
    },
    {
      "text": "and we'll look at that also later each",
      "start": 790.76,
      "duration": 4.759
    },
    {
      "text": "each head will have a dimension of head",
      "start": 793.76,
      "duration": 5.16
    },
    {
      "text": "dim which is equal",
      "start": 795.519,
      "duration": 3.401
    },
    {
      "text": "to head dim which is equal to",
      "start": 799.44,
      "duration": 5.519
    },
    {
      "text": "essentially the D",
      "start": 802.0,
      "duration": 2.959
    },
    {
      "text": "out divided by the number of heads which",
      "start": 805.16,
      "duration": 8.0
    },
    {
      "text": "is equal to 6 / 2 and that will be equal",
      "start": 808.279,
      "duration": 8.161
    },
    {
      "text": "to three so then the dimension of each",
      "start": 813.16,
      "duration": 5.64
    },
    {
      "text": "head is equal to three and since there",
      "start": 816.44,
      "duration": 4.36
    },
    {
      "text": "are two heads the total D out will be",
      "start": 818.8,
      "duration": 4.56
    },
    {
      "text": "equal to six okay so this is the second",
      "start": 820.8,
      "duration": 4.92
    },
    {
      "text": "decision point the third decision point",
      "start": 823.36,
      "duration": 4.08
    },
    {
      "text": "which I have to make is that I have to",
      "start": 825.72,
      "duration": 3.84
    },
    {
      "text": "initialize or it's not a decision Point",
      "start": 827.44,
      "duration": 4.24
    },
    {
      "text": "rather but it's the third step so the",
      "start": 829.56,
      "duration": 3.719
    },
    {
      "text": "third step which we have to do is",
      "start": 831.68,
      "duration": 3.959
    },
    {
      "text": "initialize trainable weight matrices for",
      "start": 833.279,
      "duration": 5.321
    },
    {
      "text": "the key query and the value so we have",
      "start": 835.639,
      "duration": 6.281
    },
    {
      "text": "to initi w k",
      "start": 838.6,
      "duration": 7.679
    },
    {
      "text": "WQ and WV okay so remember that the",
      "start": 841.92,
      "duration": 6.88
    },
    {
      "text": "input which we have which for now can be",
      "start": 846.279,
      "duration": 4.441
    },
    {
      "text": "thought to be six rows and three columns",
      "start": 848.8,
      "duration": 4.32
    },
    {
      "text": "has to be multiplied sorry three rows",
      "start": 850.72,
      "duration": 3.919
    },
    {
      "text": "and six columns so the input is three",
      "start": 853.12,
      "duration": 3.8
    },
    {
      "text": "rows and six columns so when you",
      "start": 854.639,
      "duration": 3.601
    },
    {
      "text": "construct these trainable weight",
      "start": 856.92,
      "duration": 3.8
    },
    {
      "text": "matrices for the keys query and value",
      "start": 858.24,
      "duration": 4.36
    },
    {
      "text": "their first Dimension has to be equal to",
      "start": 860.72,
      "duration": 4.6
    },
    {
      "text": "D in because if you look at the input",
      "start": 862.6,
      "duration": 5.72
    },
    {
      "text": "Dimension the input the number of the",
      "start": 865.32,
      "duration": 5.84
    },
    {
      "text": "last dimension of of the input is D in",
      "start": 868.32,
      "duration": 5.56
    },
    {
      "text": "and for these for d for this input to be",
      "start": 871.16,
      "duration": 6.039
    },
    {
      "text": "compatible with this WK WQ and WV in",
      "start": 873.88,
      "duration": 5.28
    },
    {
      "text": "multiplication you need the first",
      "start": 877.199,
      "duration": 4.241
    },
    {
      "text": "argument here to be equal to D",
      "start": 879.16,
      "duration": 5.359
    },
    {
      "text": "in so actually the dimensions of the",
      "start": 881.44,
      "duration": 6.72
    },
    {
      "text": "trainable key query and value matrices",
      "start": 884.519,
      "duration": 6.081
    },
    {
      "text": "are D in multiplied by D out which is 6",
      "start": 888.16,
      "duration": 4.44
    },
    {
      "text": "by 6 because D in is equal to 6 and D",
      "start": 890.6,
      "duration": 4.28
    },
    {
      "text": "out is equal to 6 so we have to",
      "start": 892.6,
      "duration": 4.76
    },
    {
      "text": "initialize the these three vectors these",
      "start": 894.88,
      "duration": 4.68
    },
    {
      "text": "three matrices rather and I have shown",
      "start": 897.36,
      "duration": 3.96
    },
    {
      "text": "these random initializations here so you",
      "start": 899.56,
      "duration": 5.639
    },
    {
      "text": "can see that WQ is a six diens or a 6x6",
      "start": 901.32,
      "duration": 8.879
    },
    {
      "text": "tensor WK is a 6x6 tensor and WV is a",
      "start": 905.199,
      "duration": 8.281
    },
    {
      "text": "6x6 tensor let us let me show you in",
      "start": 910.199,
      "duration": 5.241
    },
    {
      "text": "code where these matrices are actually",
      "start": 913.48,
      "duration": 4.32
    },
    {
      "text": "initialized so if you look at the code",
      "start": 915.44,
      "duration": 4.12
    },
    {
      "text": "these matrices are actually initialized",
      "start": 917.8,
      "duration": 6.52
    },
    {
      "text": "in the init Constructor so w query W Key",
      "start": 919.56,
      "duration": 6.88
    },
    {
      "text": "and W value these are trainable weight",
      "start": 924.32,
      "duration": 3.879
    },
    {
      "text": "matrices as you can see the dimensions",
      "start": 926.44,
      "duration": 5.399
    },
    {
      "text": "are D in D out and we are using the",
      "start": 928.199,
      "duration": 6.041
    },
    {
      "text": "linear layer of neural networks with the",
      "start": 931.839,
      "duration": 5.92
    },
    {
      "text": "bias equal to zero to set the uh initial",
      "start": 934.24,
      "duration": 5.64
    },
    {
      "text": "values for these why do we use a neural",
      "start": 937.759,
      "duration": 3.841
    },
    {
      "text": "network linear layer because it's",
      "start": 939.88,
      "duration": 3.68
    },
    {
      "text": "optimized for initializing the weights",
      "start": 941.6,
      "duration": 4.64
    },
    {
      "text": "so it's much better when we do the back",
      "start": 943.56,
      "duration": 5.719
    },
    {
      "text": "propagation later so this is where the",
      "start": 946.24,
      "duration": 5.36
    },
    {
      "text": "the trainable weight matrices for query",
      "start": 949.279,
      "duration": 4.201
    },
    {
      "text": "key and value are initialized in the",
      "start": 951.6,
      "duration": 4.32
    },
    {
      "text": "init Constructor so it's called by",
      "start": 953.48,
      "duration": 5.08
    },
    {
      "text": "default when we create or it's these",
      "start": 955.92,
      "duration": 4.919
    },
    {
      "text": "Matrix are created by default when we",
      "start": 958.56,
      "duration": 3.839
    },
    {
      "text": "create an instance of the multihead",
      "start": 960.839,
      "duration": 3.161
    },
    {
      "text": "attention",
      "start": 962.399,
      "duration": 4.321
    },
    {
      "text": "class all right so up till now we have",
      "start": 964.0,
      "duration": 4.48
    },
    {
      "text": "essentially initialize these trainable",
      "start": 966.72,
      "duration": 5.799
    },
    {
      "text": "weight matrices w k WQ and WV Now we",
      "start": 968.48,
      "duration": 6.479
    },
    {
      "text": "move to step number four step number",
      "start": 972.519,
      "duration": 4.76
    },
    {
      "text": "four is the step from which computations",
      "start": 974.959,
      "duration": 4.761
    },
    {
      "text": "actually start so we have the input now",
      "start": 977.279,
      "duration": 5.841
    },
    {
      "text": "right and uh we have the these matrices",
      "start": 979.72,
      "duration": 5.76
    },
    {
      "text": "we have trainable Keys the trainable",
      "start": 983.12,
      "duration": 5.12
    },
    {
      "text": "queries and the trainable values what",
      "start": 985.48,
      "duration": 4.44
    },
    {
      "text": "we'll now be doing doing is that we will",
      "start": 988.24,
      "duration": 4.719
    },
    {
      "text": "multiply the input with these matrices",
      "start": 989.92,
      "duration": 5.68
    },
    {
      "text": "to ultimately get the keys the queries",
      "start": 992.959,
      "duration": 4.841
    },
    {
      "text": "and the values so what are the",
      "start": 995.6,
      "duration": 4.2
    },
    {
      "text": "dimensions of the input the dimensions",
      "start": 997.8,
      "duration": 4.8
    },
    {
      "text": "of the input are",
      "start": 999.8,
      "duration": 2.8
    },
    {
      "text": "one one M",
      "start": 1002.68,
      "duration": 5.88
    },
    {
      "text": "uh me just write this again the",
      "start": 1005.72,
      "duration": 5.32
    },
    {
      "text": "dimensions of the input are 1 multiplied",
      "start": 1008.56,
      "duration": 5.12
    },
    {
      "text": "by 3 because we have three rows",
      "start": 1011.04,
      "duration": 6.56
    },
    {
      "text": "multiplied by six columns",
      "start": 1013.68,
      "duration": 6.76
    },
    {
      "text": "correct and the dimensions of each of",
      "start": 1017.6,
      "duration": 5.919
    },
    {
      "text": "these qu key query and value trainable",
      "start": 1020.44,
      "duration": 5.999
    },
    {
      "text": "weight matrices are six which is D in",
      "start": 1023.519,
      "duration": 6.32
    },
    {
      "text": "multiplied by 6 which is D",
      "start": 1026.439,
      "duration": 6.36
    },
    {
      "text": "out so when you multiply the input with",
      "start": 1029.839,
      "duration": 4.96
    },
    {
      "text": "these weight matrices the result which",
      "start": 1032.799,
      "duration": 5.361
    },
    {
      "text": "you'll get is 1x 3x 6 so you'll get the",
      "start": 1034.799,
      "duration": 5.921
    },
    {
      "text": "keys you'll get the keys Matrix which is",
      "start": 1038.16,
      "duration": 5.56
    },
    {
      "text": "1x 3x 6 you'll get the queries Matrix",
      "start": 1040.72,
      "duration": 5.839
    },
    {
      "text": "which is 1x 3x 6 and you will also get",
      "start": 1043.72,
      "duration": 5.24
    },
    {
      "text": "the values Matrix here which is 1x 3x",
      "start": 1046.559,
      "duration": 5.081
    },
    {
      "text": "six let's try to understand what this 1",
      "start": 1048.96,
      "duration": 5.56
    },
    {
      "text": "3 and six is so I as I've written over",
      "start": 1051.64,
      "duration": 5.56
    },
    {
      "text": "here one is the batch size which we are",
      "start": 1054.52,
      "duration": 6.36
    },
    {
      "text": "taken to be one three is the number of",
      "start": 1057.2,
      "duration": 6.04
    },
    {
      "text": "tokens because we have three tokens and",
      "start": 1060.88,
      "duration": 5.159
    },
    {
      "text": "D out is basically the output Dimension",
      "start": 1063.24,
      "duration": 4.679
    },
    {
      "text": "so the way to interpret these keys",
      "start": 1066.039,
      "duration": 5.281
    },
    {
      "text": "saries and value Matrix is that each row",
      "start": 1067.919,
      "duration": 5.801
    },
    {
      "text": "basically corresponds to one token so",
      "start": 1071.32,
      "duration": 3.96
    },
    {
      "text": "the first row corresponds to the first",
      "start": 1073.72,
      "duration": 3.72
    },
    {
      "text": "token the second row corresponds to the",
      "start": 1075.28,
      "duration": 3.96
    },
    {
      "text": "second token and the third row",
      "start": 1077.44,
      "duration": 4.04
    },
    {
      "text": "corresponds to the third token and there",
      "start": 1079.24,
      "duration": 5.24
    },
    {
      "text": "are six dimensions in each row because",
      "start": 1081.48,
      "duration": 5.52
    },
    {
      "text": "each token is a six dimensional",
      "start": 1084.48,
      "duration": 5.319
    },
    {
      "text": "representation because D out is equal to",
      "start": 1087.0,
      "duration": 5.6
    },
    {
      "text": "6 now let me show you in the code where",
      "start": 1089.799,
      "duration": 3.801
    },
    {
      "text": "this is",
      "start": 1092.6,
      "duration": 3.52
    },
    {
      "text": "calculated so if you go down below here",
      "start": 1093.6,
      "duration": 5.319
    },
    {
      "text": "you see the keys queries and values what",
      "start": 1096.12,
      "duration": 4.439
    },
    {
      "text": "we have done is that we have passed in",
      "start": 1098.919,
      "duration": 4.24
    },
    {
      "text": "the input to this neural network linear",
      "start": 1100.559,
      "duration": 4.681
    },
    {
      "text": "layer so what this does is that the",
      "start": 1103.159,
      "duration": 4.201
    },
    {
      "text": "trainable weight mates for the key query",
      "start": 1105.24,
      "duration": 4.16
    },
    {
      "text": "and value are applied on this input and",
      "start": 1107.36,
      "duration": 3.84
    },
    {
      "text": "we get the keys queries and the value",
      "start": 1109.4,
      "duration": 4.759
    },
    {
      "text": "Matrix as we saw on the Whiteboard the",
      "start": 1111.2,
      "duration": 4.719
    },
    {
      "text": "shape of this is B which is the batch",
      "start": 1114.159,
      "duration": 4.041
    },
    {
      "text": "size the number of rows is equal to the",
      "start": 1115.919,
      "duration": 4.0
    },
    {
      "text": "number of tokens and the number of",
      "start": 1118.2,
      "duration": 4.04
    },
    {
      "text": "columns is equal to the D out which is",
      "start": 1119.919,
      "duration": 3.721
    },
    {
      "text": "equal to",
      "start": 1122.24,
      "duration": 5.439
    },
    {
      "text": "six okay now we move to the next step",
      "start": 1123.64,
      "duration": 5.8
    },
    {
      "text": "and this is the step where four",
      "start": 1127.679,
      "duration": 3.681
    },
    {
      "text": "dimensional tensor start to come into",
      "start": 1129.44,
      "duration": 4.2
    },
    {
      "text": "the picture right so until now we have",
      "start": 1131.36,
      "duration": 4.319
    },
    {
      "text": "three dimensional tensors for the keys",
      "start": 1133.64,
      "duration": 3.88
    },
    {
      "text": "queries and values",
      "start": 1135.679,
      "duration": 4.401
    },
    {
      "text": "right why the fourth dimension needs to",
      "start": 1137.52,
      "duration": 4.519
    },
    {
      "text": "come into the picture is that until now",
      "start": 1140.08,
      "duration": 4.24
    },
    {
      "text": "these three dimensions are for batch",
      "start": 1142.039,
      "duration": 4.481
    },
    {
      "text": "size number of tokens and D out but",
      "start": 1144.32,
      "duration": 4.599
    },
    {
      "text": "there is no dimension for the number of",
      "start": 1146.52,
      "duration": 6.399
    },
    {
      "text": "heads uh or the head Dimension rather so",
      "start": 1148.919,
      "duration": 6.12
    },
    {
      "text": "this is where we come to next so what we",
      "start": 1152.919,
      "duration": 3.961
    },
    {
      "text": "are now going to do is that we are going",
      "start": 1155.039,
      "duration": 5.361
    },
    {
      "text": "to unroll the last dimension of the keys",
      "start": 1156.88,
      "duration": 6.0
    },
    {
      "text": "queries and values to include the number",
      "start": 1160.4,
      "duration": 5.08
    },
    {
      "text": "of heads and the head Dimension what",
      "start": 1162.88,
      "duration": 4.039
    },
    {
      "text": "this means is that if you look at this",
      "start": 1165.48,
      "duration": 3.8
    },
    {
      "text": "last dimension of the keys here in fact",
      "start": 1166.919,
      "duration": 4.161
    },
    {
      "text": "even for queries and values this is D",
      "start": 1169.28,
      "duration": 4.48
    },
    {
      "text": "out right and D out is essentially",
      "start": 1171.08,
      "duration": 4.479
    },
    {
      "text": "number of heads into head Dimension as",
      "start": 1173.76,
      "duration": 4.96
    },
    {
      "text": "we have seen earlier so let me take you",
      "start": 1175.559,
      "duration": 5.761
    },
    {
      "text": "yeah so here remember what we saw head",
      "start": 1178.72,
      "duration": 5.64
    },
    {
      "text": "Dimension is equal to D out divided by",
      "start": 1181.32,
      "duration": 5.28
    },
    {
      "text": "number of heads so D out is equal to",
      "start": 1184.36,
      "duration": 4.04
    },
    {
      "text": "head Dimension multiplied by the number",
      "start": 1186.6,
      "duration": 3.48
    },
    {
      "text": "of",
      "start": 1188.4,
      "duration": 4.56
    },
    {
      "text": "heads so that's what we are actually",
      "start": 1190.08,
      "duration": 4.599
    },
    {
      "text": "going to do we are going to unroll the",
      "start": 1192.96,
      "duration": 3.64
    },
    {
      "text": "last dimension of the keys saries and",
      "start": 1194.679,
      "duration": 3.681
    },
    {
      "text": "values to include number of heads and",
      "start": 1196.6,
      "duration": 4.48
    },
    {
      "text": "head Dimension right so we have D out",
      "start": 1198.36,
      "duration": 4.52
    },
    {
      "text": "which is equal to 6 which is a decision",
      "start": 1201.08,
      "duration": 3.719
    },
    {
      "text": "which we have made and we have also made",
      "start": 1202.88,
      "duration": 3.679
    },
    {
      "text": "a decision with respect to the number of",
      "start": 1204.799,
      "duration": 4.081
    },
    {
      "text": "attention heads which is equal to two So",
      "start": 1206.559,
      "duration": 4.0
    },
    {
      "text": "based on these two decision points the",
      "start": 1208.88,
      "duration": 3.039
    },
    {
      "text": "head Dimension is",
      "start": 1210.559,
      "duration": 3.641
    },
    {
      "text": "fixed and the head Dimension will be",
      "start": 1211.919,
      "duration": 5.521
    },
    {
      "text": "equal to 6 / 2 which is equal to 3 so",
      "start": 1214.2,
      "duration": 5.04
    },
    {
      "text": "what we are going to do next is that we",
      "start": 1217.44,
      "duration": 4.96
    },
    {
      "text": "had this 1X 3x 6 matrices right for the",
      "start": 1219.24,
      "duration": 5.48
    },
    {
      "text": "key SAR and value now we are going to",
      "start": 1222.4,
      "duration": 7.2
    },
    {
      "text": "roll them into 1 by 3 by 2 by 3",
      "start": 1224.72,
      "duration": 7.24
    },
    {
      "text": "so now instead of D out we will have",
      "start": 1229.6,
      "duration": 5.28
    },
    {
      "text": "number of heads which is equal to",
      "start": 1231.96,
      "duration": 7.959
    },
    {
      "text": "two and head Dimension which is equal to",
      "start": 1234.88,
      "duration": 5.039
    },
    {
      "text": "three so let's see what the reshaped",
      "start": 1240.48,
      "duration": 4.76
    },
    {
      "text": "keys queries and values Matrix actually",
      "start": 1243.159,
      "duration": 5.201
    },
    {
      "text": "look like uh so when you reshape the",
      "start": 1245.24,
      "duration": 5.08
    },
    {
      "text": "keys queries and value Matrix they start",
      "start": 1248.36,
      "duration": 4.0
    },
    {
      "text": "looking like this and I'll tell you how",
      "start": 1250.32,
      "duration": 4.08
    },
    {
      "text": "to interpret four dimensional tensors",
      "start": 1252.36,
      "duration": 5.24
    },
    {
      "text": "also so for the sake of Simplicity uh",
      "start": 1254.4,
      "duration": 5.24
    },
    {
      "text": "let's first analyze the queries Matrix",
      "start": 1257.6,
      "duration": 6.16
    },
    {
      "text": "so this Matrix is 1x 3x 2x 3 how do you",
      "start": 1259.64,
      "duration": 6.48
    },
    {
      "text": "Analyze This four dimensional tensor for",
      "start": 1263.76,
      "duration": 4.64
    },
    {
      "text": "now forget about the first which is the",
      "start": 1266.12,
      "duration": 5.24
    },
    {
      "text": "number of batches okay so next look at",
      "start": 1268.4,
      "duration": 5.08
    },
    {
      "text": "three so this three is the number of",
      "start": 1271.36,
      "duration": 4.319
    },
    {
      "text": "rows so this is my first",
      "start": 1273.48,
      "duration": 5.24
    },
    {
      "text": "row and uh this is my first token also",
      "start": 1275.679,
      "duration": 6.521
    },
    {
      "text": "right this is my second token and this",
      "start": 1278.72,
      "duration": 6.64
    },
    {
      "text": "is my third token correct that's why",
      "start": 1282.2,
      "duration": 5.599
    },
    {
      "text": "there there is this three now let's look",
      "start": 1285.36,
      "duration": 3.84
    },
    {
      "text": "at this two",
      "start": 1287.799,
      "duration": 3.561
    },
    {
      "text": "what is this two this two is the number",
      "start": 1289.2,
      "duration": 4.28
    },
    {
      "text": "of heads so if I go in each token right",
      "start": 1291.36,
      "duration": 4.12
    },
    {
      "text": "now let's see if I go in first if I go",
      "start": 1293.48,
      "duration": 4.439
    },
    {
      "text": "in the first token the first row",
      "start": 1295.48,
      "duration": 4.28
    },
    {
      "text": "corresponds to the first head and the",
      "start": 1297.919,
      "duration": 3.521
    },
    {
      "text": "second row corresponds to the second",
      "start": 1299.76,
      "duration": 4.12
    },
    {
      "text": "head that's why there is this two and if",
      "start": 1301.44,
      "duration": 4.56
    },
    {
      "text": "I go within each head I'll see that",
      "start": 1303.88,
      "duration": 4.56
    },
    {
      "text": "there is there are three dimension the",
      "start": 1306.0,
      "duration": 4.84
    },
    {
      "text": "First Dimension the second dimension and",
      "start": 1308.44,
      "duration": 5.04
    },
    {
      "text": "the third dimension so remember the head",
      "start": 1310.84,
      "duration": 4.719
    },
    {
      "text": "Dimension is equal to three so the way",
      "start": 1313.48,
      "duration": 4.12
    },
    {
      "text": "to interpret this Matrix is start from",
      "start": 1315.559,
      "duration": 4.761
    },
    {
      "text": "the outermost value so three why three",
      "start": 1317.6,
      "duration": 4.52
    },
    {
      "text": "because there are three tokens then go",
      "start": 1320.32,
      "duration": 3.8
    },
    {
      "text": "to each token why two because there are",
      "start": 1322.12,
      "duration": 4.28
    },
    {
      "text": "two heads in each token then go within",
      "start": 1324.12,
      "duration": 4.439
    },
    {
      "text": "each head why three because the",
      "start": 1326.4,
      "duration": 4.36
    },
    {
      "text": "dimension of each head is three each",
      "start": 1328.559,
      "duration": 4.081
    },
    {
      "text": "head is a three dimensional",
      "start": 1330.76,
      "duration": 4.24
    },
    {
      "text": "Vector so in this same way we can",
      "start": 1332.64,
      "duration": 4.159
    },
    {
      "text": "analyze the queries the keys and the",
      "start": 1335.0,
      "duration": 4.48
    },
    {
      "text": "value Matrix also so the keys Matrix",
      "start": 1336.799,
      "duration": 7.24
    },
    {
      "text": "will also be uh 1X 3x 2x3 and the values",
      "start": 1339.48,
      "duration": 7.92
    },
    {
      "text": "Matrix will also be 1X 3x 2x 3 as I",
      "start": 1344.039,
      "duration": 5.64
    },
    {
      "text": "mentioned before let me repeat it again",
      "start": 1347.4,
      "duration": 4.36
    },
    {
      "text": "each row over here is a token so if you",
      "start": 1349.679,
      "duration": 4.201
    },
    {
      "text": "look at the value Matrix let's look at",
      "start": 1351.76,
      "duration": 3.68
    },
    {
      "text": "the second row the second row",
      "start": 1353.88,
      "duration": 4.279
    },
    {
      "text": "corresponds to the second token if you",
      "start": 1355.44,
      "duration": 6.0
    },
    {
      "text": "now look at the first row of the second",
      "start": 1358.159,
      "duration": 6.481
    },
    {
      "text": "row this that is the threedimensional",
      "start": 1361.44,
      "duration": 7.28
    },
    {
      "text": "head Vector for the first head if you",
      "start": 1364.64,
      "duration": 5.72
    },
    {
      "text": "look at the second row that's the",
      "start": 1368.72,
      "duration": 3.24
    },
    {
      "text": "three-dimensional head Vector for the",
      "start": 1370.36,
      "duration": 4.4
    },
    {
      "text": "second head remember every token has two",
      "start": 1371.96,
      "duration": 4.68
    },
    {
      "text": "attention heads so you can think of as",
      "start": 1374.76,
      "duration": 3.48
    },
    {
      "text": "two people paying attention to each",
      "start": 1376.64,
      "duration": 3.279
    },
    {
      "text": "token token because we have two",
      "start": 1378.24,
      "duration": 3.72
    },
    {
      "text": "attention heads that's why there are two",
      "start": 1379.919,
      "duration": 4.36
    },
    {
      "text": "rows corresponding to every",
      "start": 1381.96,
      "duration": 5.0
    },
    {
      "text": "token now if we come to the code this",
      "start": 1384.279,
      "duration": 5.64
    },
    {
      "text": "line has been mentioned over here so see",
      "start": 1386.96,
      "duration": 5.0
    },
    {
      "text": "we have to unroll the last Dimension so",
      "start": 1389.919,
      "duration": 4.601
    },
    {
      "text": "now the D out will be replaced with the",
      "start": 1391.96,
      "duration": 5.12
    },
    {
      "text": "number of heads and head Dimension so",
      "start": 1394.52,
      "duration": 4.2
    },
    {
      "text": "this is exactly what has been done over",
      "start": 1397.08,
      "duration": 4.52
    },
    {
      "text": "here keys. view so now keys will be",
      "start": 1398.72,
      "duration": 5.16
    },
    {
      "text": "replaced with keys do view B common",
      "start": 1401.6,
      "duration": 4.72
    },
    {
      "text": "number of tokens common number of heads",
      "start": 1403.88,
      "duration": 5.6
    },
    {
      "text": "and head Dimension this is exactly",
      "start": 1406.32,
      "duration": 5.479
    },
    {
      "text": "uh what we we just saw on the Whiteboard",
      "start": 1409.48,
      "duration": 4.0
    },
    {
      "text": "so in this step the three dimensional",
      "start": 1411.799,
      "duration": 3.48
    },
    {
      "text": "tensors have been converted into four",
      "start": 1413.48,
      "duration": 3.92
    },
    {
      "text": "dimensional tensors to include the",
      "start": 1415.279,
      "duration": 4.841
    },
    {
      "text": "number of heads and the head Dimension",
      "start": 1417.4,
      "duration": 5.68
    },
    {
      "text": "great now we move to the next step so if",
      "start": 1420.12,
      "duration": 6.6
    },
    {
      "text": "you see uh if you see this let's look at",
      "start": 1423.08,
      "duration": 5.479
    },
    {
      "text": "this argument which is",
      "start": 1426.72,
      "duration": 5.76
    },
    {
      "text": "three um so the shape of this is 1A 3A 2",
      "start": 1428.559,
      "duration": 6.561
    },
    {
      "text": "comma 3 right now let's look at this",
      "start": 1432.48,
      "duration": 7.199
    },
    {
      "text": "first this this entry this is three",
      "start": 1435.12,
      "duration": 7.28
    },
    {
      "text": "now this three is the number of tokens",
      "start": 1439.679,
      "duration": 4.201
    },
    {
      "text": "which means that currently these",
      "start": 1442.4,
      "duration": 3.44
    },
    {
      "text": "matrices are grouped according to number",
      "start": 1443.88,
      "duration": 5.039
    },
    {
      "text": "of tokens right so I'm saying that this",
      "start": 1445.84,
      "duration": 4.88
    },
    {
      "text": "is the first token this is the second",
      "start": 1448.919,
      "duration": 3.841
    },
    {
      "text": "token and this is the third",
      "start": 1450.72,
      "duration": 4.52
    },
    {
      "text": "token and then I further dive into",
      "start": 1452.76,
      "duration": 4.08
    },
    {
      "text": "number of heads and the dimensions in",
      "start": 1455.24,
      "duration": 4.439
    },
    {
      "text": "each head but it turns out that later",
      "start": 1456.84,
      "duration": 4.4
    },
    {
      "text": "when we want to compute the attention",
      "start": 1459.679,
      "duration": 4.161
    },
    {
      "text": "scores the only way the computation can",
      "start": 1461.24,
      "duration": 4.24
    },
    {
      "text": "proceed ahead is if we Group by the",
      "start": 1463.84,
      "duration": 4.12
    },
    {
      "text": "number of heads so instead of grouping",
      "start": 1465.48,
      "duration": 4.919
    },
    {
      "text": "by the number number of tokens I",
      "start": 1467.96,
      "duration": 4.12
    },
    {
      "text": "actually want to group by the number of",
      "start": 1470.399,
      "duration": 3.921
    },
    {
      "text": "heads and we have two heads here right",
      "start": 1472.08,
      "duration": 3.88
    },
    {
      "text": "so I want to flip these",
      "start": 1474.32,
      "duration": 3.599
    },
    {
      "text": "Dimensions I want to flip these",
      "start": 1475.96,
      "duration": 5.16
    },
    {
      "text": "Dimensions here so that the first row",
      "start": 1477.919,
      "duration": 4.841
    },
    {
      "text": "represents the first head the second",
      "start": 1481.12,
      "duration": 3.679
    },
    {
      "text": "will represent the second head and each",
      "start": 1482.76,
      "duration": 4.88
    },
    {
      "text": "will have a 3X3 let me show you what I",
      "start": 1484.799,
      "duration": 5.281
    },
    {
      "text": "mean so now what we are going to do is",
      "start": 1487.64,
      "duration": 4.12
    },
    {
      "text": "we are going to group The matrices by",
      "start": 1490.08,
      "duration": 4.599
    },
    {
      "text": "the number of heads okay so currently",
      "start": 1491.76,
      "duration": 5.2
    },
    {
      "text": "the keys queries and the values Matrix",
      "start": 1494.679,
      "duration": 4.36
    },
    {
      "text": "have the dimensions of one which is the",
      "start": 1496.96,
      "duration": 3.959
    },
    {
      "text": "batch three which is the number of",
      "start": 1499.039,
      "duration": 4.481
    },
    {
      "text": "tokens two which is the number of heads",
      "start": 1500.919,
      "duration": 5.201
    },
    {
      "text": "and three which is the head Dimension so",
      "start": 1503.52,
      "duration": 4.24
    },
    {
      "text": "we are grouping with respect to the",
      "start": 1506.12,
      "duration": 4.2
    },
    {
      "text": "number of tokens but now I want to group",
      "start": 1507.76,
      "duration": 5.279
    },
    {
      "text": "with respect to number of heads so again",
      "start": 1510.32,
      "duration": 5.4
    },
    {
      "text": "I want to switch the dimensions to be I",
      "start": 1513.039,
      "duration": 5.561
    },
    {
      "text": "want to switch this this",
      "start": 1515.72,
      "duration": 5.8
    },
    {
      "text": "to uh let me write it",
      "start": 1518.6,
      "duration": 6.0
    },
    {
      "text": "again yeah I want to switch this two",
      "start": 1521.52,
      "duration": 4.759
    },
    {
      "text": "with three and this three should come",
      "start": 1524.6,
      "duration": 4.6
    },
    {
      "text": "over here so I want the the matri to",
      "start": 1526.279,
      "duration": 4.921
    },
    {
      "text": "have the dimensions of B comma number of",
      "start": 1529.2,
      "duration": 3.959
    },
    {
      "text": "heads comma number of tokens and head",
      "start": 1531.2,
      "duration": 4.24
    },
    {
      "text": "Dimension so I want the dimensions to be",
      "start": 1533.159,
      "duration": 5.361
    },
    {
      "text": "1 comma 2 comma 3 comma 3 so what I'm",
      "start": 1535.44,
      "duration": 4.719
    },
    {
      "text": "going to do in the code also you'll see",
      "start": 1538.52,
      "duration": 3.519
    },
    {
      "text": "we are going to transpose keys quaries",
      "start": 1540.159,
      "duration": 3.88
    },
    {
      "text": "and value and we are going to transpose",
      "start": 1542.039,
      "duration": 4.76
    },
    {
      "text": "one comma 2 now why do we do one comma 2",
      "start": 1544.039,
      "duration": 4.88
    },
    {
      "text": "over here because python has zero",
      "start": 1546.799,
      "duration": 4.801
    },
    {
      "text": "indexing so index zero is this since we",
      "start": 1548.919,
      "duration": 4.48
    },
    {
      "text": "want to Interchange the number of tokens",
      "start": 1551.6,
      "duration": 3.88
    },
    {
      "text": "and the number of heads the indexes",
      "start": 1553.399,
      "duration": 3.64
    },
    {
      "text": "which we need to transpose are index",
      "start": 1555.48,
      "duration": 4.04
    },
    {
      "text": "number one and index number two that's",
      "start": 1557.039,
      "duration": 4.681
    },
    {
      "text": "why we are doing Keys queries and value",
      "start": 1559.52,
      "duration": 5.68
    },
    {
      "text": "and transpose 1A 2 so let's see what the",
      "start": 1561.72,
      "duration": 5.959
    },
    {
      "text": "result actually looks like so when you",
      "start": 1565.2,
      "duration": 5.92
    },
    {
      "text": "when you make the 1A 3A 2A 3 to 1A 2",
      "start": 1567.679,
      "duration": 6.24
    },
    {
      "text": "comma 3A 3 now the transposed queries",
      "start": 1571.12,
      "duration": 5.159
    },
    {
      "text": "keys and Valu start looking like this",
      "start": 1573.919,
      "duration": 3.681
    },
    {
      "text": "and now you will see that they are",
      "start": 1576.279,
      "duration": 3.921
    },
    {
      "text": "grouped by head so the first thing what",
      "start": 1577.6,
      "duration": 4.28
    },
    {
      "text": "we can do is that let's look at this",
      "start": 1580.2,
      "duration": 4.12
    },
    {
      "text": "block in the queries so we are analyzing",
      "start": 1581.88,
      "duration": 4.56
    },
    {
      "text": "the queries Matrix now the shape of the",
      "start": 1584.32,
      "duration": 4.8
    },
    {
      "text": "queries Matrix is what",
      "start": 1586.44,
      "duration": 4.359
    },
    {
      "text": "1",
      "start": 1589.12,
      "duration": 6.72
    },
    {
      "text": "comma let me write it here again 1 comma",
      "start": 1590.799,
      "duration": 7.921
    },
    {
      "text": "2",
      "start": 1595.84,
      "duration": 4.88
    },
    {
      "text": "comma",
      "start": 1598.72,
      "duration": 6.8
    },
    {
      "text": "3 comma 3 right that is the essentially",
      "start": 1600.72,
      "duration": 7.48
    },
    {
      "text": "the shape of the queries Matrix and we",
      "start": 1605.52,
      "duration": 5.0
    },
    {
      "text": "are going to analyze this so let's start",
      "start": 1608.2,
      "duration": 4.76
    },
    {
      "text": "with this two which is the number of",
      "start": 1610.52,
      "duration": 4.44
    },
    {
      "text": "heads so if you look at the first block",
      "start": 1612.96,
      "duration": 5.64
    },
    {
      "text": "here uh let me erase this right now now",
      "start": 1614.96,
      "duration": 6.16
    },
    {
      "text": "and then draw it again yeah so if you",
      "start": 1618.6,
      "duration": 4.64
    },
    {
      "text": "look at the first block over here which",
      "start": 1621.12,
      "duration": 3.76
    },
    {
      "text": "are marking with these curly braces",
      "start": 1623.24,
      "duration": 3.679
    },
    {
      "text": "that's the first head if you look at the",
      "start": 1624.88,
      "duration": 4.12
    },
    {
      "text": "second block here that's the second head",
      "start": 1626.919,
      "duration": 4.521
    },
    {
      "text": "so see now this two because this two",
      "start": 1629.0,
      "duration": 4.159
    },
    {
      "text": "comes over here now we can group with",
      "start": 1631.44,
      "duration": 3.92
    },
    {
      "text": "respect to number of heads so the first",
      "start": 1633.159,
      "duration": 3.961
    },
    {
      "text": "block shows everything with respect to",
      "start": 1635.36,
      "duration": 4.199
    },
    {
      "text": "head one and the first row over here is",
      "start": 1637.12,
      "duration": 4.439
    },
    {
      "text": "the first token the second row over here",
      "start": 1639.559,
      "duration": 4.321
    },
    {
      "text": "is the second token and the third row",
      "start": 1641.559,
      "duration": 4.681
    },
    {
      "text": "over here is the third token similarly",
      "start": 1643.88,
      "duration": 4.36
    },
    {
      "text": "if you look at head number two the first",
      "start": 1646.24,
      "duration": 4.0
    },
    {
      "text": "row is the first token the second row is",
      "start": 1648.24,
      "duration": 4.799
    },
    {
      "text": "the second token and the third row is",
      "start": 1650.24,
      "duration": 4.2
    },
    {
      "text": "the third",
      "start": 1653.039,
      "duration": 4.24
    },
    {
      "text": "token so now we have the dimensions as",
      "start": 1654.44,
      "duration": 4.719
    },
    {
      "text": "number of tokens and head Dimensions",
      "start": 1657.279,
      "duration": 5.081
    },
    {
      "text": "come last so each token if you see each",
      "start": 1659.159,
      "duration": 5.36
    },
    {
      "text": "token has a three dimensional Vector",
      "start": 1662.36,
      "duration": 4.12
    },
    {
      "text": "because the head Dimension is equal to",
      "start": 1664.519,
      "duration": 5.16
    },
    {
      "text": "three so the the reason this helps is",
      "start": 1666.48,
      "duration": 4.72
    },
    {
      "text": "because since we can now group with",
      "start": 1669.679,
      "duration": 4.24
    },
    {
      "text": "respect to heads we can compute the",
      "start": 1671.2,
      "duration": 5.52
    },
    {
      "text": "attention score for each head separately",
      "start": 1673.919,
      "duration": 4.841
    },
    {
      "text": "so remember there is one attention score",
      "start": 1676.72,
      "duration": 3.52
    },
    {
      "text": "there are there is an attention score",
      "start": 1678.76,
      "duration": 3.48
    },
    {
      "text": "Matrix for the head one and there is an",
      "start": 1680.24,
      "duration": 4.439
    },
    {
      "text": "attention score Matrix for the head two",
      "start": 1682.24,
      "duration": 4.679
    },
    {
      "text": "and then we we are going to U",
      "start": 1684.679,
      "duration": 4.201
    },
    {
      "text": "concatenate them together right so it",
      "start": 1686.919,
      "duration": 3.961
    },
    {
      "text": "makes sense to group with respect to the",
      "start": 1688.88,
      "duration": 4.679
    },
    {
      "text": "head and that's why this step exist this",
      "start": 1690.88,
      "duration": 5.159
    },
    {
      "text": "keys. transpose it's very difficult for",
      "start": 1693.559,
      "duration": 4.321
    },
    {
      "text": "students to understand this unless you",
      "start": 1696.039,
      "duration": 4.041
    },
    {
      "text": "see this visual example of why we are",
      "start": 1697.88,
      "duration": 5.0
    },
    {
      "text": "essentially doing this transpose the",
      "start": 1700.08,
      "duration": 4.76
    },
    {
      "text": "main reason we do do this transpose is",
      "start": 1702.88,
      "duration": 3.56
    },
    {
      "text": "that here you see we are grouping with",
      "start": 1704.84,
      "duration": 4.28
    },
    {
      "text": "respect to uh",
      "start": 1706.44,
      "duration": 4.2
    },
    {
      "text": "we are grouping with respect to number",
      "start": 1709.12,
      "duration": 4.039
    },
    {
      "text": "of tokens here but that's not good if",
      "start": 1710.64,
      "duration": 4.44
    },
    {
      "text": "you want to compute the attention scores",
      "start": 1713.159,
      "duration": 4.321
    },
    {
      "text": "for each head parall so we group with",
      "start": 1715.08,
      "duration": 4.24
    },
    {
      "text": "respect to number of heads so that's why",
      "start": 1717.48,
      "duration": 4.0
    },
    {
      "text": "it's important to flip number of tokens",
      "start": 1719.32,
      "duration": 4.68
    },
    {
      "text": "and number of head Dimension and that's",
      "start": 1721.48,
      "duration": 5.36
    },
    {
      "text": "exactly what we have done okay now let's",
      "start": 1724.0,
      "duration": 5.039
    },
    {
      "text": "go to the next step the next step is to",
      "start": 1726.84,
      "duration": 4.12
    },
    {
      "text": "find the attention scores so remember",
      "start": 1729.039,
      "duration": 5.24
    },
    {
      "text": "now we have the uh we have the queries",
      "start": 1730.96,
      "duration": 5.199
    },
    {
      "text": "Matrix we have the keys Matrix and we",
      "start": 1734.279,
      "duration": 3.481
    },
    {
      "text": "have the values Matrix in exactly the",
      "start": 1736.159,
      "duration": 4.961
    },
    {
      "text": "shape which we want so now we can do uh",
      "start": 1737.76,
      "duration": 6.519
    },
    {
      "text": "we can go ahead and find the key queries",
      "start": 1741.12,
      "duration": 5.6
    },
    {
      "text": "and the keys transpose to get the",
      "start": 1744.279,
      "duration": 4.161
    },
    {
      "text": "attention score so let me show you how",
      "start": 1746.72,
      "duration": 6.679
    },
    {
      "text": "this is done first let me rub all of",
      "start": 1748.44,
      "duration": 4.959
    },
    {
      "text": "this",
      "start": 1753.919,
      "duration": 3.0
    },
    {
      "text": "okay okay so now I have rubbed all of",
      "start": 1760.519,
      "duration": 4.241
    },
    {
      "text": "this so what we are now essentially",
      "start": 1763.24,
      "duration": 5.48
    },
    {
      "text": "going to do is that um this is the",
      "start": 1764.76,
      "duration": 5.96
    },
    {
      "text": "head number one right this is the head",
      "start": 1768.72,
      "duration": 3.959
    },
    {
      "text": "number one of the queries and this is",
      "start": 1770.72,
      "duration": 4.92
    },
    {
      "text": "the head number one of the keys so what",
      "start": 1772.679,
      "duration": 5.041
    },
    {
      "text": "this this shape will help us do is that",
      "start": 1775.64,
      "duration": 3.919
    },
    {
      "text": "when we do queries multiplied by Keys",
      "start": 1777.72,
      "duration": 3.28
    },
    {
      "text": "transpose it will",
      "start": 1779.559,
      "duration": 4.801
    },
    {
      "text": "directly uh take the equivalent product",
      "start": 1781.0,
      "duration": 5.159
    },
    {
      "text": "of head one of the queries with head one",
      "start": 1784.36,
      "duration": 3.799
    },
    {
      "text": "of the keys and then head two of the",
      "start": 1786.159,
      "duration": 4.76
    },
    {
      "text": "queries and head two of the",
      "start": 1788.159,
      "duration": 5.681
    },
    {
      "text": "keys but remember when we take the keys",
      "start": 1790.919,
      "duration": 4.76
    },
    {
      "text": "transpose what's really important to us",
      "start": 1793.84,
      "duration": 4.36
    },
    {
      "text": "is that now the shape of the keys is B B",
      "start": 1795.679,
      "duration": 4.0
    },
    {
      "text": "common number of head is common number",
      "start": 1798.2,
      "duration": 4.24
    },
    {
      "text": "of tokens and head Dimension so what",
      "start": 1799.679,
      "duration": 4.641
    },
    {
      "text": "it's really important to us is number of",
      "start": 1802.44,
      "duration": 3.28
    },
    {
      "text": "tokens and head",
      "start": 1804.32,
      "duration": 5.079
    },
    {
      "text": "Dimension so remember the formula",
      "start": 1805.72,
      "duration": 6.199
    },
    {
      "text": "for calculating the attention score is",
      "start": 1809.399,
      "duration": 4.441
    },
    {
      "text": "queries multiplied by Keys transpose",
      "start": 1811.919,
      "duration": 3.36
    },
    {
      "text": "right so here also we are going to do",
      "start": 1813.84,
      "duration": 3.6
    },
    {
      "text": "queries with respect to Keys transpose",
      "start": 1815.279,
      "duration": 4.12
    },
    {
      "text": "but what exactly do we have to transpose",
      "start": 1817.44,
      "duration": 3.04
    },
    {
      "text": "we have to",
      "start": 1819.399,
      "duration": 5.241
    },
    {
      "text": "transpose uh we have to transpose this",
      "start": 1820.48,
      "duration": 5.88
    },
    {
      "text": "so we have to transpose the last two",
      "start": 1824.64,
      "duration": 3.919
    },
    {
      "text": "dimensions and let me show you what that",
      "start": 1826.36,
      "duration": 4.72
    },
    {
      "text": "transposed key Matrix looks like yeah so",
      "start": 1828.559,
      "duration": 5.201
    },
    {
      "text": "this is the transposed key Matrix now",
      "start": 1831.08,
      "duration": 4.68
    },
    {
      "text": "here you can see the key",
      "start": 1833.76,
      "duration": 5.399
    },
    {
      "text": "Matrix uh if you see the first row it's",
      "start": 1835.76,
      "duration": 5.96
    },
    {
      "text": "4143 -1.",
      "start": 1839.159,
      "duration": 7.841
    },
    {
      "text": "423 and - 2.71 31 right so when we do",
      "start": 1841.72,
      "duration": 7.92
    },
    {
      "text": "Keys transpose Keys transpose 2 comma 3",
      "start": 1847.0,
      "duration": 4.279
    },
    {
      "text": "it will transpose along the last two",
      "start": 1849.64,
      "duration": 4.08
    },
    {
      "text": "Dimensions so now that that row which we",
      "start": 1851.279,
      "duration": 5.28
    },
    {
      "text": "saw has now become a column over here so",
      "start": 1853.72,
      "duration": 5.48
    },
    {
      "text": "this is the keys transposed",
      "start": 1856.559,
      "duration": 4.441
    },
    {
      "text": "and here is the queries Matrix and I've",
      "start": 1859.2,
      "duration": 3.88
    },
    {
      "text": "shown the keys transpose over here so",
      "start": 1861.0,
      "duration": 5.12
    },
    {
      "text": "the queries matrix dimensions is 1A 2A 3",
      "start": 1863.08,
      "duration": 5.64
    },
    {
      "text": "comma 3 the keys transpose Dimension is",
      "start": 1866.12,
      "duration": 5.519
    },
    {
      "text": "1A 2 comma 3 comma 3 so they they they",
      "start": 1868.72,
      "duration": 5.52
    },
    {
      "text": "are compatible for multiplication and",
      "start": 1871.639,
      "duration": 4.081
    },
    {
      "text": "the way the multiplication will now",
      "start": 1874.24,
      "duration": 4.08
    },
    {
      "text": "proceed is that the head one will only",
      "start": 1875.72,
      "duration": 4.28
    },
    {
      "text": "be multiplied by the head one of the",
      "start": 1878.32,
      "duration": 4.04
    },
    {
      "text": "keys transposed the head two here will",
      "start": 1880.0,
      "duration": 4.2
    },
    {
      "text": "only be multiplied with the head two of",
      "start": 1882.36,
      "duration": 4.12
    },
    {
      "text": "the keys transposed and ultimately when",
      "start": 1884.2,
      "duration": 4.16
    },
    {
      "text": "we do this multiplication we we will get",
      "start": 1886.48,
      "duration": 5.079
    },
    {
      "text": "the attention scores Matrix so this is",
      "start": 1888.36,
      "duration": 5.679
    },
    {
      "text": "the attention score Matrix which we have",
      "start": 1891.559,
      "duration": 5.48
    },
    {
      "text": "and the dimensions of this are B number",
      "start": 1894.039,
      "duration": 6.0
    },
    {
      "text": "of heads number of tokens and number of",
      "start": 1897.039,
      "duration": 6.52
    },
    {
      "text": "tokens let me show you why",
      "start": 1900.039,
      "duration": 6.441
    },
    {
      "text": "um okay so if you look at what we are",
      "start": 1903.559,
      "duration": 5.161
    },
    {
      "text": "multiplying here the query's dimensions",
      "start": 1906.48,
      "duration": 4.36
    },
    {
      "text": "are B comma number of heads comma number",
      "start": 1908.72,
      "duration": 4.439
    },
    {
      "text": "of tokens comma head Dimension right and",
      "start": 1910.84,
      "duration": 3.679
    },
    {
      "text": "when we do Keys",
      "start": 1913.159,
      "duration": 4.161
    },
    {
      "text": "transpose uh 2 comma 3 the dimensions",
      "start": 1914.519,
      "duration": 5.481
    },
    {
      "text": "here are B number of heads head",
      "start": 1917.32,
      "duration": 5.12
    },
    {
      "text": "Dimensions comma number of tokens so",
      "start": 1920.0,
      "duration": 4.08
    },
    {
      "text": "essentially you can think about it like",
      "start": 1922.44,
      "duration": 3.32
    },
    {
      "text": "we are multiplying two matrices with the",
      "start": 1924.08,
      "duration": 3.479
    },
    {
      "text": "dimensions number of tokens comma head",
      "start": 1925.76,
      "duration": 4.799
    },
    {
      "text": "Dimension multiplied by head Dimension",
      "start": 1927.559,
      "duration": 5.12
    },
    {
      "text": "number of tokens so what will the",
      "start": 1930.559,
      "duration": 3.641
    },
    {
      "text": "resulted Matrix will have number of",
      "start": 1932.679,
      "duration": 4.681
    },
    {
      "text": "tokens rows and number of tokens columns",
      "start": 1934.2,
      "duration": 4.719
    },
    {
      "text": "and the first two Dimensions here will",
      "start": 1937.36,
      "duration": 3.0
    },
    {
      "text": "stay the same because they are the same",
      "start": 1938.919,
      "duration": 3.24
    },
    {
      "text": "in both of these matrices we are",
      "start": 1940.36,
      "duration": 4.159
    },
    {
      "text": "multiplying so the resultant attention",
      "start": 1942.159,
      "duration": 4.561
    },
    {
      "text": "scores will have the dimensions of B",
      "start": 1944.519,
      "duration": 3.961
    },
    {
      "text": "number of heads number of tokens and",
      "start": 1946.72,
      "duration": 4.4
    },
    {
      "text": "number of tokens it's fine if you forget",
      "start": 1948.48,
      "duration": 4.159
    },
    {
      "text": "these Dimensions but you should be able",
      "start": 1951.12,
      "duration": 4.76
    },
    {
      "text": "to interpret what is going on here so",
      "start": 1952.639,
      "duration": 5.361
    },
    {
      "text": "let's see what is going on here remember",
      "start": 1955.88,
      "duration": 3.639
    },
    {
      "text": "we have we are grouping with respect to",
      "start": 1958.0,
      "duration": 4.6
    },
    {
      "text": "head so that stays the same this first",
      "start": 1959.519,
      "duration": 4.64
    },
    {
      "text": "uh this first block which I've",
      "start": 1962.6,
      "duration": 3.0
    },
    {
      "text": "highlighted right now that is head",
      "start": 1964.159,
      "duration": 4.4
    },
    {
      "text": "number one and the second block which",
      "start": 1965.6,
      "duration": 4.76
    },
    {
      "text": "which I've highlighted right now that is",
      "start": 1968.559,
      "duration": 3.401
    },
    {
      "text": "essentially head number",
      "start": 1970.36,
      "duration": 3.88
    },
    {
      "text": "two this is the first thing to",
      "start": 1971.96,
      "duration": 4.439
    },
    {
      "text": "understand okay then what we are doing",
      "start": 1974.24,
      "duration": 4.08
    },
    {
      "text": "when you look at the let's look at head",
      "start": 1976.399,
      "duration": 4.081
    },
    {
      "text": "number one for now if you look at the",
      "start": 1978.32,
      "duration": 4.719
    },
    {
      "text": "first row the first row essentially",
      "start": 1980.48,
      "duration": 4.52
    },
    {
      "text": "consists of the attention score between",
      "start": 1983.039,
      "duration": 3.48
    },
    {
      "text": "of the first word with all the other",
      "start": 1985.0,
      "duration": 6.519
    },
    {
      "text": "words right so remember our sentence was",
      "start": 1986.519,
      "duration": 5.0
    },
    {
      "text": "the actually let me write it over here",
      "start": 1992.96,
      "duration": 6.199
    },
    {
      "text": "that will be much better so our sentence",
      "start": 1996.639,
      "duration": 4.841
    },
    {
      "text": "was",
      "start": 1999.159,
      "duration": 2.321
    },
    {
      "text": "the the cat",
      "start": 2001.919,
      "duration": 4.64
    },
    {
      "text": "the",
      "start": 2008.919,
      "duration": 5.76
    },
    {
      "text": "cat and here it was sleeps",
      "start": 2010.24,
      "duration": 4.439
    },
    {
      "text": "right the",
      "start": 2015.039,
      "duration": 3.321
    },
    {
      "text": "cat let me just write it over here yeah",
      "start": 2019.799,
      "duration": 5.84
    },
    {
      "text": "the cat sleeps and the same words I'm",
      "start": 2023.0,
      "duration": 4.559
    },
    {
      "text": "also going to write over here so the",
      "start": 2025.639,
      "duration": 3.4
    },
    {
      "text": "first row",
      "start": 2027.559,
      "duration": 5.401
    },
    {
      "text": "is let me write it over here actually",
      "start": 2029.039,
      "duration": 7.321
    },
    {
      "text": "the first row is",
      "start": 2032.96,
      "duration": 3.4
    },
    {
      "text": "the the second row is",
      "start": 2036.44,
      "duration": 5.079
    },
    {
      "text": "cat and the third row is",
      "start": 2042.96,
      "duration": 4.0
    },
    {
      "text": "sleeps so that's why the final two",
      "start": 2048.399,
      "duration": 4.641
    },
    {
      "text": "dimensions are number of tokens comma",
      "start": 2051.2,
      "duration": 4.08
    },
    {
      "text": "number of tokens because if you look at",
      "start": 2053.04,
      "duration": 4.559
    },
    {
      "text": "the second row now if you look at the",
      "start": 2055.28,
      "duration": 4.639
    },
    {
      "text": "second row now the first element of the",
      "start": 2057.599,
      "duration": 5.08
    },
    {
      "text": "second row tells us information about",
      "start": 2059.919,
      "duration": 5.601
    },
    {
      "text": "the attention between cat and",
      "start": 2062.679,
      "duration": 5.4
    },
    {
      "text": "the the second element of the the second",
      "start": 2065.52,
      "duration": 5.44
    },
    {
      "text": "row tells us the information between cat",
      "start": 2068.079,
      "duration": 5.121
    },
    {
      "text": "and cat so if the query is cat how much",
      "start": 2070.96,
      "duration": 4.24
    },
    {
      "text": "attention should you pay to cat the",
      "start": 2073.2,
      "duration": 3.719
    },
    {
      "text": "third element here tells us the",
      "start": 2075.2,
      "duration": 3.8
    },
    {
      "text": "information between cat and sleep which",
      "start": 2076.919,
      "duration": 4.361
    },
    {
      "text": "means if the qu if the query is cat how",
      "start": 2079.0,
      "duration": 4.399
    },
    {
      "text": "much attention should you pay to sleep",
      "start": 2081.28,
      "duration": 4.319
    },
    {
      "text": "so that's why the shape of the attention",
      "start": 2083.399,
      "duration": 4.401
    },
    {
      "text": "Matrix for every head is number of token",
      "start": 2085.599,
      "duration": 4.401
    },
    {
      "text": "rows and the number of token columns",
      "start": 2087.8,
      "duration": 3.96
    },
    {
      "text": "because an attention score exists",
      "start": 2090.0,
      "duration": 4.2
    },
    {
      "text": "between each token for every other token",
      "start": 2091.76,
      "duration": 4.119
    },
    {
      "text": "so whenever you see these Dimensions",
      "start": 2094.2,
      "duration": 3.56
    },
    {
      "text": "right don't get confused by it try to",
      "start": 2095.879,
      "duration": 4.001
    },
    {
      "text": "always understand the meaning behind it",
      "start": 2097.76,
      "duration": 3.72
    },
    {
      "text": "that's why we had so many lectures on",
      "start": 2099.88,
      "duration": 3.479
    },
    {
      "text": "the attention mechanism before just so",
      "start": 2101.48,
      "duration": 3.8
    },
    {
      "text": "that when we reach this stage",
      "start": 2103.359,
      "duration": 4.081
    },
    {
      "text": "understanding all of this becomes easy",
      "start": 2105.28,
      "duration": 3.88
    },
    {
      "text": "so remember until this stage we have",
      "start": 2107.44,
      "duration": 3.52
    },
    {
      "text": "computed the attention score so this is",
      "start": 2109.16,
      "duration": 4.0
    },
    {
      "text": "exactly what is done here remember what",
      "start": 2110.96,
      "duration": 5.08
    },
    {
      "text": "we saw on the Whiteboard to compute the",
      "start": 2113.16,
      "duration": 5.52
    },
    {
      "text": "attention scores we'll take the queries",
      "start": 2116.04,
      "duration": 4.88
    },
    {
      "text": "and we'll multiply with the keys.",
      "start": 2118.68,
      "duration": 5.6
    },
    {
      "text": "transpose 2 comma 3 because uh in",
      "start": 2120.92,
      "duration": 5.439
    },
    {
      "text": "transposing 2 comma 3 we'll make sure",
      "start": 2124.28,
      "duration": 5.36
    },
    {
      "text": "that the correct queries and the attent",
      "start": 2126.359,
      "duration": 6.601
    },
    {
      "text": "and the KE transpose product is taken to",
      "start": 2129.64,
      "duration": 5.64
    },
    {
      "text": "calculate the attention",
      "start": 2132.96,
      "duration": 5.08
    },
    {
      "text": "scores and uh this is also implemented",
      "start": 2135.28,
      "duration": 4.68
    },
    {
      "text": "in the code so if you see in the code",
      "start": 2138.04,
      "duration": 3.88
    },
    {
      "text": "the attention score is the product is",
      "start": 2139.96,
      "duration": 4.159
    },
    {
      "text": "the scaled product between queries and",
      "start": 2141.92,
      "duration": 3.28
    },
    {
      "text": "the",
      "start": 2144.119,
      "duration": 4.48
    },
    {
      "text": "keys great so here it shown dot product",
      "start": 2145.2,
      "duration": 5.24
    },
    {
      "text": "for each head now you'll understand why",
      "start": 2148.599,
      "duration": 3.48
    },
    {
      "text": "I'm saying each head because as I showed",
      "start": 2150.44,
      "duration": 3.679
    },
    {
      "text": "you before each head has number of",
      "start": 2152.079,
      "duration": 3.921
    },
    {
      "text": "tokens comma number of tokens attention",
      "start": 2154.119,
      "duration": 4.681
    },
    {
      "text": "scores and for for one head it's here",
      "start": 2156.0,
      "duration": 5.76
    },
    {
      "text": "and for the second head it's below okay",
      "start": 2158.8,
      "duration": 4.76
    },
    {
      "text": "now we come to the next step the next",
      "start": 2161.76,
      "duration": 3.319
    },
    {
      "text": "step is to essentially find the",
      "start": 2163.56,
      "duration": 5.92
    },
    {
      "text": "attention attention weights okay so uh",
      "start": 2165.079,
      "duration": 6.121
    },
    {
      "text": "if you look at this attention score over",
      "start": 2169.48,
      "duration": 3.44
    },
    {
      "text": "here right now you'll see that for every",
      "start": 2171.2,
      "duration": 3.879
    },
    {
      "text": "token there is an attention score with",
      "start": 2172.92,
      "duration": 4.52
    },
    {
      "text": "respect to every other token right but",
      "start": 2175.079,
      "duration": 4.161
    },
    {
      "text": "that's not what's the mechanism in",
      "start": 2177.44,
      "duration": 3.679
    },
    {
      "text": "causal attention what causal attention",
      "start": 2179.24,
      "duration": 4.4
    },
    {
      "text": "says is that when you look at the you",
      "start": 2181.119,
      "duration": 4.801
    },
    {
      "text": "should only look at the attention score",
      "start": 2183.64,
      "duration": 4.36
    },
    {
      "text": "between the and what comes before it so",
      "start": 2185.92,
      "duration": 5.12
    },
    {
      "text": "the and the all the other elements here",
      "start": 2188.0,
      "duration": 5.079
    },
    {
      "text": "so let me show them with a different let",
      "start": 2191.04,
      "duration": 6.4
    },
    {
      "text": "me first rub this uh so",
      "start": 2193.079,
      "duration": 4.361
    },
    {
      "text": "that yeah so what causal attention",
      "start": 2199.079,
      "duration": 5.361
    },
    {
      "text": "mechanism dictates is that when you look",
      "start": 2202.68,
      "duration": 4.919
    },
    {
      "text": "at the first word which is the only the",
      "start": 2204.44,
      "duration": 5.52
    },
    {
      "text": "attention score between the and what",
      "start": 2207.599,
      "duration": 4.52
    },
    {
      "text": "comes before it should survive so all of",
      "start": 2209.96,
      "duration": 5.08
    },
    {
      "text": "this should go to zero if you look at",
      "start": 2212.119,
      "duration": 5.48
    },
    {
      "text": "cat only the attention score of the",
      "start": 2215.04,
      "duration": 4.36
    },
    {
      "text": "Words which come before so the and Cat",
      "start": 2217.599,
      "duration": 4.081
    },
    {
      "text": "should survive this should go to zero",
      "start": 2219.4,
      "duration": 3.959
    },
    {
      "text": "and when you look at sleeps attention",
      "start": 2221.68,
      "duration": 3.56
    },
    {
      "text": "scores of all Will Survive because all",
      "start": 2223.359,
      "duration": 4.201
    },
    {
      "text": "the words come before it this is what we",
      "start": 2225.24,
      "duration": 4.16
    },
    {
      "text": "are actually going to implement",
      "start": 2227.56,
      "duration": 4.559
    },
    {
      "text": "next so to do that first what we are",
      "start": 2229.4,
      "duration": 4.4
    },
    {
      "text": "going to do is we are going to take the",
      "start": 2232.119,
      "duration": 3.601
    },
    {
      "text": "attention scores which we have and",
      "start": 2233.8,
      "duration": 3.68
    },
    {
      "text": "replace all of the elements above the",
      "start": 2235.72,
      "duration": 3.52
    },
    {
      "text": "diagonal with negative",
      "start": 2237.48,
      "duration": 5.04
    },
    {
      "text": "Infinity the reason we uh replace this",
      "start": 2239.24,
      "duration": 5.16
    },
    {
      "text": "with negative Infinity is because after",
      "start": 2242.52,
      "duration": 3.72
    },
    {
      "text": "this point we are going to implement the",
      "start": 2244.4,
      "duration": 4.4
    },
    {
      "text": "soft Max function so that each row sums",
      "start": 2246.24,
      "duration": 4.96
    },
    {
      "text": "up to one and when we Implement soft Max",
      "start": 2248.8,
      "duration": 3.92
    },
    {
      "text": "whatever is there in the infinity will",
      "start": 2251.2,
      "duration": 4.08
    },
    {
      "text": "automatically go to zero so it will kill",
      "start": 2252.72,
      "duration": 4.52
    },
    {
      "text": "it will kill two birds in the same Stone",
      "start": 2255.28,
      "duration": 3.559
    },
    {
      "text": "we will implement the causal attention",
      "start": 2257.24,
      "duration": 4.119
    },
    {
      "text": "mechanism and we'll also make sure that",
      "start": 2258.839,
      "duration": 5.321
    },
    {
      "text": "all the rows sum up to one but before we",
      "start": 2261.359,
      "duration": 5.041
    },
    {
      "text": "Implement soft Max we do one more thing",
      "start": 2264.16,
      "duration": 4.32
    },
    {
      "text": "we divide every single element here with",
      "start": 2266.4,
      "duration": 4.84
    },
    {
      "text": "the square root of the head Dimension",
      "start": 2268.48,
      "duration": 4.92
    },
    {
      "text": "and this when we looked at the lecture",
      "start": 2271.24,
      "duration": 5.0
    },
    {
      "text": "for uh self attention we saw why this is",
      "start": 2273.4,
      "duration": 6.08
    },
    {
      "text": "done this is essenti to make sure that",
      "start": 2276.24,
      "duration": 6.2
    },
    {
      "text": "the variance between the when we take",
      "start": 2279.48,
      "duration": 4.72
    },
    {
      "text": "the dot product between the queries and",
      "start": 2282.44,
      "duration": 4.04
    },
    {
      "text": "the keys the variance scales up with the",
      "start": 2284.2,
      "duration": 4.48
    },
    {
      "text": "number of dimensions and to prevent the",
      "start": 2286.48,
      "duration": 4.359
    },
    {
      "text": "variance from blowing up we have to",
      "start": 2288.68,
      "duration": 3.88
    },
    {
      "text": "divide by the square root of the head",
      "start": 2290.839,
      "duration": 4.841
    },
    {
      "text": "Dimension this also makes sure that the",
      "start": 2292.56,
      "duration": 5.2
    },
    {
      "text": "values in",
      "start": 2295.68,
      "duration": 4.76
    },
    {
      "text": "the values before we compute the soft",
      "start": 2297.76,
      "duration": 4.8
    },
    {
      "text": "Max are not very high and that's",
      "start": 2300.44,
      "duration": 4.52
    },
    {
      "text": "generally useful for back propagation",
      "start": 2302.56,
      "duration": 5.08
    },
    {
      "text": "and leads to stable gradients so what",
      "start": 2304.96,
      "duration": 4.119
    },
    {
      "text": "we'll be doing is that the head",
      "start": 2307.64,
      "duration": 3.719
    },
    {
      "text": "Dimension as we saw is three right",
      "start": 2309.079,
      "duration": 4.721
    },
    {
      "text": "because the D out is equal to 6 and the",
      "start": 2311.359,
      "duration": 4.681
    },
    {
      "text": "number of heads is equal to 2 so each",
      "start": 2313.8,
      "duration": 4.24
    },
    {
      "text": "head Dimension is equal to three so",
      "start": 2316.04,
      "duration": 5.92
    },
    {
      "text": "we'll divide this after replacing the",
      "start": 2318.04,
      "duration": 5.559
    },
    {
      "text": "elements above the diagonal with",
      "start": 2321.96,
      "duration": 4.04
    },
    {
      "text": "negative Infinity we'll divide this with",
      "start": 2323.599,
      "duration": 4.401
    },
    {
      "text": "square root of 3 which is square root of",
      "start": 2326.0,
      "duration": 4.319
    },
    {
      "text": "head Dimension and that leads to this",
      "start": 2328.0,
      "duration": 4.56
    },
    {
      "text": "Matrix over here or this tensor I should",
      "start": 2330.319,
      "duration": 5.081
    },
    {
      "text": "say and then we apply soft Max to this",
      "start": 2332.56,
      "duration": 5.759
    },
    {
      "text": "tensor so we make sure that every row",
      "start": 2335.4,
      "duration": 5.48
    },
    {
      "text": "here sums up to essentially so if you",
      "start": 2338.319,
      "duration": 4.881
    },
    {
      "text": "look at each row in this you'll see that",
      "start": 2340.88,
      "duration": 4.84
    },
    {
      "text": "it it's summing up to one and the reason",
      "start": 2343.2,
      "duration": 3.919
    },
    {
      "text": "it sums up to one is we are applying",
      "start": 2345.72,
      "duration": 3.48
    },
    {
      "text": "soft Max so now I can make claims",
      "start": 2347.119,
      "duration": 4.441
    },
    {
      "text": "interpretable claims so when I say that",
      "start": 2349.2,
      "duration": 4.68
    },
    {
      "text": "when I look at the second token cat I",
      "start": 2351.56,
      "duration": 4.96
    },
    {
      "text": "should pay 96% attention to the and I",
      "start": 2353.88,
      "duration": 5.68
    },
    {
      "text": "should pay 4% attention to cat when I",
      "start": 2356.52,
      "duration": 6.0
    },
    {
      "text": "look at sleeps I should pay 4% attention",
      "start": 2359.56,
      "duration": 7.16
    },
    {
      "text": "to the I should pay 26% attention to cat",
      "start": 2362.52,
      "duration": 6.76
    },
    {
      "text": "and I should pay pay 69% attention to",
      "start": 2366.72,
      "duration": 4.879
    },
    {
      "text": "sleeps remember these values are not",
      "start": 2369.28,
      "duration": 4.76
    },
    {
      "text": "optimized but when they are optimized uh",
      "start": 2371.599,
      "duration": 4.281
    },
    {
      "text": "when we look at back propagation",
      "start": 2374.04,
      "duration": 4.92
    },
    {
      "text": "later uh the fact that these values sum",
      "start": 2375.88,
      "duration": 5.439
    },
    {
      "text": "up to one will carry meaning because we",
      "start": 2378.96,
      "duration": 4.08
    },
    {
      "text": "can make interpretable statements such",
      "start": 2381.319,
      "duration": 4.401
    },
    {
      "text": "as what I was making right now remember",
      "start": 2383.04,
      "duration": 4.4
    },
    {
      "text": "that after we are going to apply soft",
      "start": 2385.72,
      "duration": 4.08
    },
    {
      "text": "Max uh the attention weights have",
      "start": 2387.44,
      "duration": 4.2
    },
    {
      "text": "exactly the same dimensions as the",
      "start": 2389.8,
      "duration": 4.279
    },
    {
      "text": "attention scores which is going to be",
      "start": 2391.64,
      "duration": 5.4
    },
    {
      "text": "the batch size number of heads number of",
      "start": 2394.079,
      "duration": 5.28
    },
    {
      "text": "token tokens and number of",
      "start": 2397.04,
      "duration": 5.079
    },
    {
      "text": "tokens so this is the dimension of the",
      "start": 2399.359,
      "duration": 6.921
    },
    {
      "text": "attention weights which is 1A 2A 3A",
      "start": 2402.119,
      "duration": 7.801
    },
    {
      "text": "3 uh so if you look closely to go from",
      "start": 2406.28,
      "duration": 5.559
    },
    {
      "text": "attention scores to attention weights we",
      "start": 2409.92,
      "duration": 4.679
    },
    {
      "text": "actually M we actually have very we have",
      "start": 2411.839,
      "duration": 5.721
    },
    {
      "text": "a rich number of steps and it's",
      "start": 2414.599,
      "duration": 4.681
    },
    {
      "text": "important for you to understand all of",
      "start": 2417.56,
      "duration": 4.0
    },
    {
      "text": "these first what we did is we applied a",
      "start": 2419.28,
      "duration": 4.039
    },
    {
      "text": "mask so that all elements above the",
      "start": 2421.56,
      "duration": 4.2
    },
    {
      "text": "diagonal are negative Infinity then we",
      "start": 2423.319,
      "duration": 4.321
    },
    {
      "text": "divided by the square root of the the",
      "start": 2425.76,
      "duration": 4.48
    },
    {
      "text": "head Dimension then we applied soft Max",
      "start": 2427.64,
      "duration": 4.84
    },
    {
      "text": "this is how we got the attention weights",
      "start": 2430.24,
      "duration": 4.04
    },
    {
      "text": "now let's see how that is done in the",
      "start": 2432.48,
      "duration": 4.639
    },
    {
      "text": "code H before that one thing usually we",
      "start": 2434.28,
      "duration": 5.24
    },
    {
      "text": "can also Implement Dropout after this so",
      "start": 2437.119,
      "duration": 4.641
    },
    {
      "text": "you can mention a dropout rate which is",
      "start": 2439.52,
      "duration": 4.96
    },
    {
      "text": "actually one of the arguments in the",
      "start": 2441.76,
      "duration": 5.52
    },
    {
      "text": "multi-ad attention class but here I'm",
      "start": 2444.48,
      "duration": 4.879
    },
    {
      "text": "not implementing Dropout for the sake of",
      "start": 2447.28,
      "duration": 4.24
    },
    {
      "text": "simplicity so if you look at the code",
      "start": 2449.359,
      "duration": 4.281
    },
    {
      "text": "here we have got the attention scores",
      "start": 2451.52,
      "duration": 4.4
    },
    {
      "text": "the first step as I said is to create",
      "start": 2453.64,
      "duration": 4.8
    },
    {
      "text": "this mask and and then apply this mask",
      "start": 2455.92,
      "duration": 4.28
    },
    {
      "text": "to the attention score so that all the",
      "start": 2458.44,
      "duration": 4.0
    },
    {
      "text": "elements above the diagonal are negative",
      "start": 2460.2,
      "duration": 5.0
    },
    {
      "text": "Infinity that's what this step is doing",
      "start": 2462.44,
      "duration": 6.44
    },
    {
      "text": "uh here the mask actually has also been",
      "start": 2465.2,
      "duration": 5.48
    },
    {
      "text": "defined over here see this is the upper",
      "start": 2468.88,
      "duration": 3.88
    },
    {
      "text": "triangular mask which is all the",
      "start": 2470.68,
      "duration": 4.2
    },
    {
      "text": "elements about the diagonal one then",
      "start": 2472.76,
      "duration": 3.88
    },
    {
      "text": "they are replaced with negative infinity",
      "start": 2474.88,
      "duration": 3.439
    },
    {
      "text": "and that's applied to the attention",
      "start": 2476.64,
      "duration": 3.64
    },
    {
      "text": "scores so this will ensure that all the",
      "start": 2478.319,
      "duration": 3.641
    },
    {
      "text": "elements above the diagonal of the",
      "start": 2480.28,
      "duration": 3.799
    },
    {
      "text": "attention scores are equal to negative",
      "start": 2481.96,
      "duration": 4.92
    },
    {
      "text": "infinity and we are only considering",
      "start": 2484.079,
      "duration": 5.601
    },
    {
      "text": "context length here why context length",
      "start": 2486.88,
      "duration": 4.959
    },
    {
      "text": "because let's say context length is",
      "start": 2489.68,
      "duration": 4.8
    },
    {
      "text": "three it means that maximum if three",
      "start": 2491.839,
      "duration": 4.641
    },
    {
      "text": "words are given we can make prediction",
      "start": 2494.48,
      "duration": 5.08
    },
    {
      "text": "of the next word so when we implement",
      "start": 2496.48,
      "duration": 4.96
    },
    {
      "text": "this Dropout mask we only Implement a",
      "start": 2499.56,
      "duration": 3.72
    },
    {
      "text": "mask of context length comma context",
      "start": 2501.44,
      "duration": 4.0
    },
    {
      "text": "length there is no point in implementing",
      "start": 2503.28,
      "duration": 4.12
    },
    {
      "text": "a bigger mask because anyway we are not",
      "start": 2505.44,
      "duration": 4.56
    },
    {
      "text": "going to look at more tokens than the",
      "start": 2507.4,
      "duration": 5.56
    },
    {
      "text": "context length at a time and if it",
      "start": 2510.0,
      "duration": 4.599
    },
    {
      "text": "happens that we are looking at a batch",
      "start": 2512.96,
      "duration": 3.32
    },
    {
      "text": "where the number of tokens are less than",
      "start": 2514.599,
      "duration": 3.881
    },
    {
      "text": "the context size this statement makes",
      "start": 2516.28,
      "duration": 4.839
    },
    {
      "text": "sure that then the mask stops at number",
      "start": 2518.48,
      "duration": 6.0
    },
    {
      "text": "of tokens but this is a detail which",
      "start": 2521.119,
      "duration": 5.681
    },
    {
      "text": "probably uh you can Overlook right now",
      "start": 2524.48,
      "duration": 3.96
    },
    {
      "text": "if you're understanding all the other",
      "start": 2526.8,
      "duration": 4.12
    },
    {
      "text": "things that's what the most important if",
      "start": 2528.44,
      "duration": 4.76
    },
    {
      "text": "you understand this Minor Detail it's",
      "start": 2530.92,
      "duration": 4.919
    },
    {
      "text": "awesome then the next step is to apply",
      "start": 2533.2,
      "duration": 4.48
    },
    {
      "text": "soft Max but as I told you before",
      "start": 2535.839,
      "duration": 3.961
    },
    {
      "text": "applying soft Max we defi we divide",
      "start": 2537.68,
      "duration": 3.76
    },
    {
      "text": "every element with the square root of",
      "start": 2539.8,
      "duration": 3.64
    },
    {
      "text": "the head Dimension if you look at the",
      "start": 2541.44,
      "duration": 5.919
    },
    {
      "text": "keys. shape let's look at keys. shape uh",
      "start": 2543.44,
      "duration": 6.24
    },
    {
      "text": "this is going to be the keys do shape so",
      "start": 2547.359,
      "duration": 5.281
    },
    {
      "text": "keys do shape of minus one which means",
      "start": 2549.68,
      "duration": 4.56
    },
    {
      "text": "that we are going to look at the last",
      "start": 2552.64,
      "duration": 3.719
    },
    {
      "text": "Dimension which is the head Dimension so",
      "start": 2554.24,
      "duration": 3.72
    },
    {
      "text": "we are essentially dividing by square",
      "start": 2556.359,
      "duration": 4.801
    },
    {
      "text": "root of head Dimension here and then we",
      "start": 2557.96,
      "duration": 5.8
    },
    {
      "text": "apply the soft Max y Dimension equal to",
      "start": 2561.16,
      "duration": 4.52
    },
    {
      "text": "minus one because we need to make sure",
      "start": 2563.76,
      "duration": 4.52
    },
    {
      "text": "that all The Columns of a row sum up to",
      "start": 2565.68,
      "duration": 5.0
    },
    {
      "text": "one and then as I said we can even",
      "start": 2568.28,
      "duration": 4.96
    },
    {
      "text": "Implement Dropout if needed towards the",
      "start": 2570.68,
      "duration": 4.8
    },
    {
      "text": "end so up till now we have reached a",
      "start": 2573.24,
      "duration": 4.96
    },
    {
      "text": "stage where we have obtained the",
      "start": 2575.48,
      "duration": 4.8
    },
    {
      "text": "attention weights is basically and I",
      "start": 2578.2,
      "duration": 4.6
    },
    {
      "text": "hope you understand the meaning behind",
      "start": 2580.28,
      "duration": 5.279
    },
    {
      "text": "this final attention weight matrix it's",
      "start": 2582.8,
      "duration": 4.559
    },
    {
      "text": "not just important to understand how the",
      "start": 2585.559,
      "duration": 4.56
    },
    {
      "text": "dimensions work so to make sure you",
      "start": 2587.359,
      "duration": 4.081
    },
    {
      "text": "understand the meaning let me go through",
      "start": 2590.119,
      "duration": 2.801
    },
    {
      "text": "the meaning of this attention weight",
      "start": 2591.44,
      "duration": 5.04
    },
    {
      "text": "Matrix once more um this what I'm",
      "start": 2592.92,
      "duration": 5.199
    },
    {
      "text": "highlighting right now is the attention",
      "start": 2596.48,
      "duration": 3.8
    },
    {
      "text": "weights for the first head this second",
      "start": 2598.119,
      "duration": 3.72
    },
    {
      "text": "block is the attention weights for the",
      "start": 2600.28,
      "duration": 5.319
    },
    {
      "text": "second head in each attention uh head",
      "start": 2601.839,
      "duration": 6.28
    },
    {
      "text": "block you will see that the size is",
      "start": 2605.599,
      "duration": 4.52
    },
    {
      "text": "number of tokens rows and the number of",
      "start": 2608.119,
      "duration": 3.561
    },
    {
      "text": "tokens",
      "start": 2610.119,
      "duration": 4.96
    },
    {
      "text": "columns so each value is essentially the",
      "start": 2611.68,
      "duration": 5.72
    },
    {
      "text": "attention weight between let's say this",
      "start": 2615.079,
      "duration": 3.681
    },
    {
      "text": "is the attention weight between this is",
      "start": 2617.4,
      "duration": 3.52
    },
    {
      "text": "the attention weight between the second",
      "start": 2618.76,
      "duration": 4.2
    },
    {
      "text": "row which is the second token and the",
      "start": 2620.92,
      "duration": 4.0
    },
    {
      "text": "second token this is the attention",
      "start": 2622.96,
      "duration": 2.92
    },
    {
      "text": "weight",
      "start": 2624.92,
      "duration": 4.439
    },
    {
      "text": "between uh the third token as the query",
      "start": 2625.88,
      "duration": 6.6
    },
    {
      "text": "and the first token as the key so",
      "start": 2629.359,
      "duration": 6.121
    },
    {
      "text": "basically you'll see that every single",
      "start": 2632.48,
      "duration": 5.079
    },
    {
      "text": "element here has some meaning it",
      "start": 2635.48,
      "duration": 4.0
    },
    {
      "text": "essentially encodes the attention weight",
      "start": 2637.559,
      "duration": 5.52
    },
    {
      "text": "between the query and the particular key",
      "start": 2639.48,
      "duration": 6.48
    },
    {
      "text": "okay now let's go ahead the last step",
      "start": 2643.079,
      "duration": 5.201
    },
    {
      "text": "which we are going to implement is that",
      "start": 2645.96,
      "duration": 4.68
    },
    {
      "text": "we have to calculate the context Vector",
      "start": 2648.28,
      "duration": 4.279
    },
    {
      "text": "remember the aim of all the attention",
      "start": 2650.64,
      "duration": 3.919
    },
    {
      "text": "mechanisms is to ultimately compute the",
      "start": 2652.559,
      "duration": 3.681
    },
    {
      "text": "context Vector Matrix and this is",
      "start": 2654.559,
      "duration": 4.0
    },
    {
      "text": "exactly what we are going to do and to",
      "start": 2656.24,
      "duration": 4.079
    },
    {
      "text": "compute the context Vector Matrix we",
      "start": 2658.559,
      "duration": 3.641
    },
    {
      "text": "take the attention weights and we",
      "start": 2660.319,
      "duration": 4.24
    },
    {
      "text": "multiply them with",
      "start": 2662.2,
      "duration": 5.08
    },
    {
      "text": "values remember the value Matrix was The",
      "start": 2664.559,
      "duration": 5.161
    },
    {
      "text": "Matrix which we had computed earlier let",
      "start": 2667.28,
      "duration": 4.4
    },
    {
      "text": "me show you where the value Matrix was",
      "start": 2669.72,
      "duration": 3.599
    },
    {
      "text": "in case you have forgotten it because we",
      "start": 2671.68,
      "duration": 4.439
    },
    {
      "text": "have done so many",
      "start": 2673.319,
      "duration": 5.28
    },
    {
      "text": "things uh yeah so here was the value",
      "start": 2676.119,
      "duration": 4.681
    },
    {
      "text": "Matrix which we had computed we have not",
      "start": 2678.599,
      "duration": 4.0
    },
    {
      "text": "used it until now it will only be used",
      "start": 2680.8,
      "duration": 3.36
    },
    {
      "text": "in this last",
      "start": 2682.599,
      "duration": 3.96
    },
    {
      "text": "step okay so the attention weights will",
      "start": 2684.16,
      "duration": 4.48
    },
    {
      "text": "be multiplied by the values Matrix to",
      "start": 2686.559,
      "duration": 4.161
    },
    {
      "text": "get the context Vector Matrix so let's",
      "start": 2688.64,
      "duration": 3.959
    },
    {
      "text": "see how the dimensions work out here",
      "start": 2690.72,
      "duration": 4.68
    },
    {
      "text": "okay the attention weights as we looked",
      "start": 2692.599,
      "duration": 5.081
    },
    {
      "text": "earlier over here",
      "start": 2695.4,
      "duration": 3.679
    },
    {
      "text": "the attention weights have the",
      "start": 2697.68,
      "duration": 6.12
    },
    {
      "text": "dimensions of B comma number of heads",
      "start": 2699.079,
      "duration": 6.961
    },
    {
      "text": "comma number of tokens comma number of",
      "start": 2703.8,
      "duration": 5.08
    },
    {
      "text": "tokens and as we saw earlier the values",
      "start": 2706.04,
      "duration": 5.92
    },
    {
      "text": "Matrix has the dimensions B comma number",
      "start": 2708.88,
      "duration": 3.959
    },
    {
      "text": "of",
      "start": 2711.96,
      "duration": 4.0
    },
    {
      "text": "heads uh comma number of tokens and head",
      "start": 2712.839,
      "duration": 5.321
    },
    {
      "text": "Dimension so effectively let's see",
      "start": 2715.96,
      "duration": 4.48
    },
    {
      "text": "whether these matrices can be multiplied",
      "start": 2718.16,
      "duration": 4.04
    },
    {
      "text": "so this is number of tokens and number",
      "start": 2720.44,
      "duration": 4.159
    },
    {
      "text": "of tokens and that will be multiplied by",
      "start": 2722.2,
      "duration": 5.52
    },
    {
      "text": "number of tokens and head dim so the",
      "start": 2724.599,
      "duration": 5.281
    },
    {
      "text": "number of columns here is number of",
      "start": 2727.72,
      "duration": 4.76
    },
    {
      "text": "tokens and the number of rows here is",
      "start": 2729.88,
      "duration": 4.719
    },
    {
      "text": "number of tokens so the number of",
      "start": 2732.48,
      "duration": 4.879
    },
    {
      "text": "columns in the first Matrix are matching",
      "start": 2734.599,
      "duration": 5.361
    },
    {
      "text": "the number of rows in the values Matrix",
      "start": 2737.359,
      "duration": 5.0
    },
    {
      "text": "so we can see that these two matrices",
      "start": 2739.96,
      "duration": 4.8
    },
    {
      "text": "can essentially be multiplied so",
      "start": 2742.359,
      "duration": 4.441
    },
    {
      "text": "multiplication is possible and now let",
      "start": 2744.76,
      "duration": 3.599
    },
    {
      "text": "us see how the multiplication will",
      "start": 2746.8,
      "duration": 3.44
    },
    {
      "text": "actually work in practice this is the",
      "start": 2748.359,
      "duration": 4.921
    },
    {
      "text": "final attention weights Matrix here it's",
      "start": 2750.24,
      "duration": 5.0
    },
    {
      "text": "mentioned attention scores but I should",
      "start": 2753.28,
      "duration": 3.76
    },
    {
      "text": "have called it attention weights",
      "start": 2755.24,
      "duration": 3.16
    },
    {
      "text": "remember there is a difference between",
      "start": 2757.04,
      "duration": 4.039
    },
    {
      "text": "scores and weights attention weights in",
      "start": 2758.4,
      "duration": 4.28
    },
    {
      "text": "in the attention weights each row sums",
      "start": 2761.079,
      "duration": 3.441
    },
    {
      "text": "up to one that's not the case with",
      "start": 2762.68,
      "duration": 5.24
    },
    {
      "text": "attention scores okay so this is the",
      "start": 2764.52,
      "duration": 6.799
    },
    {
      "text": "attention weights and this is my values",
      "start": 2767.92,
      "duration": 7.439
    },
    {
      "text": "this my values Matrix so this 1A 2A 3A 3",
      "start": 2771.319,
      "duration": 8.121
    },
    {
      "text": "and this 1A 2A 3A 3 and when we multiply",
      "start": 2775.359,
      "duration": 7.0
    },
    {
      "text": "the resultant output will be B comma",
      "start": 2779.44,
      "duration": 5.52
    },
    {
      "text": "number of heads comma number of",
      "start": 2782.359,
      "duration": 5.801
    },
    {
      "text": "tokens uh comma the head",
      "start": 2784.96,
      "duration": 6.32
    },
    {
      "text": "Dimension so here you can see that the",
      "start": 2788.16,
      "duration": 6.12
    },
    {
      "text": "context Vector output is B comma number",
      "start": 2791.28,
      "duration": 5.36
    },
    {
      "text": "of heads comma number of tokens comma",
      "start": 2794.28,
      "duration": 4.92
    },
    {
      "text": "head Dimension which is 1 comma 2 comma",
      "start": 2796.64,
      "duration": 5.719
    },
    {
      "text": "3 comma 3 Let's interpret this again uh",
      "start": 2799.2,
      "duration": 4.72
    },
    {
      "text": "so here you can see that there are two",
      "start": 2802.359,
      "duration": 3.161
    },
    {
      "text": "heads so this is head number one and",
      "start": 2803.92,
      "duration": 4.679
    },
    {
      "text": "this is head number two and in each head",
      "start": 2805.52,
      "duration": 4.92
    },
    {
      "text": "there are number of tokens so if you",
      "start": 2808.599,
      "duration": 4.081
    },
    {
      "text": "look at each head there are three rows",
      "start": 2810.44,
      "duration": 4.8
    },
    {
      "text": "so each row corresponds to one token but",
      "start": 2812.68,
      "duration": 5.0
    },
    {
      "text": "now if you look at what what each row",
      "start": 2815.24,
      "duration": 4.24
    },
    {
      "text": "represents each row represents the",
      "start": 2817.68,
      "duration": 5.28
    },
    {
      "text": "context Vector for that particular token",
      "start": 2819.48,
      "duration": 5.839
    },
    {
      "text": "and it has the dimensions equal to head",
      "start": 2822.96,
      "duration": 5.76
    },
    {
      "text": "dim because head dim is equal to three",
      "start": 2825.319,
      "duration": 6.52
    },
    {
      "text": "so that's the meaning of this uh context",
      "start": 2828.72,
      "duration": 5.48
    },
    {
      "text": "Vector Matrix which we have reached but",
      "start": 2831.839,
      "duration": 4.081
    },
    {
      "text": "now remember there is a problem here",
      "start": 2834.2,
      "duration": 5.639
    },
    {
      "text": "right or not a problem uh but we have to",
      "start": 2835.92,
      "duration": 6.28
    },
    {
      "text": "somehow merge this number of heads and",
      "start": 2839.839,
      "duration": 5.28
    },
    {
      "text": "head Dimension back together because the",
      "start": 2842.2,
      "duration": 5.159
    },
    {
      "text": "resultant context Vector Matrix remember",
      "start": 2845.119,
      "duration": 5.48
    },
    {
      "text": "what we saw earlier the let me scroll up",
      "start": 2847.359,
      "duration": 6.921
    },
    {
      "text": "a bit so if you if you if you looked at",
      "start": 2850.599,
      "duration": 5.441
    },
    {
      "text": "the goal which we had when we started",
      "start": 2854.28,
      "duration": 4.6
    },
    {
      "text": "this lecture the goal was that the",
      "start": 2856.04,
      "duration": 6.0
    },
    {
      "text": "resultant context Vector Matrix should",
      "start": 2858.88,
      "duration": 6.32
    },
    {
      "text": "have the dimensions",
      "start": 2862.04,
      "duration": 3.16
    },
    {
      "text": "of uh yeah as I mentioned to you the",
      "start": 2865.44,
      "duration": 5.639
    },
    {
      "text": "goal was that the resultant context meor",
      "start": 2868.96,
      "duration": 5.159
    },
    {
      "text": "Matrix should have D out right as the",
      "start": 2871.079,
      "duration": 6.28
    },
    {
      "text": "dimension so we should again",
      "start": 2874.119,
      "duration": 5.041
    },
    {
      "text": "pull back the head Dimension and the",
      "start": 2877.359,
      "duration": 4.2
    },
    {
      "text": "number of heads together so that we can",
      "start": 2879.16,
      "duration": 4.439
    },
    {
      "text": "get the resultant Matrix which has the D",
      "start": 2881.559,
      "duration": 3.76
    },
    {
      "text": "out Dimension",
      "start": 2883.599,
      "duration": 4.281
    },
    {
      "text": "preserved uh whereas let's see what we",
      "start": 2885.319,
      "duration": 5.081
    },
    {
      "text": "have obtained until",
      "start": 2887.88,
      "duration": 5.76
    },
    {
      "text": "now well until now the context Vector",
      "start": 2890.4,
      "duration": 6.199
    },
    {
      "text": "Matrix which you have",
      "start": 2893.64,
      "duration": 2.959
    },
    {
      "text": "obtained yeah the context Vector Matrix",
      "start": 2900.04,
      "duration": 4.48
    },
    {
      "text": "which we have obtained has number of",
      "start": 2902.68,
      "duration": 3.52
    },
    {
      "text": "heads and head dimensions in separate",
      "start": 2904.52,
      "duration": 4.0
    },
    {
      "text": "positions so first what we'll do is that",
      "start": 2906.2,
      "duration": 4.24
    },
    {
      "text": "we'll bring them closer together so that",
      "start": 2908.52,
      "duration": 4.92
    },
    {
      "text": "we can then merge them to get the D out",
      "start": 2910.44,
      "duration": 4.84
    },
    {
      "text": "so what we are going to do is now we are",
      "start": 2913.44,
      "duration": 5.44
    },
    {
      "text": "going to swap this this number of tokens",
      "start": 2915.28,
      "duration": 5.64
    },
    {
      "text": "index with the number of heads",
      "start": 2918.88,
      "duration": 4.64
    },
    {
      "text": "index so that the dimension of the",
      "start": 2920.92,
      "duration": 5.32
    },
    {
      "text": "context Vector Matrix is so that the",
      "start": 2923.52,
      "duration": 4.68
    },
    {
      "text": "shape of the context Vector Matrix is",
      "start": 2926.24,
      "duration": 5.24
    },
    {
      "text": "changed so the next step is basically",
      "start": 2928.2,
      "duration": 5.84
    },
    {
      "text": "step number 10 and that is to reformat",
      "start": 2931.48,
      "duration": 5.599
    },
    {
      "text": "the context vectors So currently the",
      "start": 2934.04,
      "duration": 5.12
    },
    {
      "text": "context Vector shape is B comma number",
      "start": 2937.079,
      "duration": 3.921
    },
    {
      "text": "of heads comma number of tokens comma",
      "start": 2939.16,
      "duration": 3.76
    },
    {
      "text": "head Dimension right and I want the",
      "start": 2941.0,
      "duration": 4.04
    },
    {
      "text": "number of heads to come here so that",
      "start": 2942.92,
      "duration": 4.08
    },
    {
      "text": "they're closer to the Head dim and I",
      "start": 2945.04,
      "duration": 3.84
    },
    {
      "text": "want the number of Tok number of tokens",
      "start": 2947.0,
      "duration": 4.839
    },
    {
      "text": "to go here so I want the resultant",
      "start": 2948.88,
      "duration": 6.679
    },
    {
      "text": "Matrix to be B comma number of tokens",
      "start": 2951.839,
      "duration": 8.081
    },
    {
      "text": "comma number of heads comma head",
      "start": 2955.559,
      "duration": 4.361
    },
    {
      "text": "Dimension so essentially what I will do",
      "start": 2960.359,
      "duration": 5.081
    },
    {
      "text": "is after I compute the context Vector",
      "start": 2962.76,
      "duration": 5.16
    },
    {
      "text": "Matrix I'll do a transpose of the first",
      "start": 2965.44,
      "duration": 4.159
    },
    {
      "text": "index and the second",
      "start": 2967.92,
      "duration": 4.159
    },
    {
      "text": "index and so the resulting context",
      "start": 2969.599,
      "duration": 4.361
    },
    {
      "text": "Vector Matrix now which has the",
      "start": 2972.079,
      "duration": 4.081
    },
    {
      "text": "dimensions of B comma number of tokens",
      "start": 2973.96,
      "duration": 3.639
    },
    {
      "text": "comma number of heads comma head",
      "start": 2976.16,
      "duration": 3.959
    },
    {
      "text": "Dimension looks like this so here you",
      "start": 2977.599,
      "duration": 4.2
    },
    {
      "text": "see now the interpretation is different",
      "start": 2980.119,
      "duration": 4.041
    },
    {
      "text": "now this is my first token now the",
      "start": 2981.799,
      "duration": 4.601
    },
    {
      "text": "grouping is with respect to tokens this",
      "start": 2984.16,
      "duration": 4.28
    },
    {
      "text": "is my second token and this is my third",
      "start": 2986.4,
      "duration": 4.36
    },
    {
      "text": "token and in each token there are two",
      "start": 2988.44,
      "duration": 4.08
    },
    {
      "text": "heads so if you look at the first token",
      "start": 2990.76,
      "duration": 3.76
    },
    {
      "text": "there are two heads and if you look at",
      "start": 2992.52,
      "duration": 4.68
    },
    {
      "text": "the first head this is the vector with",
      "start": 2994.52,
      "duration": 4.24
    },
    {
      "text": "respect to the first head the context",
      "start": 2997.2,
      "duration": 4.399
    },
    {
      "text": "Vector context vector and the second row",
      "start": 2998.76,
      "duration": 4.48
    },
    {
      "text": "is the context Vector with respect to",
      "start": 3001.599,
      "duration": 3.76
    },
    {
      "text": "the second head for the first",
      "start": 3003.24,
      "duration": 4.4
    },
    {
      "text": "token now let's see how all of this is",
      "start": 3005.359,
      "duration": 4.24
    },
    {
      "text": "implemented in code actually all of what",
      "start": 3007.64,
      "duration": 3.76
    },
    {
      "text": "we saw right now is just implemented in",
      "start": 3009.599,
      "duration": 4.361
    },
    {
      "text": "one line of code but to understand this",
      "start": 3011.4,
      "duration": 4.159
    },
    {
      "text": "we really have to understand first of",
      "start": 3013.96,
      "duration": 2.92
    },
    {
      "text": "all how the attention weights are",
      "start": 3015.559,
      "duration": 3.121
    },
    {
      "text": "multiplied with values the",
      "start": 3016.88,
      "duration": 3.52
    },
    {
      "text": "multiplication really makes sense and",
      "start": 3018.68,
      "duration": 4.36
    },
    {
      "text": "why do we do this transpose 1A 2 the",
      "start": 3020.4,
      "duration": 4.8
    },
    {
      "text": "reason we do this transpose 1 comma 2 is",
      "start": 3023.04,
      "duration": 4.2
    },
    {
      "text": "to get the context Vector mat in this",
      "start": 3025.2,
      "duration": 4.48
    },
    {
      "text": "shape the reason we get it in this shape",
      "start": 3027.24,
      "duration": 4.04
    },
    {
      "text": "is now you can see the number of heads",
      "start": 3029.68,
      "duration": 4.28
    },
    {
      "text": "and head Dimension are closer together",
      "start": 3031.28,
      "duration": 6.44
    },
    {
      "text": "so we can merge them um into the D out",
      "start": 3033.96,
      "duration": 6.92
    },
    {
      "text": "more easily so here you can see this is",
      "start": 3037.72,
      "duration": 5.119
    },
    {
      "text": "what we have reached until now where the",
      "start": 3040.88,
      "duration": 4.679
    },
    {
      "text": "context Vector is obtained and it's in",
      "start": 3042.839,
      "duration": 4.881
    },
    {
      "text": "the correct format now the last step",
      "start": 3045.559,
      "duration": 4.721
    },
    {
      "text": "what we have to do is that we have",
      "start": 3047.72,
      "duration": 5.119
    },
    {
      "text": "to um let me show",
      "start": 3050.28,
      "duration": 6.36
    },
    {
      "text": "you the last step what we have to do is",
      "start": 3052.839,
      "duration": 5.601
    },
    {
      "text": "essentially we have to combine the",
      "start": 3056.64,
      "duration": 4.0
    },
    {
      "text": "results from multiple heads so see this",
      "start": 3058.44,
      "duration": 4.24
    },
    {
      "text": "is the context Vector Matrix which we",
      "start": 3060.64,
      "duration": 3.8
    },
    {
      "text": "have obtained right now right so if you",
      "start": 3062.68,
      "duration": 3.48
    },
    {
      "text": "look at the first token which I've",
      "start": 3064.44,
      "duration": 3.879
    },
    {
      "text": "highlighted over here this is the first",
      "start": 3066.16,
      "duration": 4.439
    },
    {
      "text": "head and this is the second head now",
      "start": 3068.319,
      "duration": 4.0
    },
    {
      "text": "what I will do is that when I look at",
      "start": 3070.599,
      "duration": 3.881
    },
    {
      "text": "the first token I will combine these two",
      "start": 3072.319,
      "duration": 3.641
    },
    {
      "text": "together into one",
      "start": 3074.48,
      "duration": 5.359
    },
    {
      "text": "row so that it will be uh six the",
      "start": 3075.96,
      "duration": 6.599
    },
    {
      "text": "dimension will be six so so here these",
      "start": 3079.839,
      "duration": 5.361
    },
    {
      "text": "are three and these are three right so",
      "start": 3082.559,
      "duration": 4.321
    },
    {
      "text": "I'll combine the outputs from both of",
      "start": 3085.2,
      "duration": 3.76
    },
    {
      "text": "these heads into one output so let's see",
      "start": 3086.88,
      "duration": 4.8
    },
    {
      "text": "how this looks like so then the first",
      "start": 3088.96,
      "duration": 5.0
    },
    {
      "text": "row will so then we'll flatten this is",
      "start": 3091.68,
      "duration": 3.76
    },
    {
      "text": "called flattening will flatten each",
      "start": 3093.96,
      "duration": 5.52
    },
    {
      "text": "token output into each row so the head",
      "start": 3095.44,
      "duration": 6.28
    },
    {
      "text": "one and head two outputs are combined",
      "start": 3099.48,
      "duration": 4.48
    },
    {
      "text": "together so if you look at the final",
      "start": 3101.72,
      "duration": 4.839
    },
    {
      "text": "output the first row consists of merging",
      "start": 3103.96,
      "duration": 6.44
    },
    {
      "text": "of the two heads for the first token the",
      "start": 3106.559,
      "duration": 5.8
    },
    {
      "text": "second row consist of the merging of the",
      "start": 3110.4,
      "duration": 4.36
    },
    {
      "text": "two heads for the second token so for",
      "start": 3112.359,
      "duration": 4.2
    },
    {
      "text": "the second row we merge these two out",
      "start": 3114.76,
      "duration": 4.16
    },
    {
      "text": "outputs into one single row and for the",
      "start": 3116.559,
      "duration": 4.56
    },
    {
      "text": "third token we merge these two outputs",
      "start": 3118.92,
      "duration": 5.159
    },
    {
      "text": "into a single row so you'll see that the",
      "start": 3121.119,
      "duration": 6.2
    },
    {
      "text": "F the this is the third row so this now",
      "start": 3124.079,
      "duration": 4.961
    },
    {
      "text": "what I what I'm showing on the screen",
      "start": 3127.319,
      "duration": 4.841
    },
    {
      "text": "here is my final context Vector Matrix",
      "start": 3129.04,
      "duration": 5.48
    },
    {
      "text": "and how to interpret this if you look at",
      "start": 3132.16,
      "duration": 3.639
    },
    {
      "text": "the first",
      "start": 3134.52,
      "duration": 5.039
    },
    {
      "text": "row the first row is the context Vector",
      "start": 3135.799,
      "duration": 6.28
    },
    {
      "text": "context Vector for the first token first",
      "start": 3139.559,
      "duration": 4.201
    },
    {
      "text": "row is the context Vector for the first",
      "start": 3142.079,
      "duration": 3.76
    },
    {
      "text": "token why does it have six elements",
      "start": 3143.76,
      "duration": 4.96
    },
    {
      "text": "because d out is equal to 6 the second",
      "start": 3145.839,
      "duration": 4.76
    },
    {
      "text": "row is the context Vector for the second",
      "start": 3148.72,
      "duration": 4.16
    },
    {
      "text": "token and the third row is the context",
      "start": 3150.599,
      "duration": 4.441
    },
    {
      "text": "Vector for the third token so overall",
      "start": 3152.88,
      "duration": 4.36
    },
    {
      "text": "you see we first split the D out into",
      "start": 3155.04,
      "duration": 4.12
    },
    {
      "text": "number of heads and head Dimension and",
      "start": 3157.24,
      "duration": 4.16
    },
    {
      "text": "now we brought it back together to get",
      "start": 3159.16,
      "duration": 4.08
    },
    {
      "text": "the D out so in the final shape you will",
      "start": 3161.4,
      "duration": 3.88
    },
    {
      "text": "not see the number of heads it's all",
      "start": 3163.24,
      "duration": 4.599
    },
    {
      "text": "merged into this D out so this is my",
      "start": 3165.28,
      "duration": 6.12
    },
    {
      "text": "final answer right now and the shape of",
      "start": 3167.839,
      "duration": 6.841
    },
    {
      "text": "this is 1A 3 comma 6 which is B comma",
      "start": 3171.4,
      "duration": 6.0
    },
    {
      "text": "number of tokens comma D out",
      "start": 3174.68,
      "duration": 4.679
    },
    {
      "text": "so this is exactly what is done here",
      "start": 3177.4,
      "duration": 5.08
    },
    {
      "text": "what we do is that we uh we take this",
      "start": 3179.359,
      "duration": 5.72
    },
    {
      "text": "context vector and we reshape it into B",
      "start": 3182.48,
      "duration": 5.119
    },
    {
      "text": "comma tokens comma D out why this",
      "start": 3185.079,
      "duration": 4.921
    },
    {
      "text": "continuous is needed is because we want",
      "start": 3187.599,
      "duration": 3.96
    },
    {
      "text": "to make sure that when we reshape",
      "start": 3190.0,
      "duration": 3.359
    },
    {
      "text": "matrices they are in the same blocks of",
      "start": 3191.559,
      "duration": 4.921
    },
    {
      "text": "memory so when we reshape uh tensors",
      "start": 3193.359,
      "duration": 4.361
    },
    {
      "text": "let's say and if they're in different",
      "start": 3196.48,
      "duration": 3.4
    },
    {
      "text": "memory blocks it becomes difficult so",
      "start": 3197.72,
      "duration": 3.639
    },
    {
      "text": "first we make sure that using this",
      "start": 3199.88,
      "duration": 3.84
    },
    {
      "text": "continuous they in the same memory block",
      "start": 3201.359,
      "duration": 5.2
    },
    {
      "text": "then we reshape them so that the final",
      "start": 3203.72,
      "duration": 4.8
    },
    {
      "text": "output is B which is the batch size",
      "start": 3206.559,
      "duration": 4.161
    },
    {
      "text": "number of tokens which is equal to three",
      "start": 3208.52,
      "duration": 5.279
    },
    {
      "text": "in the example we saw and D out which is",
      "start": 3210.72,
      "duration": 4.72
    },
    {
      "text": "the output Dimension which is equal to",
      "start": 3213.799,
      "duration": 3.721
    },
    {
      "text": "six and then there is an optional",
      "start": 3215.44,
      "duration": 3.96
    },
    {
      "text": "projection layer towards the end so if",
      "start": 3217.52,
      "duration": 3.72
    },
    {
      "text": "you look at the out out output",
      "start": 3219.4,
      "duration": 4.399
    },
    {
      "text": "projection it's again a linear layer and",
      "start": 3221.24,
      "duration": 5.28
    },
    {
      "text": "whose parameters can be learned this is",
      "start": 3223.799,
      "duration": 4.641
    },
    {
      "text": "not really necessary but sometimes it is",
      "start": 3226.52,
      "duration": 4.64
    },
    {
      "text": "implemented in practice now this is",
      "start": 3228.44,
      "duration": 5.119
    },
    {
      "text": "exactly the entire procedure for how the",
      "start": 3231.16,
      "duration": 4.28
    },
    {
      "text": "multi-ad attention is implemented from",
      "start": 3233.559,
      "duration": 4.681
    },
    {
      "text": "scratch and here we saw the multi-head",
      "start": 3235.44,
      "duration": 5.8
    },
    {
      "text": "attention for the example which we have",
      "start": 3238.24,
      "duration": 7.559
    },
    {
      "text": "so the first token is again",
      "start": 3241.24,
      "duration": 9.92
    },
    {
      "text": "the the the second token is",
      "start": 3245.799,
      "duration": 5.361
    },
    {
      "text": "cat and the third token is",
      "start": 3251.68,
      "duration": 7.28
    },
    {
      "text": "sleeps me write this",
      "start": 3255.799,
      "duration": 6.241
    },
    {
      "text": "again yeah the third token is sleeps so",
      "start": 3258.96,
      "duration": 5.0
    },
    {
      "text": "you see through this entire procedure we",
      "start": 3262.04,
      "duration": 3.759
    },
    {
      "text": "obtained the enriched context Vector",
      "start": 3263.96,
      "duration": 4.0
    },
    {
      "text": "representation for these tokens",
      "start": 3265.799,
      "duration": 3.961
    },
    {
      "text": "similarly when you deal with large",
      "start": 3267.96,
      "duration": 4.72
    },
    {
      "text": "volumes of text you take sentences you",
      "start": 3269.76,
      "duration": 4.64
    },
    {
      "text": "break them down into tokens then into",
      "start": 3272.68,
      "duration": 4.2
    },
    {
      "text": "token IDs then into input embeddings and",
      "start": 3274.4,
      "duration": 4.88
    },
    {
      "text": "similar to this procedure you get",
      "start": 3276.88,
      "duration": 5.04
    },
    {
      "text": "context vectors for each token which you",
      "start": 3279.28,
      "duration": 5.44
    },
    {
      "text": "have ideally when we run the actual code",
      "start": 3281.92,
      "duration": 4.679
    },
    {
      "text": "we will have multiple batches but I",
      "start": 3284.72,
      "duration": 4.24
    },
    {
      "text": "showed only one batch right now for",
      "start": 3286.599,
      "duration": 5.921
    },
    {
      "text": "Simplicity so uh to whoever who have",
      "start": 3288.96,
      "duration": 5.399
    },
    {
      "text": "reached until this stage I want to say",
      "start": 3292.52,
      "duration": 3.559
    },
    {
      "text": "that thank you for following with me for",
      "start": 3294.359,
      "duration": 3.361
    },
    {
      "text": "so to many lectures I know these",
      "start": 3296.079,
      "duration": 4.401
    },
    {
      "text": "lectures are becoming very long but",
      "start": 3297.72,
      "duration": 4.76
    },
    {
      "text": "unless I explain every single thing in",
      "start": 3300.48,
      "duration": 4.4
    },
    {
      "text": "detail it's very difficult for you to",
      "start": 3302.48,
      "duration": 4.28
    },
    {
      "text": "understand all the details so",
      "start": 3304.88,
      "duration": 3.8
    },
    {
      "text": "congratulations if you have reached this",
      "start": 3306.76,
      "duration": 4.319
    },
    {
      "text": "lecture you have successfully understood",
      "start": 3308.68,
      "duration": 4.76
    },
    {
      "text": "how the multi-head attention works and I",
      "start": 3311.079,
      "duration": 3.801
    },
    {
      "text": "think there are very few people who",
      "start": 3313.44,
      "duration": 3.399
    },
    {
      "text": "really understand this entire piece of",
      "start": 3314.88,
      "duration": 4.28
    },
    {
      "text": "code block by",
      "start": 3316.839,
      "duration": 5.641
    },
    {
      "text": "block okay so I'll share this notebook",
      "start": 3319.16,
      "duration": 5.36
    },
    {
      "text": "with you and whatever I explain to you",
      "start": 3322.48,
      "duration": 3.839
    },
    {
      "text": "on the Whiteboard all the steps which",
      "start": 3324.52,
      "duration": 4.799
    },
    {
      "text": "which I laid out in front of you on the",
      "start": 3326.319,
      "duration": 5.201
    },
    {
      "text": "Whiteboard uh all of those have been",
      "start": 3329.319,
      "duration": 5.201
    },
    {
      "text": "explained here as step one to Step 11",
      "start": 3331.52,
      "duration": 5.68
    },
    {
      "text": "and uh I have explained added a detailed",
      "start": 3334.52,
      "duration": 4.319
    },
    {
      "text": "explanation of the multi-head attention",
      "start": 3337.2,
      "duration": 3.76
    },
    {
      "text": "class in today's lecture I did not just",
      "start": 3338.839,
      "duration": 3.801
    },
    {
      "text": "want to read this but I wanted to",
      "start": 3340.96,
      "duration": 3.599
    },
    {
      "text": "construct a practical example to show",
      "start": 3342.64,
      "duration": 5.199
    },
    {
      "text": "you how the dimensions actually work and",
      "start": 3344.559,
      "duration": 5.081
    },
    {
      "text": "uh it took me a long time to make this",
      "start": 3347.839,
      "duration": 3.681
    },
    {
      "text": "example but now I think it's worth it",
      "start": 3349.64,
      "duration": 3.56
    },
    {
      "text": "because it really helped me explain it",
      "start": 3351.52,
      "duration": 4.2
    },
    {
      "text": "and I hope you understood it better so",
      "start": 3353.2,
      "duration": 4.119
    },
    {
      "text": "now we can actually test out the",
      "start": 3355.72,
      "duration": 3.72
    },
    {
      "text": "multi-head attention class so here are",
      "start": 3357.319,
      "duration": 5.28
    },
    {
      "text": "my inputs uh as I showed you on the",
      "start": 3359.44,
      "duration": 5.2
    },
    {
      "text": "Whiteboard we have three tokens and we",
      "start": 3362.599,
      "duration": 5.601
    },
    {
      "text": "have six the embedding Dimension is six",
      "start": 3364.64,
      "duration": 5.04
    },
    {
      "text": "the only change here what I'm going to",
      "start": 3368.2,
      "duration": 3.359
    },
    {
      "text": "do I'm going to create a batch so I'm",
      "start": 3369.68,
      "duration": 3.72
    },
    {
      "text": "going to going to create a batch of two",
      "start": 3371.559,
      "duration": 3.56
    },
    {
      "text": "such inputs and I'm going to stack this",
      "start": 3373.4,
      "duration": 4.639
    },
    {
      "text": "batch on top of each other so I'm going",
      "start": 3375.119,
      "duration": 5.161
    },
    {
      "text": "to assume a d out equal to six exactly",
      "start": 3378.039,
      "duration": 4.241
    },
    {
      "text": "what we saw on the Whiteboard and",
      "start": 3380.28,
      "duration": 4.559
    },
    {
      "text": "context length equal to six and then we",
      "start": 3382.28,
      "duration": 4.079
    },
    {
      "text": "are going to implement the multihead",
      "start": 3384.839,
      "duration": 6.161
    },
    {
      "text": "attention class so D in is equal to 6 uh",
      "start": 3386.359,
      "duration": 8.561
    },
    {
      "text": "D out is equal so D in is equal to six",
      "start": 3391.0,
      "duration": 7.24
    },
    {
      "text": "right because each um each token has the",
      "start": 3394.92,
      "duration": 5.72
    },
    {
      "text": "input embedding dimension of six D out",
      "start": 3398.24,
      "duration": 4.359
    },
    {
      "text": "equal to 6 the context length which we",
      "start": 3400.64,
      "duration": 4.919
    },
    {
      "text": "are using is equal to uh",
      "start": 3402.599,
      "duration": 5.44
    },
    {
      "text": "six then",
      "start": 3405.559,
      "duration": 5.401
    },
    {
      "text": "uh yeah the dropout rate which we are",
      "start": 3408.039,
      "duration": 5.241
    },
    {
      "text": "considering is zero we can even include",
      "start": 3410.96,
      "duration": 4.56
    },
    {
      "text": "the dropout rate so the dropout rate",
      "start": 3413.28,
      "duration": 4.48
    },
    {
      "text": "will change this this last layer of",
      "start": 3415.52,
      "duration": 4.519
    },
    {
      "text": "Dropout and randomly block out some",
      "start": 3417.76,
      "duration": 4.039
    },
    {
      "text": "attention weights this is good for",
      "start": 3420.039,
      "duration": 3.721
    },
    {
      "text": "generalization and the number of heads",
      "start": 3421.799,
      "duration": 4.56
    },
    {
      "text": "equal to two so we create an instance of",
      "start": 3423.76,
      "duration": 4.279
    },
    {
      "text": "this class and then create the context",
      "start": 3426.359,
      "duration": 4.081
    },
    {
      "text": "Vector Matrix and you'll see for the",
      "start": 3428.039,
      "duration": 4.641
    },
    {
      "text": "first batch the context Vector Matrix",
      "start": 3430.44,
      "duration": 5.2
    },
    {
      "text": "has three rows and six columns let's see",
      "start": 3432.68,
      "duration": 4.919
    },
    {
      "text": "if the shape matches what we had seen on",
      "start": 3435.64,
      "duration": 2.959
    },
    {
      "text": "the",
      "start": 3437.599,
      "duration": 2.641
    },
    {
      "text": "Whiteboard",
      "start": 3438.599,
      "duration": 5.72
    },
    {
      "text": "uh okay so let me scroll down",
      "start": 3440.24,
      "duration": 6.559
    },
    {
      "text": "below yeah this is the final context",
      "start": 3444.319,
      "duration": 5.72
    },
    {
      "text": "Vector Matrix which we had",
      "start": 3446.799,
      "duration": 3.24
    },
    {
      "text": "obtained yeah so this also had three",
      "start": 3451.24,
      "duration": 6.64
    },
    {
      "text": "rows okay I think I need to scroll up a",
      "start": 3454.119,
      "duration": 6.44
    },
    {
      "text": "bit yeah this is the final context",
      "start": 3457.88,
      "duration": 4.4
    },
    {
      "text": "Vector Matrix which we obtained and this",
      "start": 3460.559,
      "duration": 3.961
    },
    {
      "text": "also had three rows and six columns the",
      "start": 3462.28,
      "duration": 3.72
    },
    {
      "text": "values might be different because we",
      "start": 3464.52,
      "duration": 2.839
    },
    {
      "text": "have done the initializations",
      "start": 3466.0,
      "duration": 3.24
    },
    {
      "text": "differently here I have taken random",
      "start": 3467.359,
      "duration": 3.881
    },
    {
      "text": "initializations in the python code there",
      "start": 3469.24,
      "duration": 4.24
    },
    {
      "text": "are some other initializations every",
      "start": 3471.24,
      "duration": 4.4
    },
    {
      "text": "time you initialize we take from a goian",
      "start": 3473.48,
      "duration": 4.04
    },
    {
      "text": "distribution so the values might be",
      "start": 3475.64,
      "duration": 3.84
    },
    {
      "text": "different but let's check the shape so",
      "start": 3477.52,
      "duration": 3.96
    },
    {
      "text": "this is 3 comma 6 three rows and six",
      "start": 3479.48,
      "duration": 4.44
    },
    {
      "text": "columns and here also we can see that",
      "start": 3481.48,
      "duration": 4.28
    },
    {
      "text": "three rows and six columns awesome so",
      "start": 3483.92,
      "duration": 4.08
    },
    {
      "text": "the shape matches but you'll see that",
      "start": 3485.76,
      "duration": 4.12
    },
    {
      "text": "since there are two batches here's the",
      "start": 3488.0,
      "duration": 3.88
    },
    {
      "text": "context Vector Matrix for the first",
      "start": 3489.88,
      "duration": 3.56
    },
    {
      "text": "batch and here's the context Vector",
      "start": 3491.88,
      "duration": 4.199
    },
    {
      "text": "Matrix for the second batch so the",
      "start": 3493.44,
      "duration": 4.359
    },
    {
      "text": "multi-head attention class which we",
      "start": 3496.079,
      "duration": 3.72
    },
    {
      "text": "defined is extremely powerful because it",
      "start": 3497.799,
      "duration": 4.481
    },
    {
      "text": "can also handle multiple batches at once",
      "start": 3499.799,
      "duration": 5.161
    },
    {
      "text": "we can even do 50 data batches and then",
      "start": 3502.28,
      "duration": 5.64
    },
    {
      "text": "it will just have one 1 two it will have",
      "start": 3504.96,
      "duration": 5.92
    },
    {
      "text": "50 such context Vector",
      "start": 3507.92,
      "duration": 5.6
    },
    {
      "text": "matrices okay that's it that brings me",
      "start": 3510.88,
      "duration": 4.919
    },
    {
      "text": "to the end of this section or the end of",
      "start": 3513.52,
      "duration": 4.0
    },
    {
      "text": "this lecture so in this lecture we",
      "start": 3515.799,
      "duration": 3.401
    },
    {
      "text": "implemented the multi-head attention",
      "start": 3517.52,
      "duration": 3.68
    },
    {
      "text": "class that we'll be using in the",
      "start": 3519.2,
      "duration": 4.0
    },
    {
      "text": "upcoming lectures to implement and train",
      "start": 3521.2,
      "duration": 2.72
    },
    {
      "text": "the",
      "start": 3523.2,
      "duration": 4.24
    },
    {
      "text": "llm this code is fully functional but we",
      "start": 3523.92,
      "duration": 5.48
    },
    {
      "text": "we used relatively small embedding sizes",
      "start": 3527.44,
      "duration": 3.679
    },
    {
      "text": "and number of attention heads to keep",
      "start": 3529.4,
      "duration": 4.199
    },
    {
      "text": "the outputs readable so as I showed you",
      "start": 3531.119,
      "duration": 5.0
    },
    {
      "text": "we only use two attention heads but gpt3",
      "start": 3533.599,
      "duration": 5.601
    },
    {
      "text": "actually was 96 attention heads so the",
      "start": 3536.119,
      "duration": 5.44
    },
    {
      "text": "smallest gpt2 model had 12 attention",
      "start": 3539.2,
      "duration": 4.159
    },
    {
      "text": "heads and a context Vector embedding",
      "start": 3541.559,
      "duration": 5.48
    },
    {
      "text": "size of 768 the largest gpt2 model had",
      "start": 3543.359,
      "duration": 6.041
    },
    {
      "text": "25 attention heads and a context Vector",
      "start": 3547.039,
      "duration": 3.721
    },
    {
      "text": "embedding size of",
      "start": 3549.4,
      "duration": 4.8
    },
    {
      "text": "1600 and gpt3 has even higher so the",
      "start": 3550.76,
      "duration": 5.839
    },
    {
      "text": "gpt3 largest model has 96 attention",
      "start": 3554.2,
      "duration": 5.52
    },
    {
      "text": "heads I think and generally in GPT",
      "start": 3556.599,
      "duration": 5.881
    },
    {
      "text": "models the D in is equal to D out in the",
      "start": 3559.72,
      "duration": 4.839
    },
    {
      "text": "example which we saw D in was equal to D",
      "start": 3562.48,
      "duration": 5.119
    },
    {
      "text": "out equal to 6 but here the D in and D",
      "start": 3564.559,
      "duration": 7.321
    },
    {
      "text": "out are much larger around 768 Etc again",
      "start": 3567.599,
      "duration": 6.601
    },
    {
      "text": "thank you so much everyone for reaching",
      "start": 3571.88,
      "duration": 5.0
    },
    {
      "text": "the end of this attention series it's",
      "start": 3574.2,
      "duration": 4.399
    },
    {
      "text": "been one of the longest and most",
      "start": 3576.88,
      "duration": 3.88
    },
    {
      "text": "comprehensive series which I have",
      "start": 3578.599,
      "duration": 5.041
    },
    {
      "text": "covered and uh I really enjoyed learning",
      "start": 3580.76,
      "duration": 5.64
    },
    {
      "text": "about all of these things I can see that",
      "start": 3583.64,
      "duration": 5.719
    },
    {
      "text": "many llm practitioners cannot understand",
      "start": 3586.4,
      "duration": 4.56
    },
    {
      "text": "these Dimensions or they do not take",
      "start": 3589.359,
      "duration": 4.041
    },
    {
      "text": "time to go through understanding the",
      "start": 3590.96,
      "duration": 5.24
    },
    {
      "text": "theory the building blocks behind how",
      "start": 3593.4,
      "duration": 4.8
    },
    {
      "text": "the attention mechanism Works they just",
      "start": 3596.2,
      "duration": 4.2
    },
    {
      "text": "implement the code bases which are",
      "start": 3598.2,
      "duration": 4.44
    },
    {
      "text": "available which completely abstract away",
      "start": 3600.4,
      "duration": 5.639
    },
    {
      "text": "all of these things so I don't think",
      "start": 3602.64,
      "duration": 4.959
    },
    {
      "text": "that's the good way or the correct way",
      "start": 3606.039,
      "duration": 3.641
    },
    {
      "text": "to learn about large language models if",
      "start": 3607.599,
      "duration": 4.161
    },
    {
      "text": "you want to be a true llm engineer or a",
      "start": 3609.68,
      "duration": 4.439
    },
    {
      "text": "machine learning engineer you have to",
      "start": 3611.76,
      "duration": 4.72
    },
    {
      "text": "understand how nuts and bols work",
      "start": 3614.119,
      "duration": 4.041
    },
    {
      "text": "otherwise you might be able to deploy",
      "start": 3616.48,
      "duration": 4.0
    },
    {
      "text": "applications but to make real inventions",
      "start": 3618.16,
      "duration": 4.08
    },
    {
      "text": "you will have to go into the code base",
      "start": 3620.48,
      "duration": 4.079
    },
    {
      "text": "change a few things understand",
      "start": 3622.24,
      "duration": 4.64
    },
    {
      "text": "Dimensions as you might have seen and I",
      "start": 3624.559,
      "duration": 4.161
    },
    {
      "text": "have stressed this many times dimensions",
      "start": 3626.88,
      "duration": 3.84
    },
    {
      "text": "and linear algebra are at the heart of",
      "start": 3628.72,
      "duration": 4.48
    },
    {
      "text": "becoming a very strong ml engineer it",
      "start": 3630.72,
      "duration": 4.48
    },
    {
      "text": "all comes down to Dimensions other",
      "start": 3633.2,
      "duration": 3.32
    },
    {
      "text": "students might be scared of a",
      "start": 3635.2,
      "duration": 3.76
    },
    {
      "text": "four-dimensional tensor right but if you",
      "start": 3636.52,
      "duration": 3.96
    },
    {
      "text": "understand how it works based on what I",
      "start": 3638.96,
      "duration": 3.72
    },
    {
      "text": "showed to you on the Whiteboard my aim",
      "start": 3640.48,
      "duration": 3.839
    },
    {
      "text": "is that you should not be scared of",
      "start": 3642.68,
      "duration": 3.6
    },
    {
      "text": "these higher dimensional matrices once",
      "start": 3644.319,
      "duration": 4.361
    },
    {
      "text": "you write it down and once you",
      "start": 3646.28,
      "duration": 4.16
    },
    {
      "text": "understand what's going on it really",
      "start": 3648.68,
      "duration": 3.84
    },
    {
      "text": "becomes easy that's why I really",
      "start": 3650.44,
      "duration": 3.84
    },
    {
      "text": "recommend writing things down you can",
      "start": 3652.52,
      "duration": 3.24
    },
    {
      "text": "write on a whiteboard you can even write",
      "start": 3654.28,
      "duration": 3.64
    },
    {
      "text": "on a piece of paper but make sure you",
      "start": 3655.76,
      "duration": 4.039
    },
    {
      "text": "write things down then you'll remember",
      "start": 3657.92,
      "duration": 4.199
    },
    {
      "text": "them for a longer period of time I hope",
      "start": 3659.799,
      "duration": 4.481
    },
    {
      "text": "you all are enjoying these lectures",
      "start": 3662.119,
      "duration": 3.561
    },
    {
      "text": "thank you so much everyone and look",
      "start": 3664.28,
      "duration": 3.0
    },
    {
      "text": "forward to seeing you in the next next",
      "start": 3665.68,
      "duration": 3.159
    },
    {
      "text": "lecture where we'll actually start",
      "start": 3667.28,
      "duration": 6.799
    },
    {
      "text": "building the llm model thanks a lot",
      "start": 3668.839,
      "duration": 5.24
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch Series this is the second part of the multihead attention lectures in the previous part we looked at implementing multi-head attention in the following way what we did is that we had the input tokens so let me show you this figure which summarizes everything yeah so this was the input Matrix which we had essentially the number of rows here represent the number of tokens which we have and each token was encoded as a three-dimensional input embedding Vector right in the first part of the multi-ad attention what we essentially did was we created multiple weight matrices for the query key and the value so if we have two attention heads we'll create two weight matrices for the query two weight matrices for the keys and two weight matrices for the values and then we will multiply the inputs with these weight matrices to get two queries two keys and two values so the main problem with this approach is that here you can see that there are two Matrix multiplications which are needed as we saw earlier gpt3 used 996 attention attention heads so if you have 96 attention heads you'll need 96 multiplications to get the queries Matrix 96 multiplications to get the keys Matrix and 96 multiplications to get the values Matrix that's not very efficient right so today what we are going to see is that how can we make sure that the number of multiplications are reduced in particular what if we just need to do one multiplication for quick Keys one one multiplication for queries and one multiplication for values and then once we do one multiplication then we can split the queries again into two parts we can split the keys into two parts and we can split the values also into two parts for two heads and then we can perform the rest so once we get the copies or the multiple matrices for queries keys and values what we can do is multiply queries with keys transposed to get the attention scores then we can get the attention weights and we can multiply them with the values to get the context vectors so the rest of the procedure can be bit similar but what if we can reduce the number of Matrix multiplications at the start this is what we are going to look at in today's lecture so let's get started this procedure is called implementing multi-head attention with weight splits and it's definitely much more efficient than the multi-head attention which we saw previously so in the previous lecture the way we implemented multi-head attention was something like this we had the the causal attention module and we calculated the causal attention module result which is the context Vector the context Vector for every single attention head and then we concatenated the results from the different uh context vectors together and that led to a large Matrix this was the process now what we are going to do is we are going to follow a slightly different procedure and that's called multihead attention with weight splits and it's much more computationally efficient so the main idea is that in the previous code we had maintained two separate classes we had maintained a class for the multi-head attention rapper and we had maintained a class for the causal attention and then we combine both of them into a single multi-head attention class so here in the in the top what you can see over here is what we did previously We performed two Matrix multiplications to obtain the two query matrices q1 and Q2 Q2 what we are going to do right now is what if the weight Matrix which we start start out initially itself was a larger weight Matrix and then uh we multiply the inputs x with the query Matrix to get the queries and then after that we split the queries into two components so here you see the difference in the previous case we multiplied x with wq1 and we multiplied x with wq2 but what if we multiply x with a WQ which is already a large Matrix which consists of D out so here you see the dimensions of this initial weight Matrix are larger and that these Dimensions already include the number of heads so this Dimension 4 is the D out which is two multiplied by the number of heads so this D out is already specified before so the weight Matrix for the queries keys and the values which we specify already will kind of include the head Dimension and then we when we get the queries keys and values we'll split them based on the number of heads so here we can see where we split the queries Matrix into two q1 and Q2 because there are two heads so ultimately the rest of the procedure will remain the same but we are just reducing the number of Matrix multiplications so if you look at this weight Matrix right now the number of attention head is specified right uh so how is it specified so D out is equal to 4 and D out is equal to head Dimension multiplied by the number of heads so the head Dimension is equal to 2 because each head has a dimension of two this is what we had done here each head had a dimension of two each attention head here you see D out equal to two which was implemented in the previous case so each head had a dimension of Two And there are two heads so the D out in this larger trainable Q Matrix already includes number of heads I'll explain to you in detail what this means right now if you just get an intuitive idea of what we are trying to do that will be very helpful okay so now uh let us get started with the code so we are going to implement multi-head attention with weight splits right so instead of maintaining two separate classes so here you can see earlier in our code we had the we had a causal attention class which did all the computations of attention scores attention weights Etc and then we integrated this causal attention class with the multi-head attention rapper so we had a multi-head attention rapper and we created multiple instances or multiple causal attention objects within this rapper now the idea is instead of maintaining two separate classes why don't we combine both of these Concepts into a single multi-head attention class also in addition to just merging the multi-head attention rapper with the causal attention code let's make some other modifications to implement multi-head attention more effectively so as I told you earlier in the multi-ad attention rapper which we had earlier multiple heads are implemented by creating causal attention objects and uh the causal attention class independently performed the attention mechanism earlier and then the results from each attention head were effectively concatenated now we are going to implement a class which is called as multi-head attention class and we are going to integrate the multi-head functionality as well as the causal attention functionality everything within a single class the way we are going to do do this is that the this class splits the input into multiple heads by reshaping the query key and value tensors let's see what this means don't worry about this sentence in this lecture I have constructed a Hands-On example so that you understand the code which we are about to write so first let's look at the multi-ad attention class and how we are going to Define it this code right here which I'm showing on the screen is at the heart of the Transformer mechanism so you see we have the in init Constructor which is invoked by default and then there is the forward method at the end of the forward Method All We are going to do is calc calculate the context Vector for each of the input embedding vectors but what happens in the middle that is the main key which you really need to understand okay so I could have just taken you through this code but I have seen that if I take students through this code it becomes very difficult for them to wrap their heads around what exactly is going on especially because if you see the dimensions there are four dimensional tensors which are involved in this code and there is a very good reason for why we need four dimensional tensors so if you if you just go through the code you will you will think that you have understood it but you would not have because there are lot of subtleties with respect to the dimensions so what we are going to do is that we are going to go to the Whiteboard and I constructed this example completely from scratch so we are going to take a simple example we are directly going to start from the input and we are going to do all the steps on the Whiteboard which are implemented in this code then you will find that understanding the code is extremely easy at every step of the code I'm going to take you to the Whiteboard and I'm going to explain to you what exactly is going on okay so let's get started I've tried to distill this down to 11 steps and uh I I will explain everything related to matrices Dimensions extremely clearly I will not assume anything everything is written down on the Whiteboard so that you will not be afraid of this code I have seen several other YouTube videos and even lectures where uh people just explain this as if it's very easy to understand but you need to decompose it into individual layers and explain every single one of them okay so I'm going to start with the forward method and I will explain every single line here step by step so first the forward method takes the input X right let's see what that input looks like and what it means so the first step to the attention make the multi-ad attention with weight splits that code or the multihead attention class is that we have to start with the input the way we will specify the input is that the input has three dimensions the First Dimension is the batch the second is the number of tokens and the third is the input Dimension right what is this D in the D in is basically every token is represented by a vector embedding so this D in is the dimension of that Vector embedding so 1A 3 comma 6 means that I have three tokens you can think of one token as one word for Simplicity so let's say the three tokens are the cat sleeps let's say these are my let's say these are my three words then what we are essentially doing here is that we are converting each of these words we are converting each of these words into six dimensional vectors so the will be a six dimensional Vector which is the first row so which is the first row over here so look at the first row over here this is the six dimensional Vector for the this is the six dimensional Vector for cat and this is the six dimensional Vector for sleep so you'll see that this is a 1x 3x 6 for Simplicity I have taken the batch size equal to 1 okay so this is a 1x 3x 6 tensor why 3x 6 because we have three rows here and we have six columns uh each row consists of six six dimensional vectors so I hope you have understood how the input has been defined so here you can see the input shape is B comma number of tokens comma input Dimensions so I hope you you have understood this okay now let's come to the next step the next step is what we have to do is we have to essentially decide two things we have to decide what our output Dimension is going to be and we have to determine the number of Heads This output Dimension is basically we have the input input embedding Vector for each token right ultimately we will get a context Vector for every token so ultimately similar to this input embedding Matrix which is 3x which is 3x 6 we will have a context embedding Vector which is three because we have three tokens multiplied by D out so now we have to also decide what is D out which is the so each each token will have a context vector what is the dimension of that Vector we have to decide so now I am deciding that D out will be equal to 6 which is same as D in this is typically done in GPT based models the D in and the D out are the same second thing we also have to decide is how many attention heads do we want to have so I have decided that we are having two attention heads right now in GPT the number of attention heads are 96 and the D out is also pretty large but exactly what we are doing right now can be scaled to a larger D out and larger number of heads okay so I'm using D out is equal to 6 and number of heads equal to three so then each head will have a dimension which is called as head dim and we'll look at that also later each each head will have a dimension of head dim which is equal to head dim which is equal to essentially the D out divided by the number of heads which is equal to 6 / 2 and that will be equal to three so then the dimension of each head is equal to three and since there are two heads the total D out will be equal to six okay so this is the second decision point the third decision point which I have to make is that I have to initialize or it's not a decision Point rather but it's the third step so the third step which we have to do is initialize trainable weight matrices for the key query and the value so we have to initi w k WQ and WV okay so remember that the input which we have which for now can be thought to be six rows and three columns has to be multiplied sorry three rows and six columns so the input is three rows and six columns so when you construct these trainable weight matrices for the keys query and value their first Dimension has to be equal to D in because if you look at the input Dimension the input the number of the last dimension of of the input is D in and for these for d for this input to be compatible with this WK WQ and WV in multiplication you need the first argument here to be equal to D in so actually the dimensions of the trainable key query and value matrices are D in multiplied by D out which is 6 by 6 because D in is equal to 6 and D out is equal to 6 so we have to initialize the these three vectors these three matrices rather and I have shown these random initializations here so you can see that WQ is a six diens or a 6x6 tensor WK is a 6x6 tensor and WV is a 6x6 tensor let us let me show you in code where these matrices are actually initialized so if you look at the code these matrices are actually initialized in the init Constructor so w query W Key and W value these are trainable weight matrices as you can see the dimensions are D in D out and we are using the linear layer of neural networks with the bias equal to zero to set the uh initial values for these why do we use a neural network linear layer because it's optimized for initializing the weights so it's much better when we do the back propagation later so this is where the the trainable weight matrices for query key and value are initialized in the init Constructor so it's called by default when we create or it's these Matrix are created by default when we create an instance of the multihead attention class all right so up till now we have essentially initialize these trainable weight matrices w k WQ and WV Now we move to step number four step number four is the step from which computations actually start so we have the input now right and uh we have the these matrices we have trainable Keys the trainable queries and the trainable values what we'll now be doing doing is that we will multiply the input with these matrices to ultimately get the keys the queries and the values so what are the dimensions of the input the dimensions of the input are one one M uh me just write this again the dimensions of the input are 1 multiplied by 3 because we have three rows multiplied by six columns correct and the dimensions of each of these qu key query and value trainable weight matrices are six which is D in multiplied by 6 which is D out so when you multiply the input with these weight matrices the result which you'll get is 1x 3x 6 so you'll get the keys you'll get the keys Matrix which is 1x 3x 6 you'll get the queries Matrix which is 1x 3x 6 and you will also get the values Matrix here which is 1x 3x six let's try to understand what this 1 3 and six is so I as I've written over here one is the batch size which we are taken to be one three is the number of tokens because we have three tokens and D out is basically the output Dimension so the way to interpret these keys saries and value Matrix is that each row basically corresponds to one token so the first row corresponds to the first token the second row corresponds to the second token and the third row corresponds to the third token and there are six dimensions in each row because each token is a six dimensional representation because D out is equal to 6 now let me show you in the code where this is calculated so if you go down below here you see the keys queries and values what we have done is that we have passed in the input to this neural network linear layer so what this does is that the trainable weight mates for the key query and value are applied on this input and we get the keys queries and the value Matrix as we saw on the Whiteboard the shape of this is B which is the batch size the number of rows is equal to the number of tokens and the number of columns is equal to the D out which is equal to six okay now we move to the next step and this is the step where four dimensional tensor start to come into the picture right so until now we have three dimensional tensors for the keys queries and values right why the fourth dimension needs to come into the picture is that until now these three dimensions are for batch size number of tokens and D out but there is no dimension for the number of heads uh or the head Dimension rather so this is where we come to next so what we are now going to do is that we are going to unroll the last dimension of the keys queries and values to include the number of heads and the head Dimension what this means is that if you look at this last dimension of the keys here in fact even for queries and values this is D out right and D out is essentially number of heads into head Dimension as we have seen earlier so let me take you yeah so here remember what we saw head Dimension is equal to D out divided by number of heads so D out is equal to head Dimension multiplied by the number of heads so that's what we are actually going to do we are going to unroll the last dimension of the keys saries and values to include number of heads and head Dimension right so we have D out which is equal to 6 which is a decision which we have made and we have also made a decision with respect to the number of attention heads which is equal to two So based on these two decision points the head Dimension is fixed and the head Dimension will be equal to 6 / 2 which is equal to 3 so what we are going to do next is that we had this 1X 3x 6 matrices right for the key SAR and value now we are going to roll them into 1 by 3 by 2 by 3 so now instead of D out we will have number of heads which is equal to two and head Dimension which is equal to three so let's see what the reshaped keys queries and values Matrix actually look like uh so when you reshape the keys queries and value Matrix they start looking like this and I'll tell you how to interpret four dimensional tensors also so for the sake of Simplicity uh let's first analyze the queries Matrix so this Matrix is 1x 3x 2x 3 how do you Analyze This four dimensional tensor for now forget about the first which is the number of batches okay so next look at three so this three is the number of rows so this is my first row and uh this is my first token also right this is my second token and this is my third token correct that's why there there is this three now let's look at this two what is this two this two is the number of heads so if I go in each token right now let's see if I go in first if I go in the first token the first row corresponds to the first head and the second row corresponds to the second head that's why there is this two and if I go within each head I'll see that there is there are three dimension the First Dimension the second dimension and the third dimension so remember the head Dimension is equal to three so the way to interpret this Matrix is start from the outermost value so three why three because there are three tokens then go to each token why two because there are two heads in each token then go within each head why three because the dimension of each head is three each head is a three dimensional Vector so in this same way we can analyze the queries the keys and the value Matrix also so the keys Matrix will also be uh 1X 3x 2x3 and the values Matrix will also be 1X 3x 2x 3 as I mentioned before let me repeat it again each row over here is a token so if you look at the value Matrix let's look at the second row the second row corresponds to the second token if you now look at the first row of the second row this that is the threedimensional head Vector for the first head if you look at the second row that's the three-dimensional head Vector for the second head remember every token has two attention heads so you can think of as two people paying attention to each token token because we have two attention heads that's why there are two rows corresponding to every token now if we come to the code this line has been mentioned over here so see we have to unroll the last Dimension so now the D out will be replaced with the number of heads and head Dimension so this is exactly what has been done over here keys. view so now keys will be replaced with keys do view B common number of tokens common number of heads and head Dimension this is exactly uh what we we just saw on the Whiteboard so in this step the three dimensional tensors have been converted into four dimensional tensors to include the number of heads and the head Dimension great now we move to the next step so if you see uh if you see this let's look at this argument which is three um so the shape of this is 1A 3A 2 comma 3 right now let's look at this first this this entry this is three now this three is the number of tokens which means that currently these matrices are grouped according to number of tokens right so I'm saying that this is the first token this is the second token and this is the third token and then I further dive into number of heads and the dimensions in each head but it turns out that later when we want to compute the attention scores the only way the computation can proceed ahead is if we Group by the number of heads so instead of grouping by the number number of tokens I actually want to group by the number of heads and we have two heads here right so I want to flip these Dimensions I want to flip these Dimensions here so that the first row represents the first head the second will represent the second head and each will have a 3X3 let me show you what I mean so now what we are going to do is we are going to group The matrices by the number of heads okay so currently the keys queries and the values Matrix have the dimensions of one which is the batch three which is the number of tokens two which is the number of heads and three which is the head Dimension so we are grouping with respect to the number of tokens but now I want to group with respect to number of heads so again I want to switch the dimensions to be I want to switch this this to uh let me write it again yeah I want to switch this two with three and this three should come over here so I want the the matri to have the dimensions of B comma number of heads comma number of tokens and head Dimension so I want the dimensions to be 1 comma 2 comma 3 comma 3 so what I'm going to do in the code also you'll see we are going to transpose keys quaries and value and we are going to transpose one comma 2 now why do we do one comma 2 over here because python has zero indexing so index zero is this since we want to Interchange the number of tokens and the number of heads the indexes which we need to transpose are index number one and index number two that's why we are doing Keys queries and value and transpose 1A 2 so let's see what the result actually looks like so when you when you make the 1A 3A 2A 3 to 1A 2 comma 3A 3 now the transposed queries keys and Valu start looking like this and now you will see that they are grouped by head so the first thing what we can do is that let's look at this block in the queries so we are analyzing the queries Matrix now the shape of the queries Matrix is what 1 comma let me write it here again 1 comma 2 comma 3 comma 3 right that is the essentially the shape of the queries Matrix and we are going to analyze this so let's start with this two which is the number of heads so if you look at the first block here uh let me erase this right now now and then draw it again yeah so if you look at the first block over here which are marking with these curly braces that's the first head if you look at the second block here that's the second head so see now this two because this two comes over here now we can group with respect to number of heads so the first block shows everything with respect to head one and the first row over here is the first token the second row over here is the second token and the third row over here is the third token similarly if you look at head number two the first row is the first token the second row is the second token and the third row is the third token so now we have the dimensions as number of tokens and head Dimensions come last so each token if you see each token has a three dimensional Vector because the head Dimension is equal to three so the the reason this helps is because since we can now group with respect to heads we can compute the attention score for each head separately so remember there is one attention score there are there is an attention score Matrix for the head one and there is an attention score Matrix for the head two and then we we are going to U concatenate them together right so it makes sense to group with respect to the head and that's why this step exist this keys. transpose it's very difficult for students to understand this unless you see this visual example of why we are essentially doing this transpose the main reason we do do this transpose is that here you see we are grouping with respect to uh we are grouping with respect to number of tokens here but that's not good if you want to compute the attention scores for each head parall so we group with respect to number of heads so that's why it's important to flip number of tokens and number of head Dimension and that's exactly what we have done okay now let's go to the next step the next step is to find the attention scores so remember now we have the uh we have the queries Matrix we have the keys Matrix and we have the values Matrix in exactly the shape which we want so now we can do uh we can go ahead and find the key queries and the keys transpose to get the attention score so let me show you how this is done first let me rub all of this okay okay so now I have rubbed all of this so what we are now essentially going to do is that um this is the head number one right this is the head number one of the queries and this is the head number one of the keys so what this this shape will help us do is that when we do queries multiplied by Keys transpose it will directly uh take the equivalent product of head one of the queries with head one of the keys and then head two of the queries and head two of the keys but remember when we take the keys transpose what's really important to us is that now the shape of the keys is B B common number of head is common number of tokens and head Dimension so what it's really important to us is number of tokens and head Dimension so remember the formula for calculating the attention score is queries multiplied by Keys transpose right so here also we are going to do queries with respect to Keys transpose but what exactly do we have to transpose we have to transpose uh we have to transpose this so we have to transpose the last two dimensions and let me show you what that transposed key Matrix looks like yeah so this is the transposed key Matrix now here you can see the key Matrix uh if you see the first row it's 4143 -1. 423 and - 2.71 31 right so when we do Keys transpose Keys transpose 2 comma 3 it will transpose along the last two Dimensions so now that that row which we saw has now become a column over here so this is the keys transposed and here is the queries Matrix and I've shown the keys transpose over here so the queries matrix dimensions is 1A 2A 3 comma 3 the keys transpose Dimension is 1A 2 comma 3 comma 3 so they they they are compatible for multiplication and the way the multiplication will now proceed is that the head one will only be multiplied by the head one of the keys transposed the head two here will only be multiplied with the head two of the keys transposed and ultimately when we do this multiplication we we will get the attention scores Matrix so this is the attention score Matrix which we have and the dimensions of this are B number of heads number of tokens and number of tokens let me show you why um okay so if you look at what we are multiplying here the query's dimensions are B comma number of heads comma number of tokens comma head Dimension right and when we do Keys transpose uh 2 comma 3 the dimensions here are B number of heads head Dimensions comma number of tokens so essentially you can think about it like we are multiplying two matrices with the dimensions number of tokens comma head Dimension multiplied by head Dimension number of tokens so what will the resulted Matrix will have number of tokens rows and number of tokens columns and the first two Dimensions here will stay the same because they are the same in both of these matrices we are multiplying so the resultant attention scores will have the dimensions of B number of heads number of tokens and number of tokens it's fine if you forget these Dimensions but you should be able to interpret what is going on here so let's see what is going on here remember we have we are grouping with respect to head so that stays the same this first uh this first block which I've highlighted right now that is head number one and the second block which which I've highlighted right now that is essentially head number two this is the first thing to understand okay then what we are doing when you look at the let's look at head number one for now if you look at the first row the first row essentially consists of the attention score between of the first word with all the other words right so remember our sentence was the actually let me write it over here that will be much better so our sentence was the the cat the cat and here it was sleeps right the cat let me just write it over here yeah the cat sleeps and the same words I'm also going to write over here so the first row is let me write it over here actually the first row is the the second row is cat and the third row is sleeps so that's why the final two dimensions are number of tokens comma number of tokens because if you look at the second row now if you look at the second row now the first element of the second row tells us information about the attention between cat and the the second element of the the second row tells us the information between cat and cat so if the query is cat how much attention should you pay to cat the third element here tells us the information between cat and sleep which means if the qu if the query is cat how much attention should you pay to sleep so that's why the shape of the attention Matrix for every head is number of token rows and the number of token columns because an attention score exists between each token for every other token so whenever you see these Dimensions right don't get confused by it try to always understand the meaning behind it that's why we had so many lectures on the attention mechanism before just so that when we reach this stage understanding all of this becomes easy so remember until this stage we have computed the attention score so this is exactly what is done here remember what we saw on the Whiteboard to compute the attention scores we'll take the queries and we'll multiply with the keys. transpose 2 comma 3 because uh in transposing 2 comma 3 we'll make sure that the correct queries and the attent and the KE transpose product is taken to calculate the attention scores and uh this is also implemented in the code so if you see in the code the attention score is the product is the scaled product between queries and the keys great so here it shown dot product for each head now you'll understand why I'm saying each head because as I showed you before each head has number of tokens comma number of tokens attention scores and for for one head it's here and for the second head it's below okay now we come to the next step the next step is to essentially find the attention attention weights okay so uh if you look at this attention score over here right now you'll see that for every token there is an attention score with respect to every other token right but that's not what's the mechanism in causal attention what causal attention says is that when you look at the you should only look at the attention score between the and what comes before it so the and the all the other elements here so let me show them with a different let me first rub this uh so that yeah so what causal attention mechanism dictates is that when you look at the first word which is the only the attention score between the and what comes before it should survive so all of this should go to zero if you look at cat only the attention score of the Words which come before so the and Cat should survive this should go to zero and when you look at sleeps attention scores of all Will Survive because all the words come before it this is what we are actually going to implement next so to do that first what we are going to do is we are going to take the attention scores which we have and replace all of the elements above the diagonal with negative Infinity the reason we uh replace this with negative Infinity is because after this point we are going to implement the soft Max function so that each row sums up to one and when we Implement soft Max whatever is there in the infinity will automatically go to zero so it will kill it will kill two birds in the same Stone we will implement the causal attention mechanism and we'll also make sure that all the rows sum up to one but before we Implement soft Max we do one more thing we divide every single element here with the square root of the head Dimension and this when we looked at the lecture for uh self attention we saw why this is done this is essenti to make sure that the variance between the when we take the dot product between the queries and the keys the variance scales up with the number of dimensions and to prevent the variance from blowing up we have to divide by the square root of the head Dimension this also makes sure that the values in the values before we compute the soft Max are not very high and that's generally useful for back propagation and leads to stable gradients so what we'll be doing is that the head Dimension as we saw is three right because the D out is equal to 6 and the number of heads is equal to 2 so each head Dimension is equal to three so we'll divide this after replacing the elements above the diagonal with negative Infinity we'll divide this with square root of 3 which is square root of head Dimension and that leads to this Matrix over here or this tensor I should say and then we apply soft Max to this tensor so we make sure that every row here sums up to essentially so if you look at each row in this you'll see that it it's summing up to one and the reason it sums up to one is we are applying soft Max so now I can make claims interpretable claims so when I say that when I look at the second token cat I should pay 96% attention to the and I should pay 4% attention to cat when I look at sleeps I should pay 4% attention to the I should pay 26% attention to cat and I should pay pay 69% attention to sleeps remember these values are not optimized but when they are optimized uh when we look at back propagation later uh the fact that these values sum up to one will carry meaning because we can make interpretable statements such as what I was making right now remember that after we are going to apply soft Max uh the attention weights have exactly the same dimensions as the attention scores which is going to be the batch size number of heads number of token tokens and number of tokens so this is the dimension of the attention weights which is 1A 2A 3A 3 uh so if you look closely to go from attention scores to attention weights we actually M we actually have very we have a rich number of steps and it's important for you to understand all of these first what we did is we applied a mask so that all elements above the diagonal are negative Infinity then we divided by the square root of the the head Dimension then we applied soft Max this is how we got the attention weights now let's see how that is done in the code H before that one thing usually we can also Implement Dropout after this so you can mention a dropout rate which is actually one of the arguments in the multi-ad attention class but here I'm not implementing Dropout for the sake of simplicity so if you look at the code here we have got the attention scores the first step as I said is to create this mask and and then apply this mask to the attention score so that all the elements above the diagonal are negative Infinity that's what this step is doing uh here the mask actually has also been defined over here see this is the upper triangular mask which is all the elements about the diagonal one then they are replaced with negative infinity and that's applied to the attention scores so this will ensure that all the elements above the diagonal of the attention scores are equal to negative infinity and we are only considering context length here why context length because let's say context length is three it means that maximum if three words are given we can make prediction of the next word so when we implement this Dropout mask we only Implement a mask of context length comma context length there is no point in implementing a bigger mask because anyway we are not going to look at more tokens than the context length at a time and if it happens that we are looking at a batch where the number of tokens are less than the context size this statement makes sure that then the mask stops at number of tokens but this is a detail which probably uh you can Overlook right now if you're understanding all the other things that's what the most important if you understand this Minor Detail it's awesome then the next step is to apply soft Max but as I told you before applying soft Max we defi we divide every element with the square root of the head Dimension if you look at the keys. shape let's look at keys. shape uh this is going to be the keys do shape so keys do shape of minus one which means that we are going to look at the last Dimension which is the head Dimension so we are essentially dividing by square root of head Dimension here and then we apply the soft Max y Dimension equal to minus one because we need to make sure that all The Columns of a row sum up to one and then as I said we can even Implement Dropout if needed towards the end so up till now we have reached a stage where we have obtained the attention weights is basically and I hope you understand the meaning behind this final attention weight matrix it's not just important to understand how the dimensions work so to make sure you understand the meaning let me go through the meaning of this attention weight Matrix once more um this what I'm highlighting right now is the attention weights for the first head this second block is the attention weights for the second head in each attention uh head block you will see that the size is number of tokens rows and the number of tokens columns so each value is essentially the attention weight between let's say this is the attention weight between this is the attention weight between the second row which is the second token and the second token this is the attention weight between uh the third token as the query and the first token as the key so basically you'll see that every single element here has some meaning it essentially encodes the attention weight between the query and the particular key okay now let's go ahead the last step which we are going to implement is that we have to calculate the context Vector remember the aim of all the attention mechanisms is to ultimately compute the context Vector Matrix and this is exactly what we are going to do and to compute the context Vector Matrix we take the attention weights and we multiply them with values remember the value Matrix was The Matrix which we had computed earlier let me show you where the value Matrix was in case you have forgotten it because we have done so many things uh yeah so here was the value Matrix which we had computed we have not used it until now it will only be used in this last step okay so the attention weights will be multiplied by the values Matrix to get the context Vector Matrix so let's see how the dimensions work out here okay the attention weights as we looked earlier over here the attention weights have the dimensions of B comma number of heads comma number of tokens comma number of tokens and as we saw earlier the values Matrix has the dimensions B comma number of heads uh comma number of tokens and head Dimension so effectively let's see whether these matrices can be multiplied so this is number of tokens and number of tokens and that will be multiplied by number of tokens and head dim so the number of columns here is number of tokens and the number of rows here is number of tokens so the number of columns in the first Matrix are matching the number of rows in the values Matrix so we can see that these two matrices can essentially be multiplied so multiplication is possible and now let us see how the multiplication will actually work in practice this is the final attention weights Matrix here it's mentioned attention scores but I should have called it attention weights remember there is a difference between scores and weights attention weights in in the attention weights each row sums up to one that's not the case with attention scores okay so this is the attention weights and this is my values this my values Matrix so this 1A 2A 3A 3 and this 1A 2A 3A 3 and when we multiply the resultant output will be B comma number of heads comma number of tokens uh comma the head Dimension so here you can see that the context Vector output is B comma number of heads comma number of tokens comma head Dimension which is 1 comma 2 comma 3 comma 3 Let's interpret this again uh so here you can see that there are two heads so this is head number one and this is head number two and in each head there are number of tokens so if you look at each head there are three rows so each row corresponds to one token but now if you look at what what each row represents each row represents the context Vector for that particular token and it has the dimensions equal to head dim because head dim is equal to three so that's the meaning of this uh context Vector Matrix which we have reached but now remember there is a problem here right or not a problem uh but we have to somehow merge this number of heads and head Dimension back together because the resultant context Vector Matrix remember what we saw earlier the let me scroll up a bit so if you if you if you looked at the goal which we had when we started this lecture the goal was that the resultant context Vector Matrix should have the dimensions of uh yeah as I mentioned to you the goal was that the resultant context meor Matrix should have D out right as the dimension so we should again pull back the head Dimension and the number of heads together so that we can get the resultant Matrix which has the D out Dimension preserved uh whereas let's see what we have obtained until now well until now the context Vector Matrix which you have obtained yeah the context Vector Matrix which we have obtained has number of heads and head dimensions in separate positions so first what we'll do is that we'll bring them closer together so that we can then merge them to get the D out so what we are going to do is now we are going to swap this this number of tokens index with the number of heads index so that the dimension of the context Vector Matrix is so that the shape of the context Vector Matrix is changed so the next step is basically step number 10 and that is to reformat the context vectors So currently the context Vector shape is B comma number of heads comma number of tokens comma head Dimension right and I want the number of heads to come here so that they're closer to the Head dim and I want the number of Tok number of tokens to go here so I want the resultant Matrix to be B comma number of tokens comma number of heads comma head Dimension so essentially what I will do is after I compute the context Vector Matrix I'll do a transpose of the first index and the second index and so the resulting context Vector Matrix now which has the dimensions of B comma number of tokens comma number of heads comma head Dimension looks like this so here you see now the interpretation is different now this is my first token now the grouping is with respect to tokens this is my second token and this is my third token and in each token there are two heads so if you look at the first token there are two heads and if you look at the first head this is the vector with respect to the first head the context Vector context vector and the second row is the context Vector with respect to the second head for the first token now let's see how all of this is implemented in code actually all of what we saw right now is just implemented in one line of code but to understand this we really have to understand first of all how the attention weights are multiplied with values the multiplication really makes sense and why do we do this transpose 1A 2 the reason we do this transpose 1 comma 2 is to get the context Vector mat in this shape the reason we get it in this shape is now you can see the number of heads and head Dimension are closer together so we can merge them um into the D out more easily so here you can see this is what we have reached until now where the context Vector is obtained and it's in the correct format now the last step what we have to do is that we have to um let me show you the last step what we have to do is essentially we have to combine the results from multiple heads so see this is the context Vector Matrix which we have obtained right now right so if you look at the first token which I've highlighted over here this is the first head and this is the second head now what I will do is that when I look at the first token I will combine these two together into one row so that it will be uh six the dimension will be six so so here these are three and these are three right so I'll combine the outputs from both of these heads into one output so let's see how this looks like so then the first row will so then we'll flatten this is called flattening will flatten each token output into each row so the head one and head two outputs are combined together so if you look at the final output the first row consists of merging of the two heads for the first token the second row consist of the merging of the two heads for the second token so for the second row we merge these two out outputs into one single row and for the third token we merge these two outputs into a single row so you'll see that the F the this is the third row so this now what I what I'm showing on the screen here is my final context Vector Matrix and how to interpret this if you look at the first row the first row is the context Vector context Vector for the first token first row is the context Vector for the first token why does it have six elements because d out is equal to 6 the second row is the context Vector for the second token and the third row is the context Vector for the third token so overall you see we first split the D out into number of heads and head Dimension and now we brought it back together to get the D out so in the final shape you will not see the number of heads it's all merged into this D out so this is my final answer right now and the shape of this is 1A 3 comma 6 which is B comma number of tokens comma D out so this is exactly what is done here what we do is that we uh we take this context vector and we reshape it into B comma tokens comma D out why this continuous is needed is because we want to make sure that when we reshape matrices they are in the same blocks of memory so when we reshape uh tensors let's say and if they're in different memory blocks it becomes difficult so first we make sure that using this continuous they in the same memory block then we reshape them so that the final output is B which is the batch size number of tokens which is equal to three in the example we saw and D out which is the output Dimension which is equal to six and then there is an optional projection layer towards the end so if you look at the out out output projection it's again a linear layer and whose parameters can be learned this is not really necessary but sometimes it is implemented in practice now this is exactly the entire procedure for how the multi-ad attention is implemented from scratch and here we saw the multi-head attention for the example which we have so the first token is again the the the second token is cat and the third token is sleeps me write this again yeah the third token is sleeps so you see through this entire procedure we obtained the enriched context Vector representation for these tokens similarly when you deal with large volumes of text you take sentences you break them down into tokens then into token IDs then into input embeddings and similar to this procedure you get context vectors for each token which you have ideally when we run the actual code we will have multiple batches but I showed only one batch right now for Simplicity so uh to whoever who have reached until this stage I want to say that thank you for following with me for so to many lectures I know these lectures are becoming very long but unless I explain every single thing in detail it's very difficult for you to understand all the details so congratulations if you have reached this lecture you have successfully understood how the multi-head attention works and I think there are very few people who really understand this entire piece of code block by block okay so I'll share this notebook with you and whatever I explain to you on the Whiteboard all the steps which which I laid out in front of you on the Whiteboard uh all of those have been explained here as step one to Step 11 and uh I have explained added a detailed explanation of the multi-head attention class in today's lecture I did not just want to read this but I wanted to construct a practical example to show you how the dimensions actually work and uh it took me a long time to make this example but now I think it's worth it because it really helped me explain it and I hope you understood it better so now we can actually test out the multi-head attention class so here are my inputs uh as I showed you on the Whiteboard we have three tokens and we have six the embedding Dimension is six the only change here what I'm going to do I'm going to create a batch so I'm going to going to create a batch of two such inputs and I'm going to stack this batch on top of each other so I'm going to assume a d out equal to six exactly what we saw on the Whiteboard and context length equal to six and then we are going to implement the multihead attention class so D in is equal to 6 uh D out is equal so D in is equal to six right because each um each token has the input embedding dimension of six D out equal to 6 the context length which we are using is equal to uh six then uh yeah the dropout rate which we are considering is zero we can even include the dropout rate so the dropout rate will change this this last layer of Dropout and randomly block out some attention weights this is good for generalization and the number of heads equal to two so we create an instance of this class and then create the context Vector Matrix and you'll see for the first batch the context Vector Matrix has three rows and six columns let's see if the shape matches what we had seen on the Whiteboard uh okay so let me scroll down below yeah this is the final context Vector Matrix which we had obtained yeah so this also had three rows okay I think I need to scroll up a bit yeah this is the final context Vector Matrix which we obtained and this also had three rows and six columns the values might be different because we have done the initializations differently here I have taken random initializations in the python code there are some other initializations every time you initialize we take from a goian distribution so the values might be different but let's check the shape so this is 3 comma 6 three rows and six columns and here also we can see that three rows and six columns awesome so the shape matches but you'll see that since there are two batches here's the context Vector Matrix for the first batch and here's the context Vector Matrix for the second batch so the multi-head attention class which we defined is extremely powerful because it can also handle multiple batches at once we can even do 50 data batches and then it will just have one 1 two it will have 50 such context Vector matrices okay that's it that brings me to the end of this section or the end of this lecture so in this lecture we implemented the multi-head attention class that we'll be using in the upcoming lectures to implement and train the llm this code is fully functional but we we used relatively small embedding sizes and number of attention heads to keep the outputs readable so as I showed you we only use two attention heads but gpt3 actually was 96 attention heads so the smallest gpt2 model had 12 attention heads and a context Vector embedding size of 768 the largest gpt2 model had 25 attention heads and a context Vector embedding size of 1600 and gpt3 has even higher so the gpt3 largest model has 96 attention heads I think and generally in GPT models the D in is equal to D out in the example which we saw D in was equal to D out equal to 6 but here the D in and D out are much larger around 768 Etc again thank you so much everyone for reaching the end of this attention series it's been one of the longest and most comprehensive series which I have covered and uh I really enjoyed learning about all of these things I can see that many llm practitioners cannot understand these Dimensions or they do not take time to go through understanding the theory the building blocks behind how the attention mechanism Works they just implement the code bases which are available which completely abstract away all of these things so I don't think that's the good way or the correct way to learn about large language models if you want to be a true llm engineer or a machine learning engineer you have to understand how nuts and bols work otherwise you might be able to deploy applications but to make real inventions you will have to go into the code base change a few things understand Dimensions as you might have seen and I have stressed this many times dimensions and linear algebra are at the heart of becoming a very strong ml engineer it all comes down to Dimensions other students might be scared of a four-dimensional tensor right but if you understand how it works based on what I showed to you on the Whiteboard my aim is that you should not be scared of these higher dimensional matrices once you write it down and once you understand what's going on it really becomes easy that's why I really recommend writing things down you can write on a whiteboard you can even write on a piece of paper but make sure you write things down then you'll remember them for a longer period of time I hope you all are enjoying these lectures thank you so much everyone and look forward to seeing you in the next next lecture where we'll actually start building the llm model thanks a lot"
}