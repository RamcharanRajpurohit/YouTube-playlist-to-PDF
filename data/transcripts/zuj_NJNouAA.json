{
  "video": {
    "video_id": "zuj_NJNouAA",
    "title": "Evaluating LLM performance on real dataset | Hands on project | Book data",
    "duration": 3516.0,
    "index": 26
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 3.51
    },
    {
      "text": "hello everyone and welcome to this",
      "start": 5.08,
      "duration": 4.28
    },
    {
      "text": "lecture in the build large language",
      "start": 7.439,
      "duration": 3.681
    },
    {
      "text": "models from scratch",
      "start": 9.36,
      "duration": 6.359
    },
    {
      "text": "series let me recap the stage of",
      "start": 11.12,
      "duration": 6.96
    },
    {
      "text": "building the llm in which we are at",
      "start": 15.719,
      "duration": 6.121
    },
    {
      "text": "currently in terms of a diagram okay so",
      "start": 18.08,
      "duration": 6.32
    },
    {
      "text": "we are in stage number two right now",
      "start": 21.84,
      "duration": 4.04
    },
    {
      "text": "based on the schematic which is",
      "start": 24.4,
      "duration": 3.52
    },
    {
      "text": "mentioned in front of you and in the",
      "start": 25.88,
      "duration": 3.639
    },
    {
      "text": "stage number two we are essentially",
      "start": 27.92,
      "duration": 3.92
    },
    {
      "text": "learning how to build this foundational",
      "start": 29.519,
      "duration": 5.04
    },
    {
      "text": "model or rather we are going to learn",
      "start": 31.84,
      "duration": 5.039
    },
    {
      "text": "how to train or pre-train a large",
      "start": 34.559,
      "duration": 4.601
    },
    {
      "text": "language model in this stage in the",
      "start": 36.879,
      "duration": 4.241
    },
    {
      "text": "previous stage in the all the previous",
      "start": 39.16,
      "duration": 3.8
    },
    {
      "text": "set of lectures we have looked at data",
      "start": 41.12,
      "duration": 4.68
    },
    {
      "text": "preparation attention mechanism and llm",
      "start": 42.96,
      "duration": 6.119
    },
    {
      "text": "architecture in fact in this stage we",
      "start": 45.8,
      "duration": 7.8
    },
    {
      "text": "even looked at um how to calculate the",
      "start": 49.079,
      "duration": 7.601
    },
    {
      "text": "loss in terms of a large language model",
      "start": 53.6,
      "duration": 5.52
    },
    {
      "text": "we looked at how to define the cross",
      "start": 56.68,
      "duration": 5.76
    },
    {
      "text": "entropy loss based on the input and",
      "start": 59.12,
      "duration": 6.28
    },
    {
      "text": "targets of a large language model and",
      "start": 62.44,
      "duration": 4.76
    },
    {
      "text": "that was the previous lecture which we",
      "start": 65.4,
      "duration": 4.96
    },
    {
      "text": "had now we will be looking at a much",
      "start": 67.2,
      "duration": 5.559
    },
    {
      "text": "larger data set in fact this is going to",
      "start": 70.36,
      "duration": 4.68
    },
    {
      "text": "be a very interesting Hands-On lecture",
      "start": 72.759,
      "duration": 3.961
    },
    {
      "text": "in which we are going to collect data",
      "start": 75.04,
      "duration": 5.24
    },
    {
      "text": "from a story book and we are going toh",
      "start": 76.72,
      "duration": 5.48
    },
    {
      "text": "make predictions using the large",
      "start": 80.28,
      "duration": 3.92
    },
    {
      "text": "language model which we have built in",
      "start": 82.2,
      "duration": 5.4
    },
    {
      "text": "the first stage and then we are going to",
      "start": 84.2,
      "duration": 5.48
    },
    {
      "text": "measure the loss of our large language",
      "start": 87.6,
      "duration": 2.879
    },
    {
      "text": "model",
      "start": 89.68,
      "duration": 3.479
    },
    {
      "text": "model based on the actual data which we",
      "start": 90.479,
      "duration": 5.28
    },
    {
      "text": "need so in the previous lecture what we",
      "start": 93.159,
      "duration": 4.96
    },
    {
      "text": "did is we looked at some simple examples",
      "start": 95.759,
      "duration": 4.801
    },
    {
      "text": "we looked at just two inputs the first",
      "start": 98.119,
      "duration": 4.081
    },
    {
      "text": "input which we looked at was every",
      "start": 100.56,
      "duration": 4.68
    },
    {
      "text": "effort moves and the second input was I",
      "start": 102.2,
      "duration": 5.12
    },
    {
      "text": "really like we looked at these two",
      "start": 105.24,
      "duration": 4.68
    },
    {
      "text": "inputs and we predicted outputs from our",
      "start": 107.32,
      "duration": 4.52
    },
    {
      "text": "large language model and then we",
      "start": 109.92,
      "duration": 3.519
    },
    {
      "text": "calculated the loss between those",
      "start": 111.84,
      "duration": 3.959
    },
    {
      "text": "outputs and the target values which we",
      "start": 113.439,
      "duration": 5.04
    },
    {
      "text": "need now what we are going to do is we",
      "start": 115.799,
      "duration": 5.28
    },
    {
      "text": "are going to look at an actual data set",
      "start": 118.479,
      "duration": 4.361
    },
    {
      "text": "and on that data set we are going to",
      "start": 121.079,
      "duration": 3.841
    },
    {
      "text": "find the training loss and we are going",
      "start": 122.84,
      "duration": 3.8
    },
    {
      "text": "to find the validation",
      "start": 124.92,
      "duration": 4.36
    },
    {
      "text": "loss so if you zoom into stage two of",
      "start": 126.64,
      "duration": 5.04
    },
    {
      "text": "building an llm further in the previous",
      "start": 129.28,
      "duration": 3.72
    },
    {
      "text": "lecture we have looked at text",
      "start": 131.68,
      "duration": 4.32
    },
    {
      "text": "generation and text evaluation so we saw",
      "start": 133.0,
      "duration": 5.519
    },
    {
      "text": "the cross entropy loss in the previous",
      "start": 136.0,
      "duration": 5.12
    },
    {
      "text": "lecture and how to calculate it for just",
      "start": 138.519,
      "duration": 5.201
    },
    {
      "text": "two simple input text today we are going",
      "start": 141.12,
      "duration": 4.96
    },
    {
      "text": "to calculate the training and validation",
      "start": 143.72,
      "duration": 5.4
    },
    {
      "text": "losses on the entire data set we'll also",
      "start": 146.08,
      "duration": 6.08
    },
    {
      "text": "split the data set into training set and",
      "start": 149.12,
      "duration": 5.56
    },
    {
      "text": "validation set so let's get started it",
      "start": 152.16,
      "duration": 4.88
    },
    {
      "text": "will be a completely Hands-On lecture",
      "start": 154.68,
      "duration": 4.639
    },
    {
      "text": "and after this lecture actually you will",
      "start": 157.04,
      "duration": 4.559
    },
    {
      "text": "be equipped to take any story book of",
      "start": 159.319,
      "duration": 4.881
    },
    {
      "text": "your choice or for that matter any data",
      "start": 161.599,
      "duration": 6.401
    },
    {
      "text": "set and uh train the large language",
      "start": 164.2,
      "duration": 5.8
    },
    {
      "text": "model not really train it because we are",
      "start": 168.0,
      "duration": 3.64
    },
    {
      "text": "not training the parameters in this",
      "start": 170.0,
      "duration": 4.28
    },
    {
      "text": "lecture do a forward pass of the large",
      "start": 171.64,
      "duration": 4.4
    },
    {
      "text": "language model get the",
      "start": 174.28,
      "duration": 5.959
    },
    {
      "text": "outputs and then get the loss based on",
      "start": 176.04,
      "duration": 6.16
    },
    {
      "text": "the True Values which you need and you",
      "start": 180.239,
      "duration": 4.36
    },
    {
      "text": "will get this loss for any data set",
      "start": 182.2,
      "duration": 4.64
    },
    {
      "text": "which you use you'll just have to follow",
      "start": 184.599,
      "duration": 4.441
    },
    {
      "text": "the same code which I'll provide you at",
      "start": 186.84,
      "duration": 4.72
    },
    {
      "text": "the end of this lecture we have not yet",
      "start": 189.04,
      "duration": 4.559
    },
    {
      "text": "started doing the training procedure but",
      "start": 191.56,
      "duration": 4.039
    },
    {
      "text": "once you get this loss for the data set",
      "start": 193.599,
      "duration": 3.761
    },
    {
      "text": "in the next lecture we are going to look",
      "start": 195.599,
      "duration": 4.64
    },
    {
      "text": "at the llm training function where we'll",
      "start": 197.36,
      "duration": 5.76
    },
    {
      "text": "also dive dive deep into back",
      "start": 200.239,
      "duration": 5.041
    },
    {
      "text": "propagation so at the end of today's",
      "start": 203.12,
      "duration": 4.44
    },
    {
      "text": "lecture you will have a very cool result",
      "start": 205.28,
      "duration": 3.959
    },
    {
      "text": "which is applicable to a wide range of",
      "start": 207.56,
      "duration": 2.759
    },
    {
      "text": "data sets",
      "start": 209.239,
      "duration": 3.36
    },
    {
      "text": "so let's get started the data set which",
      "start": 210.319,
      "duration": 4.64
    },
    {
      "text": "I'm going to consider is called the",
      "start": 212.599,
      "duration": 4.961
    },
    {
      "text": "verdict it's a book written by an author",
      "start": 214.959,
      "duration": 5.28
    },
    {
      "text": "named edit Warton and this book was",
      "start": 217.56,
      "duration": 5.92
    },
    {
      "text": "published in 1906 I think here's how the",
      "start": 220.239,
      "duration": 6.761
    },
    {
      "text": "book looks like and uh the book is",
      "start": 223.48,
      "duration": 5.2
    },
    {
      "text": "publicly available to download I'll",
      "start": 227.0,
      "duration": 3.72
    },
    {
      "text": "share the link you can download the book",
      "start": 228.68,
      "duration": 5.08
    },
    {
      "text": "from this link and it's not a very long",
      "start": 230.72,
      "duration": 5.159
    },
    {
      "text": "book it's a pretty short book in fact",
      "start": 233.76,
      "duration": 3.92
    },
    {
      "text": "but we are using this just because I",
      "start": 235.879,
      "duration": 5.041
    },
    {
      "text": "want to demonstrate uh an example which",
      "start": 237.68,
      "duration": 5.839
    },
    {
      "text": "runs very fast on my laptop and it will",
      "start": 240.92,
      "duration": 5.72
    },
    {
      "text": "run very fast on your laptop also in",
      "start": 243.519,
      "duration": 5.44
    },
    {
      "text": "fact if you take a look at this data set",
      "start": 246.64,
      "duration": 4.319
    },
    {
      "text": "and if you count the number of",
      "start": 248.959,
      "duration": 4.241
    },
    {
      "text": "characters you'll see that the number of",
      "start": 250.959,
      "duration": 5.96
    },
    {
      "text": "characters in this data set is 20,000",
      "start": 253.2,
      "duration": 7.879
    },
    {
      "text": "characters what we'll do uh on the data",
      "start": 256.919,
      "duration": 6.041
    },
    {
      "text": "set first is that we'll first convert",
      "start": 261.079,
      "duration": 5.001
    },
    {
      "text": "this data set into tokens so we'll use a",
      "start": 262.96,
      "duration": 4.959
    },
    {
      "text": "tokenization scheme for that which is",
      "start": 266.08,
      "duration": 5.52
    },
    {
      "text": "called as bite pair en code B B pair",
      "start": 267.919,
      "duration": 6.201
    },
    {
      "text": "encoding so bite pair encoding is the",
      "start": 271.6,
      "duration": 4.2
    },
    {
      "text": "tokenization scheme which we are going",
      "start": 274.12,
      "duration": 5.04
    },
    {
      "text": "to use and what this will do is that in",
      "start": 275.8,
      "duration": 5.839
    },
    {
      "text": "bite pair encoding one word is not one",
      "start": 279.16,
      "duration": 5.8
    },
    {
      "text": "token it's a subword based tokenization",
      "start": 281.639,
      "duration": 5.961
    },
    {
      "text": "scheme where even characters can be",
      "start": 284.96,
      "duration": 6.6
    },
    {
      "text": "tokens just uh pairs of letters such as",
      "start": 287.6,
      "duration": 6.92
    },
    {
      "text": "T and H this can be one token just T can",
      "start": 291.56,
      "duration": 6.479
    },
    {
      "text": "be one token Etc so if you use bite pair",
      "start": 294.52,
      "duration": 5.239
    },
    {
      "text": "encoding you'll find that this data",
      "start": 298.039,
      "duration": 4.321
    },
    {
      "text": "asset has 5,000 tokens don't worry I'm",
      "start": 299.759,
      "duration": 4.72
    },
    {
      "text": "going to show all of this in the code",
      "start": 302.36,
      "duration": 5.76
    },
    {
      "text": "but just if you search tick token you'll",
      "start": 304.479,
      "duration": 6.201
    },
    {
      "text": "see that uh this is the bite pair",
      "start": 308.12,
      "duration": 4.799
    },
    {
      "text": "encoder which we are using and it's the",
      "start": 310.68,
      "duration": 5.4
    },
    {
      "text": "same encoder which open AI actually used",
      "start": 312.919,
      "duration": 5.881
    },
    {
      "text": "and using this encoder we can encode our",
      "start": 316.08,
      "duration": 6.16
    },
    {
      "text": "data uh data set into",
      "start": 318.8,
      "duration": 6.119
    },
    {
      "text": "tokens great so this is the data set",
      "start": 322.24,
      "duration": 4.16
    },
    {
      "text": "which I'm considering and remember you",
      "start": 324.919,
      "duration": 3.601
    },
    {
      "text": "can use any data set which you want on",
      "start": 326.4,
      "duration": 4.359
    },
    {
      "text": "publicly available",
      "start": 328.52,
      "duration": 4.72
    },
    {
      "text": "books now the next step what we are",
      "start": 330.759,
      "duration": 4.681
    },
    {
      "text": "going to do is that uh we are going to",
      "start": 333.24,
      "duration": 4.16
    },
    {
      "text": "divide the data set into training and",
      "start": 335.44,
      "duration": 4.08
    },
    {
      "text": "validation remember this is the same",
      "start": 337.4,
      "duration": 3.48
    },
    {
      "text": "thing what we do for all machine",
      "start": 339.52,
      "duration": 3.56
    },
    {
      "text": "learning problems because the training",
      "start": 340.88,
      "duration": 4.24
    },
    {
      "text": "loss is not the one which really matters",
      "start": 343.08,
      "duration": 4.08
    },
    {
      "text": "what matters is how our large language",
      "start": 345.12,
      "duration": 4.359
    },
    {
      "text": "model is doing on text which it has not",
      "start": 347.16,
      "duration": 4.92
    },
    {
      "text": "seen before so we'll do a simple thing",
      "start": 349.479,
      "duration": 4.801
    },
    {
      "text": "over here let's say this is the entire",
      "start": 352.08,
      "duration": 5.119
    },
    {
      "text": "data set we are going to use a train",
      "start": 354.28,
      "duration": 5.84
    },
    {
      "text": "test ratio of90 and we we are just going",
      "start": 357.199,
      "duration": 4.881
    },
    {
      "text": "to do a simple split the training data",
      "start": 360.12,
      "duration": 5.799
    },
    {
      "text": "will be the first 90% of the input and",
      "start": 362.08,
      "duration": 7.239
    },
    {
      "text": "the validation data will be the latter",
      "start": 365.919,
      "duration": 6.921
    },
    {
      "text": "half so the remaining 10%",
      "start": 369.319,
      "duration": 5.681
    },
    {
      "text": "right um and that's how we are going to",
      "start": 372.84,
      "duration": 3.84
    },
    {
      "text": "split the training and the validation",
      "start": 375.0,
      "duration": 3.919
    },
    {
      "text": "set so if you imagine the entire data to",
      "start": 376.68,
      "duration": 4.68
    },
    {
      "text": "be like this what I'm going to do is",
      "start": 378.919,
      "duration": 6.161
    },
    {
      "text": "that I'm going to reserve the first 90%",
      "start": 381.36,
      "duration": 7.08
    },
    {
      "text": "so this is 90% And this is 10% so I'm",
      "start": 385.08,
      "duration": 5.32
    },
    {
      "text": "going to reserve this for training",
      "start": 388.44,
      "duration": 4.28
    },
    {
      "text": "purposes and I'm going to reserve the",
      "start": 390.4,
      "duration": 5.919
    },
    {
      "text": "10% for validation or testing",
      "start": 392.72,
      "duration": 6.199
    },
    {
      "text": "purpose now what I'm going to do after",
      "start": 396.319,
      "duration": 5.641
    },
    {
      "text": "this is pretty interesting usually in",
      "start": 398.919,
      "duration": 5.241
    },
    {
      "text": "other machine learning problems getting",
      "start": 401.96,
      "duration": 3.84
    },
    {
      "text": "the inputs and the outputs is pretty",
      "start": 404.16,
      "duration": 3.68
    },
    {
      "text": "easy right you just have images of cats",
      "start": 405.8,
      "duration": 3.88
    },
    {
      "text": "and dogs and the output is whether it's",
      "start": 407.84,
      "duration": 3.96
    },
    {
      "text": "a cat or whether it's a dog so it's",
      "start": 409.68,
      "duration": 3.68
    },
    {
      "text": "pretty simple you have inputs which are",
      "start": 411.8,
      "duration": 3.08
    },
    {
      "text": "images and you have outputs which are",
      "start": 413.36,
      "duration": 3.72
    },
    {
      "text": "the labels you don't have to do anything",
      "start": 414.88,
      "duration": 3.999
    },
    {
      "text": "special to create these input output",
      "start": 417.08,
      "duration": 4.32
    },
    {
      "text": "pairs but it's not as simple in the case",
      "start": 418.879,
      "duration": 4.521
    },
    {
      "text": "of a large language model because large",
      "start": 421.4,
      "duration": 4.44
    },
    {
      "text": "language models are Auto regressive",
      "start": 423.4,
      "duration": 5.359
    },
    {
      "text": "models we don't uh label anything",
      "start": 425.84,
      "duration": 4.919
    },
    {
      "text": "beforehand but from the text itself we",
      "start": 428.759,
      "duration": 4.801
    },
    {
      "text": "construct the inputs and the output and",
      "start": 430.759,
      "duration": 5.28
    },
    {
      "text": "so we'll need to understand the process",
      "start": 433.56,
      "duration": 4.28
    },
    {
      "text": "through which we conu construct these",
      "start": 436.039,
      "duration": 3.201
    },
    {
      "text": "input output",
      "start": 437.84,
      "duration": 4.359
    },
    {
      "text": "pairs so in the code we are going to use",
      "start": 439.24,
      "duration": 5.359
    },
    {
      "text": "the data loader to chunk the training",
      "start": 442.199,
      "duration": 4.72
    },
    {
      "text": "and the validation data into input and",
      "start": 444.599,
      "duration": 4.44
    },
    {
      "text": "output data sets or input and output",
      "start": 446.919,
      "duration": 4.481
    },
    {
      "text": "pairs and let me show you how I'm going",
      "start": 449.039,
      "duration": 4.28
    },
    {
      "text": "to do that if you understand this part",
      "start": 451.4,
      "duration": 3.44
    },
    {
      "text": "it will be much easier for you to",
      "start": 453.319,
      "duration": 4.361
    },
    {
      "text": "visualize what comes next right so the",
      "start": 454.84,
      "duration": 4.32
    },
    {
      "text": "first thing which we have to decide is",
      "start": 457.68,
      "duration": 3.199
    },
    {
      "text": "what is the context size which I'm going",
      "start": 459.16,
      "duration": 4.2
    },
    {
      "text": "to use in other words what is the",
      "start": 460.879,
      "duration": 4.921
    },
    {
      "text": "maximum number of tokens the llm can see",
      "start": 463.36,
      "duration": 5.399
    },
    {
      "text": "before it predicts the next token so I'm",
      "start": 465.8,
      "duration": 4.64
    },
    {
      "text": "going to show you how to construct the",
      "start": 468.759,
      "duration": 3.961
    },
    {
      "text": "input output data pairs on this data set",
      "start": 470.44,
      "duration": 5.039
    },
    {
      "text": "using a context size of four so that's",
      "start": 472.72,
      "duration": 4.72
    },
    {
      "text": "the first thing which I need to decide",
      "start": 475.479,
      "duration": 5.84
    },
    {
      "text": "context size equal to four",
      "start": 477.44,
      "duration": 6.759
    },
    {
      "text": "okay great so now let me look at this",
      "start": 481.319,
      "duration": 4.761
    },
    {
      "text": "data set and here's how I create the",
      "start": 484.199,
      "duration": 4.641
    },
    {
      "text": "input output pairs my first input is",
      "start": 486.08,
      "duration": 5.079
    },
    {
      "text": "this I had always",
      "start": 488.84,
      "duration": 5.16
    },
    {
      "text": "thought and since the context size is",
      "start": 491.159,
      "duration": 4.681
    },
    {
      "text": "four I'm looking at four tokens at a",
      "start": 494.0,
      "duration": 3.84
    },
    {
      "text": "time and although one word is not one",
      "start": 495.84,
      "duration": 4.72
    },
    {
      "text": "token I'm just assuming it here for the",
      "start": 497.84,
      "duration": 4.96
    },
    {
      "text": "sake of demonstration so this is my",
      "start": 500.56,
      "duration": 4.24
    },
    {
      "text": "first input and I'm going to label it",
      "start": 502.8,
      "duration": 4.119
    },
    {
      "text": "let me use the same color here and I'm",
      "start": 504.8,
      "duration": 5.119
    },
    {
      "text": "going to label this as X1 right this is",
      "start": 506.919,
      "duration": 5.12
    },
    {
      "text": "my first input now what's the first",
      "start": 509.919,
      "duration": 4.521
    },
    {
      "text": "output the first output is just this",
      "start": 512.039,
      "duration": 5.761
    },
    {
      "text": "input shifted by one so then it will be",
      "start": 514.44,
      "duration": 6.32
    },
    {
      "text": "this had always thought",
      "start": 517.8,
      "duration": 6.56
    },
    {
      "text": "Jack this is y1 let me write this down",
      "start": 520.76,
      "duration": 8.0
    },
    {
      "text": "over here so uh ultimately the input",
      "start": 524.36,
      "duration": 6.28
    },
    {
      "text": "will be a tensor X and I'm currently I'm",
      "start": 528.76,
      "duration": 3.8
    },
    {
      "text": "just writing the first row of this so",
      "start": 530.64,
      "duration": 4.319
    },
    {
      "text": "this will be I",
      "start": 532.56,
      "duration": 7.519
    },
    {
      "text": "had always thought that's my first input",
      "start": 534.959,
      "duration": 7.961
    },
    {
      "text": "put and let me also collect the output",
      "start": 540.079,
      "duration": 5.601
    },
    {
      "text": "tensor over here with a different color",
      "start": 542.92,
      "duration": 6.479
    },
    {
      "text": "of course and uh this will be",
      "start": 545.68,
      "duration": 6.24
    },
    {
      "text": "had",
      "start": 549.399,
      "duration": 7.801
    },
    {
      "text": "always thought Jack right now let us",
      "start": 551.92,
      "duration": 7.28
    },
    {
      "text": "focus on this first input output pair",
      "start": 557.2,
      "duration": 4.48
    },
    {
      "text": "for just a moment the first thing to",
      "start": 559.2,
      "duration": 4.28
    },
    {
      "text": "notice is that of course the output is",
      "start": 561.68,
      "duration": 3.719
    },
    {
      "text": "just the input shifted to the right by",
      "start": 563.48,
      "duration": 4.2
    },
    {
      "text": "one but another thing to notice is that",
      "start": 565.399,
      "duration": 4.161
    },
    {
      "text": "this one input output pair essentially",
      "start": 567.68,
      "duration": 4.24
    },
    {
      "text": "has four prediction tasks what are the",
      "start": 569.56,
      "duration": 4.399
    },
    {
      "text": "four prediction tasks the first is that",
      "start": 571.92,
      "duration": 6.32
    },
    {
      "text": "when I is the input had is the output so",
      "start": 573.959,
      "duration": 6.801
    },
    {
      "text": "the index corresponds that way when I",
      "start": 578.24,
      "duration": 5.68
    },
    {
      "text": "had is the input always is the output",
      "start": 580.76,
      "duration": 5.4
    },
    {
      "text": "when I had always is the input so when",
      "start": 583.92,
      "duration": 3.919
    },
    {
      "text": "these three are the input thought is the",
      "start": 586.16,
      "duration": 5.0
    },
    {
      "text": "output and when I had always thought is",
      "start": 587.839,
      "duration": 4.361
    },
    {
      "text": "the",
      "start": 591.16,
      "duration": 5.0
    },
    {
      "text": "input then Jack is the output right so",
      "start": 592.2,
      "duration": 6.56
    },
    {
      "text": "essentially when X is given an input",
      "start": 596.16,
      "duration": 4.76
    },
    {
      "text": "when we pass it through an llm it will",
      "start": 598.76,
      "duration": 4.68
    },
    {
      "text": "produce these four tokens as the",
      "start": 600.92,
      "duration": 5.88
    },
    {
      "text": "output uh and then the output produced",
      "start": 603.44,
      "duration": 5.399
    },
    {
      "text": "by llm will be compared to this which is",
      "start": 606.8,
      "duration": 4.84
    },
    {
      "text": "the actual output which we want for this",
      "start": 608.839,
      "duration": 5.321
    },
    {
      "text": "input let me repeat that again if this",
      "start": 611.64,
      "duration": 4.56
    },
    {
      "text": "is an input I had always thought the",
      "start": 614.16,
      "duration": 4.04
    },
    {
      "text": "actual output which we want is had",
      "start": 616.2,
      "duration": 4.4
    },
    {
      "text": "always thought Jack but you'll see that",
      "start": 618.2,
      "duration": 4.0
    },
    {
      "text": "when you pass these four tokens through",
      "start": 620.6,
      "duration": 3.0
    },
    {
      "text": "the large language model it's not",
      "start": 622.2,
      "duration": 3.44
    },
    {
      "text": "trained currently right so it will",
      "start": 623.6,
      "duration": 4.799
    },
    {
      "text": "predict some random words over here and",
      "start": 625.64,
      "duration": 4.52
    },
    {
      "text": "then we have to find the loss between",
      "start": 628.399,
      "duration": 4.521
    },
    {
      "text": "those random tokens and what we want",
      "start": 630.16,
      "duration": 4.72
    },
    {
      "text": "that's how you get the loss between the",
      "start": 632.92,
      "duration": 4.0
    },
    {
      "text": "first input pair and the second first",
      "start": 634.88,
      "duration": 4.68
    },
    {
      "text": "input and the first output right now",
      "start": 636.92,
      "duration": 5.08
    },
    {
      "text": "let's move ahead a bit and let let us",
      "start": 639.56,
      "duration": 4.56
    },
    {
      "text": "see how to construct the",
      "start": 642.0,
      "duration": 4.44
    },
    {
      "text": "second uh second input so I'm going to",
      "start": 644.12,
      "duration": 5.519
    },
    {
      "text": "rub this output pair right now uh okay",
      "start": 646.44,
      "duration": 5.199
    },
    {
      "text": "so we have the first input now we have",
      "start": 649.639,
      "duration": 3.521
    },
    {
      "text": "an option of how we are going to",
      "start": 651.639,
      "duration": 3.801
    },
    {
      "text": "actually construct the second input and",
      "start": 653.16,
      "duration": 5.72
    },
    {
      "text": "let me show you uh how we are going to",
      "start": 655.44,
      "duration": 4.44
    },
    {
      "text": "do that",
      "start": 658.88,
      "duration": 3.399
    },
    {
      "text": "okay so the first option is that you",
      "start": 659.88,
      "duration": 5.12
    },
    {
      "text": "just move it to the right so then X1 is",
      "start": 662.279,
      "duration": 5.401
    },
    {
      "text": "I had always thought right X2 will be",
      "start": 665.0,
      "duration": 4.2
    },
    {
      "text": "had always thought",
      "start": 667.68,
      "duration": 4.159
    },
    {
      "text": "Jack now this movement which we are",
      "start": 669.2,
      "duration": 5.4
    },
    {
      "text": "going to do is also called as",
      "start": 671.839,
      "duration": 5.24
    },
    {
      "text": "stride and that's the second parameter",
      "start": 674.6,
      "duration": 3.88
    },
    {
      "text": "which you have to decide along with the",
      "start": 677.079,
      "duration": 4.281
    },
    {
      "text": "context size so if you put stride equal",
      "start": 678.48,
      "duration": 4.599
    },
    {
      "text": "to one like I have done in this case the",
      "start": 681.36,
      "duration": 4.2
    },
    {
      "text": "second input will have will be X2 and",
      "start": 683.079,
      "duration": 4.241
    },
    {
      "text": "that has lot of overlap with the first",
      "start": 685.56,
      "duration": 3.32
    },
    {
      "text": "input right had always thought is",
      "start": 687.32,
      "duration": 3.72
    },
    {
      "text": "overlap",
      "start": 688.88,
      "duration": 5.0
    },
    {
      "text": "um so you can also do this but usually",
      "start": 691.04,
      "duration": 5.12
    },
    {
      "text": "What's Done in models such as GPT is",
      "start": 693.88,
      "duration": 4.56
    },
    {
      "text": "that the stride",
      "start": 696.16,
      "duration": 4.64
    },
    {
      "text": "which uh we are going to Define right",
      "start": 698.44,
      "duration": 5.36
    },
    {
      "text": "now is is fixed to be equal to the",
      "start": 700.8,
      "duration": 6.479
    },
    {
      "text": "context size and that is equal to four",
      "start": 703.8,
      "duration": 5.279
    },
    {
      "text": "so we are going to use a stride of four",
      "start": 707.279,
      "duration": 4.481
    },
    {
      "text": "what this will do is that when X1 if X1",
      "start": 709.079,
      "duration": 6.121
    },
    {
      "text": "is this input we are going to have 1 2 3",
      "start": 711.76,
      "duration": 6.639
    },
    {
      "text": "and four so then X2 will start from here",
      "start": 715.2,
      "duration": 5.28
    },
    {
      "text": "so then X2 will will be Jack gisburn",
      "start": 718.399,
      "duration": 5.721
    },
    {
      "text": "rather a this will be X2 you see what",
      "start": 720.48,
      "duration": 6.08
    },
    {
      "text": "stride equal to 4 does it it makes sure",
      "start": 724.12,
      "duration": 4.32
    },
    {
      "text": "that we don't have any overlap but it",
      "start": 726.56,
      "duration": 3.68
    },
    {
      "text": "will make sure that we also don't skip",
      "start": 728.44,
      "duration": 4.28
    },
    {
      "text": "any token as the input when you look at",
      "start": 730.24,
      "duration": 4.399
    },
    {
      "text": "X3 this will be",
      "start": 732.72,
      "duration": 4.6
    },
    {
      "text": "the um this will be X3 which will be the",
      "start": 734.639,
      "duration": 5.2
    },
    {
      "text": "third input when you look at",
      "start": 737.32,
      "duration": 5.0
    },
    {
      "text": "X4 uh this will be X4 which is the",
      "start": 739.839,
      "duration": 6.24
    },
    {
      "text": "fourth input right uh this is how the",
      "start": 742.32,
      "duration": 6.44
    },
    {
      "text": "inputs are created and you don't skip",
      "start": 746.079,
      "duration": 4.081
    },
    {
      "text": "anything",
      "start": 748.76,
      "duration": 2.759
    },
    {
      "text": "but also you make sure there are no",
      "start": 750.16,
      "duration": 3.44
    },
    {
      "text": "overlaps between inputs so now if you",
      "start": 751.519,
      "duration": 4.44
    },
    {
      "text": "look at the input tensor the first is I",
      "start": 753.6,
      "duration": 5.08
    },
    {
      "text": "had always thought right the second will",
      "start": 755.959,
      "duration": 7.361
    },
    {
      "text": "be Jack gizan rather a",
      "start": 758.68,
      "duration": 4.64
    },
    {
      "text": "jack",
      "start": 763.519,
      "duration": 5.921
    },
    {
      "text": "gisburn rather o that's the second input",
      "start": 765.6,
      "duration": 5.919
    },
    {
      "text": "which is X2 the third input would be",
      "start": 769.44,
      "duration": 5.24
    },
    {
      "text": "cheap genius dash dash do so the third",
      "start": 771.519,
      "duration": 4.401
    },
    {
      "text": "will be",
      "start": 774.68,
      "duration": 5.279
    },
    {
      "text": "cheap genius dash dash do",
      "start": 775.92,
      "duration": 6.4
    },
    {
      "text": "so that's how this entire input tensor",
      "start": 779.959,
      "duration": 4.44
    },
    {
      "text": "Matrix will be created sorry this input",
      "start": 782.32,
      "duration": 4.48
    },
    {
      "text": "tensor will be created like this we'll",
      "start": 784.399,
      "duration": 4.321
    },
    {
      "text": "stride through the entire data set and",
      "start": 786.8,
      "duration": 3.56
    },
    {
      "text": "we'll collect these pairs so then the",
      "start": 788.72,
      "duration": 4.119
    },
    {
      "text": "first row will be X1 the second row will",
      "start": 790.36,
      "duration": 4.88
    },
    {
      "text": "be X2 the third row will be X3 and we'll",
      "start": 792.839,
      "duration": 4.281
    },
    {
      "text": "keep on accumulating these rows until we",
      "start": 795.24,
      "duration": 4.68
    },
    {
      "text": "reach the end and how are the outputs",
      "start": 797.12,
      "duration": 5.399
    },
    {
      "text": "created once the inputs are created the",
      "start": 799.92,
      "duration": 5.039
    },
    {
      "text": "output is just the input shifted by one",
      "start": 802.519,
      "duration": 4.281
    },
    {
      "text": "so for the first input I had always",
      "start": 804.959,
      "duration": 3.481
    },
    {
      "text": "thought the output was had always",
      "start": 806.8,
      "duration": 2.92
    },
    {
      "text": "thought Jack",
      "start": 808.44,
      "duration": 3.92
    },
    {
      "text": "right now Jack gisburn rather a if this",
      "start": 809.72,
      "duration": 4.919
    },
    {
      "text": "is the input the output would",
      "start": 812.36,
      "duration": 4.599
    },
    {
      "text": "be",
      "start": 814.639,
      "duration": 6.0
    },
    {
      "text": "gisburn gisburn rather a",
      "start": 816.959,
      "duration": 7.24
    },
    {
      "text": "cheap so this will be Y2 the first row",
      "start": 820.639,
      "duration": 6.481
    },
    {
      "text": "will be y1 and similarly we'll construct",
      "start": 824.199,
      "duration": 5.161
    },
    {
      "text": "all the other outputs output",
      "start": 827.12,
      "duration": 4.839
    },
    {
      "text": "pairs till we reach the end of the data",
      "start": 829.36,
      "duration": 5.2
    },
    {
      "text": "set so that is how input output pairs",
      "start": 831.959,
      "duration": 5.961
    },
    {
      "text": "are created in the case of a uh large",
      "start": 834.56,
      "duration": 5.36
    },
    {
      "text": "language model so if you have a data set",
      "start": 837.92,
      "duration": 3.8
    },
    {
      "text": "like this what's very important is this",
      "start": 839.92,
      "duration": 5.68
    },
    {
      "text": "X and this y so the x is the input and",
      "start": 841.72,
      "duration": 6.239
    },
    {
      "text": "the Y is the target which is the actual",
      "start": 845.6,
      "duration": 5.28
    },
    {
      "text": "value so let me write this here x is the",
      "start": 847.959,
      "duration": 6.56
    },
    {
      "text": "input and Y is the Target now the loss",
      "start": 850.88,
      "duration": 5.639
    },
    {
      "text": "will be between the llm output which we",
      "start": 854.519,
      "duration": 4.401
    },
    {
      "text": "have not seen yet but we'll also get llm",
      "start": 856.519,
      "duration": 6.32
    },
    {
      "text": "output so this input will be passed into",
      "start": 858.92,
      "duration": 7.76
    },
    {
      "text": "this input will be passed into our llm",
      "start": 862.839,
      "duration": 6.92
    },
    {
      "text": "model and then we'll get the output from",
      "start": 866.68,
      "duration": 5.519
    },
    {
      "text": "the llm right and that will also be a",
      "start": 869.759,
      "duration": 4.88
    },
    {
      "text": "tensor with the same format as this",
      "start": 872.199,
      "duration": 4.88
    },
    {
      "text": "target tensor and then what we are going",
      "start": 874.639,
      "duration": 4.64
    },
    {
      "text": "to do is that we are going to then find",
      "start": 877.079,
      "duration": 5.32
    },
    {
      "text": "the loss between the we are going to",
      "start": 879.279,
      "duration": 5.401
    },
    {
      "text": "find the loss between the llm output and",
      "start": 882.399,
      "duration": 4.56
    },
    {
      "text": "the Target and this is the loss which we",
      "start": 884.68,
      "duration": 4.56
    },
    {
      "text": "are going to find in today's lecture and",
      "start": 886.959,
      "duration": 3.8
    },
    {
      "text": "this is the loss which we eventually",
      "start": 889.24,
      "duration": 5.279
    },
    {
      "text": "want to minimize okay I hope you have",
      "start": 890.759,
      "duration": 5.44
    },
    {
      "text": "understood this this part and I",
      "start": 894.519,
      "duration": 3.841
    },
    {
      "text": "deliberately wanted to show you visually",
      "start": 896.199,
      "duration": 3.76
    },
    {
      "text": "because students are generally quite",
      "start": 898.36,
      "duration": 4.88
    },
    {
      "text": "unclear regarding how input output pairs",
      "start": 899.959,
      "duration": 4.961
    },
    {
      "text": "are created in the context of large",
      "start": 903.24,
      "duration": 4.159
    },
    {
      "text": "language models now here I showed you",
      "start": 904.92,
      "duration": 4.68
    },
    {
      "text": "the input input Target pairs I should",
      "start": 907.399,
      "duration": 3.88
    },
    {
      "text": "call them here I showed you the input",
      "start": 909.6,
      "duration": 4.359
    },
    {
      "text": "Target pairs for the training data right",
      "start": 911.279,
      "duration": 4.601
    },
    {
      "text": "these are the input and let me call them",
      "start": 913.959,
      "duration": 4.56
    },
    {
      "text": "targets because outputs will be what I",
      "start": 915.88,
      "duration": 4.519
    },
    {
      "text": "refer to as the llm",
      "start": 918.519,
      "duration": 5.0
    },
    {
      "text": "predictions so I created the input and",
      "start": 920.399,
      "duration": 5.36
    },
    {
      "text": "the target pairs for the training data",
      "start": 923.519,
      "duration": 3.88
    },
    {
      "text": "similarly we'll have the input and the",
      "start": 925.759,
      "duration": 3.88
    },
    {
      "text": "target pairs for the validation dat data",
      "start": 927.399,
      "duration": 4.44
    },
    {
      "text": "and that will give us the validation",
      "start": 929.639,
      "duration": 5.2
    },
    {
      "text": "loss the input and Target pairs will",
      "start": 931.839,
      "duration": 5.44
    },
    {
      "text": "give us the train loss for the training",
      "start": 934.839,
      "duration": 5.201
    },
    {
      "text": "data and the similar tensors will give",
      "start": 937.279,
      "duration": 4.36
    },
    {
      "text": "us the validation loss for the",
      "start": 940.04,
      "duration": 4.799
    },
    {
      "text": "validation data now let me explain to",
      "start": 941.639,
      "duration": 5.361
    },
    {
      "text": "you the rest of the process and then",
      "start": 944.839,
      "duration": 4.161
    },
    {
      "text": "we'll I'll take you through code but",
      "start": 947.0,
      "duration": 3.88
    },
    {
      "text": "first I want you to understand how",
      "start": 949.0,
      "duration": 4.48
    },
    {
      "text": "exactly is the loss function calculated",
      "start": 950.88,
      "duration": 5.48
    },
    {
      "text": "so that uh the code becomes so much more",
      "start": 953.48,
      "duration": 5.12
    },
    {
      "text": "easier to understand now let's say you",
      "start": 956.36,
      "duration": 4.599
    },
    {
      "text": "create the input and Target pairs like",
      "start": 958.6,
      "duration": 4.359
    },
    {
      "text": "this which is the first step first you",
      "start": 960.959,
      "duration": 4.081
    },
    {
      "text": "look at the first row of the input and",
      "start": 962.959,
      "duration": 5.081
    },
    {
      "text": "the first row of the target uh and then",
      "start": 965.04,
      "duration": 5.32
    },
    {
      "text": "let's see how do we get the loss so",
      "start": 968.04,
      "duration": 4.44
    },
    {
      "text": "let's say let's look at the first input",
      "start": 970.36,
      "duration": 3.68
    },
    {
      "text": "so I'm going to look at the first input",
      "start": 972.48,
      "duration": 4.0
    },
    {
      "text": "now which is I had always thought which",
      "start": 974.04,
      "duration": 5.12
    },
    {
      "text": "is the first row over here U and",
      "start": 976.48,
      "duration": 5.039
    },
    {
      "text": "remember that based on the input we have",
      "start": 979.16,
      "duration": 4.64
    },
    {
      "text": "to get the llm model output right what",
      "start": 981.519,
      "duration": 4.081
    },
    {
      "text": "is our model predicting we need to know",
      "start": 983.8,
      "duration": 4.24
    },
    {
      "text": "that because I I have the actual output",
      "start": 985.6,
      "duration": 4.12
    },
    {
      "text": "corresponding to this input so if the",
      "start": 988.04,
      "duration": 4.08
    },
    {
      "text": "input is this I know that the output is",
      "start": 989.72,
      "duration": 4.239
    },
    {
      "text": "had always thought Jack that is my",
      "start": 992.12,
      "duration": 3.88
    },
    {
      "text": "output so let me even write that here",
      "start": 993.959,
      "duration": 4.641
    },
    {
      "text": "the output or rather the target I should",
      "start": 996.0,
      "duration": 5.24
    },
    {
      "text": "call it the target value for this input",
      "start": 998.6,
      "duration": 4.159
    },
    {
      "text": "is",
      "start": 1001.24,
      "duration": 3.959
    },
    {
      "text": "had had",
      "start": 1002.759,
      "duration": 5.921
    },
    {
      "text": "always thought Jack right that's the",
      "start": 1005.199,
      "duration": 5.281
    },
    {
      "text": "target output which I need but of course",
      "start": 1008.68,
      "duration": 4.2
    },
    {
      "text": "my llm is not going to uh give me this",
      "start": 1010.48,
      "duration": 4.76
    },
    {
      "text": "at the first uh first shot because we",
      "start": 1012.88,
      "duration": 4.0
    },
    {
      "text": "have not trained it yet so I need to",
      "start": 1015.24,
      "duration": 4.039
    },
    {
      "text": "first know what my llm is predicting and",
      "start": 1016.88,
      "duration": 4.56
    },
    {
      "text": "for that we are going to go through the",
      "start": 1019.279,
      "duration": 5.241
    },
    {
      "text": "entire uh not right now but uh I'm just",
      "start": 1021.44,
      "duration": 5.519
    },
    {
      "text": "going to show you the schematic of what",
      "start": 1024.52,
      "duration": 4.48
    },
    {
      "text": "we are going to put the input through so",
      "start": 1026.959,
      "duration": 3.84
    },
    {
      "text": "the input which we have that will go",
      "start": 1029.0,
      "duration": 3.959
    },
    {
      "text": "through this entire GPT architecture or",
      "start": 1030.799,
      "duration": 3.88
    },
    {
      "text": "the entire llm architecture which we",
      "start": 1032.959,
      "duration": 4.921
    },
    {
      "text": "have trained in stage number one uh so",
      "start": 1034.679,
      "duration": 4.601
    },
    {
      "text": "in the previous lectures we have",
      "start": 1037.88,
      "duration": 3.24
    },
    {
      "text": "actually built this entire architecture",
      "start": 1039.28,
      "duration": 4.039
    },
    {
      "text": "from scratch and you can see that there",
      "start": 1041.12,
      "duration": 4.079
    },
    {
      "text": "are so many different modules in this",
      "start": 1043.319,
      "duration": 4.081
    },
    {
      "text": "architecture and let me quickly explain",
      "start": 1045.199,
      "duration": 4.281
    },
    {
      "text": "to you what we are actually doing so we",
      "start": 1047.4,
      "duration": 4.159
    },
    {
      "text": "take in the input imagine those four",
      "start": 1049.48,
      "duration": 4.559
    },
    {
      "text": "tokens I had always thought we tokenize",
      "start": 1051.559,
      "duration": 5.36
    },
    {
      "text": "them using the tick token or the bite",
      "start": 1054.039,
      "duration": 5.241
    },
    {
      "text": "pair encoder then we convert them into",
      "start": 1056.919,
      "duration": 4.281
    },
    {
      "text": "token embeddings which are vector",
      "start": 1059.28,
      "duration": 3.639
    },
    {
      "text": "representations in higher dimensional",
      "start": 1061.2,
      "duration": 4.16
    },
    {
      "text": "spaces we add positional embeddings to",
      "start": 1062.919,
      "duration": 5.441
    },
    {
      "text": "it we add the Dropout layer at this",
      "start": 1065.36,
      "duration": 5.559
    },
    {
      "text": "stage uh our input passes through the",
      "start": 1068.36,
      "duration": 4.439
    },
    {
      "text": "Transformer block so the Blue Block",
      "start": 1070.919,
      "duration": 3.441
    },
    {
      "text": "which I've shown over here this is the",
      "start": 1072.799,
      "duration": 3.921
    },
    {
      "text": "Transformer block and this is the main",
      "start": 1074.36,
      "duration": 5.439
    },
    {
      "text": "engine of the GPT architecture Ure the",
      "start": 1076.72,
      "duration": 4.28
    },
    {
      "text": "main thing which happens in the",
      "start": 1079.799,
      "duration": 3.281
    },
    {
      "text": "Transformer block is that the input",
      "start": 1081.0,
      "duration": 4.36
    },
    {
      "text": "embedding vectors are transformed into",
      "start": 1083.08,
      "duration": 4.479
    },
    {
      "text": "something which is called as context",
      "start": 1085.36,
      "duration": 5.04
    },
    {
      "text": "vectors now what are context vectors and",
      "start": 1087.559,
      "duration": 5.081
    },
    {
      "text": "how do they differ from input embedding",
      "start": 1090.4,
      "duration": 5.159
    },
    {
      "text": "vectors context vectors are more richer",
      "start": 1092.64,
      "duration": 4.6
    },
    {
      "text": "so if you look at one word let's say if",
      "start": 1095.559,
      "duration": 4.48
    },
    {
      "text": "you look at effort here if you look at",
      "start": 1097.24,
      "duration": 4.679
    },
    {
      "text": "the input embedding Vector for effort it",
      "start": 1100.039,
      "duration": 4.441
    },
    {
      "text": "just encodes santic meaning about effort",
      "start": 1101.919,
      "duration": 4.801
    },
    {
      "text": "it does not contain any information",
      "start": 1104.48,
      "duration": 4.319
    },
    {
      "text": "about how effort relates to the other to",
      "start": 1106.72,
      "duration": 5.0
    },
    {
      "text": "tokens but the context Vector is much",
      "start": 1108.799,
      "duration": 4.961
    },
    {
      "text": "richer because the context Vector for",
      "start": 1111.72,
      "duration": 4.24
    },
    {
      "text": "effort not only contains semantic",
      "start": 1113.76,
      "duration": 4.0
    },
    {
      "text": "meaning about effort but it also",
      "start": 1115.96,
      "duration": 4.839
    },
    {
      "text": "contains information about when effort",
      "start": 1117.76,
      "duration": 4.56
    },
    {
      "text": "when we are looking at effort how much",
      "start": 1120.799,
      "duration": 3.721
    },
    {
      "text": "information should we pay to every moves",
      "start": 1122.32,
      "duration": 4.839
    },
    {
      "text": "and youu so in other in other words it",
      "start": 1124.52,
      "duration": 4.48
    },
    {
      "text": "pays attention the context Vector",
      "start": 1127.159,
      "duration": 4.241
    },
    {
      "text": "includes attention which is given to the",
      "start": 1129.0,
      "duration": 3.84
    },
    {
      "text": "other tokens when we look at a",
      "start": 1131.4,
      "duration": 3.8
    },
    {
      "text": "particular token and that's what gives",
      "start": 1132.84,
      "duration": 4.64
    },
    {
      "text": "all the power to the large language",
      "start": 1135.2,
      "duration": 5.8
    },
    {
      "text": "model so this multi-ad attention is the",
      "start": 1137.48,
      "duration": 6.52
    },
    {
      "text": "main driver behind the Transformer block",
      "start": 1141.0,
      "duration": 4.76
    },
    {
      "text": "so if you look at the whole GPT",
      "start": 1144.0,
      "duration": 3.64
    },
    {
      "text": "architecture the Transformer block is",
      "start": 1145.76,
      "duration": 5.56
    },
    {
      "text": "the engine of the uh GPT architecture",
      "start": 1147.64,
      "duration": 5.68
    },
    {
      "text": "and within the Transformer block the",
      "start": 1151.32,
      "duration": 4.359
    },
    {
      "text": "multi-head attention is what allows us",
      "start": 1153.32,
      "duration": 4.4
    },
    {
      "text": "to convert these input embedding vectors",
      "start": 1155.679,
      "duration": 4.161
    },
    {
      "text": "into context vectors which encodes",
      "start": 1157.72,
      "duration": 4.04
    },
    {
      "text": "information about how tokens relate to",
      "start": 1159.84,
      "duration": 4.319
    },
    {
      "text": "other tokens that's how the llms capture",
      "start": 1161.76,
      "duration": 4.56
    },
    {
      "text": "meaning and that's how they do so well",
      "start": 1164.159,
      "duration": 4.681
    },
    {
      "text": "so GPT does so well because it has this",
      "start": 1166.32,
      "duration": 3.96
    },
    {
      "text": "multi-ad attention",
      "start": 1168.84,
      "duration": 3.44
    },
    {
      "text": "mechanism so you can see that the",
      "start": 1170.28,
      "duration": 3.6
    },
    {
      "text": "Transformer has a number of building",
      "start": 1172.28,
      "duration": 3.36
    },
    {
      "text": "blocks we have the layer normalization",
      "start": 1173.88,
      "duration": 3.6
    },
    {
      "text": "multi-ad attention Drop Out shortcut",
      "start": 1175.64,
      "duration": 4.0
    },
    {
      "text": "connection feed forward neural network",
      "start": 1177.48,
      "duration": 5.8
    },
    {
      "text": "Etc and then finally when the input",
      "start": 1179.64,
      "duration": 8.12
    },
    {
      "text": "comes out of the uh out of the GPT model",
      "start": 1183.28,
      "duration": 6.12
    },
    {
      "text": "architecture we get something which is",
      "start": 1187.76,
      "duration": 4.2
    },
    {
      "text": "called as the logits uh we get something",
      "start": 1189.4,
      "duration": 4.279
    },
    {
      "text": "which is called as the logit sensor and",
      "start": 1191.96,
      "duration": 4.079
    },
    {
      "text": "it's very important to understand what",
      "start": 1193.679,
      "duration": 4.161
    },
    {
      "text": "the logit sensor is and what's the",
      "start": 1196.039,
      "duration": 4.281
    },
    {
      "text": "dimensions of this logic sensor so let's",
      "start": 1197.84,
      "duration": 5.12
    },
    {
      "text": "say the input is I had always thought",
      "start": 1200.32,
      "duration": 4.28
    },
    {
      "text": "and when we pass it through the GPT",
      "start": 1202.96,
      "duration": 3.199
    },
    {
      "text": "model which is when it passes through",
      "start": 1204.6,
      "duration": 3.12
    },
    {
      "text": "the entire architecture which I just",
      "start": 1206.159,
      "duration": 4.88
    },
    {
      "text": "showed you we get this logic sensor now",
      "start": 1207.72,
      "duration": 6.16
    },
    {
      "text": "I had always thought uh those are the",
      "start": 1211.039,
      "duration": 4.841
    },
    {
      "text": "tokens and when you look at the logits",
      "start": 1213.88,
      "duration": 4.12
    },
    {
      "text": "tensor corresponding to every token",
      "start": 1215.88,
      "duration": 4.52
    },
    {
      "text": "there are these logits there is a logits",
      "start": 1218.0,
      "duration": 5.12
    },
    {
      "text": "vector who whose dimensions are equal to",
      "start": 1220.4,
      "duration": 5.8
    },
    {
      "text": "the vocabulary size so if you look at I",
      "start": 1223.12,
      "duration": 7.039
    },
    {
      "text": "the Logics for I are 0257 because that's",
      "start": 1226.2,
      "duration": 6.56
    },
    {
      "text": "our vocabulary size if you look at had",
      "start": 1230.159,
      "duration": 5.161
    },
    {
      "text": "the logits for had are 50257 because",
      "start": 1232.76,
      "duration": 4.64
    },
    {
      "text": "that's the vocabulary if you look at",
      "start": 1235.32,
      "duration": 5.0
    },
    {
      "text": "always the logits for always are 50257",
      "start": 1237.4,
      "duration": 4.48
    },
    {
      "text": "because that's the vocabulary size and",
      "start": 1240.32,
      "duration": 2.88
    },
    {
      "text": "similarly for",
      "start": 1241.88,
      "duration": 3.84
    },
    {
      "text": "thought now when the logits tensor come",
      "start": 1243.2,
      "duration": 4.479
    },
    {
      "text": "out of the GPT architecture they are not",
      "start": 1245.72,
      "duration": 3.68
    },
    {
      "text": "normalized so if you look at the logits",
      "start": 1247.679,
      "duration": 4.12
    },
    {
      "text": "for I they don't add up to one so the",
      "start": 1249.4,
      "duration": 4.32
    },
    {
      "text": "next step is to convert this logic",
      "start": 1251.799,
      "duration": 5.041
    },
    {
      "text": "tensor into a probability tensor and",
      "start": 1253.72,
      "duration": 4.56
    },
    {
      "text": "that makes sure that when you look at",
      "start": 1256.84,
      "duration": 3.68
    },
    {
      "text": "every token the logits or the",
      "start": 1258.28,
      "duration": 4.32
    },
    {
      "text": "probabilities add up to one so now you",
      "start": 1260.52,
      "duration": 4.039
    },
    {
      "text": "can see that every logic essentially has",
      "start": 1262.6,
      "duration": 4.64
    },
    {
      "text": "a meaning because it adds up to one the",
      "start": 1264.559,
      "duration": 4.561
    },
    {
      "text": "way to predict the output now is that",
      "start": 1267.24,
      "duration": 4.72
    },
    {
      "text": "you look at I and you look at that logic",
      "start": 1269.12,
      "duration": 5.28
    },
    {
      "text": "which has the highest value okay and",
      "start": 1271.96,
      "duration": 4.28
    },
    {
      "text": "let's say it corresponds to index number",
      "start": 1274.4,
      "duration": 5.519
    },
    {
      "text": "50 in this vocabulary of 50257 or maybe",
      "start": 1276.24,
      "duration": 6.84
    },
    {
      "text": "5,000 and then you look at the token",
      "start": 1279.919,
      "duration": 4.961
    },
    {
      "text": "corresponding to that index and maybe",
      "start": 1283.08,
      "duration": 5.24
    },
    {
      "text": "it's something random like a c h now you",
      "start": 1284.88,
      "duration": 5.96
    },
    {
      "text": "look at had and you look at the logic",
      "start": 1288.32,
      "duration": 4.56
    },
    {
      "text": "which has the highest value you get the",
      "start": 1290.84,
      "duration": 3.839
    },
    {
      "text": "index corresponding to this maybe it is",
      "start": 1292.88,
      "duration": 5.399
    },
    {
      "text": "31 1 01 and then you find the token",
      "start": 1294.679,
      "duration": 5.201
    },
    {
      "text": "corresponding to this index maybe it's",
      "start": 1298.279,
      "duration": 3.4
    },
    {
      "text": "something completely random similarly",
      "start": 1299.88,
      "duration": 4.52
    },
    {
      "text": "for always you get the index which has",
      "start": 1301.679,
      "duration": 6.721
    },
    {
      "text": "the highest probability and you get the",
      "start": 1304.4,
      "duration": 9.879
    },
    {
      "text": "outputs like a am m m o let's say uh am",
      "start": 1308.4,
      "duration": 7.2
    },
    {
      "text": "and then you look at thought and you",
      "start": 1314.279,
      "duration": 3.28
    },
    {
      "text": "find the index corresponding to the",
      "start": 1315.6,
      "duration": 3.959
    },
    {
      "text": "highest probability and let's say this",
      "start": 1317.559,
      "duration": 7.48
    },
    {
      "text": "is 611 6111 and then you get the uh",
      "start": 1319.559,
      "duration": 8.0
    },
    {
      "text": "outputs so these are the output tokens",
      "start": 1325.039,
      "duration": 4.681
    },
    {
      "text": "now which are llm is predicting and",
      "start": 1327.559,
      "duration": 3.761
    },
    {
      "text": "these are the output tokens which we",
      "start": 1329.72,
      "duration": 4.559
    },
    {
      "text": "need to make sure that uh they need to",
      "start": 1331.32,
      "duration": 5.8
    },
    {
      "text": "be as close to our Target as possible",
      "start": 1334.279,
      "duration": 4.441
    },
    {
      "text": "okay I hope you have understood the",
      "start": 1337.12,
      "duration": 3.24
    },
    {
      "text": "workflow which we are trying to follow",
      "start": 1338.72,
      "duration": 3.839
    },
    {
      "text": "here we first have a logic sensor we",
      "start": 1340.36,
      "duration": 4.96
    },
    {
      "text": "convert it into a soft Max sensor uh",
      "start": 1342.559,
      "duration": 5.281
    },
    {
      "text": "which which is a probability sensor and",
      "start": 1345.32,
      "duration": 4.4
    },
    {
      "text": "I just gave you the intuition that based",
      "start": 1347.84,
      "duration": 3.76
    },
    {
      "text": "on this tensor how do you make the",
      "start": 1349.72,
      "duration": 5.72
    },
    {
      "text": "output for this input right so what's",
      "start": 1351.6,
      "duration": 6.92
    },
    {
      "text": "done now is that after this point let's",
      "start": 1355.44,
      "duration": 4.96
    },
    {
      "text": "say we have this probability tensor",
      "start": 1358.52,
      "duration": 4.36
    },
    {
      "text": "right then what we actually do is that",
      "start": 1360.4,
      "duration": 5.72
    },
    {
      "text": "we look at the targets uh this is the",
      "start": 1362.88,
      "duration": 5.08
    },
    {
      "text": "actual value which we want and we look",
      "start": 1366.12,
      "duration": 4.6
    },
    {
      "text": "at the index which each token in the",
      "start": 1367.96,
      "duration": 5.04
    },
    {
      "text": "Target corresponds to so had has index",
      "start": 1370.72,
      "duration": 5.04
    },
    {
      "text": "23 in the vocabulary always has index",
      "start": 1373.0,
      "duration": 5.48
    },
    {
      "text": "3881 in our vocabulary thought has IND",
      "start": 1375.76,
      "duration": 5.44
    },
    {
      "text": "index 1 1 2 2 3 in our vocabulary Jack",
      "start": 1378.48,
      "duration": 5.16
    },
    {
      "text": "has index 15 in the vocabulary I'm",
      "start": 1381.2,
      "duration": 4.2
    },
    {
      "text": "assigning these random values for now",
      "start": 1383.64,
      "duration": 4.36
    },
    {
      "text": "just for demonstration purposes now let",
      "start": 1385.4,
      "duration": 4.519
    },
    {
      "text": "me show you one thing here so first let",
      "start": 1388.0,
      "duration": 3.039
    },
    {
      "text": "me rub",
      "start": 1389.919,
      "duration": 4.561
    },
    {
      "text": "these uh rub these colors Okay now",
      "start": 1391.039,
      "duration": 5.681
    },
    {
      "text": "what's done next is that based on these",
      "start": 1394.48,
      "duration": 5.64
    },
    {
      "text": "based on these IND indexes uh let's say",
      "start": 1396.72,
      "duration": 6.079
    },
    {
      "text": "we look at I and we look at index number",
      "start": 1400.12,
      "duration": 5.88
    },
    {
      "text": "23 let's say this is index number 23 and",
      "start": 1402.799,
      "duration": 7.0
    },
    {
      "text": "we take its uh value that will be P1",
      "start": 1406.0,
      "duration": 8.12
    },
    {
      "text": "then we look at had and then we uh that",
      "start": 1409.799,
      "duration": 6.521
    },
    {
      "text": "so the target for had is always and its",
      "start": 1414.12,
      "duration": 4.2
    },
    {
      "text": "index is 3881 let's say we look at that",
      "start": 1416.32,
      "duration": 3.479
    },
    {
      "text": "index and find the probability",
      "start": 1418.32,
      "duration": 4.04
    },
    {
      "text": "corresponding to that index that's P2",
      "start": 1419.799,
      "duration": 5.0
    },
    {
      "text": "then we look at the third row and find",
      "start": 1422.36,
      "duration": 4.88
    },
    {
      "text": "index number 1 1 22 3 and find the",
      "start": 1424.799,
      "duration": 4.161
    },
    {
      "text": "probability corresponding to that that's",
      "start": 1427.24,
      "duration": 7.08
    },
    {
      "text": "P3 and then we uh find uh the index",
      "start": 1428.96,
      "duration": 7.8
    },
    {
      "text": "number 15 in the last row and then we",
      "start": 1434.32,
      "duration": 4.28
    },
    {
      "text": "find the probability corresponding to to",
      "start": 1436.76,
      "duration": 4.88
    },
    {
      "text": "that so then that will be P4 ideally if",
      "start": 1438.6,
      "duration": 5.12
    },
    {
      "text": "in an Ideal World if the llm is trained",
      "start": 1441.64,
      "duration": 4.12
    },
    {
      "text": "perfectly these probabilities will be",
      "start": 1443.72,
      "duration": 3.4
    },
    {
      "text": "close to",
      "start": 1445.76,
      "duration": 4.32
    },
    {
      "text": "one and if these probabilities are close",
      "start": 1447.12,
      "duration": 4.84
    },
    {
      "text": "to one It means that our llm is also",
      "start": 1450.08,
      "duration": 5.479
    },
    {
      "text": "predicting these values right um as the",
      "start": 1451.96,
      "duration": 5.839
    },
    {
      "text": "output but when the llm is not trained",
      "start": 1455.559,
      "duration": 4.081
    },
    {
      "text": "at all these probabilities will not be",
      "start": 1457.799,
      "duration": 3.48
    },
    {
      "text": "close to one at all they might be very",
      "start": 1459.64,
      "duration": 4.44
    },
    {
      "text": "low which means that our llm does not",
      "start": 1461.279,
      "duration": 4.52
    },
    {
      "text": "think that this target needs to be the",
      "start": 1464.08,
      "duration": 4.68
    },
    {
      "text": "output because it has not been trained",
      "start": 1465.799,
      "duration": 5.201
    },
    {
      "text": "so the whole goal is that to make sure",
      "start": 1468.76,
      "duration": 4.2
    },
    {
      "text": "that these probabilities get as close to",
      "start": 1471.0,
      "duration": 4.559
    },
    {
      "text": "one as possible and that's why we uh",
      "start": 1472.96,
      "duration": 5.0
    },
    {
      "text": "employ the categorical or I should call",
      "start": 1475.559,
      "duration": 4.761
    },
    {
      "text": "it the cross entropy loss so at first",
      "start": 1477.96,
      "duration": 4.76
    },
    {
      "text": "take the logarithm of all these values I",
      "start": 1480.32,
      "duration": 4.8
    },
    {
      "text": "add up these log values then I take the",
      "start": 1482.72,
      "duration": 4.48
    },
    {
      "text": "mean and then I take the negative this",
      "start": 1485.12,
      "duration": 5.559
    },
    {
      "text": "is also called as the negative log",
      "start": 1487.2,
      "duration": 5.44
    },
    {
      "text": "likelihood uh this is called as the",
      "start": 1490.679,
      "duration": 4.24
    },
    {
      "text": "negative log likelihood and the whole",
      "start": 1492.64,
      "duration": 4.56
    },
    {
      "text": "goal of training the llm is to make sure",
      "start": 1494.919,
      "duration": 4.76
    },
    {
      "text": "that this this negative log likelihood",
      "start": 1497.2,
      "duration": 5.32
    },
    {
      "text": "which I'm calling n LL and if you plot",
      "start": 1499.679,
      "duration": 6.24
    },
    {
      "text": "nnl of X as a function of X so it's",
      "start": 1502.52,
      "duration": 5.2
    },
    {
      "text": "negative of logarithm right so it will",
      "start": 1505.919,
      "duration": 4.321
    },
    {
      "text": "look something like uh it will look",
      "start": 1507.72,
      "duration": 4.76
    },
    {
      "text": "something like this since it's the",
      "start": 1510.24,
      "duration": 4.64
    },
    {
      "text": "negative it will it will look something",
      "start": 1512.48,
      "duration": 5.919
    },
    {
      "text": "like this and uh our whole goal is to",
      "start": 1514.88,
      "duration": 5.48
    },
    {
      "text": "make sure that the loss comes down and",
      "start": 1518.399,
      "duration": 4.0
    },
    {
      "text": "it comes down to zero as much as",
      "start": 1520.36,
      "duration": 4.6
    },
    {
      "text": "possible so now today what we are going",
      "start": 1522.399,
      "duration": 4.16
    },
    {
      "text": "to do is that today we are not going to",
      "start": 1524.96,
      "duration": 3.199
    },
    {
      "text": "train the llm we are just going to see",
      "start": 1526.559,
      "duration": 4.72
    },
    {
      "text": "this starting point of this loss uh",
      "start": 1528.159,
      "duration": 5.441
    },
    {
      "text": "which which will be very high value but",
      "start": 1531.279,
      "duration": 3.76
    },
    {
      "text": "then in the subsequent lecture we are",
      "start": 1533.6,
      "duration": 3.079
    },
    {
      "text": "going to train the large language model",
      "start": 1535.039,
      "duration": 3.601
    },
    {
      "text": "so that this loss comes as down as",
      "start": 1536.679,
      "duration": 4.161
    },
    {
      "text": "possible so this is the workflow which I",
      "start": 1538.64,
      "duration": 4.039
    },
    {
      "text": "showed you for one input right I had",
      "start": 1540.84,
      "duration": 4.76
    },
    {
      "text": "always thought uh and then how do we get",
      "start": 1542.679,
      "duration": 3.641
    },
    {
      "text": "the",
      "start": 1545.6,
      "duration": 3.6
    },
    {
      "text": "loss now remember that we don't just",
      "start": 1546.32,
      "duration": 5.68
    },
    {
      "text": "have one input we have all these inputs",
      "start": 1549.2,
      "duration": 5.8
    },
    {
      "text": "uh which are stacked together in a batch",
      "start": 1552.0,
      "duration": 4.72
    },
    {
      "text": "so remember that this data loader",
      "start": 1555.0,
      "duration": 3.76
    },
    {
      "text": "processes inputs in a batch",
      "start": 1556.72,
      "duration": 4.0
    },
    {
      "text": "so each batch has an accumulation of",
      "start": 1558.76,
      "duration": 4.72
    },
    {
      "text": "inputs right based on the size of the",
      "start": 1560.72,
      "duration": 4.4
    },
    {
      "text": "batch so now let me show you how",
      "start": 1563.48,
      "duration": 4.439
    },
    {
      "text": "multiple inputs in a batch are processed",
      "start": 1565.12,
      "duration": 4.6
    },
    {
      "text": "so let's say we have a batch which has",
      "start": 1567.919,
      "duration": 4.0
    },
    {
      "text": "two inputs together so let's say this is",
      "start": 1569.72,
      "duration": 5.079
    },
    {
      "text": "a batch whose batch size is equal to two",
      "start": 1571.919,
      "duration": 5.281
    },
    {
      "text": "which means that there are two inputs",
      "start": 1574.799,
      "duration": 6.48
    },
    {
      "text": "together uh in a batch at a time right",
      "start": 1577.2,
      "duration": 5.92
    },
    {
      "text": "so this is one batch and this has two",
      "start": 1581.279,
      "duration": 4.52
    },
    {
      "text": "inputs I had always thought Jack gpan",
      "start": 1583.12,
      "duration": 6.48
    },
    {
      "text": "rather so this is X1 and this is X2 and",
      "start": 1585.799,
      "duration": 5.921
    },
    {
      "text": "this is y1 and these are this is Y2",
      "start": 1589.6,
      "duration": 3.799
    },
    {
      "text": "these are the target outputs so for the",
      "start": 1591.72,
      "duration": 4.24
    },
    {
      "text": "first input X1 my output should be had",
      "start": 1593.399,
      "duration": 4.601
    },
    {
      "text": "always thought Jack which we also saw in",
      "start": 1595.96,
      "duration": 3.599
    },
    {
      "text": "the previous example where just one",
      "start": 1598.0,
      "duration": 4.12
    },
    {
      "text": "input was there and my second input is",
      "start": 1599.559,
      "duration": 5.081
    },
    {
      "text": "Jack gpan rather a the output should be",
      "start": 1602.12,
      "duration": 4.72
    },
    {
      "text": "gizan rather or cheap that's what I want",
      "start": 1604.64,
      "duration": 4.88
    },
    {
      "text": "these are the targets right now similar",
      "start": 1606.84,
      "duration": 5.12
    },
    {
      "text": "to the similar to what we saw for one",
      "start": 1609.52,
      "duration": 4.639
    },
    {
      "text": "input the steps are pretty similar for",
      "start": 1611.96,
      "duration": 4.599
    },
    {
      "text": "the case of batches as well I'm trying",
      "start": 1614.159,
      "duration": 4.12
    },
    {
      "text": "to see a color which would work the best",
      "start": 1616.559,
      "duration": 3.72
    },
    {
      "text": "here yeah so what we'll do is that we'll",
      "start": 1618.279,
      "duration": 3.681
    },
    {
      "text": "first take the input and we'll pass it",
      "start": 1620.279,
      "duration": 4.081
    },
    {
      "text": "through the entire GPT architecture in",
      "start": 1621.96,
      "duration": 4.079
    },
    {
      "text": "this case what we'll get is that we'll",
      "start": 1624.36,
      "duration": 4.199
    },
    {
      "text": "get two logic sensors the first logic",
      "start": 1626.039,
      "duration": 5.401
    },
    {
      "text": "sensor is for the first bat first input",
      "start": 1628.559,
      "duration": 4.521
    },
    {
      "text": "the second logic sensor is for the",
      "start": 1631.44,
      "duration": 3.959
    },
    {
      "text": "second input so if you look at the size",
      "start": 1633.08,
      "duration": 4.199
    },
    {
      "text": "of this tensor now we have two batches",
      "start": 1635.399,
      "duration": 3.961
    },
    {
      "text": "here and in each batch there are four",
      "start": 1637.279,
      "duration": 4.921
    },
    {
      "text": "rows and in each row there are 5 to 57",
      "start": 1639.36,
      "duration": 4.72
    },
    {
      "text": "columns in the previous case where there",
      "start": 1642.2,
      "duration": 4.12
    },
    {
      "text": "was just one input the size was just 4",
      "start": 1644.08,
      "duration": 5.52
    },
    {
      "text": "into 50257 but now we have two such",
      "start": 1646.32,
      "duration": 5.599
    },
    {
      "text": "batches right two such input so this is",
      "start": 1649.6,
      "duration": 4.52
    },
    {
      "text": "input number one and here is input",
      "start": 1651.919,
      "duration": 5.201
    },
    {
      "text": "number two right so in the code what",
      "start": 1654.12,
      "duration": 4.799
    },
    {
      "text": "we'll do is that when we get this logic",
      "start": 1657.12,
      "duration": 3.72
    },
    {
      "text": "sensor we'll flatten this out we'll",
      "start": 1658.919,
      "duration": 4.041
    },
    {
      "text": "flatten the batch Dimension out which",
      "start": 1660.84,
      "duration": 5.12
    },
    {
      "text": "means we'll merge both of these together",
      "start": 1662.96,
      "duration": 5.0
    },
    {
      "text": "uh we'll merge both of these together so",
      "start": 1665.96,
      "duration": 4.12
    },
    {
      "text": "that now the my cumulative logic sensor",
      "start": 1667.96,
      "duration": 3.88
    },
    {
      "text": "looks something like this this is my",
      "start": 1670.08,
      "duration": 3.8
    },
    {
      "text": "first input this is my second input all",
      "start": 1671.84,
      "duration": 4.319
    },
    {
      "text": "merged together the size of this now",
      "start": 1673.88,
      "duration": 5.519
    },
    {
      "text": "will be eight rows multiplied by",
      "start": 1676.159,
      "duration": 6.4
    },
    {
      "text": "50257 that will be the size of this okay",
      "start": 1679.399,
      "duration": 5.081
    },
    {
      "text": "and the next steps are pretty similar we",
      "start": 1682.559,
      "duration": 4.041
    },
    {
      "text": "add we apply soft Max we convert this",
      "start": 1684.48,
      "duration": 5.319
    },
    {
      "text": "into a tensor of probabilities and then",
      "start": 1686.6,
      "duration": 5.24
    },
    {
      "text": "we look at the Target we look at the",
      "start": 1689.799,
      "duration": 4.441
    },
    {
      "text": "Target tokens and we stack this target",
      "start": 1691.84,
      "duration": 5.04
    },
    {
      "text": "tokens also so for the first input these",
      "start": 1694.24,
      "duration": 4.4
    },
    {
      "text": "four are the target outputs for the",
      "start": 1696.88,
      "duration": 3.679
    },
    {
      "text": "first for the second input these four",
      "start": 1698.64,
      "duration": 4.0
    },
    {
      "text": "are the target outputs and we get the",
      "start": 1700.559,
      "duration": 4.041
    },
    {
      "text": "token IDs corresponding to these Target",
      "start": 1702.64,
      "duration": 4.32
    },
    {
      "text": "outputs we stack them together and then",
      "start": 1704.6,
      "duration": 5.439
    },
    {
      "text": "we find for each row we find the value",
      "start": 1706.96,
      "duration": 5.76
    },
    {
      "text": "corresponding to these token IDs and",
      "start": 1710.039,
      "duration": 5.161
    },
    {
      "text": "then these values are noted down as P11",
      "start": 1712.72,
      "duration": 7.439
    },
    {
      "text": "p12 p13 p14 this for input 1 p21 P22 p23",
      "start": 1715.2,
      "duration": 7.599
    },
    {
      "text": "p24 that's for input number two and then",
      "start": 1720.159,
      "duration": 5.321
    },
    {
      "text": "similarly we get the cross entropy loss",
      "start": 1722.799,
      "duration": 5.081
    },
    {
      "text": "so this is the loss for the first input",
      "start": 1725.48,
      "duration": 4.039
    },
    {
      "text": "this is the loss for the second input",
      "start": 1727.88,
      "duration": 3.799
    },
    {
      "text": "and then we add add it together and",
      "start": 1729.519,
      "duration": 3.841
    },
    {
      "text": "ultimately it will also look something",
      "start": 1731.679,
      "duration": 5.201
    },
    {
      "text": "like this uh and the whole goal is that",
      "start": 1733.36,
      "duration": 5.4
    },
    {
      "text": "in this case also we want to minimize",
      "start": 1736.88,
      "duration": 3.84
    },
    {
      "text": "this loss and bring it as close to zero",
      "start": 1738.76,
      "duration": 4.6
    },
    {
      "text": "as possible so now I hope you see that",
      "start": 1740.72,
      "duration": 4.799
    },
    {
      "text": "even for a batch of two inputs the",
      "start": 1743.36,
      "duration": 3.96
    },
    {
      "text": "process of getting the loss is pretty",
      "start": 1745.519,
      "duration": 4.321
    },
    {
      "text": "similar as what the process was for just",
      "start": 1747.32,
      "duration": 5.719
    },
    {
      "text": "one input right uh and I want you to",
      "start": 1749.84,
      "duration": 5.079
    },
    {
      "text": "keep this visual workflow in mind so",
      "start": 1753.039,
      "duration": 3.52
    },
    {
      "text": "that when we come to the code you will",
      "start": 1754.919,
      "duration": 3.48
    },
    {
      "text": "really understand what is happening over",
      "start": 1756.559,
      "duration": 3.921
    },
    {
      "text": "here so in the code there is going to",
      "start": 1758.399,
      "duration": 4.801
    },
    {
      "text": "come a time when uh we are going to",
      "start": 1760.48,
      "duration": 4.48
    },
    {
      "text": "apply the data loader to the training",
      "start": 1763.2,
      "duration": 4.64
    },
    {
      "text": "and validation set and the training data",
      "start": 1764.96,
      "duration": 4.959
    },
    {
      "text": "set is going to look like this after it",
      "start": 1767.84,
      "duration": 5.04
    },
    {
      "text": "passes the data loader and I want you to",
      "start": 1769.919,
      "duration": 5.401
    },
    {
      "text": "analyze this right I told you that the",
      "start": 1772.88,
      "duration": 4.88
    },
    {
      "text": "input is processed into batches right so",
      "start": 1775.32,
      "duration": 5.68
    },
    {
      "text": "here you can see that uh let's look at",
      "start": 1777.76,
      "duration": 6.12
    },
    {
      "text": "first the each row for now right each",
      "start": 1781.0,
      "duration": 5.84
    },
    {
      "text": "row corresponds to one batch so this is",
      "start": 1783.88,
      "duration": 7.679
    },
    {
      "text": "X and this is y so what this 2 comma 256",
      "start": 1786.84,
      "duration": 7.12
    },
    {
      "text": "means is that the so let's look at our",
      "start": 1791.559,
      "duration": 4.081
    },
    {
      "text": "case right now let's look at the case",
      "start": 1793.96,
      "duration": 4.719
    },
    {
      "text": "which we took for the batch uh we looked",
      "start": 1795.64,
      "duration": 5.56
    },
    {
      "text": "at X1 we looked at X and Y right this is",
      "start": 1798.679,
      "duration": 5.161
    },
    {
      "text": "the first batch and the size here was 2A",
      "start": 1801.2,
      "duration": 6.199
    },
    {
      "text": "4 because there were two uh there were",
      "start": 1803.84,
      "duration": 5.679
    },
    {
      "text": "two inputs and each input had four",
      "start": 1807.399,
      "duration": 5.041
    },
    {
      "text": "tokens and here also it was 2A 4 because",
      "start": 1809.519,
      "duration": 5.16
    },
    {
      "text": "there are two outputs and four tokens",
      "start": 1812.44,
      "duration": 4.2
    },
    {
      "text": "similarly what I want you to note over",
      "start": 1814.679,
      "duration": 3.48
    },
    {
      "text": "here is that when you look at this",
      "start": 1816.64,
      "duration": 3.919
    },
    {
      "text": "training data loader it's exactly",
      "start": 1818.159,
      "duration": 4.76
    },
    {
      "text": "similar to the example which we saw so",
      "start": 1820.559,
      "duration": 4.761
    },
    {
      "text": "let's look at the first row over",
      "start": 1822.919,
      "duration": 4.841
    },
    {
      "text": "here let's look at the first row over",
      "start": 1825.32,
      "duration": 3.52
    },
    {
      "text": "here",
      "start": 1827.76,
      "duration": 4.08
    },
    {
      "text": "uh it has two inputs but it has 256",
      "start": 1828.84,
      "duration": 5.04
    },
    {
      "text": "tokens because that's the context size",
      "start": 1831.84,
      "duration": 4.24
    },
    {
      "text": "we are going to use when we go to code",
      "start": 1833.88,
      "duration": 5.76
    },
    {
      "text": "so that's X the first input and it has",
      "start": 1836.08,
      "duration": 7.0
    },
    {
      "text": "two uh it has two inputs so it's X1",
      "start": 1839.64,
      "duration": 6.56
    },
    {
      "text": "comma X2 um and that's exactly what we",
      "start": 1843.08,
      "duration": 4.92
    },
    {
      "text": "saw right why there is two here because",
      "start": 1846.2,
      "duration": 5.16
    },
    {
      "text": "there is X1 uh so if you zoom this in",
      "start": 1848.0,
      "duration": 5.0
    },
    {
      "text": "further let me show you how it looks",
      "start": 1851.36,
      "duration": 5.4
    },
    {
      "text": "like it will be um",
      "start": 1853.0,
      "duration": 10.0
    },
    {
      "text": "I uh I had always this will be 256 now",
      "start": 1856.76,
      "duration": 7.919
    },
    {
      "text": "not four like what I had shown you",
      "start": 1863.0,
      "duration": 4.0
    },
    {
      "text": "because the context size is 256 this",
      "start": 1864.679,
      "duration": 4.72
    },
    {
      "text": "will be X1 and then X2 will be another",
      "start": 1867.0,
      "duration": 5.559
    },
    {
      "text": "input batch this will be X1 X2 and then",
      "start": 1869.399,
      "duration": 5.361
    },
    {
      "text": "similarly we have y1 Y2 which are",
      "start": 1872.559,
      "duration": 4.681
    },
    {
      "text": "shifted by one I already told you right",
      "start": 1874.76,
      "duration": 4.6
    },
    {
      "text": "how to construct the y1 and Y2 they are",
      "start": 1877.24,
      "duration": 4.559
    },
    {
      "text": "the inputs just shifted by one so these",
      "start": 1879.36,
      "duration": 4.679
    },
    {
      "text": "are y1 and Y2 and they will also have",
      "start": 1881.799,
      "duration": 4.281
    },
    {
      "text": "the size of",
      "start": 1884.039,
      "duration": 4.441
    },
    {
      "text": "256 this is just the first badge the",
      "start": 1886.08,
      "duration": 5.4
    },
    {
      "text": "first row is just the first batch uh",
      "start": 1888.48,
      "duration": 5.919
    },
    {
      "text": "where each batch had two inputs of 256",
      "start": 1891.48,
      "duration": 5.28
    },
    {
      "text": "tokens each similarly here we can see",
      "start": 1894.399,
      "duration": 4.24
    },
    {
      "text": "that there are nine batches so there are",
      "start": 1896.76,
      "duration": 4.32
    },
    {
      "text": "nine training set batches each batch has",
      "start": 1898.639,
      "duration": 5.321
    },
    {
      "text": "two samples and each sample has 256",
      "start": 1901.08,
      "duration": 4.88
    },
    {
      "text": "tokens each so I don't want you to be",
      "start": 1903.96,
      "duration": 4.079
    },
    {
      "text": "scared when you see this in the code",
      "start": 1905.96,
      "duration": 4.04
    },
    {
      "text": "similarly in the validation we just have",
      "start": 1908.039,
      "duration": 4.721
    },
    {
      "text": "one batch because only 10% is used each",
      "start": 1910.0,
      "duration": 6.2
    },
    {
      "text": "batch has h two samples and each sample",
      "start": 1912.76,
      "duration": 6.56
    },
    {
      "text": "has 256 tokens each",
      "start": 1916.2,
      "duration": 4.64
    },
    {
      "text": "okay I hope you have understood this",
      "start": 1919.32,
      "duration": 3.44
    },
    {
      "text": "visual workflow which I have constructed",
      "start": 1920.84,
      "duration": 3.839
    },
    {
      "text": "over here I could have directly taken",
      "start": 1922.76,
      "duration": 3.279
    },
    {
      "text": "you through the code but then it would",
      "start": 1924.679,
      "duration": 2.88
    },
    {
      "text": "have been very difficult to understand",
      "start": 1926.039,
      "duration": 4.12
    },
    {
      "text": "the different steps in the code right",
      "start": 1927.559,
      "duration": 4.441
    },
    {
      "text": "now I'm going to take you through the",
      "start": 1930.159,
      "duration": 3.441
    },
    {
      "text": "code and we are going to see how to",
      "start": 1932.0,
      "duration": 3.36
    },
    {
      "text": "calculate the loss for this verdict",
      "start": 1933.6,
      "duration": 4.72
    },
    {
      "text": "short story step by step but please keep",
      "start": 1935.36,
      "duration": 4.919
    },
    {
      "text": "this intuition of this workflow in mind",
      "start": 1938.32,
      "duration": 3.839
    },
    {
      "text": "then everything will be very clear for",
      "start": 1940.279,
      "duration": 4.52
    },
    {
      "text": "you okay so now we are at the code",
      "start": 1942.159,
      "duration": 4.681
    },
    {
      "text": "before we get started I want to clarify",
      "start": 1944.799,
      "duration": 4.441
    },
    {
      "text": "that we are actually using a relatively",
      "start": 1946.84,
      "duration": 4.76
    },
    {
      "text": "small data set and that is because we",
      "start": 1949.24,
      "duration": 4.76
    },
    {
      "text": "want to run the code in a few minutes on",
      "start": 1951.6,
      "duration": 4.799
    },
    {
      "text": "our laptop computer I could have used a",
      "start": 1954.0,
      "duration": 4.08
    },
    {
      "text": "larger data set but it would have taken",
      "start": 1956.399,
      "duration": 3.481
    },
    {
      "text": "a huge amount of",
      "start": 1958.08,
      "duration": 4.16
    },
    {
      "text": "time uh just to give you a sense of how",
      "start": 1959.88,
      "duration": 4.72
    },
    {
      "text": "much time it takes to run big data sets",
      "start": 1962.24,
      "duration": 7.279
    },
    {
      "text": "Lama 7 billion the that model required",
      "start": 1964.6,
      "duration": 7.0
    },
    {
      "text": "84320 GPU",
      "start": 1969.519,
      "duration": 4.681
    },
    {
      "text": "hours um and was trained on two trillion",
      "start": 1971.6,
      "duration": 5.16
    },
    {
      "text": "tokens and training this llm would cost",
      "start": 1974.2,
      "duration": 4.599
    },
    {
      "text": "about $700,000",
      "start": 1976.76,
      "duration": 3.799
    },
    {
      "text": "that's why I cannot show it to you on",
      "start": 1978.799,
      "duration": 4.0
    },
    {
      "text": "the full scale data set but I'm showing",
      "start": 1980.559,
      "duration": 4.321
    },
    {
      "text": "it to you on a smaller data set and the",
      "start": 1982.799,
      "duration": 4.0
    },
    {
      "text": "whole logic is completely scalable for",
      "start": 1984.88,
      "duration": 4.96
    },
    {
      "text": "larger data as well okay so here's the",
      "start": 1986.799,
      "duration": 4.561
    },
    {
      "text": "code the first thing what we are doing",
      "start": 1989.84,
      "duration": 3.799
    },
    {
      "text": "is that we are getting this data set",
      "start": 1991.36,
      "duration": 4.96
    },
    {
      "text": "link from here uh and I'll share this",
      "start": 1993.639,
      "duration": 4.841
    },
    {
      "text": "link with you in the YouTube description",
      "start": 1996.32,
      "duration": 4.04
    },
    {
      "text": "and I'm importing this data set over",
      "start": 1998.48,
      "duration": 6.199
    },
    {
      "text": "here so here you can see that uh um I'm",
      "start": 2000.36,
      "duration": 8.08
    },
    {
      "text": "reading I'm reading the data set and I'm",
      "start": 2004.679,
      "duration": 5.96
    },
    {
      "text": "storing the all the information all the",
      "start": 2008.44,
      "duration": 4.839
    },
    {
      "text": "text in this variable called Text data",
      "start": 2010.639,
      "duration": 4.121
    },
    {
      "text": "and let's check whether the text is",
      "start": 2013.279,
      "duration": 3.801
    },
    {
      "text": "loaded fine by printing out the first",
      "start": 2014.76,
      "duration": 4.919
    },
    {
      "text": "100 words so here I'm printing the first",
      "start": 2017.08,
      "duration": 5.719
    },
    {
      "text": "100 characters uh and here you can see",
      "start": 2019.679,
      "duration": 5.12
    },
    {
      "text": "that the first 100 characters have been",
      "start": 2022.799,
      "duration": 4.12
    },
    {
      "text": "printed and they look very closely",
      "start": 2024.799,
      "duration": 4.0
    },
    {
      "text": "matching with what was actually there in",
      "start": 2026.919,
      "duration": 4.681
    },
    {
      "text": "my text I had always thought Jack gispan",
      "start": 2028.799,
      "duration": 5.281
    },
    {
      "text": "rather a cheap genius and here also I",
      "start": 2031.6,
      "duration": 4.199
    },
    {
      "text": "had always thought Jack is rather a",
      "start": 2034.08,
      "duration": 4.24
    },
    {
      "text": "cheap genius awesome let let's print the",
      "start": 2035.799,
      "duration": 5.24
    },
    {
      "text": "last 100 characters it for me the stoud",
      "start": 2038.32,
      "duration": 4.64
    },
    {
      "text": "Strand alone and let's see whether that",
      "start": 2041.039,
      "duration": 4.201
    },
    {
      "text": "also",
      "start": 2042.96,
      "duration": 2.28
    },
    {
      "text": "matches okay I think this also matches",
      "start": 2048.44,
      "duration": 3.639
    },
    {
      "text": "the whole data set is not being",
      "start": 2050.76,
      "duration": 3.28
    },
    {
      "text": "displayed over here but now we are",
      "start": 2052.079,
      "duration": 3.32
    },
    {
      "text": "pretty sure that the data has been",
      "start": 2054.04,
      "duration": 4.079
    },
    {
      "text": "loaded fine let's move to the next step",
      "start": 2055.399,
      "duration": 4.52
    },
    {
      "text": "in the next step what I want to show is",
      "start": 2058.119,
      "duration": 3.681
    },
    {
      "text": "that we can print out the total number",
      "start": 2059.919,
      "duration": 3.68
    },
    {
      "text": "of characters in this data and it's",
      "start": 2061.8,
      "duration": 4.72
    },
    {
      "text": "20479 that's fine but remember I told",
      "start": 2063.599,
      "duration": 4.841
    },
    {
      "text": "you about the bite pair encode encoder",
      "start": 2066.52,
      "duration": 3.28
    },
    {
      "text": "what we are going to do is that we are",
      "start": 2068.44,
      "duration": 2.959
    },
    {
      "text": "going to use the bite pair encoder",
      "start": 2069.8,
      "duration": 4.4
    },
    {
      "text": "tokenizer to encode this text data and",
      "start": 2071.399,
      "duration": 4.96
    },
    {
      "text": "we had already defined the tokenizer in",
      "start": 2074.2,
      "duration": 4.04
    },
    {
      "text": "the previous lecture but I'm going to do",
      "start": 2076.359,
      "duration": 4.921
    },
    {
      "text": "it once more here so that",
      "start": 2078.24,
      "duration": 7.0
    },
    {
      "text": "uh um yeah so that everything is from",
      "start": 2081.28,
      "duration": 5.879
    },
    {
      "text": "scratch so what we are actually going to",
      "start": 2085.24,
      "duration": 4.2
    },
    {
      "text": "do is that we are going to import tick",
      "start": 2087.159,
      "duration": 4.881
    },
    {
      "text": "token uh and we we are going to Define",
      "start": 2089.44,
      "duration": 4.76
    },
    {
      "text": "this tokenizer from The Tick token",
      "start": 2092.04,
      "duration": 7.6
    },
    {
      "text": "Library so let me write it down here",
      "start": 2094.2,
      "duration": 5.44
    },
    {
      "text": "okay yeah so we are importing this tick",
      "start": 2100.52,
      "duration": 5.72
    },
    {
      "text": "token Library which is the same Library",
      "start": 2103.4,
      "duration": 5.56
    },
    {
      "text": "open AI uses for their tokenization and",
      "start": 2106.24,
      "duration": 4.44
    },
    {
      "text": "we are going to get the encoding from",
      "start": 2108.96,
      "duration": 4.08
    },
    {
      "text": "tick token which is a bite pair encoder",
      "start": 2110.68,
      "duration": 4.399
    },
    {
      "text": "character and subord level encoder",
      "start": 2113.04,
      "duration": 5.72
    },
    {
      "text": "basically and we are going to encode the",
      "start": 2115.079,
      "duration": 6.641
    },
    {
      "text": "entire text data using this encoder and",
      "start": 2118.76,
      "duration": 4.359
    },
    {
      "text": "we are going to print out the number of",
      "start": 2121.72,
      "duration": 4.399
    },
    {
      "text": "tokens right so the number of tokens are",
      "start": 2123.119,
      "duration": 6.441
    },
    {
      "text": "5145 so with 5145 tokens the text is",
      "start": 2126.119,
      "duration": 5.281
    },
    {
      "text": "very short for training and llm but",
      "start": 2129.56,
      "duration": 3.76
    },
    {
      "text": "again it's for educational",
      "start": 2131.4,
      "duration": 5.32
    },
    {
      "text": "purposes uh okay the next step is that",
      "start": 2133.32,
      "duration": 5.12
    },
    {
      "text": "we are going to divide the data set into",
      "start": 2136.72,
      "duration": 3.879
    },
    {
      "text": "training and validation data and we are",
      "start": 2138.44,
      "duration": 4.159
    },
    {
      "text": "going to use the data loader exactly",
      "start": 2140.599,
      "duration": 3.801
    },
    {
      "text": "what I told you over here right so we",
      "start": 2142.599,
      "duration": 4.041
    },
    {
      "text": "have loaded the data set now and we have",
      "start": 2144.4,
      "duration": 4.56
    },
    {
      "text": "to div divide it into training and",
      "start": 2146.64,
      "duration": 5.08
    },
    {
      "text": "validation okay let's do that before",
      "start": 2148.96,
      "duration": 4.92
    },
    {
      "text": "that what I'm going to show you is that",
      "start": 2151.72,
      "duration": 4.08
    },
    {
      "text": "remember I told you what our data loader",
      "start": 2153.88,
      "duration": 4.84
    },
    {
      "text": "does our data loader we have to specify",
      "start": 2155.8,
      "duration": 5.24
    },
    {
      "text": "this uh context size and we have to",
      "start": 2158.72,
      "duration": 5.0
    },
    {
      "text": "specify the stride our data loader Loops",
      "start": 2161.04,
      "duration": 4.559
    },
    {
      "text": "over this entire data set and creates",
      "start": 2163.72,
      "duration": 4.84
    },
    {
      "text": "this input output pairs that's what I've",
      "start": 2165.599,
      "duration": 6.0
    },
    {
      "text": "implemented in the code right now uh so",
      "start": 2168.56,
      "duration": 5.039
    },
    {
      "text": "here you can see that max length is the",
      "start": 2171.599,
      "duration": 5.24
    },
    {
      "text": "context size and stride is the uh how",
      "start": 2173.599,
      "duration": 4.921
    },
    {
      "text": "many steps we want to leave before",
      "start": 2176.839,
      "duration": 4.76
    },
    {
      "text": "creating the next input so first so we",
      "start": 2178.52,
      "duration": 5.4
    },
    {
      "text": "create two tensors input and the target",
      "start": 2181.599,
      "duration": 4.201
    },
    {
      "text": "so these are the X and Y tensors which I",
      "start": 2183.92,
      "duration": 4.96
    },
    {
      "text": "showed and then we Loop over the entire",
      "start": 2185.8,
      "duration": 4.88
    },
    {
      "text": "we Loop over the entire data set we",
      "start": 2188.88,
      "duration": 3.479
    },
    {
      "text": "create the input Chunk we create the",
      "start": 2190.68,
      "duration": 4.08
    },
    {
      "text": "target chunk which is based on the",
      "start": 2192.359,
      "duration": 4.601
    },
    {
      "text": "context length and we append it to the",
      "start": 2194.76,
      "duration": 5.319
    },
    {
      "text": "tensor so uh the first row here will be",
      "start": 2196.96,
      "duration": 5.359
    },
    {
      "text": "the first input chunk the first row in",
      "start": 2200.079,
      "duration": 3.681
    },
    {
      "text": "the Target will be the first Target",
      "start": 2202.319,
      "duration": 3.481
    },
    {
      "text": "chunk then we move over in the second",
      "start": 2203.76,
      "duration": 4.68
    },
    {
      "text": "Loop then we fill the second row of the",
      "start": 2205.8,
      "duration": 4.44
    },
    {
      "text": "input and the target similarly we Loop",
      "start": 2208.44,
      "duration": 3.52
    },
    {
      "text": "over the entire data set and fill the",
      "start": 2210.24,
      "duration": 4.68
    },
    {
      "text": "input tensor and the target",
      "start": 2211.96,
      "duration": 5.84
    },
    {
      "text": "tensor uh that's what this GPT d data",
      "start": 2214.92,
      "duration": 5.399
    },
    {
      "text": "set version one creates and then we use",
      "start": 2217.8,
      "duration": 5.559
    },
    {
      "text": "the create data loader function it takes",
      "start": 2220.319,
      "duration": 5.28
    },
    {
      "text": "the input output data sets which which",
      "start": 2223.359,
      "duration": 5.0
    },
    {
      "text": "have been created in the GPT data set V1",
      "start": 2225.599,
      "duration": 5.801
    },
    {
      "text": "class and then U we create this data",
      "start": 2228.359,
      "duration": 6.121
    },
    {
      "text": "loader instance so data loader is",
      "start": 2231.4,
      "duration": 6.36
    },
    {
      "text": "already um provided by pytor so let me",
      "start": 2234.48,
      "duration": 5.359
    },
    {
      "text": "show you this I'll also add the link to",
      "start": 2237.76,
      "duration": 4.599
    },
    {
      "text": "this so data sets and data loaders right",
      "start": 2239.839,
      "duration": 4.561
    },
    {
      "text": "they are very useful for processing data",
      "start": 2242.359,
      "duration": 4.361
    },
    {
      "text": "also in batches remember we want to use",
      "start": 2244.4,
      "duration": 4.439
    },
    {
      "text": "batches over here so using a data loader",
      "start": 2246.72,
      "duration": 5.08
    },
    {
      "text": "like this just makes uh processing the",
      "start": 2248.839,
      "duration": 5.601
    },
    {
      "text": "batches much more convenient so we we",
      "start": 2251.8,
      "duration": 4.76
    },
    {
      "text": "create an instance of this data loader",
      "start": 2254.44,
      "duration": 3.8
    },
    {
      "text": "and we feed in the data set which we",
      "start": 2256.56,
      "duration": 3.559
    },
    {
      "text": "created the input output data set which",
      "start": 2258.24,
      "duration": 4.56
    },
    {
      "text": "I mentioned over here input and targets",
      "start": 2260.119,
      "duration": 5.681
    },
    {
      "text": "and then here we specify the batch size",
      "start": 2262.8,
      "duration": 4.72
    },
    {
      "text": "uh we specify",
      "start": 2265.8,
      "duration": 4.72
    },
    {
      "text": "Shuffle uh these uh arguments are not",
      "start": 2267.52,
      "duration": 5.4
    },
    {
      "text": "useful in the current context right now",
      "start": 2270.52,
      "duration": 4.16
    },
    {
      "text": "but I'll also explain them to you later",
      "start": 2272.92,
      "duration": 3.56
    },
    {
      "text": "when we are going to train the llm uh",
      "start": 2274.68,
      "duration": 4.919
    },
    {
      "text": "see the output EX ET but remember that",
      "start": 2276.48,
      "duration": 5.32
    },
    {
      "text": "for now the only important aspect here",
      "start": 2279.599,
      "duration": 3.561
    },
    {
      "text": "is that we are going to create an",
      "start": 2281.8,
      "duration": 3.24
    },
    {
      "text": "instance of this data loader feed in",
      "start": 2283.16,
      "duration": 4.64
    },
    {
      "text": "this data set and Define the batch size",
      "start": 2285.04,
      "duration": 4.4
    },
    {
      "text": "if you want to do parallel processing",
      "start": 2287.8,
      "duration": 3.4
    },
    {
      "text": "you can also set the number of workers",
      "start": 2289.44,
      "duration": 4.919
    },
    {
      "text": "Etc okay so now an instance of the data",
      "start": 2291.2,
      "duration": 5.28
    },
    {
      "text": "loader is created and let me actually",
      "start": 2294.359,
      "duration": 4.121
    },
    {
      "text": "take some time to explain the shuffle",
      "start": 2296.48,
      "duration": 5.08
    },
    {
      "text": "and the drop last so what this suff",
      "start": 2298.48,
      "duration": 4.68
    },
    {
      "text": "Shuffle essentially does is that it",
      "start": 2301.56,
      "duration": 3.559
    },
    {
      "text": "shuffles the data set order when batches",
      "start": 2303.16,
      "duration": 4.439
    },
    {
      "text": "are created that's sometimes useful for",
      "start": 2305.119,
      "duration": 4.561
    },
    {
      "text": "generalization what this drop last",
      "start": 2307.599,
      "duration": 4.401
    },
    {
      "text": "actually does is that uh if the last",
      "start": 2309.68,
      "duration": 5.72
    },
    {
      "text": "batch size is very small and uh some",
      "start": 2312.0,
      "duration": 5.92
    },
    {
      "text": "very small data is left at the last",
      "start": 2315.4,
      "duration": 4.8
    },
    {
      "text": "batch and it's not equal to the full",
      "start": 2317.92,
      "duration": 4.36
    },
    {
      "text": "batch size then it just drops that last",
      "start": 2320.2,
      "duration": 4.119
    },
    {
      "text": "last batch so here we are setting the",
      "start": 2322.28,
      "duration": 3.72
    },
    {
      "text": "drop last equal to",
      "start": 2324.319,
      "duration": 4.401
    },
    {
      "text": "true and see the thing which I want to",
      "start": 2326.0,
      "duration": 4.44
    },
    {
      "text": "mention here is that max length equal to",
      "start": 2328.72,
      "duration": 3.8
    },
    {
      "text": "256 which means that the context size",
      "start": 2330.44,
      "duration": 4.76
    },
    {
      "text": "which we are going to use is 256 and",
      "start": 2332.52,
      "duration": 5.72
    },
    {
      "text": "we'll also see that later uh we are",
      "start": 2335.2,
      "duration": 7.0
    },
    {
      "text": "going to use a context size of 256 over",
      "start": 2338.24,
      "duration": 7.24
    },
    {
      "text": "here and that set by the",
      "start": 2342.2,
      "duration": 6.0
    },
    {
      "text": "GPT the GPT configuration which we are",
      "start": 2345.48,
      "duration": 4.839
    },
    {
      "text": "going to provide so I'll also mention it",
      "start": 2348.2,
      "duration": 4.399
    },
    {
      "text": "over",
      "start": 2350.319,
      "duration": 2.28
    },
    {
      "text": "here okay so for now I hope you have",
      "start": 2352.839,
      "duration": 5.321
    },
    {
      "text": "understood the GPT data set version one",
      "start": 2355.319,
      "duration": 4.52
    },
    {
      "text": "class and this create data loader",
      "start": 2358.16,
      "duration": 3.36
    },
    {
      "text": "function which basically creates the",
      "start": 2359.839,
      "duration": 4.361
    },
    {
      "text": "input and the output data Pairs and then",
      "start": 2361.52,
      "duration": 5.52
    },
    {
      "text": "it also specifies the batch size one",
      "start": 2364.2,
      "duration": 4.639
    },
    {
      "text": "more thing I want to mention before we",
      "start": 2367.04,
      "duration": 3.36
    },
    {
      "text": "create the training and the validation",
      "start": 2368.839,
      "duration": 3.161
    },
    {
      "text": "data set is that this is the",
      "start": 2370.4,
      "duration": 3.439
    },
    {
      "text": "configuration which we are going to use",
      "start": 2372.0,
      "duration": 4.68
    },
    {
      "text": "so look at the context length that's 256",
      "start": 2373.839,
      "duration": 5.361
    },
    {
      "text": "which means that uh we are going to look",
      "start": 2376.68,
      "duration": 5.52
    },
    {
      "text": "at 256 tokens at one time whenever I",
      "start": 2379.2,
      "duration": 5.56
    },
    {
      "text": "showed you this example here I showed",
      "start": 2382.2,
      "duration": 4.36
    },
    {
      "text": "four tokens I showed the context length",
      "start": 2384.76,
      "duration": 3.72
    },
    {
      "text": "of four because that's easier to",
      "start": 2386.56,
      "duration": 3.88
    },
    {
      "text": "demonstrate so when you try to",
      "start": 2388.48,
      "duration": 4.16
    },
    {
      "text": "understand the code always try to think",
      "start": 2390.44,
      "duration": 4.919
    },
    {
      "text": "of four as being replaced with 256 rest",
      "start": 2392.64,
      "duration": 4.439
    },
    {
      "text": "all the workflow remains exactly the",
      "start": 2395.359,
      "duration": 3.801
    },
    {
      "text": "same",
      "start": 2397.079,
      "duration": 3.76
    },
    {
      "text": "okay the next thing what we are going to",
      "start": 2399.16,
      "duration": 3.52
    },
    {
      "text": "do is that we are going to split the",
      "start": 2400.839,
      "duration": 4.28
    },
    {
      "text": "data set uh so we are going to use a",
      "start": 2402.68,
      "duration": 5.28
    },
    {
      "text": "train test split of 90% the first 90% of",
      "start": 2405.119,
      "duration": 4.881
    },
    {
      "text": "the data is a training data the",
      "start": 2407.96,
      "duration": 4.8
    },
    {
      "text": "remaining 10% is the validation data and",
      "start": 2410.0,
      "duration": 5.52
    },
    {
      "text": "here's the main part where magic happens",
      "start": 2412.76,
      "duration": 4.88
    },
    {
      "text": "so we are going to create a data loader",
      "start": 2415.52,
      "duration": 3.88
    },
    {
      "text": "based on the training data what this",
      "start": 2417.64,
      "duration": 4.28
    },
    {
      "text": "does is that it uh it splits the",
      "start": 2419.4,
      "duration": 4.16
    },
    {
      "text": "training data into the input and the",
      "start": 2421.92,
      "duration": 4.64
    },
    {
      "text": "target tensor pairs which we had seen uh",
      "start": 2423.56,
      "duration": 4.559
    },
    {
      "text": "over here",
      "start": 2426.56,
      "duration": 3.12
    },
    {
      "text": "and we are also going to create a",
      "start": 2428.119,
      "duration": 3.24
    },
    {
      "text": "validation data loader which splits the",
      "start": 2429.68,
      "duration": 3.28
    },
    {
      "text": "validation data into input and the",
      "start": 2431.359,
      "duration": 3.24
    },
    {
      "text": "target pairs because we also need the",
      "start": 2432.96,
      "duration": 4.44
    },
    {
      "text": "validation loss so here you see the",
      "start": 2434.599,
      "duration": 6.081
    },
    {
      "text": "train data loader is an object so we",
      "start": 2437.4,
      "duration": 5.48
    },
    {
      "text": "create we uh we create the train data",
      "start": 2440.68,
      "duration": 3.96
    },
    {
      "text": "loader based on this create data loader",
      "start": 2442.88,
      "duration": 5.88
    },
    {
      "text": "version one function and uh we specify",
      "start": 2444.64,
      "duration": 6.28
    },
    {
      "text": "that batch size equal to two maximum",
      "start": 2448.76,
      "duration": 4.92
    },
    {
      "text": "length is GPT config context length so",
      "start": 2450.92,
      "duration": 5.159
    },
    {
      "text": "that's 256 that's the context size",
      "start": 2453.68,
      "duration": 4.439
    },
    {
      "text": "stride equal to the context size",
      "start": 2456.079,
      "duration": 3.961
    },
    {
      "text": "remember I had mentioned to you that",
      "start": 2458.119,
      "duration": 4.2
    },
    {
      "text": "generally when these llm architectures",
      "start": 2460.04,
      "duration": 4.16
    },
    {
      "text": "are run the stride and the context size",
      "start": 2462.319,
      "duration": 5.681
    },
    {
      "text": "are um matched because we make sure that",
      "start": 2464.2,
      "duration": 5.639
    },
    {
      "text": "no word is lost but at the same time",
      "start": 2468.0,
      "duration": 4.0
    },
    {
      "text": "there is no overlap between consecutive",
      "start": 2469.839,
      "duration": 4.801
    },
    {
      "text": "inputs awesome right and then drop last",
      "start": 2472.0,
      "duration": 4.52
    },
    {
      "text": "equal to True Shuffle equal to true and",
      "start": 2474.64,
      "duration": 4.199
    },
    {
      "text": "we are not doing parallel processing so",
      "start": 2476.52,
      "duration": 4.48
    },
    {
      "text": "I I'm putting number of workers equal to",
      "start": 2478.839,
      "duration": 4.76
    },
    {
      "text": "zero similarly we construct the",
      "start": 2481.0,
      "duration": 5.52
    },
    {
      "text": "validation loader with a batch size of",
      "start": 2483.599,
      "duration": 4.921
    },
    {
      "text": "two MA X length which is the context",
      "start": 2486.52,
      "duration": 4.4
    },
    {
      "text": "length of 1024 and the stride sorry",
      "start": 2488.52,
      "duration": 5.2
    },
    {
      "text": "context length of 256 and the stride of",
      "start": 2490.92,
      "duration": 5.6
    },
    {
      "text": "256 when gpt2 smallest version was",
      "start": 2493.72,
      "duration": 4.32
    },
    {
      "text": "trained they actually used a context",
      "start": 2496.52,
      "duration": 4.12
    },
    {
      "text": "length of 1024 and you can even do that",
      "start": 2498.04,
      "duration": 5.079
    },
    {
      "text": "but it just takes a long time uh all you",
      "start": 2500.64,
      "duration": 4.959
    },
    {
      "text": "need to do is just replace this with",
      "start": 2503.119,
      "duration": 4.841
    },
    {
      "text": "1024 and just run the same code which",
      "start": 2505.599,
      "duration": 4.801
    },
    {
      "text": "I'll be providing to you but please be",
      "start": 2507.96,
      "duration": 3.96
    },
    {
      "text": "patient when you run the code on your",
      "start": 2510.4,
      "duration": 3.84
    },
    {
      "text": "end it might take some time we can do",
      "start": 2511.92,
      "duration": 5.84
    },
    {
      "text": "some sanity check so ideally the number",
      "start": 2514.24,
      "duration": 5.68
    },
    {
      "text": "of uh tokens which we want in our",
      "start": 2517.76,
      "duration": 3.839
    },
    {
      "text": "training data set should not be less",
      "start": 2519.92,
      "duration": 4.36
    },
    {
      "text": "than our back context length right",
      "start": 2521.599,
      "duration": 4.24
    },
    {
      "text": "because then we don't have enough tokens",
      "start": 2524.28,
      "duration": 3.76
    },
    {
      "text": "to predict the next word so here I have",
      "start": 2525.839,
      "duration": 4.28
    },
    {
      "text": "just written that if this is the case if",
      "start": 2528.04,
      "duration": 3.92
    },
    {
      "text": "our number of training tokens is less",
      "start": 2530.119,
      "duration": 3.96
    },
    {
      "text": "than our context length then print an",
      "start": 2531.96,
      "duration": 4.08
    },
    {
      "text": "error similarly if the number of",
      "start": 2534.079,
      "duration": 3.881
    },
    {
      "text": "validation tokens is less than the",
      "start": 2536.04,
      "duration": 4.2
    },
    {
      "text": "context length print an error it does",
      "start": 2537.96,
      "duration": 3.76
    },
    {
      "text": "not print an error which means we are",
      "start": 2540.24,
      "duration": 4.879
    },
    {
      "text": "good to go uh one more thing I want to",
      "start": 2541.72,
      "duration": 4.76
    },
    {
      "text": "mention here is that we are using a",
      "start": 2545.119,
      "duration": 3.601
    },
    {
      "text": "batch size of two in large language",
      "start": 2546.48,
      "duration": 4.56
    },
    {
      "text": "models in training GPT level models they",
      "start": 2548.72,
      "duration": 4.52
    },
    {
      "text": "usually use a pretty large batch size",
      "start": 2551.04,
      "duration": 4.36
    },
    {
      "text": "but we use a relatively small batch size",
      "start": 2553.24,
      "duration": 3.8
    },
    {
      "text": "to reduce the computational resource",
      "start": 2555.4,
      "duration": 4.12
    },
    {
      "text": "demand and because the data set is also",
      "start": 2557.04,
      "duration": 5.24
    },
    {
      "text": "very small to begin with to give you a",
      "start": 2559.52,
      "duration": 5.0
    },
    {
      "text": "context Lama 2 7 billion was trained",
      "start": 2562.28,
      "duration": 4.72
    },
    {
      "text": "with a batch size of 1024 here we are",
      "start": 2564.52,
      "duration": 4.64
    },
    {
      "text": "using batch size of two because I want",
      "start": 2567.0,
      "duration": 4.8
    },
    {
      "text": "to run it very quickly on my",
      "start": 2569.16,
      "duration": 5.399
    },
    {
      "text": "laptop one more check we can do to make",
      "start": 2571.8,
      "duration": 4.519
    },
    {
      "text": "sure that the data is loaded correctly",
      "start": 2574.559,
      "duration": 4.081
    },
    {
      "text": "is that remember both in the training",
      "start": 2576.319,
      "duration": 4.401
    },
    {
      "text": "and the validation there are now X and Y",
      "start": 2578.64,
      "duration": 5.0
    },
    {
      "text": "pairs input and Target pairs uh so the",
      "start": 2580.72,
      "duration": 4.839
    },
    {
      "text": "training has inputs and targets and the",
      "start": 2583.64,
      "duration": 4.36
    },
    {
      "text": "validation is inputs and targets let's",
      "start": 2585.559,
      "duration": 3.921
    },
    {
      "text": "actually print out the shape of these",
      "start": 2588.0,
      "duration": 3.64
    },
    {
      "text": "inputs and targets so the training",
      "start": 2589.48,
      "duration": 5.32
    },
    {
      "text": "loader has this if you print out the X",
      "start": 2591.64,
      "duration": 4.919
    },
    {
      "text": "and the y shape in the training loader",
      "start": 2594.8,
      "duration": 4.24
    },
    {
      "text": "it will look like this and if you print",
      "start": 2596.559,
      "duration": 4.161
    },
    {
      "text": "out the X and Y shape in the validation",
      "start": 2599.04,
      "duration": 3.68
    },
    {
      "text": "loader it will look like this so if you",
      "start": 2600.72,
      "duration": 3.839
    },
    {
      "text": "look at the train loader let's look at",
      "start": 2602.72,
      "duration": 4.8
    },
    {
      "text": "the first row this is the X and what I'm",
      "start": 2604.559,
      "duration": 5.161
    },
    {
      "text": "highlighting now is the Y what this",
      "start": 2607.52,
      "duration": 4.2
    },
    {
      "text": "represents is that the",
      "start": 2609.72,
      "duration": 5.359
    },
    {
      "text": "input um so in one batch so this is one",
      "start": 2611.72,
      "duration": 5.2
    },
    {
      "text": "batch so first row corresponds to the",
      "start": 2615.079,
      "duration": 4.04
    },
    {
      "text": "first batch the First Column of the",
      "start": 2616.92,
      "duration": 3.919
    },
    {
      "text": "first row is the input the second column",
      "start": 2619.119,
      "duration": 3.72
    },
    {
      "text": "of the first row is the output if you",
      "start": 2620.839,
      "duration": 3.881
    },
    {
      "text": "look at the first",
      "start": 2622.839,
      "duration": 4.361
    },
    {
      "text": "batch input you'll see that there are",
      "start": 2624.72,
      "duration": 7.28
    },
    {
      "text": "two samples each sample has 256 tokens",
      "start": 2627.2,
      "duration": 7.0
    },
    {
      "text": "similarly if you look at the first batch",
      "start": 2632.0,
      "duration": 4.2
    },
    {
      "text": "output you'll see that there are two",
      "start": 2634.2,
      "duration": 4.28
    },
    {
      "text": "samples and two 56 tokens this is the",
      "start": 2636.2,
      "duration": 4.399
    },
    {
      "text": "target which we want and this is the",
      "start": 2638.48,
      "duration": 6.119
    },
    {
      "text": "input which is there similarly uh since",
      "start": 2640.599,
      "duration": 7.76
    },
    {
      "text": "256 tokens are exhausted um in the input",
      "start": 2644.599,
      "duration": 5.48
    },
    {
      "text": "and we have to Loop over the entire data",
      "start": 2648.359,
      "duration": 3.561
    },
    {
      "text": "set there are it turns out that there",
      "start": 2650.079,
      "duration": 4.641
    },
    {
      "text": "are nine such batches which are created",
      "start": 2651.92,
      "duration": 5.36
    },
    {
      "text": "uh in the training data and there is one",
      "start": 2654.72,
      "duration": 4.24
    },
    {
      "text": "batch which is created in the validation",
      "start": 2657.28,
      "duration": 4.039
    },
    {
      "text": "data similar to the training data in the",
      "start": 2658.96,
      "duration": 4.28
    },
    {
      "text": "validation data you'll see that the",
      "start": 2661.319,
      "duration": 4.081
    },
    {
      "text": "batch has two samples each sample has",
      "start": 2663.24,
      "duration": 5.079
    },
    {
      "text": "256 tokens and I also printed the length",
      "start": 2665.4,
      "duration": 4.76
    },
    {
      "text": "of the training loader here and you can",
      "start": 2668.319,
      "duration": 4.28
    },
    {
      "text": "even print the length of",
      "start": 2670.16,
      "duration": 5.159
    },
    {
      "text": "the validation loader and you will get",
      "start": 2672.599,
      "duration": 5.281
    },
    {
      "text": "that uh the length of the training",
      "start": 2675.319,
      "duration": 4.441
    },
    {
      "text": "loader is equal to 9 because there are",
      "start": 2677.88,
      "duration": 4.4
    },
    {
      "text": "nine batches each batch has two samples",
      "start": 2679.76,
      "duration": 4.12
    },
    {
      "text": "and the length of the validation loader",
      "start": 2682.28,
      "duration": 4.079
    },
    {
      "text": "is equal to one and uh there's just one",
      "start": 2683.88,
      "duration": 5.16
    },
    {
      "text": "batch with two samples I hope now this",
      "start": 2686.359,
      "duration": 6.441
    },
    {
      "text": "part is clear to you to to make sure you",
      "start": 2689.04,
      "duration": 5.799
    },
    {
      "text": "understand this part that is why I",
      "start": 2692.8,
      "duration": 3.64
    },
    {
      "text": "actually went through this entire",
      "start": 2694.839,
      "duration": 3.841
    },
    {
      "text": "whiteboard demonstration to show you",
      "start": 2696.44,
      "duration": 4.52
    },
    {
      "text": "that towards the end we are going to get",
      "start": 2698.68,
      "duration": 5.12
    },
    {
      "text": "something like this in the in the code",
      "start": 2700.96,
      "duration": 4.56
    },
    {
      "text": "and remember I spent some time to",
      "start": 2703.8,
      "duration": 5.039
    },
    {
      "text": "explain these sizes and these Dimensions",
      "start": 2705.52,
      "duration": 5.92
    },
    {
      "text": "I hope you are following along and if I",
      "start": 2708.839,
      "duration": 4.161
    },
    {
      "text": "directly went through the code and when",
      "start": 2711.44,
      "duration": 3.119
    },
    {
      "text": "you reach this part it it would have",
      "start": 2713.0,
      "duration": 3.24
    },
    {
      "text": "been impossible for you to understand",
      "start": 2714.559,
      "duration": 3.601
    },
    {
      "text": "this that's why it was very important",
      "start": 2716.24,
      "duration": 3.319
    },
    {
      "text": "for me to go through this entire",
      "start": 2718.16,
      "duration": 3.24
    },
    {
      "text": "whiteboard demonstration so that you",
      "start": 2719.559,
      "duration": 3.161
    },
    {
      "text": "understand the dimensions of what's",
      "start": 2721.4,
      "duration": 4.24
    },
    {
      "text": "really going on so up till now what we",
      "start": 2722.72,
      "duration": 4.68
    },
    {
      "text": "have created is that we have created Ed",
      "start": 2725.64,
      "duration": 4.0
    },
    {
      "text": "the we have the input and the targets",
      "start": 2727.4,
      "duration": 4.199
    },
    {
      "text": "and we have badged them into the input",
      "start": 2729.64,
      "duration": 5.12
    },
    {
      "text": "and the uh Target data but we have still",
      "start": 2731.599,
      "duration": 5.161
    },
    {
      "text": "not got the output predictions right we",
      "start": 2734.76,
      "duration": 5.28
    },
    {
      "text": "have still not um got the GPT model",
      "start": 2736.76,
      "duration": 5.319
    },
    {
      "text": "predictions so that's what we are going",
      "start": 2740.04,
      "duration": 3.279
    },
    {
      "text": "to do",
      "start": 2742.079,
      "duration": 3.961
    },
    {
      "text": "next uh one more thing before going next",
      "start": 2743.319,
      "duration": 4.24
    },
    {
      "text": "is that we can print out the training",
      "start": 2746.04,
      "duration": 3.16
    },
    {
      "text": "tokens and validation",
      "start": 2747.559,
      "duration": 3.28
    },
    {
      "text": "tokens",
      "start": 2749.2,
      "duration": 4.84
    },
    {
      "text": "uh uh just for the sake of Sanity so",
      "start": 2750.839,
      "duration": 4.681
    },
    {
      "text": "this makes sure that the data is now",
      "start": 2754.04,
      "duration": 4.799
    },
    {
      "text": "loaded correctly now we can actually go",
      "start": 2755.52,
      "duration": 5.12
    },
    {
      "text": "to the next part which is getting the",
      "start": 2758.839,
      "duration": 4.321
    },
    {
      "text": "llm model outputs so in one of the",
      "start": 2760.64,
      "duration": 4.479
    },
    {
      "text": "previous lectures we have defined this",
      "start": 2763.16,
      "duration": 4.64
    },
    {
      "text": "GPT model class what this GPT model",
      "start": 2765.119,
      "duration": 5.121
    },
    {
      "text": "class does is that uh it essentially",
      "start": 2767.8,
      "duration": 4.12
    },
    {
      "text": "implements every single thing what I've",
      "start": 2770.24,
      "duration": 4.68
    },
    {
      "text": "shown in this figure it takes the inputs",
      "start": 2771.92,
      "duration": 5.6
    },
    {
      "text": "it takes the inputs and then it returns",
      "start": 2774.92,
      "duration": 5.159
    },
    {
      "text": "a loged sensor remember the logic sensor",
      "start": 2777.52,
      "duration": 4.319
    },
    {
      "text": "as it is returned does not encode",
      "start": 2780.079,
      "duration": 4.48
    },
    {
      "text": "probabilities we need to convert it to a",
      "start": 2781.839,
      "duration": 5.641
    },
    {
      "text": "probability tensor using the soft Max so",
      "start": 2784.559,
      "duration": 4.641
    },
    {
      "text": "the GPT model class which we have",
      "start": 2787.48,
      "duration": 4.599
    },
    {
      "text": "constructed Returns the logic sensor and",
      "start": 2789.2,
      "duration": 5.0
    },
    {
      "text": "we have several lectures on this for now",
      "start": 2792.079,
      "duration": 5.441
    },
    {
      "text": "you can just um keep in mind that okay",
      "start": 2794.2,
      "duration": 4.879
    },
    {
      "text": "first the inputs are converted into",
      "start": 2797.52,
      "duration": 3.36
    },
    {
      "text": "token embeddings we add the positional",
      "start": 2799.079,
      "duration": 4.681
    },
    {
      "text": "embeddings then we add a Dropout layer",
      "start": 2800.88,
      "duration": 5.6
    },
    {
      "text": "then we pass the output of the Dropout",
      "start": 2803.76,
      "duration": 5.12
    },
    {
      "text": "layer to through this Transformer block",
      "start": 2806.48,
      "duration": 3.92
    },
    {
      "text": "this Transformer block which I",
      "start": 2808.88,
      "duration": 2.8
    },
    {
      "text": "highlighted right now that has the",
      "start": 2810.4,
      "duration": 3.679
    },
    {
      "text": "multi-head attention mechanism which is",
      "start": 2811.68,
      "duration": 5.24
    },
    {
      "text": "the main engine behind the llm power",
      "start": 2814.079,
      "duration": 4.561
    },
    {
      "text": "after coming out of the Transformer we",
      "start": 2816.92,
      "duration": 3.84
    },
    {
      "text": "have another layer normalization layer",
      "start": 2818.64,
      "duration": 4.0
    },
    {
      "text": "followed by output neural network which",
      "start": 2820.76,
      "duration": 3.52
    },
    {
      "text": "gives us this loged",
      "start": 2822.64,
      "duration": 4.04
    },
    {
      "text": "sensor then we create an instance of",
      "start": 2824.28,
      "duration": 4.68
    },
    {
      "text": "this GPT model class and we call it",
      "start": 2826.68,
      "duration": 4.76
    },
    {
      "text": "model and we are using the same GPT",
      "start": 2828.96,
      "duration": 4.96
    },
    {
      "text": "model config 124 million parameters",
      "start": 2831.44,
      "duration": 4.96
    },
    {
      "text": "which I had defined over here we have to",
      "start": 2833.92,
      "duration": 4.56
    },
    {
      "text": "specify the vocabulary size context",
      "start": 2836.4,
      "duration": 4.4
    },
    {
      "text": "length embedding Dimension number of",
      "start": 2838.48,
      "duration": 4.119
    },
    {
      "text": "attention heads number of Transformer",
      "start": 2840.8,
      "duration": 3.88
    },
    {
      "text": "blocks dropout rate and whether the",
      "start": 2842.599,
      "duration": 4.881
    },
    {
      "text": "query key value bias is set to false in",
      "start": 2844.68,
      "duration": 4.399
    },
    {
      "text": "this lecture I'm not going to explain",
      "start": 2847.48,
      "duration": 3.32
    },
    {
      "text": "all of these parameters because that was",
      "start": 2849.079,
      "duration": 4.201
    },
    {
      "text": "the subject of previous lectures uh if",
      "start": 2850.8,
      "duration": 3.84
    },
    {
      "text": "you don't understand what those",
      "start": 2853.28,
      "duration": 3.36
    },
    {
      "text": "parameters mean I encourage you to check",
      "start": 2854.64,
      "duration": 3.76
    },
    {
      "text": "out the previous lectures in a lot of",
      "start": 2856.64,
      "duration": 3.719
    },
    {
      "text": "detail we have around six lectures on",
      "start": 2858.4,
      "duration": 5.0
    },
    {
      "text": "that and uh six lectures explaining how",
      "start": 2860.359,
      "duration": 5.921
    },
    {
      "text": "we constructed this GPT model class for",
      "start": 2863.4,
      "duration": 5.52
    },
    {
      "text": "now just remember that we have got the",
      "start": 2866.28,
      "duration": 5.0
    },
    {
      "text": "output Logics and we have constructed an",
      "start": 2868.92,
      "duration": 4.399
    },
    {
      "text": "instance of the GPT model class so when",
      "start": 2871.28,
      "duration": 3.88
    },
    {
      "text": "you pass an instance when you pass an",
      "start": 2873.319,
      "duration": 4.441
    },
    {
      "text": "input to this model it will give you the",
      "start": 2875.16,
      "duration": 4.84
    },
    {
      "text": "logits now we are actually ready to",
      "start": 2877.76,
      "duration": 4.88
    },
    {
      "text": "implement the loss because we have the",
      "start": 2880.0,
      "duration": 5.0
    },
    {
      "text": "uh we have the targets over here we have",
      "start": 2882.64,
      "duration": 5.24
    },
    {
      "text": "the targets over here and we also have",
      "start": 2885.0,
      "duration": 5.359
    },
    {
      "text": "the GPT model output and now we are",
      "start": 2887.88,
      "duration": 4.08
    },
    {
      "text": "actually going to implement the exact",
      "start": 2890.359,
      "duration": 3.441
    },
    {
      "text": "same steps over here remember First We",
      "start": 2891.96,
      "duration": 4.84
    },
    {
      "text": "Take the soft Max then we index with the",
      "start": 2893.8,
      "duration": 5.0
    },
    {
      "text": "probabilities uh then we index these",
      "start": 2896.8,
      "duration": 4.319
    },
    {
      "text": "tokens based on the target tokens and",
      "start": 2898.8,
      "duration": 5.4
    },
    {
      "text": "then we get the cross entropy loss right",
      "start": 2901.119,
      "duration": 5.681
    },
    {
      "text": "using the negative log likelihood and we",
      "start": 2904.2,
      "duration": 4.359
    },
    {
      "text": "did the same thing for this batch over",
      "start": 2906.8,
      "duration": 4.0
    },
    {
      "text": "here so now I want you to keep this in",
      "start": 2908.559,
      "duration": 4.28
    },
    {
      "text": "mind when we had a batch remember what",
      "start": 2910.8,
      "duration": 4.24
    },
    {
      "text": "we did first when we had a batch we",
      "start": 2912.839,
      "duration": 4.401
    },
    {
      "text": "first flatten the logits right this is",
      "start": 2915.04,
      "duration": 3.72
    },
    {
      "text": "exactly what we are going to do when we",
      "start": 2917.24,
      "duration": 3.44
    },
    {
      "text": "calculate the loss so there is a",
      "start": 2918.76,
      "duration": 4.0
    },
    {
      "text": "function called calculate loss batch",
      "start": 2920.68,
      "duration": 3.919
    },
    {
      "text": "which takes the input batch and the",
      "start": 2922.76,
      "duration": 5.04
    },
    {
      "text": "target batch right what this means is",
      "start": 2924.599,
      "duration": 5.2
    },
    {
      "text": "that it's exactly like what I've shown",
      "start": 2927.8,
      "duration": 4.36
    },
    {
      "text": "over here this is the input batch X and",
      "start": 2929.799,
      "duration": 4.641
    },
    {
      "text": "this is the target batch y it just that",
      "start": 2932.16,
      "duration": 5.12
    },
    {
      "text": "instead of four tokens there will be 256",
      "start": 2934.44,
      "duration": 4.6
    },
    {
      "text": "then what we are going to do in the code",
      "start": 2937.28,
      "duration": 4.0
    },
    {
      "text": "is that uh we are going to pass the",
      "start": 2939.04,
      "duration": 4.68
    },
    {
      "text": "input batch through the model the GPT",
      "start": 2941.28,
      "duration": 5.2
    },
    {
      "text": "model and gets the logit tensor so until",
      "start": 2943.72,
      "duration": 4.24
    },
    {
      "text": "now in the code we are at this stage",
      "start": 2946.48,
      "duration": 3.599
    },
    {
      "text": "where we have got the logit tensor then",
      "start": 2947.96,
      "duration": 5.2
    },
    {
      "text": "we are going to flatten the logits um 0",
      "start": 2950.079,
      "duration": 4.601
    },
    {
      "text": "comma 1 so see we are going to flatten",
      "start": 2953.16,
      "duration": 6.08
    },
    {
      "text": "the logit 0 comma 1 uh and we get this",
      "start": 2954.68,
      "duration": 6.08
    },
    {
      "text": "now remember up till now we have not",
      "start": 2959.24,
      "duration": 4.72
    },
    {
      "text": "implemented soft Max we have not indexed",
      "start": 2960.76,
      "duration": 6.079
    },
    {
      "text": "this uh probability tensor with the",
      "start": 2963.96,
      "duration": 5.0
    },
    {
      "text": "Target index indices and we have not got",
      "start": 2966.839,
      "duration": 4.76
    },
    {
      "text": "the negative log likelihood it turns out",
      "start": 2968.96,
      "duration": 4.76
    },
    {
      "text": "that with just one line of code nn.",
      "start": 2971.599,
      "duration": 4.0
    },
    {
      "text": "functional doc cross entropy we can do",
      "start": 2973.72,
      "duration": 4.8
    },
    {
      "text": "all of these three steps so when you do",
      "start": 2975.599,
      "duration": 5.281
    },
    {
      "text": "the nn. functional. cross entropy on the",
      "start": 2978.52,
      "duration": 4.839
    },
    {
      "text": "flats logic tensor and the flatten",
      "start": 2980.88,
      "duration": 4.679
    },
    {
      "text": "Target batch so remember the flatten",
      "start": 2983.359,
      "duration": 5.081
    },
    {
      "text": "Target batch is this is this tensor over",
      "start": 2985.559,
      "duration": 6.76
    },
    {
      "text": "here this is the flatten targets batch",
      "start": 2988.44,
      "duration": 6.72
    },
    {
      "text": "so what the nn. functional doc cross",
      "start": 2992.319,
      "duration": 4.961
    },
    {
      "text": "entropy does is that",
      "start": 2995.16,
      "duration": 5.24
    },
    {
      "text": "it first applies soft Max to the logic",
      "start": 2997.28,
      "duration": 5.2
    },
    {
      "text": "uh tensor because that's the first",
      "start": 3000.4,
      "duration": 4.04
    },
    {
      "text": "argument it first applies softmax to",
      "start": 3002.48,
      "duration": 4.8
    },
    {
      "text": "this first argument uh which is also",
      "start": 3004.44,
      "duration": 5.56
    },
    {
      "text": "shown in this white board and then it",
      "start": 3007.28,
      "duration": 5.68
    },
    {
      "text": "takes the uh values corresponding to the",
      "start": 3010.0,
      "duration": 5.2
    },
    {
      "text": "indices in the second argument so then",
      "start": 3012.96,
      "duration": 3.52
    },
    {
      "text": "it takes the values in this",
      "start": 3015.2,
      "duration": 3.08
    },
    {
      "text": "corresponding to the indices in this",
      "start": 3016.48,
      "duration": 6.639
    },
    {
      "text": "argument so it it then gets this P11 p12",
      "start": 3018.28,
      "duration": 6.72
    },
    {
      "text": "Etc this Matrix and then it also gets",
      "start": 3023.119,
      "duration": 3.761
    },
    {
      "text": "the negative log likelihood it calculat",
      "start": 3025.0,
      "duration": 4.119
    },
    {
      "text": "the negative log likelihood so in one",
      "start": 3026.88,
      "duration": 5.6
    },
    {
      "text": "line of code we actually get the loss",
      "start": 3029.119,
      "duration": 6.081
    },
    {
      "text": "and this is an awesome function which is",
      "start": 3032.48,
      "duration": 5.16
    },
    {
      "text": "a very powerful function in pytorch you",
      "start": 3035.2,
      "duration": 4.44
    },
    {
      "text": "can take a look at this I'll also share",
      "start": 3037.64,
      "duration": 5.88
    },
    {
      "text": "the link to this uh this uh torch P",
      "start": 3039.64,
      "duration": 6.199
    },
    {
      "text": "torch function with you awesome so this",
      "start": 3043.52,
      "duration": 5.0
    },
    {
      "text": "is how we calculate the loss between an",
      "start": 3045.839,
      "duration": 5.081
    },
    {
      "text": "input batch and a Target batch but now",
      "start": 3048.52,
      "duration": 4.799
    },
    {
      "text": "remember that we have to calculate the",
      "start": 3050.92,
      "duration": 4.639
    },
    {
      "text": "loss for all of the batches right and",
      "start": 3053.319,
      "duration": 3.8
    },
    {
      "text": "that's why we are defining a function",
      "start": 3055.559,
      "duration": 3.481
    },
    {
      "text": "called calculate loss loader which",
      "start": 3057.119,
      "duration": 3.561
    },
    {
      "text": "calculates the loss from all of the",
      "start": 3059.04,
      "duration": 3.92
    },
    {
      "text": "batches together the main function in",
      "start": 3060.68,
      "duration": 4.879
    },
    {
      "text": "this the main part in this is that you",
      "start": 3062.96,
      "duration": 5.08
    },
    {
      "text": "get the input and Target batch for the",
      "start": 3065.559,
      "duration": 5.121
    },
    {
      "text": "entire data loader which means that uh",
      "start": 3068.04,
      "duration": 5.039
    },
    {
      "text": "so here we just looked at one input and",
      "start": 3070.68,
      "duration": 4.96
    },
    {
      "text": "one output batch right one target batch",
      "start": 3073.079,
      "duration": 3.961
    },
    {
      "text": "but you will see here there are many",
      "start": 3075.64,
      "duration": 4.64
    },
    {
      "text": "input and Out target batches uh so Row",
      "start": 3077.04,
      "duration": 5.16
    },
    {
      "text": "one row one of input and Row one of",
      "start": 3080.28,
      "duration": 3.64
    },
    {
      "text": "Target is the first batch row two is the",
      "start": 3082.2,
      "duration": 3.0
    },
    {
      "text": "second batch so there are multiple",
      "start": 3083.92,
      "duration": 3.28
    },
    {
      "text": "batches and we have to miate the loss",
      "start": 3085.2,
      "duration": 4.28
    },
    {
      "text": "for all of those right so similarly you",
      "start": 3087.2,
      "duration": 4.8
    },
    {
      "text": "get the input and Target batch uh and",
      "start": 3089.48,
      "duration": 4.319
    },
    {
      "text": "then you Loop over so when you're",
      "start": 3092.0,
      "duration": 3.72
    },
    {
      "text": "looking at one batch you just run this",
      "start": 3093.799,
      "duration": 4.0
    },
    {
      "text": "earlier function and then you just",
      "start": 3095.72,
      "duration": 4.24
    },
    {
      "text": "aggregate the losses together so when",
      "start": 3097.799,
      "duration": 4.28
    },
    {
      "text": "you uh run the loss for one batch you'll",
      "start": 3099.96,
      "duration": 3.68
    },
    {
      "text": "get the loss then you add it with the",
      "start": 3102.079,
      "duration": 3.28
    },
    {
      "text": "loss for the second batch and similarly",
      "start": 3103.64,
      "duration": 3.76
    },
    {
      "text": "you get the total loss and then",
      "start": 3105.359,
      "duration": 3.96
    },
    {
      "text": "ultimately you just divide the total",
      "start": 3107.4,
      "duration": 3.64
    },
    {
      "text": "loss with the number of batches which",
      "start": 3109.319,
      "duration": 3.961
    },
    {
      "text": "will give you a mean loss per",
      "start": 3111.04,
      "duration": 4.68
    },
    {
      "text": "batch the different parts of the code",
      "start": 3113.28,
      "duration": 4.24
    },
    {
      "text": "which are added before ensure that if",
      "start": 3115.72,
      "duration": 4.44
    },
    {
      "text": "the length of the data loader is zero it",
      "start": 3117.52,
      "duration": 5.599
    },
    {
      "text": "we return that the loss is not a number",
      "start": 3120.16,
      "duration": 5.0
    },
    {
      "text": "because length of data loader is zero",
      "start": 3123.119,
      "duration": 5.48
    },
    {
      "text": "does not make sense both our uh training",
      "start": 3125.16,
      "duration": 5.0
    },
    {
      "text": "and the validation data loaders",
      "start": 3128.599,
      "duration": 3.48
    },
    {
      "text": "currently training data loader is of",
      "start": 3130.16,
      "duration": 3.36
    },
    {
      "text": "length nine because there are nine",
      "start": 3132.079,
      "duration": 3.76
    },
    {
      "text": "batches validation data loader is of",
      "start": 3133.52,
      "duration": 4.4
    },
    {
      "text": "length one because there is one batch if",
      "start": 3135.839,
      "duration": 3.801
    },
    {
      "text": "the length of the data loader is itself",
      "start": 3137.92,
      "duration": 3.04
    },
    {
      "text": "zero which means that there are no",
      "start": 3139.64,
      "duration": 3.6
    },
    {
      "text": "batches and there is nothing to compute",
      "start": 3140.96,
      "duration": 5.28
    },
    {
      "text": "similarly when we uh when we call this",
      "start": 3143.24,
      "duration": 3.8
    },
    {
      "text": "Cal",
      "start": 3146.24,
      "duration": 2.8
    },
    {
      "text": "caloss loader function and if we don't",
      "start": 3147.04,
      "duration": 4.48
    },
    {
      "text": "specify the number of batches so if by",
      "start": 3149.04,
      "duration": 4.48
    },
    {
      "text": "default it's none we set the number of",
      "start": 3151.52,
      "duration": 3.559
    },
    {
      "text": "batches equal to the length of the data",
      "start": 3153.52,
      "duration": 3.76
    },
    {
      "text": "loader so for the training data loader",
      "start": 3155.079,
      "duration": 3.641
    },
    {
      "text": "that will be equal to 9 for the",
      "start": 3157.28,
      "duration": 2.96
    },
    {
      "text": "validation data loader that will be",
      "start": 3158.72,
      "duration": 4.599
    },
    {
      "text": "equal to one now if someone specifies",
      "start": 3160.24,
      "duration": 5.8
    },
    {
      "text": "the number of batches here which are",
      "start": 3163.319,
      "duration": 4.921
    },
    {
      "text": "more than the number of batches in the",
      "start": 3166.04,
      "duration": 4.6
    },
    {
      "text": "data loader we set the actual number of",
      "start": 3168.24,
      "duration": 5.04
    },
    {
      "text": "batches to be minimum of those",
      "start": 3170.64,
      "duration": 5.36
    },
    {
      "text": "two uh right so the number of batches",
      "start": 3173.28,
      "duration": 4.4
    },
    {
      "text": "equal to minimum of number of batches",
      "start": 3176.0,
      "duration": 3.76
    },
    {
      "text": "set here and remember that in the data",
      "start": 3177.68,
      "duration": 3.84
    },
    {
      "text": "loader also there is a provision to set",
      "start": 3179.76,
      "duration": 4.24
    },
    {
      "text": "the number of batches so the ultimate",
      "start": 3181.52,
      "duration": 4.279
    },
    {
      "text": "batch size which is used for computation",
      "start": 3184.0,
      "duration": 4.2
    },
    {
      "text": "will be minimum of those two that's it",
      "start": 3185.799,
      "duration": 5.361
    },
    {
      "text": "and then we take the one input and one",
      "start": 3188.2,
      "duration": 4.919
    },
    {
      "text": "target at a time we find the loss",
      "start": 3191.16,
      "duration": 3.88
    },
    {
      "text": "according to this scal loss batch",
      "start": 3193.119,
      "duration": 4.281
    },
    {
      "text": "function which implements the uh",
      "start": 3195.04,
      "duration": 4.64
    },
    {
      "text": "functional cross nn. functional. cross",
      "start": 3197.4,
      "duration": 5.679
    },
    {
      "text": "entropy loss and then we actually add",
      "start": 3199.68,
      "duration": 5.159
    },
    {
      "text": "all of the losses together from every",
      "start": 3203.079,
      "duration": 4.321
    },
    {
      "text": "input Target batch and then we just",
      "start": 3204.839,
      "duration": 4.881
    },
    {
      "text": "divide by the number of batches and this",
      "start": 3207.4,
      "duration": 6.24
    },
    {
      "text": "is how we got get the average uh cross",
      "start": 3209.72,
      "duration": 7.04
    },
    {
      "text": "entropy loss per batch this the output",
      "start": 3213.64,
      "duration": 5.0
    },
    {
      "text": "of this function is the loss of our",
      "start": 3216.76,
      "duration": 4.359
    },
    {
      "text": "large language model on this book The",
      "start": 3218.64,
      "duration": 4.439
    },
    {
      "text": "Verdict data set which we considered in",
      "start": 3221.119,
      "duration": 3.121
    },
    {
      "text": "today's",
      "start": 3223.079,
      "duration": 4.48
    },
    {
      "text": "lecture now let's actually run uh let's",
      "start": 3224.24,
      "duration": 5.0
    },
    {
      "text": "actually call this function on the data",
      "start": 3227.559,
      "duration": 3.361
    },
    {
      "text": "which we have and let's see the output",
      "start": 3229.24,
      "duration": 2.8
    },
    {
      "text": "which we get",
      "start": 3230.92,
      "duration": 3.72
    },
    {
      "text": "right okay so what I'm going to do now",
      "start": 3232.04,
      "duration": 6.16
    },
    {
      "text": "is that uh I'm going to call this Cal",
      "start": 3234.64,
      "duration": 6.479
    },
    {
      "text": "Closs loader and I'm going to uh input",
      "start": 3238.2,
      "duration": 5.72
    },
    {
      "text": "the train loader the model and the",
      "start": 3241.119,
      "duration": 6.0
    },
    {
      "text": "device and here you see tor. device if",
      "start": 3243.92,
      "duration": 5.439
    },
    {
      "text": "tor. Qi is available else it will run on",
      "start": 3247.119,
      "duration": 6.24
    },
    {
      "text": "CPU so uh my code is running on my CPU",
      "start": 3249.359,
      "duration": 6.161
    },
    {
      "text": "right now and I'm calling this scal",
      "start": 3253.359,
      "duration": 4.041
    },
    {
      "text": "Closs loader function for both the train",
      "start": 3255.52,
      "duration": 4.039
    },
    {
      "text": "loss and for the validation loss so when",
      "start": 3257.4,
      "duration": 4.959
    },
    {
      "text": "I call it for the train loss I I input",
      "start": 3259.559,
      "duration": 4.52
    },
    {
      "text": "the train loader here when I call it for",
      "start": 3262.359,
      "duration": 3.041
    },
    {
      "text": "the validation loss I input the",
      "start": 3264.079,
      "duration": 4.0
    },
    {
      "text": "validation loader and the model is",
      "start": 3265.4,
      "duration": 5.28
    },
    {
      "text": "essentially uh the instance which we",
      "start": 3268.079,
      "duration": 4.24
    },
    {
      "text": "have already created here this is the",
      "start": 3270.68,
      "duration": 5.119
    },
    {
      "text": "model an instance of the GPT model class",
      "start": 3272.319,
      "duration": 5.441
    },
    {
      "text": "and uh that's the second argument the",
      "start": 3275.799,
      "duration": 3.841
    },
    {
      "text": "third argument is the",
      "start": 3277.76,
      "duration": 6.039
    },
    {
      "text": "device so uh if if you want to run on",
      "start": 3279.64,
      "duration": 6.199
    },
    {
      "text": "CPU it can even run on CPU like I'm",
      "start": 3283.799,
      "duration": 5.04
    },
    {
      "text": "showing right now so I uncommented I've",
      "start": 3285.839,
      "duration": 4.561
    },
    {
      "text": "commented these lines right now",
      "start": 3288.839,
      "duration": 3.28
    },
    {
      "text": "uncommenting these lines will allow the",
      "start": 3290.4,
      "duration": 3.719
    },
    {
      "text": "code to run on Apple silicon chips if",
      "start": 3292.119,
      "duration": 4.401
    },
    {
      "text": "available which is approximately 2X",
      "start": 3294.119,
      "duration": 5.401
    },
    {
      "text": "faster than on Apple CPU so right now my",
      "start": 3296.52,
      "duration": 5.319
    },
    {
      "text": "code is running on Apple CPU you can",
      "start": 3299.52,
      "duration": 5.52
    },
    {
      "text": "also run it on Cuda if Cuda is available",
      "start": 3301.839,
      "duration": 4.96
    },
    {
      "text": "or if you have GPU access you can even",
      "start": 3305.04,
      "duration": 4.24
    },
    {
      "text": "run it on GPU so right now what I'm",
      "start": 3306.799,
      "duration": 3.961
    },
    {
      "text": "going to do is that I'm just going to",
      "start": 3309.28,
      "duration": 3.72
    },
    {
      "text": "click on this run and I'm going to show",
      "start": 3310.76,
      "duration": 4.2
    },
    {
      "text": "you live how much time it is taking for",
      "start": 3313.0,
      "duration": 4.839
    },
    {
      "text": "me to run this uh I just want to show",
      "start": 3314.96,
      "duration": 6.079
    },
    {
      "text": "you that uh here what we have",
      "start": 3317.839,
      "duration": 5.2
    },
    {
      "text": "essentially done is that we have loaded",
      "start": 3321.039,
      "duration": 4.201
    },
    {
      "text": "this entire data set we have converted",
      "start": 3323.039,
      "duration": 4.961
    },
    {
      "text": "this into input output pairs we have",
      "start": 3325.24,
      "duration": 7.48
    },
    {
      "text": "passed the uh data set into the GPT",
      "start": 3328.0,
      "duration": 6.52
    },
    {
      "text": "architecture block so we have passed the",
      "start": 3332.72,
      "duration": 4.24
    },
    {
      "text": "data set into this llm architecture",
      "start": 3334.52,
      "duration": 5.88
    },
    {
      "text": "block which looks like this uh and then",
      "start": 3336.96,
      "duration": 5.72
    },
    {
      "text": "we have got the llm outputs and then we",
      "start": 3340.4,
      "duration": 4.159
    },
    {
      "text": "have compared the loss with the targets",
      "start": 3342.68,
      "duration": 3.76
    },
    {
      "text": "and with these outputs and then we have",
      "start": 3344.559,
      "duration": 3.48
    },
    {
      "text": "collected an aggregate matric of this",
      "start": 3346.44,
      "duration": 3.84
    },
    {
      "text": "loss so here you can see I've got the",
      "start": 3348.039,
      "duration": 3.441
    },
    {
      "text": "training loss and I've got the",
      "start": 3350.28,
      "duration": 3.24
    },
    {
      "text": "validation loss and I got it live in",
      "start": 3351.48,
      "duration": 4.839
    },
    {
      "text": "less than 30 seconds I would say now you",
      "start": 3353.52,
      "duration": 4.88
    },
    {
      "text": "you can take this code and you can do",
      "start": 3356.319,
      "duration": 3.641
    },
    {
      "text": "whatever you want you can increase the",
      "start": 3358.4,
      "duration": 5.12
    },
    {
      "text": "context size to 1024 all you need to do",
      "start": 3359.96,
      "duration": 6.399
    },
    {
      "text": "is go here and change the context size",
      "start": 3363.52,
      "duration": 6.12
    },
    {
      "text": "to 1024 to mimic conditions more closely",
      "start": 3366.359,
      "duration": 7.521
    },
    {
      "text": "to gpt2 you can even go to internet and",
      "start": 3369.64,
      "duration": 6.719
    },
    {
      "text": "search uh Harry",
      "start": 3373.88,
      "duration": 4.8
    },
    {
      "text": "Potter book",
      "start": 3376.359,
      "duration": 4.561
    },
    {
      "text": "download um you can download the Harry",
      "start": 3378.68,
      "duration": 4.52
    },
    {
      "text": "Potter book there's an ebook series here",
      "start": 3380.92,
      "duration": 4.679
    },
    {
      "text": "just make sure the um just make sure",
      "start": 3383.2,
      "duration": 5.359
    },
    {
      "text": "about the copyright versions similarly",
      "start": 3385.599,
      "duration": 5.081
    },
    {
      "text": "you can go ahead and download any data",
      "start": 3388.559,
      "duration": 4.28
    },
    {
      "text": "set which you want and just train the",
      "start": 3390.68,
      "duration": 4.159
    },
    {
      "text": "large language or just run this code on",
      "start": 3392.839,
      "duration": 4.161
    },
    {
      "text": "the data set which you are considering",
      "start": 3394.839,
      "duration": 4.641
    },
    {
      "text": "it will be truly awesome for you to use",
      "start": 3397.0,
      "duration": 4.24
    },
    {
      "text": "your own data set and get this training",
      "start": 3399.48,
      "duration": 3.8
    },
    {
      "text": "and validation loss because once you",
      "start": 3401.24,
      "duration": 3.24
    },
    {
      "text": "have obtained the training and",
      "start": 3403.28,
      "duration": 3.16
    },
    {
      "text": "validation loss that really opens up the",
      "start": 3404.48,
      "duration": 4.72
    },
    {
      "text": "door for us to to back propagate so in",
      "start": 3406.44,
      "duration": 4.48
    },
    {
      "text": "the next lecture what we are going to do",
      "start": 3409.2,
      "duration": 3.2
    },
    {
      "text": "is that we are actually going to Define",
      "start": 3410.92,
      "duration": 3.439
    },
    {
      "text": "an llm training function which",
      "start": 3412.4,
      "duration": 3.56
    },
    {
      "text": "implements the back propagation and",
      "start": 3414.359,
      "duration": 3.361
    },
    {
      "text": "which tries to minimize the training and",
      "start": 3415.96,
      "duration": 3.96
    },
    {
      "text": "the validation loss so then it will make",
      "start": 3417.72,
      "duration": 4.96
    },
    {
      "text": "sure that the outputs being generated",
      "start": 3419.92,
      "duration": 6.24
    },
    {
      "text": "are very coherent and then even if you",
      "start": 3422.68,
      "duration": 6.399
    },
    {
      "text": "run this code on another data set even",
      "start": 3426.16,
      "duration": 4.32
    },
    {
      "text": "in the next code when we do the",
      "start": 3429.079,
      "duration": 3.881
    },
    {
      "text": "pre-training you can do the same pre-",
      "start": 3430.48,
      "duration": 5.119
    },
    {
      "text": "trining on your custom data set so the",
      "start": 3432.96,
      "duration": 4.0
    },
    {
      "text": "code which we have developed today is",
      "start": 3435.599,
      "duration": 5.76
    },
    {
      "text": "pretty generalizable and uh I hope you",
      "start": 3436.96,
      "duration": 6.119
    },
    {
      "text": "you understood what we are trying to",
      "start": 3441.359,
      "duration": 3.601
    },
    {
      "text": "demonstrate today we are trying to",
      "start": 3443.079,
      "duration": 3.48
    },
    {
      "text": "demonstrate through a real hand on",
      "start": 3444.96,
      "duration": 4.2
    },
    {
      "text": "example how we can actually take a data",
      "start": 3446.559,
      "duration": 4.76
    },
    {
      "text": "set from the internet and we can divide",
      "start": 3449.16,
      "duration": 5.639
    },
    {
      "text": "it into input Target pairs we can run",
      "start": 3451.319,
      "duration": 5.161
    },
    {
      "text": "the data set through a large language",
      "start": 3454.799,
      "duration": 3.721
    },
    {
      "text": "model which we ourselves have developed",
      "start": 3456.48,
      "duration": 3.24
    },
    {
      "text": "if you have not been through the",
      "start": 3458.52,
      "duration": 3.839
    },
    {
      "text": "previous lectures this this model I have",
      "start": 3459.72,
      "duration": 4.04
    },
    {
      "text": "not taken it from anywhere we have",
      "start": 3462.359,
      "duration": 4.521
    },
    {
      "text": "developed it live we have developed it",
      "start": 3463.76,
      "duration": 5.279
    },
    {
      "text": "from scratch without any single Library",
      "start": 3466.88,
      "duration": 4.28
    },
    {
      "text": "like Lang chain or any other Library we",
      "start": 3469.039,
      "duration": 4.0
    },
    {
      "text": "have coded this from the basic building",
      "start": 3471.16,
      "duration": 4.08
    },
    {
      "text": "blocks and that has been used to produce",
      "start": 3473.039,
      "duration": 4.601
    },
    {
      "text": "this output that's even more",
      "start": 3475.24,
      "duration": 4.64
    },
    {
      "text": "satisfying uh okay students so that",
      "start": 3477.64,
      "duration": 4.56
    },
    {
      "text": "brings me to the end of this lecture I",
      "start": 3479.88,
      "duration": 4.6
    },
    {
      "text": "deliberately wanted you to I wanted to",
      "start": 3482.2,
      "duration": 3.879
    },
    {
      "text": "give you a feel of the Whiteboard",
      "start": 3484.48,
      "duration": 3.96
    },
    {
      "text": "teaching uh so that you understand the",
      "start": 3486.079,
      "duration": 4.881
    },
    {
      "text": "intuition Theory and also the coding",
      "start": 3488.44,
      "duration": 5.159
    },
    {
      "text": "which is my main goal in every lecture",
      "start": 3490.96,
      "duration": 5.24
    },
    {
      "text": "which I conduct in the next lecture we",
      "start": 3493.599,
      "duration": 4.52
    },
    {
      "text": "are going to look at llm pre-training",
      "start": 3496.2,
      "duration": 3.76
    },
    {
      "text": "I'll be sharing this code file with you",
      "start": 3498.119,
      "duration": 3.321
    },
    {
      "text": "if you can run it before the next",
      "start": 3499.96,
      "duration": 3.639
    },
    {
      "text": "lecture it's awesome if not it's fine",
      "start": 3501.44,
      "duration": 3.679
    },
    {
      "text": "I'll try to make the next lecture so",
      "start": 3503.599,
      "duration": 3.52
    },
    {
      "text": "that it's selfcontain thank you so much",
      "start": 3505.119,
      "duration": 3.521
    },
    {
      "text": "everyone and I look forward to seeing",
      "start": 3507.119,
      "duration": 5.121
    },
    {
      "text": "you in the next lecture",
      "start": 3508.64,
      "duration": 3.6
    }
  ],
  "full_text": "[Music] hello everyone and welcome to this lecture in the build large language models from scratch series let me recap the stage of building the llm in which we are at currently in terms of a diagram okay so we are in stage number two right now based on the schematic which is mentioned in front of you and in the stage number two we are essentially learning how to build this foundational model or rather we are going to learn how to train or pre-train a large language model in this stage in the previous stage in the all the previous set of lectures we have looked at data preparation attention mechanism and llm architecture in fact in this stage we even looked at um how to calculate the loss in terms of a large language model we looked at how to define the cross entropy loss based on the input and targets of a large language model and that was the previous lecture which we had now we will be looking at a much larger data set in fact this is going to be a very interesting Hands-On lecture in which we are going to collect data from a story book and we are going toh make predictions using the large language model which we have built in the first stage and then we are going to measure the loss of our large language model model based on the actual data which we need so in the previous lecture what we did is we looked at some simple examples we looked at just two inputs the first input which we looked at was every effort moves and the second input was I really like we looked at these two inputs and we predicted outputs from our large language model and then we calculated the loss between those outputs and the target values which we need now what we are going to do is we are going to look at an actual data set and on that data set we are going to find the training loss and we are going to find the validation loss so if you zoom into stage two of building an llm further in the previous lecture we have looked at text generation and text evaluation so we saw the cross entropy loss in the previous lecture and how to calculate it for just two simple input text today we are going to calculate the training and validation losses on the entire data set we'll also split the data set into training set and validation set so let's get started it will be a completely Hands-On lecture and after this lecture actually you will be equipped to take any story book of your choice or for that matter any data set and uh train the large language model not really train it because we are not training the parameters in this lecture do a forward pass of the large language model get the outputs and then get the loss based on the True Values which you need and you will get this loss for any data set which you use you'll just have to follow the same code which I'll provide you at the end of this lecture we have not yet started doing the training procedure but once you get this loss for the data set in the next lecture we are going to look at the llm training function where we'll also dive dive deep into back propagation so at the end of today's lecture you will have a very cool result which is applicable to a wide range of data sets so let's get started the data set which I'm going to consider is called the verdict it's a book written by an author named edit Warton and this book was published in 1906 I think here's how the book looks like and uh the book is publicly available to download I'll share the link you can download the book from this link and it's not a very long book it's a pretty short book in fact but we are using this just because I want to demonstrate uh an example which runs very fast on my laptop and it will run very fast on your laptop also in fact if you take a look at this data set and if you count the number of characters you'll see that the number of characters in this data set is 20,000 characters what we'll do uh on the data set first is that we'll first convert this data set into tokens so we'll use a tokenization scheme for that which is called as bite pair en code B B pair encoding so bite pair encoding is the tokenization scheme which we are going to use and what this will do is that in bite pair encoding one word is not one token it's a subword based tokenization scheme where even characters can be tokens just uh pairs of letters such as T and H this can be one token just T can be one token Etc so if you use bite pair encoding you'll find that this data asset has 5,000 tokens don't worry I'm going to show all of this in the code but just if you search tick token you'll see that uh this is the bite pair encoder which we are using and it's the same encoder which open AI actually used and using this encoder we can encode our data uh data set into tokens great so this is the data set which I'm considering and remember you can use any data set which you want on publicly available books now the next step what we are going to do is that uh we are going to divide the data set into training and validation remember this is the same thing what we do for all machine learning problems because the training loss is not the one which really matters what matters is how our large language model is doing on text which it has not seen before so we'll do a simple thing over here let's say this is the entire data set we are going to use a train test ratio of90 and we we are just going to do a simple split the training data will be the first 90% of the input and the validation data will be the latter half so the remaining 10% right um and that's how we are going to split the training and the validation set so if you imagine the entire data to be like this what I'm going to do is that I'm going to reserve the first 90% so this is 90% And this is 10% so I'm going to reserve this for training purposes and I'm going to reserve the 10% for validation or testing purpose now what I'm going to do after this is pretty interesting usually in other machine learning problems getting the inputs and the outputs is pretty easy right you just have images of cats and dogs and the output is whether it's a cat or whether it's a dog so it's pretty simple you have inputs which are images and you have outputs which are the labels you don't have to do anything special to create these input output pairs but it's not as simple in the case of a large language model because large language models are Auto regressive models we don't uh label anything beforehand but from the text itself we construct the inputs and the output and so we'll need to understand the process through which we conu construct these input output pairs so in the code we are going to use the data loader to chunk the training and the validation data into input and output data sets or input and output pairs and let me show you how I'm going to do that if you understand this part it will be much easier for you to visualize what comes next right so the first thing which we have to decide is what is the context size which I'm going to use in other words what is the maximum number of tokens the llm can see before it predicts the next token so I'm going to show you how to construct the input output data pairs on this data set using a context size of four so that's the first thing which I need to decide context size equal to four okay great so now let me look at this data set and here's how I create the input output pairs my first input is this I had always thought and since the context size is four I'm looking at four tokens at a time and although one word is not one token I'm just assuming it here for the sake of demonstration so this is my first input and I'm going to label it let me use the same color here and I'm going to label this as X1 right this is my first input now what's the first output the first output is just this input shifted by one so then it will be this had always thought Jack this is y1 let me write this down over here so uh ultimately the input will be a tensor X and I'm currently I'm just writing the first row of this so this will be I had always thought that's my first input put and let me also collect the output tensor over here with a different color of course and uh this will be had always thought Jack right now let us focus on this first input output pair for just a moment the first thing to notice is that of course the output is just the input shifted to the right by one but another thing to notice is that this one input output pair essentially has four prediction tasks what are the four prediction tasks the first is that when I is the input had is the output so the index corresponds that way when I had is the input always is the output when I had always is the input so when these three are the input thought is the output and when I had always thought is the input then Jack is the output right so essentially when X is given an input when we pass it through an llm it will produce these four tokens as the output uh and then the output produced by llm will be compared to this which is the actual output which we want for this input let me repeat that again if this is an input I had always thought the actual output which we want is had always thought Jack but you'll see that when you pass these four tokens through the large language model it's not trained currently right so it will predict some random words over here and then we have to find the loss between those random tokens and what we want that's how you get the loss between the first input pair and the second first input and the first output right now let's move ahead a bit and let let us see how to construct the second uh second input so I'm going to rub this output pair right now uh okay so we have the first input now we have an option of how we are going to actually construct the second input and let me show you uh how we are going to do that okay so the first option is that you just move it to the right so then X1 is I had always thought right X2 will be had always thought Jack now this movement which we are going to do is also called as stride and that's the second parameter which you have to decide along with the context size so if you put stride equal to one like I have done in this case the second input will have will be X2 and that has lot of overlap with the first input right had always thought is overlap um so you can also do this but usually What's Done in models such as GPT is that the stride which uh we are going to Define right now is is fixed to be equal to the context size and that is equal to four so we are going to use a stride of four what this will do is that when X1 if X1 is this input we are going to have 1 2 3 and four so then X2 will start from here so then X2 will will be Jack gisburn rather a this will be X2 you see what stride equal to 4 does it it makes sure that we don't have any overlap but it will make sure that we also don't skip any token as the input when you look at X3 this will be the um this will be X3 which will be the third input when you look at X4 uh this will be X4 which is the fourth input right uh this is how the inputs are created and you don't skip anything but also you make sure there are no overlaps between inputs so now if you look at the input tensor the first is I had always thought right the second will be Jack gizan rather a jack gisburn rather o that's the second input which is X2 the third input would be cheap genius dash dash do so the third will be cheap genius dash dash do so that's how this entire input tensor Matrix will be created sorry this input tensor will be created like this we'll stride through the entire data set and we'll collect these pairs so then the first row will be X1 the second row will be X2 the third row will be X3 and we'll keep on accumulating these rows until we reach the end and how are the outputs created once the inputs are created the output is just the input shifted by one so for the first input I had always thought the output was had always thought Jack right now Jack gisburn rather a if this is the input the output would be gisburn gisburn rather a cheap so this will be Y2 the first row will be y1 and similarly we'll construct all the other outputs output pairs till we reach the end of the data set so that is how input output pairs are created in the case of a uh large language model so if you have a data set like this what's very important is this X and this y so the x is the input and the Y is the target which is the actual value so let me write this here x is the input and Y is the Target now the loss will be between the llm output which we have not seen yet but we'll also get llm output so this input will be passed into this input will be passed into our llm model and then we'll get the output from the llm right and that will also be a tensor with the same format as this target tensor and then what we are going to do is that we are going to then find the loss between the we are going to find the loss between the llm output and the Target and this is the loss which we are going to find in today's lecture and this is the loss which we eventually want to minimize okay I hope you have understood this this part and I deliberately wanted to show you visually because students are generally quite unclear regarding how input output pairs are created in the context of large language models now here I showed you the input input Target pairs I should call them here I showed you the input Target pairs for the training data right these are the input and let me call them targets because outputs will be what I refer to as the llm predictions so I created the input and the target pairs for the training data similarly we'll have the input and the target pairs for the validation dat data and that will give us the validation loss the input and Target pairs will give us the train loss for the training data and the similar tensors will give us the validation loss for the validation data now let me explain to you the rest of the process and then we'll I'll take you through code but first I want you to understand how exactly is the loss function calculated so that uh the code becomes so much more easier to understand now let's say you create the input and Target pairs like this which is the first step first you look at the first row of the input and the first row of the target uh and then let's see how do we get the loss so let's say let's look at the first input so I'm going to look at the first input now which is I had always thought which is the first row over here U and remember that based on the input we have to get the llm model output right what is our model predicting we need to know that because I I have the actual output corresponding to this input so if the input is this I know that the output is had always thought Jack that is my output so let me even write that here the output or rather the target I should call it the target value for this input is had had always thought Jack right that's the target output which I need but of course my llm is not going to uh give me this at the first uh first shot because we have not trained it yet so I need to first know what my llm is predicting and for that we are going to go through the entire uh not right now but uh I'm just going to show you the schematic of what we are going to put the input through so the input which we have that will go through this entire GPT architecture or the entire llm architecture which we have trained in stage number one uh so in the previous lectures we have actually built this entire architecture from scratch and you can see that there are so many different modules in this architecture and let me quickly explain to you what we are actually doing so we take in the input imagine those four tokens I had always thought we tokenize them using the tick token or the bite pair encoder then we convert them into token embeddings which are vector representations in higher dimensional spaces we add positional embeddings to it we add the Dropout layer at this stage uh our input passes through the Transformer block so the Blue Block which I've shown over here this is the Transformer block and this is the main engine of the GPT architecture Ure the main thing which happens in the Transformer block is that the input embedding vectors are transformed into something which is called as context vectors now what are context vectors and how do they differ from input embedding vectors context vectors are more richer so if you look at one word let's say if you look at effort here if you look at the input embedding Vector for effort it just encodes santic meaning about effort it does not contain any information about how effort relates to the other to tokens but the context Vector is much richer because the context Vector for effort not only contains semantic meaning about effort but it also contains information about when effort when we are looking at effort how much information should we pay to every moves and youu so in other in other words it pays attention the context Vector includes attention which is given to the other tokens when we look at a particular token and that's what gives all the power to the large language model so this multi-ad attention is the main driver behind the Transformer block so if you look at the whole GPT architecture the Transformer block is the engine of the uh GPT architecture and within the Transformer block the multi-head attention is what allows us to convert these input embedding vectors into context vectors which encodes information about how tokens relate to other tokens that's how the llms capture meaning and that's how they do so well so GPT does so well because it has this multi-ad attention mechanism so you can see that the Transformer has a number of building blocks we have the layer normalization multi-ad attention Drop Out shortcut connection feed forward neural network Etc and then finally when the input comes out of the uh out of the GPT model architecture we get something which is called as the logits uh we get something which is called as the logit sensor and it's very important to understand what the logit sensor is and what's the dimensions of this logic sensor so let's say the input is I had always thought and when we pass it through the GPT model which is when it passes through the entire architecture which I just showed you we get this logic sensor now I had always thought uh those are the tokens and when you look at the logits tensor corresponding to every token there are these logits there is a logits vector who whose dimensions are equal to the vocabulary size so if you look at I the Logics for I are 0257 because that's our vocabulary size if you look at had the logits for had are 50257 because that's the vocabulary if you look at always the logits for always are 50257 because that's the vocabulary size and similarly for thought now when the logits tensor come out of the GPT architecture they are not normalized so if you look at the logits for I they don't add up to one so the next step is to convert this logic tensor into a probability tensor and that makes sure that when you look at every token the logits or the probabilities add up to one so now you can see that every logic essentially has a meaning because it adds up to one the way to predict the output now is that you look at I and you look at that logic which has the highest value okay and let's say it corresponds to index number 50 in this vocabulary of 50257 or maybe 5,000 and then you look at the token corresponding to that index and maybe it's something random like a c h now you look at had and you look at the logic which has the highest value you get the index corresponding to this maybe it is 31 1 01 and then you find the token corresponding to this index maybe it's something completely random similarly for always you get the index which has the highest probability and you get the outputs like a am m m o let's say uh am and then you look at thought and you find the index corresponding to the highest probability and let's say this is 611 6111 and then you get the uh outputs so these are the output tokens now which are llm is predicting and these are the output tokens which we need to make sure that uh they need to be as close to our Target as possible okay I hope you have understood the workflow which we are trying to follow here we first have a logic sensor we convert it into a soft Max sensor uh which which is a probability sensor and I just gave you the intuition that based on this tensor how do you make the output for this input right so what's done now is that after this point let's say we have this probability tensor right then what we actually do is that we look at the targets uh this is the actual value which we want and we look at the index which each token in the Target corresponds to so had has index 23 in the vocabulary always has index 3881 in our vocabulary thought has IND index 1 1 2 2 3 in our vocabulary Jack has index 15 in the vocabulary I'm assigning these random values for now just for demonstration purposes now let me show you one thing here so first let me rub these uh rub these colors Okay now what's done next is that based on these based on these IND indexes uh let's say we look at I and we look at index number 23 let's say this is index number 23 and we take its uh value that will be P1 then we look at had and then we uh that so the target for had is always and its index is 3881 let's say we look at that index and find the probability corresponding to that index that's P2 then we look at the third row and find index number 1 1 22 3 and find the probability corresponding to that that's P3 and then we uh find uh the index number 15 in the last row and then we find the probability corresponding to to that so then that will be P4 ideally if in an Ideal World if the llm is trained perfectly these probabilities will be close to one and if these probabilities are close to one It means that our llm is also predicting these values right um as the output but when the llm is not trained at all these probabilities will not be close to one at all they might be very low which means that our llm does not think that this target needs to be the output because it has not been trained so the whole goal is that to make sure that these probabilities get as close to one as possible and that's why we uh employ the categorical or I should call it the cross entropy loss so at first take the logarithm of all these values I add up these log values then I take the mean and then I take the negative this is also called as the negative log likelihood uh this is called as the negative log likelihood and the whole goal of training the llm is to make sure that this this negative log likelihood which I'm calling n LL and if you plot nnl of X as a function of X so it's negative of logarithm right so it will look something like uh it will look something like this since it's the negative it will it will look something like this and uh our whole goal is to make sure that the loss comes down and it comes down to zero as much as possible so now today what we are going to do is that today we are not going to train the llm we are just going to see this starting point of this loss uh which which will be very high value but then in the subsequent lecture we are going to train the large language model so that this loss comes as down as possible so this is the workflow which I showed you for one input right I had always thought uh and then how do we get the loss now remember that we don't just have one input we have all these inputs uh which are stacked together in a batch so remember that this data loader processes inputs in a batch so each batch has an accumulation of inputs right based on the size of the batch so now let me show you how multiple inputs in a batch are processed so let's say we have a batch which has two inputs together so let's say this is a batch whose batch size is equal to two which means that there are two inputs together uh in a batch at a time right so this is one batch and this has two inputs I had always thought Jack gpan rather so this is X1 and this is X2 and this is y1 and these are this is Y2 these are the target outputs so for the first input X1 my output should be had always thought Jack which we also saw in the previous example where just one input was there and my second input is Jack gpan rather a the output should be gizan rather or cheap that's what I want these are the targets right now similar to the similar to what we saw for one input the steps are pretty similar for the case of batches as well I'm trying to see a color which would work the best here yeah so what we'll do is that we'll first take the input and we'll pass it through the entire GPT architecture in this case what we'll get is that we'll get two logic sensors the first logic sensor is for the first bat first input the second logic sensor is for the second input so if you look at the size of this tensor now we have two batches here and in each batch there are four rows and in each row there are 5 to 57 columns in the previous case where there was just one input the size was just 4 into 50257 but now we have two such batches right two such input so this is input number one and here is input number two right so in the code what we'll do is that when we get this logic sensor we'll flatten this out we'll flatten the batch Dimension out which means we'll merge both of these together uh we'll merge both of these together so that now the my cumulative logic sensor looks something like this this is my first input this is my second input all merged together the size of this now will be eight rows multiplied by 50257 that will be the size of this okay and the next steps are pretty similar we add we apply soft Max we convert this into a tensor of probabilities and then we look at the Target we look at the Target tokens and we stack this target tokens also so for the first input these four are the target outputs for the first for the second input these four are the target outputs and we get the token IDs corresponding to these Target outputs we stack them together and then we find for each row we find the value corresponding to these token IDs and then these values are noted down as P11 p12 p13 p14 this for input 1 p21 P22 p23 p24 that's for input number two and then similarly we get the cross entropy loss so this is the loss for the first input this is the loss for the second input and then we add add it together and ultimately it will also look something like this uh and the whole goal is that in this case also we want to minimize this loss and bring it as close to zero as possible so now I hope you see that even for a batch of two inputs the process of getting the loss is pretty similar as what the process was for just one input right uh and I want you to keep this visual workflow in mind so that when we come to the code you will really understand what is happening over here so in the code there is going to come a time when uh we are going to apply the data loader to the training and validation set and the training data set is going to look like this after it passes the data loader and I want you to analyze this right I told you that the input is processed into batches right so here you can see that uh let's look at first the each row for now right each row corresponds to one batch so this is X and this is y so what this 2 comma 256 means is that the so let's look at our case right now let's look at the case which we took for the batch uh we looked at X1 we looked at X and Y right this is the first batch and the size here was 2A 4 because there were two uh there were two inputs and each input had four tokens and here also it was 2A 4 because there are two outputs and four tokens similarly what I want you to note over here is that when you look at this training data loader it's exactly similar to the example which we saw so let's look at the first row over here let's look at the first row over here uh it has two inputs but it has 256 tokens because that's the context size we are going to use when we go to code so that's X the first input and it has two uh it has two inputs so it's X1 comma X2 um and that's exactly what we saw right why there is two here because there is X1 uh so if you zoom this in further let me show you how it looks like it will be um I uh I had always this will be 256 now not four like what I had shown you because the context size is 256 this will be X1 and then X2 will be another input batch this will be X1 X2 and then similarly we have y1 Y2 which are shifted by one I already told you right how to construct the y1 and Y2 they are the inputs just shifted by one so these are y1 and Y2 and they will also have the size of 256 this is just the first badge the first row is just the first batch uh where each batch had two inputs of 256 tokens each similarly here we can see that there are nine batches so there are nine training set batches each batch has two samples and each sample has 256 tokens each so I don't want you to be scared when you see this in the code similarly in the validation we just have one batch because only 10% is used each batch has h two samples and each sample has 256 tokens each okay I hope you have understood this visual workflow which I have constructed over here I could have directly taken you through the code but then it would have been very difficult to understand the different steps in the code right now I'm going to take you through the code and we are going to see how to calculate the loss for this verdict short story step by step but please keep this intuition of this workflow in mind then everything will be very clear for you okay so now we are at the code before we get started I want to clarify that we are actually using a relatively small data set and that is because we want to run the code in a few minutes on our laptop computer I could have used a larger data set but it would have taken a huge amount of time uh just to give you a sense of how much time it takes to run big data sets Lama 7 billion the that model required 84320 GPU hours um and was trained on two trillion tokens and training this llm would cost about $700,000 that's why I cannot show it to you on the full scale data set but I'm showing it to you on a smaller data set and the whole logic is completely scalable for larger data as well okay so here's the code the first thing what we are doing is that we are getting this data set link from here uh and I'll share this link with you in the YouTube description and I'm importing this data set over here so here you can see that uh um I'm reading I'm reading the data set and I'm storing the all the information all the text in this variable called Text data and let's check whether the text is loaded fine by printing out the first 100 words so here I'm printing the first 100 characters uh and here you can see that the first 100 characters have been printed and they look very closely matching with what was actually there in my text I had always thought Jack gispan rather a cheap genius and here also I had always thought Jack is rather a cheap genius awesome let let's print the last 100 characters it for me the stoud Strand alone and let's see whether that also matches okay I think this also matches the whole data set is not being displayed over here but now we are pretty sure that the data has been loaded fine let's move to the next step in the next step what I want to show is that we can print out the total number of characters in this data and it's 20479 that's fine but remember I told you about the bite pair encode encoder what we are going to do is that we are going to use the bite pair encoder tokenizer to encode this text data and we had already defined the tokenizer in the previous lecture but I'm going to do it once more here so that uh um yeah so that everything is from scratch so what we are actually going to do is that we are going to import tick token uh and we we are going to Define this tokenizer from The Tick token Library so let me write it down here okay yeah so we are importing this tick token Library which is the same Library open AI uses for their tokenization and we are going to get the encoding from tick token which is a bite pair encoder character and subord level encoder basically and we are going to encode the entire text data using this encoder and we are going to print out the number of tokens right so the number of tokens are 5145 so with 5145 tokens the text is very short for training and llm but again it's for educational purposes uh okay the next step is that we are going to divide the data set into training and validation data and we are going to use the data loader exactly what I told you over here right so we have loaded the data set now and we have to div divide it into training and validation okay let's do that before that what I'm going to show you is that remember I told you what our data loader does our data loader we have to specify this uh context size and we have to specify the stride our data loader Loops over this entire data set and creates this input output pairs that's what I've implemented in the code right now uh so here you can see that max length is the context size and stride is the uh how many steps we want to leave before creating the next input so first so we create two tensors input and the target so these are the X and Y tensors which I showed and then we Loop over the entire we Loop over the entire data set we create the input Chunk we create the target chunk which is based on the context length and we append it to the tensor so uh the first row here will be the first input chunk the first row in the Target will be the first Target chunk then we move over in the second Loop then we fill the second row of the input and the target similarly we Loop over the entire data set and fill the input tensor and the target tensor uh that's what this GPT d data set version one creates and then we use the create data loader function it takes the input output data sets which which have been created in the GPT data set V1 class and then U we create this data loader instance so data loader is already um provided by pytor so let me show you this I'll also add the link to this so data sets and data loaders right they are very useful for processing data also in batches remember we want to use batches over here so using a data loader like this just makes uh processing the batches much more convenient so we we create an instance of this data loader and we feed in the data set which we created the input output data set which I mentioned over here input and targets and then here we specify the batch size uh we specify Shuffle uh these uh arguments are not useful in the current context right now but I'll also explain them to you later when we are going to train the llm uh see the output EX ET but remember that for now the only important aspect here is that we are going to create an instance of this data loader feed in this data set and Define the batch size if you want to do parallel processing you can also set the number of workers Etc okay so now an instance of the data loader is created and let me actually take some time to explain the shuffle and the drop last so what this suff Shuffle essentially does is that it shuffles the data set order when batches are created that's sometimes useful for generalization what this drop last actually does is that uh if the last batch size is very small and uh some very small data is left at the last batch and it's not equal to the full batch size then it just drops that last last batch so here we are setting the drop last equal to true and see the thing which I want to mention here is that max length equal to 256 which means that the context size which we are going to use is 256 and we'll also see that later uh we are going to use a context size of 256 over here and that set by the GPT the GPT configuration which we are going to provide so I'll also mention it over here okay so for now I hope you have understood the GPT data set version one class and this create data loader function which basically creates the input and the output data Pairs and then it also specifies the batch size one more thing I want to mention before we create the training and the validation data set is that this is the configuration which we are going to use so look at the context length that's 256 which means that uh we are going to look at 256 tokens at one time whenever I showed you this example here I showed four tokens I showed the context length of four because that's easier to demonstrate so when you try to understand the code always try to think of four as being replaced with 256 rest all the workflow remains exactly the same okay the next thing what we are going to do is that we are going to split the data set uh so we are going to use a train test split of 90% the first 90% of the data is a training data the remaining 10% is the validation data and here's the main part where magic happens so we are going to create a data loader based on the training data what this does is that it uh it splits the training data into the input and the target tensor pairs which we had seen uh over here and we are also going to create a validation data loader which splits the validation data into input and the target pairs because we also need the validation loss so here you see the train data loader is an object so we create we uh we create the train data loader based on this create data loader version one function and uh we specify that batch size equal to two maximum length is GPT config context length so that's 256 that's the context size stride equal to the context size remember I had mentioned to you that generally when these llm architectures are run the stride and the context size are um matched because we make sure that no word is lost but at the same time there is no overlap between consecutive inputs awesome right and then drop last equal to True Shuffle equal to true and we are not doing parallel processing so I I'm putting number of workers equal to zero similarly we construct the validation loader with a batch size of two MA X length which is the context length of 1024 and the stride sorry context length of 256 and the stride of 256 when gpt2 smallest version was trained they actually used a context length of 1024 and you can even do that but it just takes a long time uh all you need to do is just replace this with 1024 and just run the same code which I'll be providing to you but please be patient when you run the code on your end it might take some time we can do some sanity check so ideally the number of uh tokens which we want in our training data set should not be less than our back context length right because then we don't have enough tokens to predict the next word so here I have just written that if this is the case if our number of training tokens is less than our context length then print an error similarly if the number of validation tokens is less than the context length print an error it does not print an error which means we are good to go uh one more thing I want to mention here is that we are using a batch size of two in large language models in training GPT level models they usually use a pretty large batch size but we use a relatively small batch size to reduce the computational resource demand and because the data set is also very small to begin with to give you a context Lama 2 7 billion was trained with a batch size of 1024 here we are using batch size of two because I want to run it very quickly on my laptop one more check we can do to make sure that the data is loaded correctly is that remember both in the training and the validation there are now X and Y pairs input and Target pairs uh so the training has inputs and targets and the validation is inputs and targets let's actually print out the shape of these inputs and targets so the training loader has this if you print out the X and the y shape in the training loader it will look like this and if you print out the X and Y shape in the validation loader it will look like this so if you look at the train loader let's look at the first row this is the X and what I'm highlighting now is the Y what this represents is that the input um so in one batch so this is one batch so first row corresponds to the first batch the First Column of the first row is the input the second column of the first row is the output if you look at the first batch input you'll see that there are two samples each sample has 256 tokens similarly if you look at the first batch output you'll see that there are two samples and two 56 tokens this is the target which we want and this is the input which is there similarly uh since 256 tokens are exhausted um in the input and we have to Loop over the entire data set there are it turns out that there are nine such batches which are created uh in the training data and there is one batch which is created in the validation data similar to the training data in the validation data you'll see that the batch has two samples each sample has 256 tokens and I also printed the length of the training loader here and you can even print the length of the validation loader and you will get that uh the length of the training loader is equal to 9 because there are nine batches each batch has two samples and the length of the validation loader is equal to one and uh there's just one batch with two samples I hope now this part is clear to you to to make sure you understand this part that is why I actually went through this entire whiteboard demonstration to show you that towards the end we are going to get something like this in the in the code and remember I spent some time to explain these sizes and these Dimensions I hope you are following along and if I directly went through the code and when you reach this part it it would have been impossible for you to understand this that's why it was very important for me to go through this entire whiteboard demonstration so that you understand the dimensions of what's really going on so up till now what we have created is that we have created Ed the we have the input and the targets and we have badged them into the input and the uh Target data but we have still not got the output predictions right we have still not um got the GPT model predictions so that's what we are going to do next uh one more thing before going next is that we can print out the training tokens and validation tokens uh uh just for the sake of Sanity so this makes sure that the data is now loaded correctly now we can actually go to the next part which is getting the llm model outputs so in one of the previous lectures we have defined this GPT model class what this GPT model class does is that uh it essentially implements every single thing what I've shown in this figure it takes the inputs it takes the inputs and then it returns a loged sensor remember the logic sensor as it is returned does not encode probabilities we need to convert it to a probability tensor using the soft Max so the GPT model class which we have constructed Returns the logic sensor and we have several lectures on this for now you can just um keep in mind that okay first the inputs are converted into token embeddings we add the positional embeddings then we add a Dropout layer then we pass the output of the Dropout layer to through this Transformer block this Transformer block which I highlighted right now that has the multi-head attention mechanism which is the main engine behind the llm power after coming out of the Transformer we have another layer normalization layer followed by output neural network which gives us this loged sensor then we create an instance of this GPT model class and we call it model and we are using the same GPT model config 124 million parameters which I had defined over here we have to specify the vocabulary size context length embedding Dimension number of attention heads number of Transformer blocks dropout rate and whether the query key value bias is set to false in this lecture I'm not going to explain all of these parameters because that was the subject of previous lectures uh if you don't understand what those parameters mean I encourage you to check out the previous lectures in a lot of detail we have around six lectures on that and uh six lectures explaining how we constructed this GPT model class for now just remember that we have got the output Logics and we have constructed an instance of the GPT model class so when you pass an instance when you pass an input to this model it will give you the logits now we are actually ready to implement the loss because we have the uh we have the targets over here we have the targets over here and we also have the GPT model output and now we are actually going to implement the exact same steps over here remember First We Take the soft Max then we index with the probabilities uh then we index these tokens based on the target tokens and then we get the cross entropy loss right using the negative log likelihood and we did the same thing for this batch over here so now I want you to keep this in mind when we had a batch remember what we did first when we had a batch we first flatten the logits right this is exactly what we are going to do when we calculate the loss so there is a function called calculate loss batch which takes the input batch and the target batch right what this means is that it's exactly like what I've shown over here this is the input batch X and this is the target batch y it just that instead of four tokens there will be 256 then what we are going to do in the code is that uh we are going to pass the input batch through the model the GPT model and gets the logit tensor so until now in the code we are at this stage where we have got the logit tensor then we are going to flatten the logits um 0 comma 1 so see we are going to flatten the logit 0 comma 1 uh and we get this now remember up till now we have not implemented soft Max we have not indexed this uh probability tensor with the Target index indices and we have not got the negative log likelihood it turns out that with just one line of code nn. functional doc cross entropy we can do all of these three steps so when you do the nn. functional. cross entropy on the flats logic tensor and the flatten Target batch so remember the flatten Target batch is this is this tensor over here this is the flatten targets batch so what the nn. functional doc cross entropy does is that it first applies soft Max to the logic uh tensor because that's the first argument it first applies softmax to this first argument uh which is also shown in this white board and then it takes the uh values corresponding to the indices in the second argument so then it takes the values in this corresponding to the indices in this argument so it it then gets this P11 p12 Etc this Matrix and then it also gets the negative log likelihood it calculat the negative log likelihood so in one line of code we actually get the loss and this is an awesome function which is a very powerful function in pytorch you can take a look at this I'll also share the link to this uh this uh torch P torch function with you awesome so this is how we calculate the loss between an input batch and a Target batch but now remember that we have to calculate the loss for all of the batches right and that's why we are defining a function called calculate loss loader which calculates the loss from all of the batches together the main function in this the main part in this is that you get the input and Target batch for the entire data loader which means that uh so here we just looked at one input and one output batch right one target batch but you will see here there are many input and Out target batches uh so Row one row one of input and Row one of Target is the first batch row two is the second batch so there are multiple batches and we have to miate the loss for all of those right so similarly you get the input and Target batch uh and then you Loop over so when you're looking at one batch you just run this earlier function and then you just aggregate the losses together so when you uh run the loss for one batch you'll get the loss then you add it with the loss for the second batch and similarly you get the total loss and then ultimately you just divide the total loss with the number of batches which will give you a mean loss per batch the different parts of the code which are added before ensure that if the length of the data loader is zero it we return that the loss is not a number because length of data loader is zero does not make sense both our uh training and the validation data loaders currently training data loader is of length nine because there are nine batches validation data loader is of length one because there is one batch if the length of the data loader is itself zero which means that there are no batches and there is nothing to compute similarly when we uh when we call this Cal caloss loader function and if we don't specify the number of batches so if by default it's none we set the number of batches equal to the length of the data loader so for the training data loader that will be equal to 9 for the validation data loader that will be equal to one now if someone specifies the number of batches here which are more than the number of batches in the data loader we set the actual number of batches to be minimum of those two uh right so the number of batches equal to minimum of number of batches set here and remember that in the data loader also there is a provision to set the number of batches so the ultimate batch size which is used for computation will be minimum of those two that's it and then we take the one input and one target at a time we find the loss according to this scal loss batch function which implements the uh functional cross nn. functional. cross entropy loss and then we actually add all of the losses together from every input Target batch and then we just divide by the number of batches and this is how we got get the average uh cross entropy loss per batch this the output of this function is the loss of our large language model on this book The Verdict data set which we considered in today's lecture now let's actually run uh let's actually call this function on the data which we have and let's see the output which we get right okay so what I'm going to do now is that uh I'm going to call this Cal Closs loader and I'm going to uh input the train loader the model and the device and here you see tor. device if tor. Qi is available else it will run on CPU so uh my code is running on my CPU right now and I'm calling this scal Closs loader function for both the train loss and for the validation loss so when I call it for the train loss I I input the train loader here when I call it for the validation loss I input the validation loader and the model is essentially uh the instance which we have already created here this is the model an instance of the GPT model class and uh that's the second argument the third argument is the device so uh if if you want to run on CPU it can even run on CPU like I'm showing right now so I uncommented I've commented these lines right now uncommenting these lines will allow the code to run on Apple silicon chips if available which is approximately 2X faster than on Apple CPU so right now my code is running on Apple CPU you can also run it on Cuda if Cuda is available or if you have GPU access you can even run it on GPU so right now what I'm going to do is that I'm just going to click on this run and I'm going to show you live how much time it is taking for me to run this uh I just want to show you that uh here what we have essentially done is that we have loaded this entire data set we have converted this into input output pairs we have passed the uh data set into the GPT architecture block so we have passed the data set into this llm architecture block which looks like this uh and then we have got the llm outputs and then we have compared the loss with the targets and with these outputs and then we have collected an aggregate matric of this loss so here you can see I've got the training loss and I've got the validation loss and I got it live in less than 30 seconds I would say now you you can take this code and you can do whatever you want you can increase the context size to 1024 all you need to do is go here and change the context size to 1024 to mimic conditions more closely to gpt2 you can even go to internet and search uh Harry Potter book download um you can download the Harry Potter book there's an ebook series here just make sure the um just make sure about the copyright versions similarly you can go ahead and download any data set which you want and just train the large language or just run this code on the data set which you are considering it will be truly awesome for you to use your own data set and get this training and validation loss because once you have obtained the training and validation loss that really opens up the door for us to to back propagate so in the next lecture what we are going to do is that we are actually going to Define an llm training function which implements the back propagation and which tries to minimize the training and the validation loss so then it will make sure that the outputs being generated are very coherent and then even if you run this code on another data set even in the next code when we do the pre-training you can do the same pre- trining on your custom data set so the code which we have developed today is pretty generalizable and uh I hope you you understood what we are trying to demonstrate today we are trying to demonstrate through a real hand on example how we can actually take a data set from the internet and we can divide it into input Target pairs we can run the data set through a large language model which we ourselves have developed if you have not been through the previous lectures this this model I have not taken it from anywhere we have developed it live we have developed it from scratch without any single Library like Lang chain or any other Library we have coded this from the basic building blocks and that has been used to produce this output that's even more satisfying uh okay students so that brings me to the end of this lecture I deliberately wanted you to I wanted to give you a feel of the Whiteboard teaching uh so that you understand the intuition Theory and also the coding which is my main goal in every lecture which I conduct in the next lecture we are going to look at llm pre-training I'll be sharing this code file with you if you can run it before the next lecture it's awesome if not it's fine I'll try to make the next lecture so that it's selfcontain thank you so much everyone and I look forward to seeing you in the next lecture"
}