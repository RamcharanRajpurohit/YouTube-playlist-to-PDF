{
  "video": {
    "video_id": "ghCSGRgVB_o",
    "title": "Lecture 10: What are token embeddings?",
    "duration": 3652.0,
    "index": 9
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.2
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.24,
      "duration": 5.279
    },
    {
      "text": "in the build large language models from",
      "start": 8.2,
      "duration": 6.12
    },
    {
      "text": "scratch Series today I am going to cover",
      "start": 10.519,
      "duration": 6.641
    },
    {
      "text": "a very very important topic and that",
      "start": 14.32,
      "duration": 5.039
    },
    {
      "text": "topic is called token",
      "start": 17.16,
      "duration": 4.44
    },
    {
      "text": "embeddings let me highlight this over",
      "start": 19.359,
      "duration": 4.68
    },
    {
      "text": "here so today we are going to learn",
      "start": 21.6,
      "duration": 4.08
    },
    {
      "text": "about this concept called token",
      "start": 24.039,
      "duration": 4.121
    },
    {
      "text": "embeddings people also call these as",
      "start": 25.68,
      "duration": 5.079
    },
    {
      "text": "vector embeddings or word embeddings",
      "start": 28.16,
      "duration": 4.76
    },
    {
      "text": "calling them Vector embeddings is fine",
      "start": 30.759,
      "duration": 5.761
    },
    {
      "text": "but word embeddings is not entirely",
      "start": 32.92,
      "duration": 8.72
    },
    {
      "text": "accurate uh tokens Can Be words can be",
      "start": 36.52,
      "duration": 8.12
    },
    {
      "text": "subwords or even can be characters and",
      "start": 41.64,
      "duration": 6.04
    },
    {
      "text": "token is a more broader term and that's",
      "start": 44.64,
      "duration": 5.759
    },
    {
      "text": "why I prefer to use the word token",
      "start": 47.68,
      "duration": 5.359
    },
    {
      "text": "embeddings so what are token embeddings",
      "start": 50.399,
      "duration": 5.041
    },
    {
      "text": "and why are they so important let's get",
      "start": 53.039,
      "duration": 5.401
    },
    {
      "text": "started with today's lecture if you look",
      "start": 55.44,
      "duration": 5.279
    },
    {
      "text": "at this workflow of how large language",
      "start": 58.44,
      "duration": 5.16
    },
    {
      "text": "models actually work there is an input",
      "start": 60.719,
      "duration": 5.521
    },
    {
      "text": "text and let's say the input text is",
      "start": 63.6,
      "duration": 5.559
    },
    {
      "text": "this is an example what happens next is",
      "start": 66.24,
      "duration": 5.519
    },
    {
      "text": "that the input text is broken down into",
      "start": 69.159,
      "duration": 5.681
    },
    {
      "text": "tokens let's say this is the one token",
      "start": 71.759,
      "duration": 5.561
    },
    {
      "text": "is is the second token as is the third",
      "start": 74.84,
      "duration": 5.639
    },
    {
      "text": "token and example is the fourth token",
      "start": 77.32,
      "duration": 5.04
    },
    {
      "text": "this is an example of a word based",
      "start": 80.479,
      "duration": 6.0
    },
    {
      "text": "tokenizer so each token will be one word",
      "start": 82.36,
      "duration": 7.2
    },
    {
      "text": "but the way tokenizers actually work in",
      "start": 86.479,
      "duration": 6.28
    },
    {
      "text": "model such as GPT is that GPT uses a",
      "start": 89.56,
      "duration": 6.44
    },
    {
      "text": "bite pair encoder as a tokenizer which",
      "start": 92.759,
      "duration": 6.161
    },
    {
      "text": "is a subword tokenizer which means that",
      "start": 96.0,
      "duration": 6.04
    },
    {
      "text": "even parts of words or even characters",
      "start": 98.92,
      "duration": 6.12
    },
    {
      "text": "can be individual tokens but that's not",
      "start": 102.04,
      "duration": 5.28
    },
    {
      "text": "the main focus of today's lecture if you",
      "start": 105.04,
      "duration": 4.719
    },
    {
      "text": "want to understand about tokenizers in",
      "start": 107.32,
      "duration": 4.92
    },
    {
      "text": "detail we have covered that in one of",
      "start": 109.759,
      "duration": 4.32
    },
    {
      "text": "the previous lectures and we have also",
      "start": 112.24,
      "duration": 4.239
    },
    {
      "text": "seen about bite pair encoding in one of",
      "start": 114.079,
      "duration": 5.561
    },
    {
      "text": "the previous lectures so tokenizing the",
      "start": 116.479,
      "duration": 5.881
    },
    {
      "text": "word is the step number one then comes",
      "start": 119.64,
      "duration": 4.799
    },
    {
      "text": "step number two which is converting",
      "start": 122.36,
      "duration": 5.079
    },
    {
      "text": "these tokens into token IDs so every",
      "start": 124.439,
      "duration": 6.44
    },
    {
      "text": "token is converted into token IDs and",
      "start": 127.439,
      "duration": 6.601
    },
    {
      "text": "then comes step number three we don't",
      "start": 130.879,
      "duration": 6.0
    },
    {
      "text": "just stop at these token IDs token IDs",
      "start": 134.04,
      "duration": 5.16
    },
    {
      "text": "are converted into something which is",
      "start": 136.879,
      "duration": 4.401
    },
    {
      "text": "called as token",
      "start": 139.2,
      "duration": 4.72
    },
    {
      "text": "embeddings and these token embeddings",
      "start": 141.28,
      "duration": 5.64
    },
    {
      "text": "then serve as the input to training the",
      "start": 143.92,
      "duration": 6.0
    },
    {
      "text": "large language model such as the GPT and",
      "start": 146.92,
      "duration": 4.2
    },
    {
      "text": "and then there are number of",
      "start": 149.92,
      "duration": 3.2
    },
    {
      "text": "postprocessing steps and then comes the",
      "start": 151.12,
      "duration": 5.16
    },
    {
      "text": "final output so today we are going to",
      "start": 153.12,
      "duration": 5.56
    },
    {
      "text": "look at step number three after you",
      "start": 156.28,
      "duration": 4.48
    },
    {
      "text": "generate the tokens after you generate",
      "start": 158.68,
      "duration": 5.199
    },
    {
      "text": "the token IDs in Step number one and two",
      "start": 160.76,
      "duration": 5.119
    },
    {
      "text": "what are token embeddings why do you",
      "start": 163.879,
      "duration": 4.321
    },
    {
      "text": "need them and why this third step is so",
      "start": 165.879,
      "duration": 6.44
    },
    {
      "text": "important especially when dealing with",
      "start": 168.2,
      "duration": 4.119
    },
    {
      "text": "language so as I have mentioned here",
      "start": 172.44,
      "duration": 4.92
    },
    {
      "text": "today we are going to learn about step",
      "start": 175.0,
      "duration": 4.519
    },
    {
      "text": "number three which is creating token",
      "start": 177.36,
      "duration": 4.92
    },
    {
      "text": "embed ICS awesome so I have broken down",
      "start": 179.519,
      "duration": 6.0
    },
    {
      "text": "today's lecture into 1 2",
      "start": 182.28,
      "duration": 4.92
    },
    {
      "text": "three",
      "start": 185.519,
      "duration": 4.72
    },
    {
      "text": "um four to five different five to six",
      "start": 187.2,
      "duration": 5.24
    },
    {
      "text": "different modules and we are going",
      "start": 190.239,
      "duration": 3.841
    },
    {
      "text": "through we are going to go through some",
      "start": 192.44,
      "duration": 4.519
    },
    {
      "text": "Jupiter notebooks some code and Ive also",
      "start": 194.08,
      "duration": 4.879
    },
    {
      "text": "constructed some presentation specially",
      "start": 196.959,
      "duration": 5.041
    },
    {
      "text": "for today's lecture the reason is that I",
      "start": 198.959,
      "duration": 4.441
    },
    {
      "text": "want this lecture to be very",
      "start": 202.0,
      "duration": 3.44
    },
    {
      "text": "comprehensive I have seen so much",
      "start": 203.4,
      "duration": 3.96
    },
    {
      "text": "content out there which really does not",
      "start": 205.44,
      "duration": 4.439
    },
    {
      "text": "motivate the concept of token embeddings",
      "start": 207.36,
      "duration": 5.04
    },
    {
      "text": "it does not show small practical",
      "start": 209.879,
      "duration": 4.401
    },
    {
      "text": "demonstrations all of this is going to",
      "start": 212.4,
      "duration": 3.88
    },
    {
      "text": "be covered in today's lecture so we are",
      "start": 214.28,
      "duration": 3.92
    },
    {
      "text": "going to start with a conceptual",
      "start": 216.28,
      "duration": 3.4
    },
    {
      "text": "understanding of why token embeddings",
      "start": 218.2,
      "duration": 3.599
    },
    {
      "text": "are important then we are going to see a",
      "start": 219.68,
      "duration": 4.4
    },
    {
      "text": "small Hands-On demo where we'll play",
      "start": 221.799,
      "duration": 4.481
    },
    {
      "text": "with token embeddings to give you an",
      "start": 224.08,
      "duration": 4.84
    },
    {
      "text": "intuitive feeling and then we are going",
      "start": 226.28,
      "duration": 4.44
    },
    {
      "text": "to look at how are token embeddings",
      "start": 228.92,
      "duration": 4.28
    },
    {
      "text": "created for large language",
      "start": 230.72,
      "duration": 5.4
    },
    {
      "text": "models so let's get started with today's",
      "start": 233.2,
      "duration": 5.52
    },
    {
      "text": "lecture in which the first part which",
      "start": 236.12,
      "duration": 5.199
    },
    {
      "text": "I'm going to cover is conceptual",
      "start": 238.72,
      "duration": 4.28
    },
    {
      "text": "understanding of why token embeddings",
      "start": 241.319,
      "duration": 3.081
    },
    {
      "text": "are",
      "start": 243.0,
      "duration": 3.959
    },
    {
      "text": "needed so as I mentioned I've created a",
      "start": 244.4,
      "duration": 4.64
    },
    {
      "text": "separate presentation in this build llms",
      "start": 246.959,
      "duration": 4.64
    },
    {
      "text": "from scratch series and this is titled",
      "start": 249.04,
      "duration": 4.839
    },
    {
      "text": "what are token embeddings and why do we",
      "start": 251.599,
      "duration": 3.841
    },
    {
      "text": "really need",
      "start": 253.879,
      "duration": 4.521
    },
    {
      "text": "them so",
      "start": 255.44,
      "duration": 5.919
    },
    {
      "text": "uh okay let's get started with this",
      "start": 258.4,
      "duration": 5.359
    },
    {
      "text": "problem of you have words",
      "start": 261.359,
      "duration": 5.12
    },
    {
      "text": "right and you want these words to be",
      "start": 263.759,
      "duration": 5.16
    },
    {
      "text": "input to the machine learning model or",
      "start": 266.479,
      "duration": 3.801
    },
    {
      "text": "to the large language anguage model",
      "start": 268.919,
      "duration": 4.12
    },
    {
      "text": "let's say but computers can't understand",
      "start": 270.28,
      "duration": 5.199
    },
    {
      "text": "words right so you need to represent the",
      "start": 273.039,
      "duration": 4.88
    },
    {
      "text": "words in the format of",
      "start": 275.479,
      "duration": 5.921
    },
    {
      "text": "numbers so let's say we assign random",
      "start": 277.919,
      "duration": 5.481
    },
    {
      "text": "numbers to each word so let's say cat is",
      "start": 281.4,
      "duration": 5.92
    },
    {
      "text": "34 book is 2.9 tablet is minus 20 kitten",
      "start": 283.4,
      "duration": 4.68
    },
    {
      "text": "is",
      "start": 287.32,
      "duration": 4.28
    },
    {
      "text": "-13 so let's say in the llm framework",
      "start": 288.08,
      "duration": 6.24
    },
    {
      "text": "which we just saw we have already",
      "start": 291.6,
      "duration": 5.36
    },
    {
      "text": "converted the tokens into token IDs",
      "start": 294.32,
      "duration": 4.04
    },
    {
      "text": "right and we have maintained the",
      "start": 296.96,
      "duration": 3.76
    },
    {
      "text": "vocabulary so let's say our vocabulary",
      "start": 298.36,
      "duration": 4.6
    },
    {
      "text": "consists of all the tokens and a token",
      "start": 300.72,
      "duration": 5.56
    },
    {
      "text": "ID corresponding to each token so then",
      "start": 302.96,
      "duration": 5.48
    },
    {
      "text": "these token IDs itself can be the",
      "start": 306.28,
      "duration": 3.8
    },
    {
      "text": "training input to the large language",
      "start": 308.44,
      "duration": 3.68
    },
    {
      "text": "model so then why do we need this step",
      "start": 310.08,
      "duration": 5.24
    },
    {
      "text": "number three so yeah words can be",
      "start": 312.12,
      "duration": 5.12
    },
    {
      "text": "represented as numbers and I have",
      "start": 315.32,
      "duration": 4.36
    },
    {
      "text": "already converted tokens into token IDs",
      "start": 317.24,
      "duration": 5.28
    },
    {
      "text": "like this so why can't I just use token",
      "start": 319.68,
      "duration": 6.359
    },
    {
      "text": "IDs as the input to the",
      "start": 322.52,
      "duration": 3.519
    },
    {
      "text": "model there is a reason for this we",
      "start": 326.72,
      "duration": 4.64
    },
    {
      "text": "cannot just use randomly assigned",
      "start": 329.199,
      "duration": 5.081
    },
    {
      "text": "numbers and the main problem is",
      "start": 331.36,
      "duration": 5.72
    },
    {
      "text": "that the beauty of language is that some",
      "start": 334.28,
      "duration": 5.08
    },
    {
      "text": "words are related to other words for",
      "start": 337.08,
      "duration": 5.239
    },
    {
      "text": "example cat and kitten cat and kitten",
      "start": 339.36,
      "duration": 6.679
    },
    {
      "text": "they are related right um dog and puppy",
      "start": 342.319,
      "duration": 6.44
    },
    {
      "text": "they're related words but just assigning",
      "start": 346.039,
      "duration": 5.521
    },
    {
      "text": "random numbers or token IDs to each word",
      "start": 348.759,
      "duration": 5.041
    },
    {
      "text": "does not really capture the semantic",
      "start": 351.56,
      "duration": 5.32
    },
    {
      "text": "meaning between these individual",
      "start": 353.8,
      "duration": 6.32
    },
    {
      "text": "words so cat and kitten are s ically",
      "start": 356.88,
      "duration": 6.28
    },
    {
      "text": "related however the associated numbers",
      "start": 360.12,
      "duration": 6.72
    },
    {
      "text": "34 and minus33 does not capture this",
      "start": 363.16,
      "duration": 6.12
    },
    {
      "text": "relation that's one of the major problem",
      "start": 366.84,
      "duration": 5.24
    },
    {
      "text": "of just using token IDs and I want to",
      "start": 369.28,
      "duration": 5.759
    },
    {
      "text": "explain this to you in another way uh",
      "start": 372.08,
      "duration": 5.16
    },
    {
      "text": "when all of us look at this image we see",
      "start": 375.039,
      "duration": 4.321
    },
    {
      "text": "that it's a cat but do you know why",
      "start": 377.24,
      "duration": 4.72
    },
    {
      "text": "convolutional neural networks work so",
      "start": 379.36,
      "duration": 5.119
    },
    {
      "text": "well because convolutional neural",
      "start": 381.96,
      "duration": 4.639
    },
    {
      "text": "networks don't just use the pixel values",
      "start": 384.479,
      "duration": 5.16
    },
    {
      "text": "and stretch it out as one input vector",
      "start": 386.599,
      "duration": 4.921
    },
    {
      "text": "they actually encode the spatial",
      "start": 389.639,
      "duration": 4.921
    },
    {
      "text": "relation between the pixels so the these",
      "start": 391.52,
      "duration": 5.88
    },
    {
      "text": "two eyes are close to each other right",
      "start": 394.56,
      "duration": 5.32
    },
    {
      "text": "uh the whiskers are closer to the eyes",
      "start": 397.4,
      "duration": 4.799
    },
    {
      "text": "these two ears are close to each other",
      "start": 399.88,
      "duration": 4.0
    },
    {
      "text": "that is an information which is",
      "start": 402.199,
      "duration": 4.361
    },
    {
      "text": "contained in the image itself and we",
      "start": 403.88,
      "duration": 5.159
    },
    {
      "text": "should exploit this information when we",
      "start": 406.56,
      "duration": 4.6
    },
    {
      "text": "give in when we feed the input to the",
      "start": 409.039,
      "duration": 4.361
    },
    {
      "text": "model if we don't exploit the",
      "start": 411.16,
      "duration": 4.12
    },
    {
      "text": "information which is inherently present",
      "start": 413.4,
      "duration": 5.0
    },
    {
      "text": "in the image we are not doing an optimal",
      "start": 415.28,
      "duration": 5.319
    },
    {
      "text": "thing I could just take these pixels",
      "start": 418.4,
      "duration": 3.96
    },
    {
      "text": "here convert them into numbers and feed",
      "start": 420.599,
      "duration": 4.44
    },
    {
      "text": "them as input but then I won't extract",
      "start": 422.36,
      "duration": 4.92
    },
    {
      "text": "the information which is already",
      "start": 425.039,
      "duration": 4.481
    },
    {
      "text": "available to me in this image like which",
      "start": 427.28,
      "duration": 4.08
    },
    {
      "text": "parts are closer to each other the ears",
      "start": 429.52,
      "duration": 4.0
    },
    {
      "text": "are closed the eyes are closed the nose",
      "start": 431.36,
      "duration": 3.839
    },
    {
      "text": "is closer to the eyes",
      "start": 433.52,
      "duration": 4.079
    },
    {
      "text": "Etc that's why convolutional neural",
      "start": 435.199,
      "duration": 4.161
    },
    {
      "text": "networks work so well because they",
      "start": 437.599,
      "duration": 4.961
    },
    {
      "text": "exploit the spatial relation between",
      "start": 439.36,
      "duration": 5.6
    },
    {
      "text": "pixels and they exploit this information",
      "start": 442.56,
      "duration": 5.319
    },
    {
      "text": "inherently present in an image consider",
      "start": 444.96,
      "duration": 4.88
    },
    {
      "text": "text",
      "start": 447.879,
      "duration": 4.281
    },
    {
      "text": "when you look at",
      "start": 449.84,
      "duration": 4.919
    },
    {
      "text": "sentences we as humans are able to",
      "start": 452.16,
      "duration": 4.12
    },
    {
      "text": "understand what sentences mean because",
      "start": 454.759,
      "duration": 4.481
    },
    {
      "text": "words carry meanings and there are some",
      "start": 456.28,
      "duration": 4.919
    },
    {
      "text": "words which are closer in meaning to",
      "start": 459.24,
      "duration": 2.88
    },
    {
      "text": "other",
      "start": 461.199,
      "duration": 4.12
    },
    {
      "text": "words this is the inherent advantage in",
      "start": 462.12,
      "duration": 5.68
    },
    {
      "text": "the text which we need to exploit we",
      "start": 465.319,
      "duration": 4.521
    },
    {
      "text": "need to exploit the fact that cat and",
      "start": 467.8,
      "duration": 3.92
    },
    {
      "text": "kitten are closer to each",
      "start": 469.84,
      "duration": 4.96
    },
    {
      "text": "other dog and puppy are somehow closer",
      "start": 471.72,
      "duration": 5.159
    },
    {
      "text": "to each other in meaning if we don't",
      "start": 474.8,
      "duration": 4.679
    },
    {
      "text": "exploit this information we will train",
      "start": 476.879,
      "duration": 4.481
    },
    {
      "text": "for a huge amount of time and we won't",
      "start": 479.479,
      "duration": 5.12
    },
    {
      "text": "be doing an optimal uh machine learning",
      "start": 481.36,
      "duration": 6.559
    },
    {
      "text": "training words are beautiful they carry",
      "start": 484.599,
      "duration": 6.201
    },
    {
      "text": "meaning so then why not exploit the",
      "start": 487.919,
      "duration": 4.441
    },
    {
      "text": "similarities in meaning between",
      "start": 490.8,
      "duration": 3.079
    },
    {
      "text": "different",
      "start": 492.36,
      "duration": 4.2
    },
    {
      "text": "words so then you might be thinking okay",
      "start": 493.879,
      "duration": 5.801
    },
    {
      "text": "what about one hot encoding so I'll take",
      "start": 496.56,
      "duration": 5.359
    },
    {
      "text": "every word so I'll first have a huge",
      "start": 499.68,
      "duration": 4.44
    },
    {
      "text": "vocabulary of all possible words and",
      "start": 501.919,
      "duration": 5.041
    },
    {
      "text": "then I'll assign one hot encoding so dog",
      "start": 504.12,
      "duration": 5.759
    },
    {
      "text": "would be 0 00 0 let's say one and then",
      "start": 506.96,
      "duration": 5.519
    },
    {
      "text": "rest will be zeros so to every word",
      "start": 509.879,
      "duration": 4.321
    },
    {
      "text": "there will be all zeros but there will",
      "start": 512.479,
      "duration": 5.321
    },
    {
      "text": "be only one one so let me do one hot",
      "start": 514.2,
      "duration": 6.399
    },
    {
      "text": "encoding for every word so dog will be",
      "start": 517.8,
      "duration": 6.0
    },
    {
      "text": "this puppy will be another one hot",
      "start": 520.599,
      "duration": 5.881
    },
    {
      "text": "encoding but this also leads to a",
      "start": 523.8,
      "duration": 5.64
    },
    {
      "text": "similar problem with random number",
      "start": 526.48,
      "duration": 6.44
    },
    {
      "text": "assignment one hot encoding also fails",
      "start": 529.44,
      "duration": 5.76
    },
    {
      "text": "to capture the semantic relationship",
      "start": 532.92,
      "duration": 4.88
    },
    {
      "text": "between words let's say for example if",
      "start": 535.2,
      "duration": 5.44
    },
    {
      "text": "you see dog and puppy how do you know",
      "start": 537.8,
      "duration": 4.719
    },
    {
      "text": "from this one hot encoding that dog and",
      "start": 540.64,
      "duration": 3.96
    },
    {
      "text": "puppy are more closer to each other",
      "start": 542.519,
      "duration": 3.601
    },
    {
      "text": "there is we don't encode this",
      "start": 544.6,
      "duration": 4.04
    },
    {
      "text": "information at all and again it leads to",
      "start": 546.12,
      "duration": 5.12
    },
    {
      "text": "the same problem words have meaning and",
      "start": 548.64,
      "duration": 4.639
    },
    {
      "text": "why don't we exploit this meaning when",
      "start": 551.24,
      "duration": 3.0
    },
    {
      "text": "we",
      "start": 553.279,
      "duration": 3.56
    },
    {
      "text": "construct uh the inputs to be given to",
      "start": 554.24,
      "duration": 5.279
    },
    {
      "text": "the large language",
      "start": 556.839,
      "duration": 6.881
    },
    {
      "text": "model so assigning random token IDs does",
      "start": 559.519,
      "duration": 7.841
    },
    {
      "text": "not work one hot encoding does not",
      "start": 563.72,
      "duration": 7.32
    },
    {
      "text": "work okay so then how do you encode",
      "start": 567.36,
      "duration": 5.719
    },
    {
      "text": "semantic relationship or how do you",
      "start": 571.04,
      "duration": 4.44
    },
    {
      "text": "encode the semantic meaning when you are",
      "start": 573.079,
      "duration": 5.44
    },
    {
      "text": "going to convert these words to",
      "start": 575.48,
      "duration": 6.12
    },
    {
      "text": "numbers then came the idea that what if",
      "start": 578.519,
      "duration": 6.44
    },
    {
      "text": "every word was encoded as a vector this",
      "start": 581.6,
      "duration": 5.84
    },
    {
      "text": "is going to be very important I'm going",
      "start": 584.959,
      "duration": 5.56
    },
    {
      "text": "to take four words here dog cat apple",
      "start": 587.44,
      "duration": 5.68
    },
    {
      "text": "and banana and I'm going to say why",
      "start": 590.519,
      "duration": 5.721
    },
    {
      "text": "don't you encode every word as a vector",
      "start": 593.12,
      "duration": 5.12
    },
    {
      "text": "then you might be saying what should be",
      "start": 596.24,
      "duration": 3.52
    },
    {
      "text": "the dimension of this vector Vector is",
      "start": 598.24,
      "duration": 3.32
    },
    {
      "text": "it a two dimensional Vector is it a",
      "start": 599.76,
      "duration": 3.44
    },
    {
      "text": "three dimensional Vector is it a",
      "start": 601.56,
      "duration": 3.12
    },
    {
      "text": "thousand Dimension",
      "start": 603.2,
      "duration": 5.0
    },
    {
      "text": "Vector then I will say well what if the",
      "start": 604.68,
      "duration": 5.719
    },
    {
      "text": "dimension is determined by",
      "start": 608.2,
      "duration": 5.24
    },
    {
      "text": "features then you might ask okay which",
      "start": 610.399,
      "duration": 5.56
    },
    {
      "text": "features then I'll say that okay let's",
      "start": 613.44,
      "duration": 5.12
    },
    {
      "text": "take these four words dog cat apple and",
      "start": 615.959,
      "duration": 5.961
    },
    {
      "text": "banana and I look at these five features",
      "start": 618.56,
      "duration": 7.64
    },
    {
      "text": "has a tail is eatable has four legs",
      "start": 621.92,
      "duration": 7.64
    },
    {
      "text": "makes sound is a pet",
      "start": 626.2,
      "duration": 5.56
    },
    {
      "text": "and based on these questions so first",
      "start": 629.56,
      "duration": 4.24
    },
    {
      "text": "I'll ask a question does it have a tail",
      "start": 631.76,
      "duration": 4.879
    },
    {
      "text": "is it eatable does it have four legs",
      "start": 633.8,
      "duration": 5.56
    },
    {
      "text": "does it make a sound is it a pet based",
      "start": 636.639,
      "duration": 5.081
    },
    {
      "text": "on the answers if the answer is yes it",
      "start": 639.36,
      "duration": 4.279
    },
    {
      "text": "has a tail the value of that feature",
      "start": 641.72,
      "duration": 4.76
    },
    {
      "text": "will be very high if the answer is yes",
      "start": 643.639,
      "duration": 4.961
    },
    {
      "text": "it's eatable the value of that feature",
      "start": 646.48,
      "duration": 4.479
    },
    {
      "text": "will be very high this is how I will",
      "start": 648.6,
      "duration": 3.96
    },
    {
      "text": "construct these",
      "start": 650.959,
      "duration": 4.161
    },
    {
      "text": "vectors so let me show you what I mean",
      "start": 652.56,
      "duration": 4.44
    },
    {
      "text": "in some detail so let's say you look at",
      "start": 655.12,
      "duration": 4.519
    },
    {
      "text": "a dog right look at the values which are",
      "start": 657.0,
      "duration": 5.68
    },
    {
      "text": "very high has a tail this value is very",
      "start": 659.639,
      "duration": 5.281
    },
    {
      "text": "high has four legs its value is high",
      "start": 662.68,
      "duration": 4.76
    },
    {
      "text": "makes sound its value is high is a pet",
      "start": 664.92,
      "duration": 5.919
    },
    {
      "text": "its value is very high great look at cat",
      "start": 667.44,
      "duration": 5.68
    },
    {
      "text": "has a tail it's again the value is very",
      "start": 670.839,
      "duration": 4.721
    },
    {
      "text": "high has four legs makes sound and is a",
      "start": 673.12,
      "duration": 5.159
    },
    {
      "text": "pet so now if you see dog and cat you",
      "start": 675.56,
      "duration": 4.6
    },
    {
      "text": "will see that they are kind of closer to",
      "start": 678.279,
      "duration": 4.041
    },
    {
      "text": "each other right because whichever",
      "start": 680.16,
      "duration": 5.679
    },
    {
      "text": "values are high in a dog are also higher",
      "start": 682.32,
      "duration": 6.319
    },
    {
      "text": "in a cat as well look at the first index",
      "start": 685.839,
      "duration": 4.641
    },
    {
      "text": "has a tail it's higher in both dog as",
      "start": 688.639,
      "duration": 4.121
    },
    {
      "text": "well as cat look at the second index",
      "start": 690.48,
      "duration": 4.56
    },
    {
      "text": "it's low in both dog as well as cat look",
      "start": 692.76,
      "duration": 4.84
    },
    {
      "text": "at the last index is a pet it's high in",
      "start": 695.04,
      "duration": 4.68
    },
    {
      "text": "both dog as well as",
      "start": 697.6,
      "duration": 4.6
    },
    {
      "text": "cat now let's look at the vector",
      "start": 699.72,
      "duration": 5.359
    },
    {
      "text": "representation for apple and banana for",
      "start": 702.2,
      "duration": 5.48
    },
    {
      "text": "apple and banana has a tail has four",
      "start": 705.079,
      "duration": 6.481
    },
    {
      "text": "legs and is a pet are very low but what",
      "start": 707.68,
      "duration": 6.719
    },
    {
      "text": "is high is is eatable makes sound is",
      "start": 711.56,
      "duration": 5.16
    },
    {
      "text": "also very low so if you look at apple",
      "start": 714.399,
      "duration": 5.161
    },
    {
      "text": "and banana you definitely see that they",
      "start": 716.72,
      "duration": 5.16
    },
    {
      "text": "are closer to each other right because",
      "start": 719.56,
      "duration": 4.6
    },
    {
      "text": "whatever is high for apple is also high",
      "start": 721.88,
      "duration": 5.12
    },
    {
      "text": "for banana whatever is low for apple is",
      "start": 724.16,
      "duration": 5.16
    },
    {
      "text": "also low for",
      "start": 727.0,
      "duration": 5.6
    },
    {
      "text": "banana so the good thing is that if we",
      "start": 729.32,
      "duration": 7.04
    },
    {
      "text": "represent words as vectors and if we",
      "start": 732.6,
      "duration": 6.039
    },
    {
      "text": "construct these vectors in a smart",
      "start": 736.36,
      "duration": 6.279
    },
    {
      "text": "manner vectors can capture semantic",
      "start": 738.639,
      "duration": 7.281
    },
    {
      "text": "meaning which was not captured before",
      "start": 742.639,
      "duration": 5.241
    },
    {
      "text": "when you did one hot encoding or when",
      "start": 745.92,
      "duration": 4.32
    },
    {
      "text": "you did random number assignment the",
      "start": 747.88,
      "duration": 5.16
    },
    {
      "text": "semantic meaning was not captured only",
      "start": 750.24,
      "duration": 4.8
    },
    {
      "text": "when we represented words as these",
      "start": 753.04,
      "duration": 5.12
    },
    {
      "text": "vectors was the semantic meaning between",
      "start": 755.04,
      "duration": 5.44
    },
    {
      "text": "different words captured one more thing",
      "start": 758.16,
      "duration": 4.4
    },
    {
      "text": "to note here is that dog and cat are",
      "start": 760.48,
      "duration": 4.0
    },
    {
      "text": "similar or closer to each other than",
      "start": 762.56,
      "duration": 4.399
    },
    {
      "text": "let's say dog and banana if you compare",
      "start": 764.48,
      "duration": 4.44
    },
    {
      "text": "dog and banana you'll see that whatever",
      "start": 766.959,
      "duration": 3.761
    },
    {
      "text": "is higher in the dog let's say has a",
      "start": 768.92,
      "duration": 4.24
    },
    {
      "text": "tail is lower in a banana whatever is",
      "start": 770.72,
      "duration": 4.96
    },
    {
      "text": "higher in banana let's say is eatable is",
      "start": 773.16,
      "duration": 5.2
    },
    {
      "text": "lower in a dog So based on Vector",
      "start": 775.68,
      "duration": 5.04
    },
    {
      "text": "representation you can group Words which",
      "start": 778.36,
      "duration": 4.279
    },
    {
      "text": "are similar to each other and also see",
      "start": 780.72,
      "duration": 4.119
    },
    {
      "text": "which words are farther away from each",
      "start": 782.639,
      "duration": 5.281
    },
    {
      "text": "other isn't this awesome similar to how",
      "start": 784.839,
      "duration": 5.0
    },
    {
      "text": "we encoded",
      "start": 787.92,
      "duration": 4.96
    },
    {
      "text": "the information inherently presented",
      "start": 789.839,
      "duration": 5.601
    },
    {
      "text": "present in an image while feeding the",
      "start": 792.88,
      "duration": 5.12
    },
    {
      "text": "input to a convolutional neural network",
      "start": 795.44,
      "duration": 4.24
    },
    {
      "text": "what we are doing here is that we are",
      "start": 798.0,
      "duration": 3.399
    },
    {
      "text": "saying that what is the information",
      "start": 799.68,
      "duration": 4.12
    },
    {
      "text": "inherently present in text and that",
      "start": 801.399,
      "duration": 5.0
    },
    {
      "text": "information is that textual words have",
      "start": 803.8,
      "duration": 4.92
    },
    {
      "text": "semantic meaning so then why don't we",
      "start": 806.399,
      "duration": 4.081
    },
    {
      "text": "convert these words into vectors that",
      "start": 808.72,
      "duration": 4.0
    },
    {
      "text": "can capture this",
      "start": 810.48,
      "duration": 5.12
    },
    {
      "text": "meaning great so these are called as",
      "start": 812.72,
      "duration": 5.359
    },
    {
      "text": "vector embeddings and they are also",
      "start": 815.6,
      "duration": 4.679
    },
    {
      "text": "called as token embeddings because every",
      "start": 818.079,
      "duration": 4.641
    },
    {
      "text": "token is converted into a vector",
      "start": 820.279,
      "duration": 4.92
    },
    {
      "text": "embedding that's where the word or",
      "start": 822.72,
      "duration": 5.119
    },
    {
      "text": "that's where the I would say phrase",
      "start": 825.199,
      "duration": 5.601
    },
    {
      "text": "token embeddings actually comes into the",
      "start": 827.839,
      "duration": 6.641
    },
    {
      "text": "picture so the first point to take away",
      "start": 830.8,
      "duration": 5.719
    },
    {
      "text": "from this lecture is that vectors can",
      "start": 834.48,
      "duration": 5.24
    },
    {
      "text": "definitely capture semantic meaning",
      "start": 836.519,
      "duration": 4.801
    },
    {
      "text": "now the next question is how do you",
      "start": 839.72,
      "duration": 4.559
    },
    {
      "text": "construct these vectors how do you make",
      "start": 841.32,
      "duration": 6.0
    },
    {
      "text": "sure that okay how how do I make these",
      "start": 844.279,
      "duration": 4.961
    },
    {
      "text": "vectors so that let's say dog and puppy",
      "start": 847.32,
      "duration": 4.439
    },
    {
      "text": "are closer dog and cat are closer but",
      "start": 849.24,
      "duration": 5.12
    },
    {
      "text": "dog and banana are farther apart how do",
      "start": 851.759,
      "duration": 5.361
    },
    {
      "text": "I make these vectors and that's all I'm",
      "start": 854.36,
      "duration": 4.64
    },
    {
      "text": "going to tell you in the next part of",
      "start": 857.12,
      "duration": 2.6
    },
    {
      "text": "this",
      "start": 859.0,
      "duration": 3.079
    },
    {
      "text": "lecture how do you come up with these",
      "start": 859.72,
      "duration": 5.359
    },
    {
      "text": "vector embeddings or token",
      "start": 862.079,
      "duration": 5.521
    },
    {
      "text": "embeddings and uh the answer here the",
      "start": 865.079,
      "duration": 4.401
    },
    {
      "text": "real simple answer is that we have to",
      "start": 867.6,
      "duration": 4.0
    },
    {
      "text": "train a neural network to create Vector",
      "start": 869.48,
      "duration": 4.919
    },
    {
      "text": "embedding so for example we have",
      "start": 871.6,
      "duration": 4.88
    },
    {
      "text": "information which are all the tokens and",
      "start": 874.399,
      "duration": 5.68
    },
    {
      "text": "we have some output and uh based on this",
      "start": 876.48,
      "duration": 5.44
    },
    {
      "text": "information and the output we have to",
      "start": 880.079,
      "duration": 5.081
    },
    {
      "text": "train a neural network to make sure that",
      "start": 881.92,
      "duration": 5.919
    },
    {
      "text": "the vector embedding is correct where",
      "start": 885.16,
      "duration": 4.28
    },
    {
      "text": "does this information come from it's",
      "start": 887.839,
      "duration": 4.201
    },
    {
      "text": "from text So based on the sentences in a",
      "start": 889.44,
      "duration": 4.839
    },
    {
      "text": "textual document we know which words are",
      "start": 892.04,
      "duration": 4.56
    },
    {
      "text": "closer to each other which words are",
      "start": 894.279,
      "duration": 4.48
    },
    {
      "text": "similar to each other and those should",
      "start": 896.6,
      "duration": 4.479
    },
    {
      "text": "have similar vectors that's the training",
      "start": 898.759,
      "duration": 5.241
    },
    {
      "text": "data and we train a neural network to",
      "start": 901.079,
      "duration": 6.041
    },
    {
      "text": "construct a vector embedding uh and I'll",
      "start": 904.0,
      "duration": 5.32
    },
    {
      "text": "explain this to you in a bit more detail",
      "start": 907.12,
      "duration": 4.24
    },
    {
      "text": "but just know this that creating these",
      "start": 909.32,
      "duration": 4.72
    },
    {
      "text": "Vector embeddings is not easy because",
      "start": 911.36,
      "duration": 4.599
    },
    {
      "text": "I'm just showing four words right now",
      "start": 914.04,
      "duration": 3.76
    },
    {
      "text": "but imagine there is a vocabulary of",
      "start": 915.959,
      "duration": 4.921
    },
    {
      "text": "50,000 words when gpt2 was trained it",
      "start": 917.8,
      "duration": 6.159
    },
    {
      "text": "had a vocabulary of 50,000 words and you",
      "start": 920.88,
      "duration": 5.44
    },
    {
      "text": "have to create a vector embedding in for",
      "start": 923.959,
      "duration": 5.961
    },
    {
      "text": "these many words remember how",
      "start": 926.32,
      "duration": 6.04
    },
    {
      "text": "how computationally expensive that would",
      "start": 929.92,
      "duration": 5.12
    },
    {
      "text": "be and that's why training GPT takes a",
      "start": 932.36,
      "duration": 5.36
    },
    {
      "text": "huge amount of",
      "start": 935.04,
      "duration": 5.359
    },
    {
      "text": "time okay so this brings me to the end",
      "start": 937.72,
      "duration": 4.44
    },
    {
      "text": "of the presentation where hopefully I",
      "start": 940.399,
      "duration": 4.281
    },
    {
      "text": "wanted to convey two points first is",
      "start": 942.16,
      "duration": 6.799
    },
    {
      "text": "that words carry meaning and to give",
      "start": 944.68,
      "duration": 6.0
    },
    {
      "text": "these words as input to the large",
      "start": 948.959,
      "duration": 3.721
    },
    {
      "text": "language models we need to exploit this",
      "start": 950.68,
      "duration": 6.04
    },
    {
      "text": "meaning if we just use random token IDs",
      "start": 952.68,
      "duration": 5.88
    },
    {
      "text": "or if you use one hot encoding this",
      "start": 956.72,
      "duration": 3.72
    },
    {
      "text": "cement IC relationship or meaning",
      "start": 958.56,
      "duration": 4.959
    },
    {
      "text": "between words is not exploited but we",
      "start": 960.44,
      "duration": 5.12
    },
    {
      "text": "saw a glimpse of if words are",
      "start": 963.519,
      "duration": 5.12
    },
    {
      "text": "represented as vectors maybe we can",
      "start": 965.56,
      "duration": 5.519
    },
    {
      "text": "incorporate this semantic relationship",
      "start": 968.639,
      "duration": 4.721
    },
    {
      "text": "between the different",
      "start": 971.079,
      "duration": 4.68
    },
    {
      "text": "words incorporating words or",
      "start": 973.36,
      "duration": 4.599
    },
    {
      "text": "representing words as vectors so that",
      "start": 975.759,
      "duration": 4.401
    },
    {
      "text": "the semantic relationship is preserved",
      "start": 977.959,
      "duration": 4.44
    },
    {
      "text": "is called as Vector embedding and it's",
      "start": 980.16,
      "duration": 4.159
    },
    {
      "text": "also called as token",
      "start": 982.399,
      "duration": 4.761
    },
    {
      "text": "embedding in the last part of this PPT",
      "start": 984.319,
      "duration": 5.041
    },
    {
      "text": "we saw that creating these vector or",
      "start": 987.16,
      "duration": 5.239
    },
    {
      "text": "token embeddings is not easy because you",
      "start": 989.36,
      "duration": 4.88
    },
    {
      "text": "need to train a neural network to make",
      "start": 992.399,
      "duration": 4.8
    },
    {
      "text": "sure that the right Vector embedding is",
      "start": 994.24,
      "duration": 6.44
    },
    {
      "text": "created great so now we have finished",
      "start": 997.199,
      "duration": 7.481
    },
    {
      "text": "the the first aspect of our agenda today",
      "start": 1000.68,
      "duration": 6.36
    },
    {
      "text": "which was essentially to show you all a",
      "start": 1004.68,
      "duration": 4.48
    },
    {
      "text": "conceptual understanding of why token",
      "start": 1007.04,
      "duration": 4.799
    },
    {
      "text": "embeddings are needed now what we are",
      "start": 1009.16,
      "duration": 4.919
    },
    {
      "text": "going to do is that we are going to see",
      "start": 1011.839,
      "duration": 4.56
    },
    {
      "text": "a small Hands-On demo so that you",
      "start": 1014.079,
      "duration": 4.56
    },
    {
      "text": "improve your conceptual understanding of",
      "start": 1016.399,
      "duration": 5.12
    },
    {
      "text": "to embeddings this demo is not related",
      "start": 1018.639,
      "duration": 4.56
    },
    {
      "text": "to the main code file which we are",
      "start": 1021.519,
      "duration": 4.241
    },
    {
      "text": "developing for uh building a large",
      "start": 1023.199,
      "duration": 5.24
    },
    {
      "text": "language model but it's just a toy",
      "start": 1025.76,
      "duration": 5.679
    },
    {
      "text": "demo uh which I have constructed over",
      "start": 1028.439,
      "duration": 6.4
    },
    {
      "text": "here so uh let's get started with this",
      "start": 1031.439,
      "duration": 6.161
    },
    {
      "text": "demo many big companies like Google",
      "start": 1034.839,
      "duration": 5.441
    },
    {
      "text": "already have pre-trained token um",
      "start": 1037.6,
      "duration": 5.28
    },
    {
      "text": "embeddings which means that this word to",
      "start": 1040.28,
      "duration": 5.0
    },
    {
      "text": "W Google News 300 let's search about it",
      "start": 1042.88,
      "duration": 3.76
    },
    {
      "text": "a",
      "start": 1045.28,
      "duration": 4.12
    },
    {
      "text": "bit so uh",
      "start": 1046.64,
      "duration": 5.0
    },
    {
      "text": "Google so there is a Google News data",
      "start": 1049.4,
      "duration": 5.6
    },
    {
      "text": "set which has about 100 billion words so",
      "start": 1051.64,
      "duration": 6.279
    },
    {
      "text": "this word to W Google News 300 are",
      "start": 1055.0,
      "duration": 5.32
    },
    {
      "text": "already pre-trained vectors on this huge",
      "start": 1057.919,
      "duration": 5.64
    },
    {
      "text": "data set so Google has already trained",
      "start": 1060.32,
      "duration": 5.16
    },
    {
      "text": "uh or trained the neural network to",
      "start": 1063.559,
      "duration": 3.801
    },
    {
      "text": "create this Vector embeddings so what",
      "start": 1065.48,
      "duration": 3.439
    },
    {
      "text": "has been done is that we get the Google",
      "start": 1067.36,
      "duration": 4.199
    },
    {
      "text": "News data set with 100 billion words and",
      "start": 1068.919,
      "duration": 5.201
    },
    {
      "text": "then we do the training to map every",
      "start": 1071.559,
      "duration": 3.761
    },
    {
      "text": "word to a",
      "start": 1074.12,
      "duration": 4.76
    },
    {
      "text": "vector now what is this 300 the 300 is B",
      "start": 1075.32,
      "duration": 5.68
    },
    {
      "text": "basically the number of Dimensions when",
      "start": 1078.88,
      "duration": 4.32
    },
    {
      "text": "we create token embeddings or vector",
      "start": 1081.0,
      "duration": 5.2
    },
    {
      "text": "embeddings words are mapped into a large",
      "start": 1083.2,
      "duration": 5.44
    },
    {
      "text": "dimensional Vector space in the",
      "start": 1086.2,
      "duration": 4.12
    },
    {
      "text": "demonstration which you just saw this",
      "start": 1088.64,
      "duration": 4.24
    },
    {
      "text": "was a five dimensional Vector space but",
      "start": 1090.32,
      "duration": 4.599
    },
    {
      "text": "five dimensions are not really enough to",
      "start": 1092.88,
      "duration": 4.6
    },
    {
      "text": "capture all the meaning so here we are",
      "start": 1094.919,
      "duration": 6.24
    },
    {
      "text": "using a 300 Dimension word to W which",
      "start": 1097.48,
      "duration": 5.96
    },
    {
      "text": "means that every word is transformed",
      "start": 1101.159,
      "duration": 4.88
    },
    {
      "text": "into a 300 dimensional vector and then",
      "start": 1103.44,
      "duration": 5.479
    },
    {
      "text": "we train based on the underlying data so",
      "start": 1106.039,
      "duration": 4.361
    },
    {
      "text": "that the semantic meaning between the",
      "start": 1108.919,
      "duration": 3.801
    },
    {
      "text": "words is preserved this is already a",
      "start": 1110.4,
      "duration": 5.56
    },
    {
      "text": "pre-trained data set when GPT was built",
      "start": 1112.72,
      "duration": 5.28
    },
    {
      "text": "or when large language models are built",
      "start": 1115.96,
      "duration": 4.48
    },
    {
      "text": "they don't use a pre-train data set they",
      "start": 1118.0,
      "duration": 5.4
    },
    {
      "text": "train these embeddings uh along with",
      "start": 1120.44,
      "duration": 5.239
    },
    {
      "text": "training the large language model itself",
      "start": 1123.4,
      "duration": 4.44
    },
    {
      "text": "I'll come to that in a moment but for",
      "start": 1125.679,
      "duration": 4.041
    },
    {
      "text": "now for the sake of this demonstration",
      "start": 1127.84,
      "duration": 4.12
    },
    {
      "text": "just know that I'm already using",
      "start": 1129.72,
      "duration": 4.76
    },
    {
      "text": "pre-trained word to we which means that",
      "start": 1131.96,
      "duration": 4.68
    },
    {
      "text": "this model it can take any word as an",
      "start": 1134.48,
      "duration": 4.84
    },
    {
      "text": "input and convert it into vectors 300",
      "start": 1136.64,
      "duration": 5.279
    },
    {
      "text": "dimensional Vector so what I'm going to",
      "start": 1139.32,
      "duration": 5.2
    },
    {
      "text": "do now is that I'm going",
      "start": 1141.919,
      "duration": 5.321
    },
    {
      "text": "to assign a dictionary which is word",
      "start": 1144.52,
      "duration": 5.96
    },
    {
      "text": "vectors and uh equal to model so model",
      "start": 1147.24,
      "duration": 6.0
    },
    {
      "text": "is basically the word to Vector",
      "start": 1150.48,
      "duration": 6.319
    },
    {
      "text": "embeddings in this word to Google News",
      "start": 1153.24,
      "duration": 6.0
    },
    {
      "text": "300 and then word vectors is the",
      "start": 1156.799,
      "duration": 4.521
    },
    {
      "text": "dictionary how is it a dictionary",
      "start": 1159.24,
      "duration": 5.28
    },
    {
      "text": "basically it will be uh every there will",
      "start": 1161.32,
      "duration": 4.92
    },
    {
      "text": "be words in this dictionary and then",
      "start": 1164.52,
      "duration": 3.6
    },
    {
      "text": "every word will be assigned to a 300",
      "start": 1166.24,
      "duration": 4.4
    },
    {
      "text": "dimensional Vector so let's see what the",
      "start": 1168.12,
      "duration": 4.48
    },
    {
      "text": "vector for computer looks like so if you",
      "start": 1170.64,
      "duration": 3.6
    },
    {
      "text": "print this out you will see this is the",
      "start": 1172.6,
      "duration": 4.48
    },
    {
      "text": "vector for computer it's a 300 Dimension",
      "start": 1174.24,
      "duration": 5.08
    },
    {
      "text": "Vector does not mean anything right now",
      "start": 1177.08,
      "duration": 4.12
    },
    {
      "text": "for now just know that it's 300",
      "start": 1179.32,
      "duration": 4.32
    },
    {
      "text": "Dimension vector and if you print the",
      "start": 1181.2,
      "duration": 5.12
    },
    {
      "text": "vector shape for any word you'll see",
      "start": 1183.64,
      "duration": 5.279
    },
    {
      "text": "that it's 300 which means that every",
      "start": 1186.32,
      "duration": 4.92
    },
    {
      "text": "word is encoded into a 300 dimensional",
      "start": 1188.919,
      "duration": 4.841
    },
    {
      "text": "Vector that's fine what I want to show",
      "start": 1191.24,
      "duration": 5.08
    },
    {
      "text": "you now is I want to prove to you that",
      "start": 1193.76,
      "duration": 5.279
    },
    {
      "text": "well trained Vector embeddings actually",
      "start": 1196.32,
      "duration": 6.599
    },
    {
      "text": "the semantic meaning right so king plus",
      "start": 1199.039,
      "duration": 6.081
    },
    {
      "text": "woman minus man what do you think this",
      "start": 1202.919,
      "duration": 4.161
    },
    {
      "text": "should be just tell me the first thing",
      "start": 1205.12,
      "duration": 4.039
    },
    {
      "text": "which comes to your mind uh you can",
      "start": 1207.08,
      "duration": 5.2
    },
    {
      "text": "pause here right now and think about if",
      "start": 1209.159,
      "duration": 6.081
    },
    {
      "text": "words are actually if vectors are",
      "start": 1212.28,
      "duration": 4.519
    },
    {
      "text": "actually encoding the meaning between",
      "start": 1215.24,
      "duration": 3.88
    },
    {
      "text": "words and if I have a vector for King",
      "start": 1216.799,
      "duration": 4.841
    },
    {
      "text": "let's say if I have a vector for woman",
      "start": 1219.12,
      "duration": 5.08
    },
    {
      "text": "and if I have a vector for man and if I",
      "start": 1221.64,
      "duration": 4.44
    },
    {
      "text": "add the vector for King with the vector",
      "start": 1224.2,
      "duration": 4.64
    },
    {
      "text": "for woman and then I subtract the vector",
      "start": 1226.08,
      "duration": 5.599
    },
    {
      "text": "for man what should I be left",
      "start": 1228.84,
      "duration": 5.28
    },
    {
      "text": "with okay I hope all of you have got the",
      "start": 1231.679,
      "duration": 3.921
    },
    {
      "text": "answer so I should be left with",
      "start": 1234.12,
      "duration": 4.24
    },
    {
      "text": "something which resembles similar to a",
      "start": 1235.6,
      "duration": 6.64
    },
    {
      "text": "queen because king plus man in in",
      "start": 1238.36,
      "duration": 6.0
    },
    {
      "text": "ideally should be equal to Queen plus",
      "start": 1242.24,
      "duration": 5.6
    },
    {
      "text": "woman let's say queen plus king plus",
      "start": 1244.36,
      "duration": 6.559
    },
    {
      "text": "woman should be equal to man plus Queen",
      "start": 1247.84,
      "duration": 5.56
    },
    {
      "text": "so king plus woman minus man should",
      "start": 1250.919,
      "duration": 5.64
    },
    {
      "text": "ideally be Queen why because we take a",
      "start": 1253.4,
      "duration": 5.2
    },
    {
      "text": "masculine aspect we subtract another",
      "start": 1256.559,
      "duration": 4.321
    },
    {
      "text": "another masculine aspect so what should",
      "start": 1258.6,
      "duration": 4.8
    },
    {
      "text": "remain is only a feminine aspect and",
      "start": 1260.88,
      "duration": 4.6
    },
    {
      "text": "woman and a man are there so ideally the",
      "start": 1263.4,
      "duration": 5.279
    },
    {
      "text": "answer to this should be Queen right uh",
      "start": 1265.48,
      "duration": 5.24
    },
    {
      "text": "then we will be satisfied that the",
      "start": 1268.679,
      "duration": 5.321
    },
    {
      "text": "vectors are indeed encoding some meaning",
      "start": 1270.72,
      "duration": 5.959
    },
    {
      "text": "so let's try to do this so we are going",
      "start": 1274.0,
      "duration": 5.24
    },
    {
      "text": "to uh what we are going to do is that we",
      "start": 1276.679,
      "duration": 4.841
    },
    {
      "text": "are going to add King and woman and we",
      "start": 1279.24,
      "duration": 4.84
    },
    {
      "text": "are going to subtract man and then we",
      "start": 1281.52,
      "duration": 5.0
    },
    {
      "text": "are going to print out the words which",
      "start": 1284.08,
      "duration": 5.079
    },
    {
      "text": "are the most similar to the answer",
      "start": 1286.52,
      "duration": 4.159
    },
    {
      "text": "and when you print this you will see",
      "start": 1289.159,
      "duration": 4.0
    },
    {
      "text": "that indeed Queen is the answer of this",
      "start": 1290.679,
      "duration": 5.401
    },
    {
      "text": "and the vector uh",
      "start": 1293.159,
      "duration": 5.721
    },
    {
      "text": "and the probability of getting queen as",
      "start": 1296.08,
      "duration": 5.16
    },
    {
      "text": "the answer is around 71% which means",
      "start": 1298.88,
      "duration": 6.52
    },
    {
      "text": "that uh here is a list of top 10 answers",
      "start": 1301.24,
      "duration": 5.96
    },
    {
      "text": "and out of this queen is the most",
      "start": 1305.4,
      "duration": 4.24
    },
    {
      "text": "preferred answer so this is the first",
      "start": 1307.2,
      "duration": 5.12
    },
    {
      "text": "indication to all of you that if you",
      "start": 1309.64,
      "duration": 5.0
    },
    {
      "text": "convert words to vectors and then you do",
      "start": 1312.32,
      "duration": 3.76
    },
    {
      "text": "addition and subtraction of these",
      "start": 1314.64,
      "duration": 4.159
    },
    {
      "text": "vectors essentially you are encoding",
      "start": 1316.08,
      "duration": 6.12
    },
    {
      "text": "some meaning here so somehow the vectors",
      "start": 1318.799,
      "duration": 5.161
    },
    {
      "text": "have this information that the vector",
      "start": 1322.2,
      "duration": 4.479
    },
    {
      "text": "for King encodes some masculinity the",
      "start": 1323.96,
      "duration": 5.56
    },
    {
      "text": "vector for woman encodes some femininity",
      "start": 1326.679,
      "duration": 5.921
    },
    {
      "text": "the vector for man encodes some",
      "start": 1329.52,
      "duration": 5.48
    },
    {
      "text": "masculinity these vectors also have the",
      "start": 1332.6,
      "duration": 5.16
    },
    {
      "text": "meaning that somehow king and queen are",
      "start": 1335.0,
      "duration": 4.52
    },
    {
      "text": "closer to each other somehow man and",
      "start": 1337.76,
      "duration": 3.56
    },
    {
      "text": "woman are closer to each other isn't",
      "start": 1339.52,
      "duration": 4.2
    },
    {
      "text": "that amazing this really blew my mind",
      "start": 1341.32,
      "duration": 4.0
    },
    {
      "text": "when I knew about this for the first",
      "start": 1343.72,
      "duration": 4.24
    },
    {
      "text": "time and when I knew about this these",
      "start": 1345.32,
      "duration": 5.12
    },
    {
      "text": "things such as as one hot encoding just",
      "start": 1347.96,
      "duration": 4.68
    },
    {
      "text": "seemed so boring because in one hot",
      "start": 1350.44,
      "duration": 4.68
    },
    {
      "text": "encoding no information is captured no",
      "start": 1352.64,
      "duration": 3.72
    },
    {
      "text": "meaning is",
      "start": 1355.12,
      "duration": 3.52
    },
    {
      "text": "captured but if you actually convert",
      "start": 1356.36,
      "duration": 4.96
    },
    {
      "text": "words to vectors and preserve meaning",
      "start": 1358.64,
      "duration": 4.639
    },
    {
      "text": "you can get some you actually get the",
      "start": 1361.32,
      "duration": 3.959
    },
    {
      "text": "meaning preserved I was not really sure",
      "start": 1363.279,
      "duration": 3.76
    },
    {
      "text": "that the meaning will be preserved as",
      "start": 1365.279,
      "duration": 4.64
    },
    {
      "text": "vectors but it it is preserved we can",
      "start": 1367.039,
      "duration": 5.52
    },
    {
      "text": "also do couple of other things uh",
      "start": 1369.919,
      "duration": 4.88
    },
    {
      "text": "ideally woman and man should be closer",
      "start": 1372.559,
      "duration": 4.081
    },
    {
      "text": "to each other king and queen should be",
      "start": 1374.799,
      "duration": 4.321
    },
    {
      "text": "closer uncle and Aunt are related",
      "start": 1376.64,
      "duration": 4.36
    },
    {
      "text": "boy and girl are related nephew and",
      "start": 1379.12,
      "duration": 4.4
    },
    {
      "text": "niece are related paper and water are",
      "start": 1381.0,
      "duration": 5.24
    },
    {
      "text": "related what we can do is that we can",
      "start": 1383.52,
      "duration": 4.8
    },
    {
      "text": "now check whether the vector embeddings",
      "start": 1386.24,
      "duration": 4.319
    },
    {
      "text": "are also related to each other so what",
      "start": 1388.32,
      "duration": 3.839
    },
    {
      "text": "we do is that we convert these words",
      "start": 1390.559,
      "duration": 3.6
    },
    {
      "text": "into vectors and test the similarity",
      "start": 1392.159,
      "duration": 4.64
    },
    {
      "text": "between vectors the way it's done is by",
      "start": 1394.159,
      "duration": 5.361
    },
    {
      "text": "I think finding the distance between the",
      "start": 1396.799,
      "duration": 4.24
    },
    {
      "text": "vectors",
      "start": 1399.52,
      "duration": 4.279
    },
    {
      "text": "so what I do here is World vectors.",
      "start": 1401.039,
      "duration": 4.721
    },
    {
      "text": "similarity woman and man and I do the",
      "start": 1403.799,
      "duration": 4.76
    },
    {
      "text": "same for all these other words so if if",
      "start": 1405.76,
      "duration": 4.519
    },
    {
      "text": "you look at the answers you'll see that",
      "start": 1408.559,
      "duration": 4.641
    },
    {
      "text": "for the first five the similarity score",
      "start": 1410.279,
      "duration": 5.361
    },
    {
      "text": "is pretty high because woman man king",
      "start": 1413.2,
      "duration": 4.88
    },
    {
      "text": "queen uncle aunt boy girl nephew niece",
      "start": 1415.64,
      "duration": 4.72
    },
    {
      "text": "are closer to each other awesome right",
      "start": 1418.08,
      "duration": 4.199
    },
    {
      "text": "but if you look at the last two words",
      "start": 1420.36,
      "duration": 4.199
    },
    {
      "text": "paper and water they are not closer to",
      "start": 1422.279,
      "duration": 3.961
    },
    {
      "text": "each other they are not related at all",
      "start": 1424.559,
      "duration": 3.321
    },
    {
      "text": "and our vectors are capturing this",
      "start": 1426.24,
      "duration": 3.52
    },
    {
      "text": "meaning let's say there are there is a",
      "start": 1427.88,
      "duration": 3.56
    },
    {
      "text": "vector for paper somewhere there's a",
      "start": 1429.76,
      "duration": 3.799
    },
    {
      "text": "vector for water somewhere these vectors",
      "start": 1431.44,
      "duration": 4.0
    },
    {
      "text": "are so far apart that they are not",
      "start": 1433.559,
      "duration": 4.161
    },
    {
      "text": "related to each other and that's why the",
      "start": 1435.44,
      "duration": 4.52
    },
    {
      "text": "similarities score is very",
      "start": 1437.72,
      "duration": 4.88
    },
    {
      "text": "low in fact if you see woman and man",
      "start": 1439.96,
      "duration": 4.719
    },
    {
      "text": "king and queen uncle and Aunt boy and",
      "start": 1442.6,
      "duration": 3.92
    },
    {
      "text": "girl nephew and niece these vectors are",
      "start": 1444.679,
      "duration": 3.841
    },
    {
      "text": "closer to each other because they",
      "start": 1446.52,
      "duration": 3.039
    },
    {
      "text": "capture the",
      "start": 1448.52,
      "duration": 3.32
    },
    {
      "text": "meaning but paper and water are not",
      "start": 1449.559,
      "duration": 3.841
    },
    {
      "text": "close to each other at all and that's",
      "start": 1451.84,
      "duration": 3.56
    },
    {
      "text": "why the similarity score between these",
      "start": 1453.4,
      "duration": 3.32
    },
    {
      "text": "two vectors is",
      "start": 1455.4,
      "duration": 4.12
    },
    {
      "text": "low we can also do some cool things like",
      "start": 1456.72,
      "duration": 4.559
    },
    {
      "text": "we can find Words which are similar to a",
      "start": 1459.52,
      "duration": 4.6
    },
    {
      "text": "given word so if you look at Tower and",
      "start": 1461.279,
      "duration": 4.64
    },
    {
      "text": "then look at the vectors which are most",
      "start": 1464.12,
      "duration": 4.439
    },
    {
      "text": "similar to Tower so the answers are",
      "start": 1465.919,
      "duration": 6.401
    },
    {
      "text": "scraper Tower Spire uh",
      "start": 1468.559,
      "duration": 6.48
    },
    {
      "text": "Etc we can also see some other things",
      "start": 1472.32,
      "duration": 4.76
    },
    {
      "text": "like similarities between man woman",
      "start": 1475.039,
      "duration": 5.321
    },
    {
      "text": "semiconductor earthor nephew n Etc and",
      "start": 1477.08,
      "duration": 5.4
    },
    {
      "text": "here we can see that the magnitude of",
      "start": 1480.36,
      "duration": 3.72
    },
    {
      "text": "the difference between the man and woman",
      "start": 1482.48,
      "duration": 3.76
    },
    {
      "text": "so this is a vector difference and if",
      "start": 1484.08,
      "duration": 4.88
    },
    {
      "text": "you see np. lin. Norm so this is finding",
      "start": 1486.24,
      "duration": 5.52
    },
    {
      "text": "the norm of the difference in difference",
      "start": 1488.96,
      "duration": 5.0
    },
    {
      "text": "in these two vectors so you'll see that",
      "start": 1491.76,
      "duration": 3.84
    },
    {
      "text": "the magnitude of the difference between",
      "start": 1493.96,
      "duration": 4.48
    },
    {
      "text": "the man and woman is 1.73",
      "start": 1495.6,
      "duration": 4.439
    },
    {
      "text": "the magnitude of the vector difference",
      "start": 1498.44,
      "duration": 4.4
    },
    {
      "text": "between nephew and N is 1.96 but the",
      "start": 1500.039,
      "duration": 4.401
    },
    {
      "text": "magnitude of the vector difference",
      "start": 1502.84,
      "duration": 4.24
    },
    {
      "text": "between semiconductor and earthor is",
      "start": 1504.44,
      "duration": 5.68
    },
    {
      "text": "5.67 this is another indication that the",
      "start": 1507.08,
      "duration": 5.36
    },
    {
      "text": "vectors actually encode some meaning and",
      "start": 1510.12,
      "duration": 3.84
    },
    {
      "text": "if you find the magnitude of the",
      "start": 1512.44,
      "duration": 3.28
    },
    {
      "text": "difference between the vectors it's an",
      "start": 1513.96,
      "duration": 4.04
    },
    {
      "text": "indication of how closer in meaning the",
      "start": 1515.72,
      "duration": 4.959
    },
    {
      "text": "words are isn't that amazing let me",
      "start": 1518.0,
      "duration": 4.64
    },
    {
      "text": "repeat that again if you take two",
      "start": 1520.679,
      "duration": 4.201
    },
    {
      "text": "vectors and if you find the magnitude of",
      "start": 1522.64,
      "duration": 4.24
    },
    {
      "text": "the difference between the vectors",
      "start": 1524.88,
      "duration": 4.44
    },
    {
      "text": "that's an indication of how how close or",
      "start": 1526.88,
      "duration": 5.6
    },
    {
      "text": "how far the words are in their",
      "start": 1529.32,
      "duration": 5.839
    },
    {
      "text": "meaning so when you do Vector embedding",
      "start": 1532.48,
      "duration": 5.16
    },
    {
      "text": "or when you do token embedding the",
      "start": 1535.159,
      "duration": 4.281
    },
    {
      "text": "beautiful thing is that you actually",
      "start": 1537.64,
      "duration": 4.44
    },
    {
      "text": "retain the information or retain the",
      "start": 1539.44,
      "duration": 6.64
    },
    {
      "text": "meaning uh of words and then you feed",
      "start": 1542.08,
      "duration": 6.719
    },
    {
      "text": "these embeddings into the large language",
      "start": 1546.08,
      "duration": 5.28
    },
    {
      "text": "model and that makes a huge amount of",
      "start": 1548.799,
      "duration": 3.841
    },
    {
      "text": "difference instead of let's say just",
      "start": 1551.36,
      "duration": 5.319
    },
    {
      "text": "feeding word one hot encodings awesome I",
      "start": 1552.64,
      "duration": 6.2
    },
    {
      "text": "hope everyone is with me until now now I",
      "start": 1556.679,
      "duration": 3.921
    },
    {
      "text": "could have directly started with step",
      "start": 1558.84,
      "duration": 3.959
    },
    {
      "text": "number three but I wanted to show you",
      "start": 1560.6,
      "duration": 4.199
    },
    {
      "text": "this small Hands-On demo so that you get",
      "start": 1562.799,
      "duration": 4.401
    },
    {
      "text": "an intuitive feel that if Vector",
      "start": 1564.799,
      "duration": 5.041
    },
    {
      "text": "embeddings are trained nicely like they",
      "start": 1567.2,
      "duration": 5.52
    },
    {
      "text": "done in this word twek Google News model",
      "start": 1569.84,
      "duration": 6.0
    },
    {
      "text": "we can actually encode meanings in these",
      "start": 1572.72,
      "duration": 6.04
    },
    {
      "text": "vectors awesome so I hope in point",
      "start": 1575.84,
      "duration": 4.6
    },
    {
      "text": "number one and two I have been",
      "start": 1578.76,
      "duration": 3.84
    },
    {
      "text": "successful in making you understand what",
      "start": 1580.44,
      "duration": 4.68
    },
    {
      "text": "is the need for token embeddings and",
      "start": 1582.6,
      "duration": 4.24
    },
    {
      "text": "that if token embeddings are created",
      "start": 1585.12,
      "duration": 5.24
    },
    {
      "text": "successfully they can indeed encode some",
      "start": 1586.84,
      "duration": 6.48
    },
    {
      "text": "meaning great now let's come to the",
      "start": 1590.36,
      "duration": 5.559
    },
    {
      "text": "third point which is how are token",
      "start": 1593.32,
      "duration": 5.68
    },
    {
      "text": "embeddings created for large language",
      "start": 1595.919,
      "duration": 5.681
    },
    {
      "text": "models so the way this is done is that",
      "start": 1599.0,
      "duration": 4.919
    },
    {
      "text": "we start with the vocabulary we start",
      "start": 1601.6,
      "duration": 4.199
    },
    {
      "text": "with a vocabulary for large language",
      "start": 1603.919,
      "duration": 5.041
    },
    {
      "text": "models and then we have tokens in that",
      "start": 1605.799,
      "duration": 5.921
    },
    {
      "text": "vocabulary and then we have token IDs so",
      "start": 1608.96,
      "duration": 4.88
    },
    {
      "text": "let me show you so this is the",
      "start": 1611.72,
      "duration": 4.92
    },
    {
      "text": "vocabulary so the first step what is",
      "start": 1613.84,
      "duration": 4.839
    },
    {
      "text": "done is that we take the Vo vocabulary",
      "start": 1616.64,
      "duration": 5.519
    },
    {
      "text": "and we have token IDs every token ID is",
      "start": 1618.679,
      "duration": 6.561
    },
    {
      "text": "converted into embedding vectors so if",
      "start": 1622.159,
      "duration": 5.481
    },
    {
      "text": "you have uh if you see this is the",
      "start": 1625.24,
      "duration": 4.12
    },
    {
      "text": "output this is also called as the",
      "start": 1627.64,
      "duration": 4.12
    },
    {
      "text": "embedding M embedding weight Matrix",
      "start": 1629.36,
      "duration": 5.199
    },
    {
      "text": "don't worry about this right now",
      "start": 1631.76,
      "duration": 5.44
    },
    {
      "text": "uh there are two things you need before",
      "start": 1634.559,
      "duration": 5.36
    },
    {
      "text": "you construct this Matrix you need first",
      "start": 1637.2,
      "duration": 4.28
    },
    {
      "text": "of all the vocabulary",
      "start": 1639.919,
      "duration": 4.041
    },
    {
      "text": "size and second thing you need is the",
      "start": 1641.48,
      "duration": 5.16
    },
    {
      "text": "vector Dimension so you need how many",
      "start": 1643.96,
      "duration": 5.28
    },
    {
      "text": "Dimension vector uh is the embedding",
      "start": 1646.64,
      "duration": 5.8
    },
    {
      "text": "going to be so for example let me",
      "start": 1649.24,
      "duration": 7.36
    },
    {
      "text": "actually ask let me go to chat",
      "start": 1652.44,
      "duration": 8.0
    },
    {
      "text": "GPT and let me ask chat",
      "start": 1656.6,
      "duration": 7.52
    },
    {
      "text": "GPT what was the vector",
      "start": 1660.44,
      "duration": 6.68
    },
    {
      "text": "embedding",
      "start": 1664.12,
      "duration": 3.0
    },
    {
      "text": "dimension for training",
      "start": 1667.76,
      "duration": 6.36
    },
    {
      "text": "gpt2",
      "start": 1671.76,
      "duration": 6.759
    },
    {
      "text": "also what was",
      "start": 1674.12,
      "duration": 4.399
    },
    {
      "text": "the vocabulary",
      "start": 1678.64,
      "duration": 2.96
    },
    {
      "text": "size vocabulary size means how many",
      "start": 1681.679,
      "duration": 4.521
    },
    {
      "text": "tokens were there and how many token IDs",
      "start": 1684.279,
      "duration": 5.0
    },
    {
      "text": "was there okay so remember this the",
      "start": 1686.2,
      "duration": 6.12
    },
    {
      "text": "vector embedding dimension for gpt2 was",
      "start": 1689.279,
      "duration": 5.721
    },
    {
      "text": "768 and uh for the smallest model and",
      "start": 1692.32,
      "duration": 5.359
    },
    {
      "text": "for the largest model it was 160 so",
      "start": 1695.0,
      "duration": 5.519
    },
    {
      "text": "let's stick with 768 for now and the",
      "start": 1697.679,
      "duration": 5.36
    },
    {
      "text": "vocabulary size for gpt2 was",
      "start": 1700.519,
      "duration": 5.361
    },
    {
      "text": "50257 so I'm going to go here right now",
      "start": 1703.039,
      "duration": 4.281
    },
    {
      "text": "and let's look at how the embedding",
      "start": 1705.88,
      "duration": 3.639
    },
    {
      "text": "Matrix was then constructed so the",
      "start": 1707.32,
      "duration": 4.64
    },
    {
      "text": "vocabulary size was 50257 right which",
      "start": 1709.519,
      "duration": 5.04
    },
    {
      "text": "means gpt2 had these many tokens those",
      "start": 1711.96,
      "duration": 5.4
    },
    {
      "text": "were subwords uh made through bite pair",
      "start": 1714.559,
      "duration": 4.24
    },
    {
      "text": "encoding so there are",
      "start": 1717.36,
      "duration": 5.08
    },
    {
      "text": "50257 tokens and token IDs so token IDs",
      "start": 1718.799,
      "duration": 7.521
    },
    {
      "text": "went from 01 2 3 up to",
      "start": 1722.44,
      "duration": 3.88
    },
    {
      "text": "50257",
      "start": 1726.799,
      "duration": 3.0
    },
    {
      "text": "50257 awesome and then what we are going",
      "start": 1730.919,
      "duration": 5.081
    },
    {
      "text": "to do is that for each of these token",
      "start": 1733.76,
      "duration": 4.96
    },
    {
      "text": "IDs each of these token ID which",
      "start": 1736.0,
      "duration": 4.399
    },
    {
      "text": "corresponds to one token there would be",
      "start": 1738.72,
      "duration": 3.959
    },
    {
      "text": "a vector and the vector Dimension was",
      "start": 1740.399,
      "duration": 4.4
    },
    {
      "text": "768 in this",
      "start": 1742.679,
      "duration": 5.801
    },
    {
      "text": "case so for the zero token ID there will",
      "start": 1744.799,
      "duration": 7.321
    },
    {
      "text": "be 768 a vector of 768 dimensions for",
      "start": 1748.48,
      "duration": 5.6
    },
    {
      "text": "the token ID of one there will be a",
      "start": 1752.12,
      "duration": 4.88
    },
    {
      "text": "vector of 768 Dimensions similarly for",
      "start": 1754.08,
      "duration": 5.68
    },
    {
      "text": "the token ID of 50257 there will be a",
      "start": 1757.0,
      "duration": 5.919
    },
    {
      "text": "vector of 768 Dimensions so for every",
      "start": 1759.76,
      "duration": 5.799
    },
    {
      "text": "token ID there will be a vector of 768",
      "start": 1762.919,
      "duration": 5.161
    },
    {
      "text": "Dimensions so think of the size of this",
      "start": 1765.559,
      "duration": 4.881
    },
    {
      "text": "embedding layer weight Matrix right for",
      "start": 1768.08,
      "duration": 4.0
    },
    {
      "text": "let's say if you look at the first token",
      "start": 1770.44,
      "duration": 5.079
    },
    {
      "text": "ID there will be 768 weights because",
      "start": 1772.08,
      "duration": 5.56
    },
    {
      "text": "when you construct the vector it has 768",
      "start": 1775.519,
      "duration": 4.921
    },
    {
      "text": "dimensions for the second token ID which",
      "start": 1777.64,
      "duration": 5.759
    },
    {
      "text": "is token ID 1 it also has 768 weights",
      "start": 1780.44,
      "duration": 5.04
    },
    {
      "text": "similarly if you reach to the end",
      "start": 1783.399,
      "duration": 4.961
    },
    {
      "text": "50257 this token ID and the token",
      "start": 1785.48,
      "duration": 5.84
    },
    {
      "text": "corresponding with it has 768 weights so",
      "start": 1788.36,
      "duration": 5.559
    },
    {
      "text": "the number of tokens in this token in",
      "start": 1791.32,
      "duration": 4.64
    },
    {
      "text": "this embedding Matrix is",
      "start": 1793.919,
      "duration": 6.12
    },
    {
      "text": "50257 into 76",
      "start": 1795.96,
      "duration": 4.079
    },
    {
      "text": "so these are the weights okay and this",
      "start": 1800.399,
      "duration": 6.481
    },
    {
      "text": "is called as the embedding layer weight",
      "start": 1805.0,
      "duration": 4.72
    },
    {
      "text": "Matrix this is extremely important so",
      "start": 1806.88,
      "duration": 5.36
    },
    {
      "text": "once you get the token IDs so if I go to",
      "start": 1809.72,
      "duration": 4.4
    },
    {
      "text": "this flow map right now once you get the",
      "start": 1812.24,
      "duration": 4.679
    },
    {
      "text": "token IDs you convert these token IDs",
      "start": 1814.12,
      "duration": 5.6
    },
    {
      "text": "into an embedding layer weight",
      "start": 1816.919,
      "duration": 5.24
    },
    {
      "text": "Matrix and initially when this weight",
      "start": 1819.72,
      "duration": 5.12
    },
    {
      "text": "Matrix is initialized we do not know how",
      "start": 1822.159,
      "duration": 4.961
    },
    {
      "text": "the vectors are right what I showed you",
      "start": 1824.84,
      "duration": 3.88
    },
    {
      "text": "over here the word model it's a",
      "start": 1827.12,
      "duration": 4.08
    },
    {
      "text": "pre-trained model but now I'm going to",
      "start": 1828.72,
      "duration": 5.6
    },
    {
      "text": "tell you how is it actually trained so",
      "start": 1831.2,
      "duration": 5.16
    },
    {
      "text": "before so you just now know the size of",
      "start": 1834.32,
      "duration": 3.8
    },
    {
      "text": "this Matrix that if you are the person",
      "start": 1836.36,
      "duration": 4.72
    },
    {
      "text": "training gpt2 you know that okay I have",
      "start": 1838.12,
      "duration": 4.84
    },
    {
      "text": "to ultimately create an embedding layer",
      "start": 1841.08,
      "duration": 3.839
    },
    {
      "text": "weight Matrix which has",
      "start": 1842.96,
      "duration": 5.64
    },
    {
      "text": "50257 rows and which has 768 columns but",
      "start": 1844.919,
      "duration": 5.401
    },
    {
      "text": "you you don't know what each weight",
      "start": 1848.6,
      "duration": 4.4
    },
    {
      "text": "value will be so then what do you do",
      "start": 1850.32,
      "duration": 5.359
    },
    {
      "text": "what you do is that you initialize the",
      "start": 1853.0,
      "duration": 4.6
    },
    {
      "text": "embedding weights with random values",
      "start": 1855.679,
      "duration": 4.72
    },
    {
      "text": "that's the First Step so all of these",
      "start": 1857.6,
      "duration": 5.4
    },
    {
      "text": "50257 into 768 values will be",
      "start": 1860.399,
      "duration": 5.361
    },
    {
      "text": "initialized randomly step number one",
      "start": 1863.0,
      "duration": 4.6
    },
    {
      "text": "this initialization serves as the",
      "start": 1865.76,
      "duration": 4.639
    },
    {
      "text": "starting point for the llm learning",
      "start": 1867.6,
      "duration": 5.959
    },
    {
      "text": "process then what do you do these",
      "start": 1870.399,
      "duration": 5.561
    },
    {
      "text": "weights are then optimized as part of",
      "start": 1873.559,
      "duration": 4.801
    },
    {
      "text": "the llm training process this is",
      "start": 1875.96,
      "duration": 5.12
    },
    {
      "text": "extremely important when gpt2 was",
      "start": 1878.36,
      "duration": 5.159
    },
    {
      "text": "trained these values were not known",
      "start": 1881.08,
      "duration": 4.559
    },
    {
      "text": "before what were the ideal weight",
      "start": 1883.519,
      "duration": 4.4
    },
    {
      "text": "values a training process was",
      "start": 1885.639,
      "duration": 4.16
    },
    {
      "text": "implemented where we had these many",
      "start": 1887.919,
      "duration": 5.201
    },
    {
      "text": "parameters 5257 by 768 parameters were",
      "start": 1889.799,
      "duration": 5.12
    },
    {
      "text": "there and each of these weight",
      "start": 1893.12,
      "duration": 3.6
    },
    {
      "text": "parameters were optimized during the",
      "start": 1894.919,
      "duration": 4.401
    },
    {
      "text": "training process and that is how vector",
      "start": 1896.72,
      "duration": 4.4
    },
    {
      "text": "embeddings or token embeddings were",
      "start": 1899.32,
      "duration": 4.359
    },
    {
      "text": "created how was the optimization process",
      "start": 1901.12,
      "duration": 5.039
    },
    {
      "text": "done we had the training data and based",
      "start": 1903.679,
      "duration": 4.24
    },
    {
      "text": "on the data we knew which words were",
      "start": 1906.159,
      "duration": 3.36
    },
    {
      "text": "closer to each other which words were",
      "start": 1907.919,
      "duration": 3.72
    },
    {
      "text": "farther apart from each other for",
      "start": 1909.519,
      "duration": 4.4
    },
    {
      "text": "example when we looked at this word to",
      "start": 1911.639,
      "duration": 4.64
    },
    {
      "text": "the training data was Google news right",
      "start": 1913.919,
      "duration": 4.64
    },
    {
      "text": "and we had so the Google news of course",
      "start": 1916.279,
      "duration": 4.561
    },
    {
      "text": "if we had 300 billion words we have the",
      "start": 1918.559,
      "duration": 3.921
    },
    {
      "text": "information that king and queen are",
      "start": 1920.84,
      "duration": 4.799
    },
    {
      "text": "similar man and woman are similar so",
      "start": 1922.48,
      "duration": 6.36
    },
    {
      "text": "that training data is used as underlying",
      "start": 1925.639,
      "duration": 6.52
    },
    {
      "text": "information to modify each of these",
      "start": 1928.84,
      "duration": 5.679
    },
    {
      "text": "parameters so if you know about neural",
      "start": 1932.159,
      "duration": 4.64
    },
    {
      "text": "network training similarly back",
      "start": 1934.519,
      "duration": 3.88
    },
    {
      "text": "propagation is implemented here to",
      "start": 1936.799,
      "duration": 3.48
    },
    {
      "text": "optimize all of these weights of the",
      "start": 1938.399,
      "duration": 4.52
    },
    {
      "text": "embedding layer weight Matrix and that",
      "start": 1940.279,
      "duration": 5.88
    },
    {
      "text": "is how token IDs are converted into",
      "start": 1942.919,
      "duration": 5.321
    },
    {
      "text": "vector embeddings",
      "start": 1946.159,
      "duration": 4.48
    },
    {
      "text": "so if you think about it at the heart of",
      "start": 1948.24,
      "duration": 4.319
    },
    {
      "text": "it large language models are just giant",
      "start": 1950.639,
      "duration": 4.441
    },
    {
      "text": "neural networks right and one part of",
      "start": 1952.559,
      "duration": 4.281
    },
    {
      "text": "this giant neural networks is training",
      "start": 1955.08,
      "duration": 4.36
    },
    {
      "text": "and embedding layer weight Matrix so if",
      "start": 1956.84,
      "duration": 4.6
    },
    {
      "text": "you look at this",
      "start": 1959.44,
      "duration": 5.079
    },
    {
      "text": "uh uh this graphic here so token",
      "start": 1961.44,
      "duration": 5.32
    },
    {
      "text": "embeddings are fed as an input to to the",
      "start": 1964.519,
      "duration": 4.16
    },
    {
      "text": "training right that's what this graphic",
      "start": 1966.76,
      "duration": 4.56
    },
    {
      "text": "shows but during the training of the GPT",
      "start": 1968.679,
      "duration": 4.401
    },
    {
      "text": "while it's training to predict the next",
      "start": 1971.32,
      "duration": 4.68
    },
    {
      "text": "word we also train that embedding itself",
      "start": 1973.08,
      "duration": 5.12
    },
    {
      "text": "so the embedding neural n network is",
      "start": 1976.0,
      "duration": 4.039
    },
    {
      "text": "trained and then we also train the",
      "start": 1978.2,
      "duration": 3.4
    },
    {
      "text": "prediction for the next World so there",
      "start": 1980.039,
      "duration": 3.041
    },
    {
      "text": "are two trainings actually which are",
      "start": 1981.6,
      "duration": 2.88
    },
    {
      "text": "kind of going on",
      "start": 1983.08,
      "duration": 4.36
    },
    {
      "text": "here for now all you need to remember is",
      "start": 1984.48,
      "duration": 6.0
    },
    {
      "text": "that uh words so we start with a",
      "start": 1987.44,
      "duration": 5.599
    },
    {
      "text": "vocabulary such as for gpt2 we start",
      "start": 1990.48,
      "duration": 5.76
    },
    {
      "text": "with this vocabulary of 50257 tokens and",
      "start": 1993.039,
      "duration": 5.0
    },
    {
      "text": "then we have to decide that okay when I",
      "start": 1996.24,
      "duration": 3.36
    },
    {
      "text": "do the vector embedding what the",
      "start": 1998.039,
      "duration": 4.201
    },
    {
      "text": "dimension of the vector I want and then",
      "start": 1999.6,
      "duration": 5.72
    },
    {
      "text": "that's 768 then that decides the size of",
      "start": 2002.24,
      "duration": 5.52
    },
    {
      "text": "your embedding layer Matrix so we have 5",
      "start": 2005.32,
      "duration": 6.0
    },
    {
      "text": "0257 rows and we have 768 column for",
      "start": 2007.76,
      "duration": 5.48
    },
    {
      "text": "each token ID we are going to have a",
      "start": 2011.32,
      "duration": 3.8
    },
    {
      "text": "vector with 768",
      "start": 2013.24,
      "duration": 4.319
    },
    {
      "text": "values how are these values decided",
      "start": 2015.12,
      "duration": 3.559
    },
    {
      "text": "through training through back",
      "start": 2017.559,
      "duration": 2.96
    },
    {
      "text": "propagation we start out with",
      "start": 2018.679,
      "duration": 3.321
    },
    {
      "text": "initializing these values of the",
      "start": 2020.519,
      "duration": 3.681
    },
    {
      "text": "embedding layer in a random Manner and",
      "start": 2022.0,
      "duration": 4.039
    },
    {
      "text": "then all of these values are optimized",
      "start": 2024.2,
      "duration": 4.68
    },
    {
      "text": "during the training",
      "start": 2026.039,
      "duration": 4.961
    },
    {
      "text": "process uh I hope you have followed",
      "start": 2028.88,
      "duration": 3.919
    },
    {
      "text": "until this point because now I'm going",
      "start": 2031.0,
      "duration": 3.84
    },
    {
      "text": "to take you through code and we are",
      "start": 2032.799,
      "duration": 3.88
    },
    {
      "text": "actually going to learn a bit about",
      "start": 2034.84,
      "duration": 3.52
    },
    {
      "text": "these token embeddings",
      "start": 2036.679,
      "duration": 3.72
    },
    {
      "text": "and we are going to learn how to",
      "start": 2038.36,
      "duration": 4.88
    },
    {
      "text": "essentially create um this embedding",
      "start": 2040.399,
      "duration": 5.441
    },
    {
      "text": "layer Matrix which we just saw over here",
      "start": 2043.24,
      "duration": 4.919
    },
    {
      "text": "so please keep this image in mind and",
      "start": 2045.84,
      "duration": 4.2
    },
    {
      "text": "remember that there are two Dimensions",
      "start": 2048.159,
      "duration": 4.2
    },
    {
      "text": "which are important the vector Dimension",
      "start": 2050.04,
      "duration": 4.4
    },
    {
      "text": "which is the size of the each vector and",
      "start": 2052.359,
      "duration": 3.361
    },
    {
      "text": "the vocabulary",
      "start": 2054.44,
      "duration": 5.04
    },
    {
      "text": "size okay so let us illustrate how the",
      "start": 2055.72,
      "duration": 5.639
    },
    {
      "text": "token ID to the embedding Vector",
      "start": 2059.48,
      "duration": 4.639
    },
    {
      "text": "conversion works with a Hands-On example",
      "start": 2061.359,
      "duration": 4.601
    },
    {
      "text": "so let's say we have the four input",
      "start": 2064.119,
      "duration": 4.24
    },
    {
      "text": "tokens which are input number or ID",
      "start": 2065.96,
      "duration": 3.32
    },
    {
      "text": "number",
      "start": 2068.359,
      "duration": 4.04
    },
    {
      "text": "2351 remember these are token IDs so",
      "start": 2069.28,
      "duration": 7.119
    },
    {
      "text": "every token ID is associated with a word",
      "start": 2072.399,
      "duration": 7.68
    },
    {
      "text": "uh to give you uh like a concrete feel",
      "start": 2076.399,
      "duration": 8.161
    },
    {
      "text": "for this let's say the example",
      "start": 2080.079,
      "duration": 4.481
    },
    {
      "text": "is uh so I'm going to look at 0 1 2 3 4",
      "start": 2084.879,
      "duration": 6.28
    },
    {
      "text": "5 so I need six words",
      "start": 2088.919,
      "duration": 5.881
    },
    {
      "text": "actually um so here it's going to be",
      "start": 2091.159,
      "duration": 8.481
    },
    {
      "text": "let's say my sentence is quick",
      "start": 2094.8,
      "duration": 7.16
    },
    {
      "text": "uh my sentence is",
      "start": 2099.64,
      "duration": 5.32
    },
    {
      "text": "quick",
      "start": 2101.96,
      "duration": 3.0
    },
    {
      "text": "fox",
      "start": 2105.4,
      "duration": 4.76
    },
    {
      "text": "is",
      "start": 2107.64,
      "duration": 6.52
    },
    {
      "text": "in the",
      "start": 2110.16,
      "duration": 7.12
    },
    {
      "text": "house let's say this is my sentence so",
      "start": 2114.16,
      "duration": 4.64
    },
    {
      "text": "what I will do is first convert this",
      "start": 2117.28,
      "duration": 3.64
    },
    {
      "text": "into tokens so let's say quick and I'm",
      "start": 2118.8,
      "duration": 3.92
    },
    {
      "text": "just showing word tokens word based",
      "start": 2120.92,
      "duration": 4.36
    },
    {
      "text": "tokenizer for Simplicity let's say quick",
      "start": 2122.72,
      "duration": 5.119
    },
    {
      "text": "is one token Fox is one token is is is",
      "start": 2125.28,
      "duration": 5.64
    },
    {
      "text": "one token in is one token the is one",
      "start": 2127.839,
      "duration": 5.841
    },
    {
      "text": "token and house is the next token then",
      "start": 2130.92,
      "duration": 5.36
    },
    {
      "text": "we arrange these tokens in ascending",
      "start": 2133.68,
      "duration": 5.439
    },
    {
      "text": "order and then assign them token",
      "start": 2136.28,
      "duration": 4.4
    },
    {
      "text": "IDs",
      "start": 2139.119,
      "duration": 4.161
    },
    {
      "text": "so house will probably come first with a",
      "start": 2140.68,
      "duration": 6.24
    },
    {
      "text": "token ID zero then uh no I think Fox",
      "start": 2143.28,
      "duration": 6.039
    },
    {
      "text": "would come first with a token ID zero",
      "start": 2146.92,
      "duration": 4.28
    },
    {
      "text": "house would come",
      "start": 2149.319,
      "duration": 4.561
    },
    {
      "text": "second uh with the token ID one I hope",
      "start": 2151.2,
      "duration": 5.76
    },
    {
      "text": "I'm not Mak making any mistake here then",
      "start": 2153.88,
      "duration": 5.76
    },
    {
      "text": "in will come two is will come three",
      "start": 2156.96,
      "duration": 4.52
    },
    {
      "text": "quick will come four and then the will",
      "start": 2159.64,
      "duration": 4.88
    },
    {
      "text": "come five right and now what I want to",
      "start": 2161.48,
      "duration": 6.16
    },
    {
      "text": "do is I want to encode uh or I want to",
      "start": 2164.52,
      "duration": 7.2
    },
    {
      "text": "rather convert token ID 235 and 1 into",
      "start": 2167.64,
      "duration": 6.6
    },
    {
      "text": "embeddings or into Vector embeddings so",
      "start": 2171.72,
      "duration": 5.56
    },
    {
      "text": "what is ID number two is this word in 2",
      "start": 2174.24,
      "duration": 7.2
    },
    {
      "text": "three is five and one so I want to",
      "start": 2177.28,
      "duration": 7.559
    },
    {
      "text": "convert these words in is the and house",
      "start": 2181.44,
      "duration": 5.12
    },
    {
      "text": "I want to convert them into embedding",
      "start": 2184.839,
      "duration": 5.681
    },
    {
      "text": "vectors 2 3 5 and 1 right so 2 3 5 and 1",
      "start": 2186.56,
      "duration": 6.96
    },
    {
      "text": "correct so then my input IDs are tor.",
      "start": 2190.52,
      "duration": 5.4
    },
    {
      "text": "tensor 2351 remember we are using",
      "start": 2193.52,
      "duration": 4.839
    },
    {
      "text": "tensors here because ultimately we are",
      "start": 2195.92,
      "duration": 3.919
    },
    {
      "text": "going to use back propagation to",
      "start": 2198.359,
      "duration": 4.24
    },
    {
      "text": "optimize the embedding layer weights so",
      "start": 2199.839,
      "duration": 4.641
    },
    {
      "text": "it's much better to represent everything",
      "start": 2202.599,
      "duration": 2.841
    },
    {
      "text": "as",
      "start": 2204.48,
      "duration": 4.119
    },
    {
      "text": "tensors great so these are my input IDs",
      "start": 2205.44,
      "duration": 5.44
    },
    {
      "text": "and now for the sake of Simplicity we",
      "start": 2208.599,
      "duration": 4.76
    },
    {
      "text": "are going to use only small vocabulary",
      "start": 2210.88,
      "duration": 6.88
    },
    {
      "text": "of six words remember gpt2 had 50257",
      "start": 2213.359,
      "duration": 6.841
    },
    {
      "text": "tokens in the vocabulary right now just",
      "start": 2217.76,
      "duration": 4.12
    },
    {
      "text": "for Simplicity we are just going to use",
      "start": 2220.2,
      "duration": 4.44
    },
    {
      "text": "a small vocabulary of only six tokens",
      "start": 2221.88,
      "duration": 5.68
    },
    {
      "text": "instead of the 5257 words in the BP",
      "start": 2224.64,
      "duration": 4.959
    },
    {
      "text": "tokenizer and let's say we want to",
      "start": 2227.56,
      "duration": 5.039
    },
    {
      "text": "create embeddings of size three uh so",
      "start": 2229.599,
      "duration": 6.361
    },
    {
      "text": "here see GPT to 768",
      "start": 2232.599,
      "duration": 6.161
    },
    {
      "text": "right so here I told you two two",
      "start": 2235.96,
      "duration": 5.68
    },
    {
      "text": "Dimensions were important the size of",
      "start": 2238.76,
      "duration": 5.04
    },
    {
      "text": "the vocabulary which now we are assuming",
      "start": 2241.64,
      "duration": 5.24
    },
    {
      "text": "six and the vector Dimension so in this",
      "start": 2243.8,
      "duration": 4.76
    },
    {
      "text": "uh code file I'm assuming the vector",
      "start": 2246.88,
      "duration": 3.92
    },
    {
      "text": "Dimension three so what I'm going to do",
      "start": 2248.56,
      "duration": 4.24
    },
    {
      "text": "here is that for each of the words in my",
      "start": 2250.8,
      "duration": 4.4
    },
    {
      "text": "vocabulary which are these words so for",
      "start": 2252.8,
      "duration": 4.2
    },
    {
      "text": "now I'm starting with these six words",
      "start": 2255.2,
      "duration": 3.399
    },
    {
      "text": "for each of these six words in my",
      "start": 2257.0,
      "duration": 4.16
    },
    {
      "text": "vocabulary I will have a vector and that",
      "start": 2258.599,
      "duration": 5.321
    },
    {
      "text": "Vector will have three dimensions okay",
      "start": 2261.16,
      "duration": 4.28
    },
    {
      "text": "this is how I'm going to construct the",
      "start": 2263.92,
      "duration": 2.52
    },
    {
      "text": "vector",
      "start": 2265.44,
      "duration": 3.48
    },
    {
      "text": "embedding so the vocabulary size will be",
      "start": 2266.44,
      "duration": 5.639
    },
    {
      "text": "six what are the six words quick fox is",
      "start": 2268.92,
      "duration": 5.84
    },
    {
      "text": "in the house then output Dimension will",
      "start": 2272.079,
      "duration": 5.161
    },
    {
      "text": "be three which means that every token of",
      "start": 2274.76,
      "duration": 5.04
    },
    {
      "text": "this vocabulary will be converted into a",
      "start": 2277.24,
      "duration": 4.92
    },
    {
      "text": "vector of three dimensions how that is",
      "start": 2279.8,
      "duration": 4.319
    },
    {
      "text": "done in practice is that we create an",
      "start": 2282.16,
      "duration": 5.8
    },
    {
      "text": "embedding layer and then we use tor. nn.",
      "start": 2284.119,
      "duration": 7.48
    },
    {
      "text": "embedding and uh then what we do is we",
      "start": 2287.96,
      "duration": 5.72
    },
    {
      "text": "we pass in two arguments the vocabulary",
      "start": 2291.599,
      "duration": 4.841
    },
    {
      "text": "size which means the number of words",
      "start": 2293.68,
      "duration": 4.399
    },
    {
      "text": "which need to be converted into",
      "start": 2296.44,
      "duration": 3.52
    },
    {
      "text": "embeddings and the output Dimension",
      "start": 2298.079,
      "duration": 4.841
    },
    {
      "text": "which is the dimension of each Vector",
      "start": 2299.96,
      "duration": 5.56
    },
    {
      "text": "embedding uh awesome right so this is",
      "start": 2302.92,
      "duration": 5.0
    },
    {
      "text": "how we use the tor. nn. embedding let me",
      "start": 2305.52,
      "duration": 5.16
    },
    {
      "text": "show you this in Python right now so if",
      "start": 2307.92,
      "duration": 4.84
    },
    {
      "text": "you look at the embedding",
      "start": 2310.68,
      "duration": 4.159
    },
    {
      "text": "documentation you will see that it's a",
      "start": 2312.76,
      "duration": 4.0
    },
    {
      "text": "simple lookup table that stores",
      "start": 2314.839,
      "duration": 4.601
    },
    {
      "text": "embedding of a fixed dictionary and size",
      "start": 2316.76,
      "duration": 4.2
    },
    {
      "text": "what this means is that we need to have",
      "start": 2319.44,
      "duration": 4.08
    },
    {
      "text": "a vocabulary and we need to give the",
      "start": 2320.96,
      "duration": 4.76
    },
    {
      "text": "size of the embedding Vector that's it",
      "start": 2323.52,
      "duration": 4.12
    },
    {
      "text": "and then it creates a dictionary I'll",
      "start": 2325.72,
      "duration": 4.879
    },
    {
      "text": "show you why is it called a lookup table",
      "start": 2327.64,
      "duration": 5.08
    },
    {
      "text": "and I'll tell you okay why does the word",
      "start": 2330.599,
      "duration": 4.201
    },
    {
      "text": "lookup table come into to the picture",
      "start": 2332.72,
      "duration": 4.24
    },
    {
      "text": "for now all you need to remember is that",
      "start": 2334.8,
      "duration": 3.88
    },
    {
      "text": "the way we initialize these Vector",
      "start": 2336.96,
      "duration": 4.84
    },
    {
      "text": "embeddings is tor. nn. embedding and",
      "start": 2338.68,
      "duration": 4.8
    },
    {
      "text": "this initialize the weights of the",
      "start": 2341.8,
      "duration": 4.039
    },
    {
      "text": "embedding Matrix in a random manner",
      "start": 2343.48,
      "duration": 4.68
    },
    {
      "text": "right so here I showed you that every",
      "start": 2345.839,
      "duration": 4.561
    },
    {
      "text": "weight in this vocab in this embedding",
      "start": 2348.16,
      "duration": 4.919
    },
    {
      "text": "Matrix is initialized randomly so let me",
      "start": 2350.4,
      "duration": 5.48
    },
    {
      "text": "again show this to you here now you see",
      "start": 2353.079,
      "duration": 4.601
    },
    {
      "text": "I have six IDs",
      "start": 2355.88,
      "duration": 11.64
    },
    {
      "text": "right so 0 1 2 3 4 and five and each of",
      "start": 2357.68,
      "duration": 11.52
    },
    {
      "text": "these will have a three dimensional",
      "start": 2367.52,
      "duration": 7.319
    },
    {
      "text": "Vector associated with it 1 2 3 1 2 3 1",
      "start": 2369.2,
      "duration": 12.08
    },
    {
      "text": "2 3 1 2 3 1 2 3 1 2 3 so I will have",
      "start": 2374.839,
      "duration": 8.52
    },
    {
      "text": "essentially six rows and I will have",
      "start": 2381.28,
      "duration": 4.16
    },
    {
      "text": "three columns this is my embedding layer",
      "start": 2383.359,
      "duration": 4.24
    },
    {
      "text": "weight Matrix and all of these values",
      "start": 2385.44,
      "duration": 4.879
    },
    {
      "text": "are initialized randomly how can you get",
      "start": 2387.599,
      "duration": 4.921
    },
    {
      "text": "this values you then just have to type",
      "start": 2390.319,
      "duration": 5.04
    },
    {
      "text": "embedding layer. weight What will what",
      "start": 2392.52,
      "duration": 4.44
    },
    {
      "text": "this will give you is it will give you",
      "start": 2395.359,
      "duration": 3.24
    },
    {
      "text": "all the weights which are initialized",
      "start": 2396.96,
      "duration": 4.24
    },
    {
      "text": "through the embedding layer so when you",
      "start": 2398.599,
      "duration": 4.72
    },
    {
      "text": "print this you'll get this and you'll",
      "start": 2401.2,
      "duration": 4.56
    },
    {
      "text": "see it's exactly the same size as what",
      "start": 2403.319,
      "duration": 4.161
    },
    {
      "text": "we had shown over here it has six rows",
      "start": 2405.76,
      "duration": 4.16
    },
    {
      "text": "and three columns here also you see we",
      "start": 2407.48,
      "duration": 4.76
    },
    {
      "text": "have six rows and three columns every",
      "start": 2409.92,
      "duration": 4.399
    },
    {
      "text": "row here corresponds to the vector",
      "start": 2412.24,
      "duration": 4.76
    },
    {
      "text": "associated with that token ID so this is",
      "start": 2414.319,
      "duration": 4.441
    },
    {
      "text": "the three-dimensional Vector with the",
      "start": 2417.0,
      "duration": 3.599
    },
    {
      "text": "zero token ID this is the",
      "start": 2418.76,
      "duration": 3.44
    },
    {
      "text": "threedimensional vector with the first",
      "start": 2420.599,
      "duration": 3.361
    },
    {
      "text": "token ID this is the threedimensional",
      "start": 2422.2,
      "duration": 5.8
    },
    {
      "text": "vector of the second token ID Etc",
      "start": 2423.96,
      "duration": 6.04
    },
    {
      "text": "uh so now you can see a tensor has been",
      "start": 2428.0,
      "duration": 3.48
    },
    {
      "text": "returned and these are the initial",
      "start": 2430.0,
      "duration": 3.16
    },
    {
      "text": "weights which need to be",
      "start": 2431.48,
      "duration": 4.0
    },
    {
      "text": "optimized so as we can see here the",
      "start": 2433.16,
      "duration": 4.159
    },
    {
      "text": "weight Matrix of the embedding layer",
      "start": 2435.48,
      "duration": 4.16
    },
    {
      "text": "consists of small random values",
      "start": 2437.319,
      "duration": 5.0
    },
    {
      "text": "initially and these are the values which",
      "start": 2439.64,
      "duration": 4.959
    },
    {
      "text": "are optimized during llm training as",
      "start": 2442.319,
      "duration": 4.441
    },
    {
      "text": "part of the llm optimization itself",
      "start": 2444.599,
      "duration": 4.201
    },
    {
      "text": "which we will see in further chapters so",
      "start": 2446.76,
      "duration": 3.68
    },
    {
      "text": "when the llm is optimized there are",
      "start": 2448.8,
      "duration": 3.279
    },
    {
      "text": "actually two broad level things which",
      "start": 2450.44,
      "duration": 3.72
    },
    {
      "text": "are optimized first is the embedding",
      "start": 2452.079,
      "duration": 4.961
    },
    {
      "text": "layer weights and second is actually the",
      "start": 2454.16,
      "duration": 4.08
    },
    {
      "text": "we",
      "start": 2457.04,
      "duration": 4.68
    },
    {
      "text": "uh which are needed later to also",
      "start": 2458.24,
      "duration": 6.2
    },
    {
      "text": "predict the next word I'll come to that",
      "start": 2461.72,
      "duration": 5.56
    },
    {
      "text": "in one of the upcoming chapters moreover",
      "start": 2464.44,
      "duration": 4.52
    },
    {
      "text": "we can see that the weight Matrix has",
      "start": 2467.28,
      "duration": 3.96
    },
    {
      "text": "six rows and three columns as we saw",
      "start": 2468.96,
      "duration": 5.0
    },
    {
      "text": "over here and there is one row for each",
      "start": 2471.24,
      "duration": 4.52
    },
    {
      "text": "of the six possible tokens in the",
      "start": 2473.96,
      "duration": 4.48
    },
    {
      "text": "vocabulary which we already discussed",
      "start": 2475.76,
      "duration": 5.52
    },
    {
      "text": "each row is essentially the vector",
      "start": 2478.44,
      "duration": 4.919
    },
    {
      "text": "embedding of each",
      "start": 2481.28,
      "duration": 6.16
    },
    {
      "text": "token or each token ID great",
      "start": 2483.359,
      "duration": 6.201
    },
    {
      "text": "now what we can do is that uh once this",
      "start": 2487.44,
      "duration": 4.72
    },
    {
      "text": "embedding layer has been created right",
      "start": 2489.56,
      "duration": 4.6
    },
    {
      "text": "uh what I want to show you is that how",
      "start": 2492.16,
      "duration": 5.36
    },
    {
      "text": "can we get the vectors for each ID and",
      "start": 2494.16,
      "duration": 5.8
    },
    {
      "text": "that's actually pretty simple because",
      "start": 2497.52,
      "duration": 4.44
    },
    {
      "text": "this this is the first row is the vector",
      "start": 2499.96,
      "duration": 4.359
    },
    {
      "text": "for the zeroth ID the second row is the",
      "start": 2501.96,
      "duration": 5.56
    },
    {
      "text": "vector for the first ID Etc so let's say",
      "start": 2504.319,
      "duration": 6.241
    },
    {
      "text": "uh if you want to get the vector for ID",
      "start": 2507.52,
      "duration": 4.079
    },
    {
      "text": "number",
      "start": 2510.56,
      "duration": 4.12
    },
    {
      "text": "three uh let me show this to you in code",
      "start": 2511.599,
      "duration": 5.641
    },
    {
      "text": "ID number three is is right and if you",
      "start": 2514.68,
      "duration": 5.08
    },
    {
      "text": "you want to get the vector for is how do",
      "start": 2517.24,
      "duration": 4.52
    },
    {
      "text": "you do it you first look at ID number",
      "start": 2519.76,
      "duration": 4.839
    },
    {
      "text": "three so ID number 0 1 2 3 so this this",
      "start": 2521.76,
      "duration": 5.04
    },
    {
      "text": "is that ID number right so then all you",
      "start": 2524.599,
      "duration": 4.441
    },
    {
      "text": "need to do is look at this corresponding",
      "start": 2526.8,
      "duration": 4.88
    },
    {
      "text": "Row in the embedding weight Matrix",
      "start": 2529.04,
      "duration": 4.96
    },
    {
      "text": "that's why it's called a lookup table to",
      "start": 2531.68,
      "duration": 4.56
    },
    {
      "text": "find the vector associated with a",
      "start": 2534.0,
      "duration": 5.72
    },
    {
      "text": "particular ID you just need to take this",
      "start": 2536.24,
      "duration": 5.8
    },
    {
      "text": "Matrix you look at the vector",
      "start": 2539.72,
      "duration": 4.399
    },
    {
      "text": "corresponding with that particular ID",
      "start": 2542.04,
      "duration": 4.64
    },
    {
      "text": "row number that's it this is exactly",
      "start": 2544.119,
      "duration": 4.761
    },
    {
      "text": "what we are going going to do over here",
      "start": 2546.68,
      "duration": 4.12
    },
    {
      "text": "uh we want to obtain the vector",
      "start": 2548.88,
      "duration": 4.0
    },
    {
      "text": "representation for ID number three right",
      "start": 2550.8,
      "duration": 4.0
    },
    {
      "text": "so that's what we are going to do we are",
      "start": 2552.88,
      "duration": 3.84
    },
    {
      "text": "going to access the embedding layer it's",
      "start": 2554.8,
      "duration": 3.799
    },
    {
      "text": "a lookup table and we are going to",
      "start": 2556.72,
      "duration": 3.399
    },
    {
      "text": "access",
      "start": 2558.599,
      "duration": 5.48
    },
    {
      "text": "the uh embedding Matrix for the token ID",
      "start": 2560.119,
      "duration": 6.561
    },
    {
      "text": "3 and what will this be this will be the",
      "start": 2564.079,
      "duration": 4.961
    },
    {
      "text": "fourth row because the zero ID is the",
      "start": 2566.68,
      "duration": 4.639
    },
    {
      "text": "first row the first ID is the second row",
      "start": 2569.04,
      "duration": 4.319
    },
    {
      "text": "second ID is the third row and third ID",
      "start": 2571.319,
      "duration": 4.28
    },
    {
      "text": "is the fourth row that's it and then",
      "start": 2573.359,
      "duration": 3.521
    },
    {
      "text": "when you print this you will get this",
      "start": 2575.599,
      "duration": 3.401
    },
    {
      "text": "vector so this is the vector which is",
      "start": 2576.88,
      "duration": 4.84
    },
    {
      "text": "the vector embedding for that particular",
      "start": 2579.0,
      "duration": 4.8
    },
    {
      "text": "ID which is ID number three so I also",
      "start": 2581.72,
      "duration": 4.48
    },
    {
      "text": "written this over here if we compare the",
      "start": 2583.8,
      "duration": 5.48
    },
    {
      "text": "embedding Vector for token ID3 we see",
      "start": 2586.2,
      "duration": 5.159
    },
    {
      "text": "that it is identical to the fourth row",
      "start": 2589.28,
      "duration": 5.279
    },
    {
      "text": "so look at this this Vector it's exactly",
      "start": 2591.359,
      "duration": 5.96
    },
    {
      "text": "the same as the fourth row",
      "start": 2594.559,
      "duration": 5.481
    },
    {
      "text": "right uh in other words the embedding",
      "start": 2597.319,
      "duration": 4.961
    },
    {
      "text": "layer is essentially a lookup operation",
      "start": 2600.04,
      "duration": 4.6
    },
    {
      "text": "that retrieves rows from the embedding",
      "start": 2602.28,
      "duration": 5.48
    },
    {
      "text": "layers weight Matrix via a token ID",
      "start": 2604.64,
      "duration": 5.439
    },
    {
      "text": "let me explain this in simpler words the",
      "start": 2607.76,
      "duration": 4.04
    },
    {
      "text": "embedding layer is essentially just a",
      "start": 2610.079,
      "duration": 4.121
    },
    {
      "text": "lookup operation and what this lookup",
      "start": 2611.8,
      "duration": 5.0
    },
    {
      "text": "operation does is that if you give it an",
      "start": 2614.2,
      "duration": 4.76
    },
    {
      "text": "ID number it looks for that particular",
      "start": 2616.8,
      "duration": 4.68
    },
    {
      "text": "row and retrieves a vector for you so",
      "start": 2618.96,
      "duration": 4.639
    },
    {
      "text": "for example if you want the vector for",
      "start": 2621.48,
      "duration": 4.839
    },
    {
      "text": "ID number five all you will need to do",
      "start": 2623.599,
      "duration": 4.96
    },
    {
      "text": "is look at this particular row which is",
      "start": 2626.319,
      "duration": 5.76
    },
    {
      "text": "row number six and then it will retrieve",
      "start": 2628.559,
      "duration": 5.401
    },
    {
      "text": "or it will give you that particular",
      "start": 2632.079,
      "duration": 4.0
    },
    {
      "text": "Vector if you look at if you want the",
      "start": 2633.96,
      "duration": 4.399
    },
    {
      "text": "vector for ID number Z just look at row",
      "start": 2636.079,
      "duration": 5.28
    },
    {
      "text": "number one if you look at the vector for",
      "start": 2638.359,
      "duration": 5.0
    },
    {
      "text": "ID number or if you want the vector for",
      "start": 2641.359,
      "duration": 4.681
    },
    {
      "text": "ID number one just look at row number",
      "start": 2643.359,
      "duration": 5.2
    },
    {
      "text": "two so that's why you can so the",
      "start": 2646.04,
      "duration": 4.36
    },
    {
      "text": "embedding weight Matrix is of course a",
      "start": 2648.559,
      "duration": 4.721
    },
    {
      "text": "matrix of the weights for Vector",
      "start": 2650.4,
      "duration": 5.32
    },
    {
      "text": "embeddings but it's also a lookup table",
      "start": 2653.28,
      "duration": 4.64
    },
    {
      "text": "if you specify the ID number you can use",
      "start": 2655.72,
      "duration": 4.68
    },
    {
      "text": "the embedding weight Matrix to find the",
      "start": 2657.92,
      "duration": 4.76
    },
    {
      "text": "exact Vector representation for that",
      "start": 2660.4,
      "duration": 4.8
    },
    {
      "text": "particular ID so that's why if you look",
      "start": 2662.68,
      "duration": 5.08
    },
    {
      "text": "at the P documentation this embedding is",
      "start": 2665.2,
      "duration": 4.919
    },
    {
      "text": "also called as a simple lookup table I",
      "start": 2667.76,
      "duration": 5.0
    },
    {
      "text": "hope you have understood this right",
      "start": 2670.119,
      "duration": 6.44
    },
    {
      "text": "now okay so uh one major portion of this",
      "start": 2672.76,
      "duration": 5.72
    },
    {
      "text": "lecture was for you to understand the",
      "start": 2676.559,
      "duration": 3.921
    },
    {
      "text": "embedding weight Matrix and why it is",
      "start": 2678.48,
      "duration": 4.92
    },
    {
      "text": "actually considered to be a lookup",
      "start": 2680.48,
      "duration": 5.839
    },
    {
      "text": "table awesome now let's come to the next",
      "start": 2683.4,
      "duration": 5.439
    },
    {
      "text": "part so previously we have seen how to",
      "start": 2686.319,
      "duration": 4.481
    },
    {
      "text": "convert a single token ID into three",
      "start": 2688.839,
      "duration": 4.24
    },
    {
      "text": "dimensional embedding right we just gave",
      "start": 2690.8,
      "duration": 4.96
    },
    {
      "text": "a single token ID and converted it into",
      "start": 2693.079,
      "duration": 5.24
    },
    {
      "text": "an embedding vector but remember what we",
      "start": 2695.76,
      "duration": 4.72
    },
    {
      "text": "started from I wanted the vector",
      "start": 2698.319,
      "duration": 4.721
    },
    {
      "text": "representations for these four IDs so I",
      "start": 2700.48,
      "duration": 4.92
    },
    {
      "text": "wanted the vector representation for ID",
      "start": 2703.04,
      "duration": 6.12
    },
    {
      "text": "number uh one ID number five ID number",
      "start": 2705.4,
      "duration": 6.28
    },
    {
      "text": "two and ID number three so how can we",
      "start": 2709.16,
      "duration": 5.08
    },
    {
      "text": "give these four to the lookup table we",
      "start": 2711.68,
      "duration": 4.84
    },
    {
      "text": "just specify the particular array so",
      "start": 2714.24,
      "duration": 5.119
    },
    {
      "text": "here we have the input IDs right uh we",
      "start": 2716.52,
      "duration": 4.839
    },
    {
      "text": "have the input IDs for which we want the",
      "start": 2719.359,
      "duration": 4.681
    },
    {
      "text": "vector representation all we do is that",
      "start": 2721.359,
      "duration": 5.321
    },
    {
      "text": "we just use the embedding layer and pass",
      "start": 2724.04,
      "duration": 4.16
    },
    {
      "text": "in the input",
      "start": 2726.68,
      "duration": 4.72
    },
    {
      "text": "IDs so similar to what happened here",
      "start": 2728.2,
      "duration": 5.04
    },
    {
      "text": "what this operation does is that it",
      "start": 2731.4,
      "duration": 4.0
    },
    {
      "text": "first looks at the input IDs and it sees",
      "start": 2733.24,
      "duration": 4.72
    },
    {
      "text": "that there are actually Four values in",
      "start": 2735.4,
      "duration": 4.679
    },
    {
      "text": "the input ID then what it does it goes",
      "start": 2737.96,
      "duration": 3.48
    },
    {
      "text": "through each",
      "start": 2740.079,
      "duration": 4.28
    },
    {
      "text": "individual ID and looks up the embedding",
      "start": 2741.44,
      "duration": 5.679
    },
    {
      "text": "Vector for that ID that's it so when you",
      "start": 2744.359,
      "duration": 5.24
    },
    {
      "text": "pass in the input IDs it will first look",
      "start": 2747.119,
      "duration": 5.24
    },
    {
      "text": "at uh this thing and it will look at",
      "start": 2749.599,
      "duration": 4.72
    },
    {
      "text": "first it will look at row number three",
      "start": 2752.359,
      "duration": 4.0
    },
    {
      "text": "then row number four row number six and",
      "start": 2754.319,
      "duration": 4.641
    },
    {
      "text": "row number two two so it will look at",
      "start": 2756.359,
      "duration": 4.72
    },
    {
      "text": "row number four row number six row",
      "start": 2758.96,
      "duration": 4.56
    },
    {
      "text": "number three and row number two and then",
      "start": 2761.079,
      "duration": 4.401
    },
    {
      "text": "it will print out the",
      "start": 2763.52,
      "duration": 4.52
    },
    {
      "text": "answer so essentially each row in this",
      "start": 2765.48,
      "duration": 4.839
    },
    {
      "text": "output Matrix is the corresponding",
      "start": 2768.04,
      "duration": 5.48
    },
    {
      "text": "Vector embedding for that particular ID",
      "start": 2770.319,
      "duration": 4.561
    },
    {
      "text": "so the only thing which you have to",
      "start": 2773.52,
      "duration": 2.799
    },
    {
      "text": "remember right now is the embedding",
      "start": 2774.88,
      "duration": 3.719
    },
    {
      "text": "layer is a lookup Matrix and you can",
      "start": 2776.319,
      "duration": 4.921
    },
    {
      "text": "pass a single ID to this lookup Matrix",
      "start": 2778.599,
      "duration": 4.881
    },
    {
      "text": "you can even pass multiple IDs or a",
      "start": 2781.24,
      "duration": 4.72
    },
    {
      "text": "group of IDs and in just one line of",
      "start": 2783.48,
      "duration": 4.28
    },
    {
      "text": "command you can get all the vector",
      "start": 2785.96,
      "duration": 6.08
    },
    {
      "text": "embeddings for that particular token ID",
      "start": 2787.76,
      "duration": 6.12
    },
    {
      "text": "this is how embedding layer is actually",
      "start": 2792.04,
      "duration": 4.559
    },
    {
      "text": "implemented in practice right now uh",
      "start": 2793.88,
      "duration": 5.56
    },
    {
      "text": "small random values have been initiated",
      "start": 2796.599,
      "duration": 6.441
    },
    {
      "text": "but um we are going to train these",
      "start": 2799.44,
      "duration": 5.8
    },
    {
      "text": "values so that they actually capture the",
      "start": 2803.04,
      "duration": 4.64
    },
    {
      "text": "meaning and that is what we'll come to",
      "start": 2805.24,
      "duration": 5.599
    },
    {
      "text": "in one of the subsequent",
      "start": 2807.68,
      "duration": 5.639
    },
    {
      "text": "lectures okay so let's see how much of",
      "start": 2810.839,
      "duration": 4.921
    },
    {
      "text": "the lecture we have covered so far we",
      "start": 2813.319,
      "duration": 5.0
    },
    {
      "text": "covered this part with where we saw that",
      "start": 2815.76,
      "duration": 5.2
    },
    {
      "text": "uh we saw that the embedding layer is",
      "start": 2818.319,
      "duration": 4.721
    },
    {
      "text": "essentially a lookup operation that",
      "start": 2820.96,
      "duration": 4.599
    },
    {
      "text": "retrieves rows from the embedding layer",
      "start": 2823.04,
      "duration": 5.559
    },
    {
      "text": "weight Matrix using a token ID here is",
      "start": 2825.559,
      "duration": 5.121
    },
    {
      "text": "an image which also explain this so",
      "start": 2828.599,
      "duration": 4.641
    },
    {
      "text": "let's say this is the weight Matrix uh",
      "start": 2830.68,
      "duration": 5.399
    },
    {
      "text": "embedding Matrix and let's say uh these",
      "start": 2833.24,
      "duration": 5.879
    },
    {
      "text": "are the token IDs which we want to embed",
      "start": 2836.079,
      "duration": 5.04
    },
    {
      "text": "or we want to find the vector",
      "start": 2839.119,
      "duration": 5.081
    },
    {
      "text": "representations so if you actually pass",
      "start": 2841.119,
      "duration": 4.96
    },
    {
      "text": "in these token IDs to the embedding",
      "start": 2844.2,
      "duration": 4.0
    },
    {
      "text": "layer what it will do is that it will",
      "start": 2846.079,
      "duration": 4.441
    },
    {
      "text": "first look at each particular ID so it",
      "start": 2848.2,
      "duration": 5.0
    },
    {
      "text": "will look at ID number two which means",
      "start": 2850.52,
      "duration": 4.559
    },
    {
      "text": "it will go to row number three which is",
      "start": 2853.2,
      "duration": 4.28
    },
    {
      "text": "highlighted in blue that will be the",
      "start": 2855.079,
      "duration": 5.081
    },
    {
      "text": "first answer of this lookup table then",
      "start": 2857.48,
      "duration": 5.079
    },
    {
      "text": "it will look at ID number three and that",
      "start": 2860.16,
      "duration": 4.52
    },
    {
      "text": "will mean row number four that is the",
      "start": 2862.559,
      "duration": 6.401
    },
    {
      "text": "second row of the final answer then ID",
      "start": 2864.68,
      "duration": 5.96
    },
    {
      "text": "number five which is essentially row",
      "start": 2868.96,
      "duration": 5.08
    },
    {
      "text": "number six and then uh it will give the",
      "start": 2870.64,
      "duration": 5.28
    },
    {
      "text": "vector corresponding to row number six",
      "start": 2874.04,
      "duration": 3.319
    },
    {
      "text": "and finally",
      "start": 2875.92,
      "duration": 5.12
    },
    {
      "text": "uh ID one which means row number two so",
      "start": 2877.359,
      "duration": 4.841
    },
    {
      "text": "then it will give the vector",
      "start": 2881.04,
      "duration": 3.0
    },
    {
      "text": "corresponding to row number two that's",
      "start": 2882.2,
      "duration": 5.0
    },
    {
      "text": "it so this is the embedding weight",
      "start": 2884.04,
      "duration": 6.2
    },
    {
      "text": "Matrix and then it just looks at the",
      "start": 2887.2,
      "duration": 5.359
    },
    {
      "text": "particular row based on these IDs and",
      "start": 2890.24,
      "duration": 4.92
    },
    {
      "text": "then it gives the vector embeddings for",
      "start": 2892.559,
      "duration": 4.441
    },
    {
      "text": "all the IDS which we asked for so here",
      "start": 2895.16,
      "duration": 4.6
    },
    {
      "text": "we asked for fox jumps over dog and then",
      "start": 2897.0,
      "duration": 4.319
    },
    {
      "text": "it gives the vector embeddings for all",
      "start": 2899.76,
      "duration": 2.559
    },
    {
      "text": "those",
      "start": 2901.319,
      "duration": 4.0
    },
    {
      "text": "IDs so if someone asks you what's an",
      "start": 2902.319,
      "duration": 4.721
    },
    {
      "text": "embedding layer you can say that it's a",
      "start": 2905.319,
      "duration": 4.601
    },
    {
      "text": "simple lookup operation that retrieves",
      "start": 2907.04,
      "duration": 5.319
    },
    {
      "text": "the vector for the particular token ID",
      "start": 2909.92,
      "duration": 3.919
    },
    {
      "text": "that's",
      "start": 2912.359,
      "duration": 5.0
    },
    {
      "text": "it now I want to show you one last thing",
      "start": 2913.839,
      "duration": 6.201
    },
    {
      "text": "actually uh it is a bit of a finer",
      "start": 2917.359,
      "duration": 4.641
    },
    {
      "text": "detail but I think it's important for",
      "start": 2920.04,
      "duration": 4.44
    },
    {
      "text": "you to also think about an embedding",
      "start": 2922.0,
      "duration": 5.119
    },
    {
      "text": "layer in some another dimension right so",
      "start": 2924.48,
      "duration": 4.72
    },
    {
      "text": "let's say we have the following three",
      "start": 2927.119,
      "duration": 4.521
    },
    {
      "text": "training examples let's say we are we",
      "start": 2929.2,
      "duration": 4.359
    },
    {
      "text": "want to have the embedding for ID number",
      "start": 2931.64,
      "duration": 5.679
    },
    {
      "text": "two ID number three and ID number one so",
      "start": 2933.559,
      "duration": 5.56
    },
    {
      "text": "let's say there are four words in our",
      "start": 2937.319,
      "duration": 6.28
    },
    {
      "text": "vocabulary and uh which means that 0 1 2",
      "start": 2939.119,
      "duration": 6.361
    },
    {
      "text": "3 these are the IDS with these four",
      "start": 2943.599,
      "duration": 4.321
    },
    {
      "text": "words and we want to encode each of",
      "start": 2945.48,
      "duration": 6.599
    },
    {
      "text": "these IDs into a vector so uh let's say",
      "start": 2947.92,
      "duration": 6.679
    },
    {
      "text": "the embedding Dimension is five so each",
      "start": 2952.079,
      "duration": 4.561
    },
    {
      "text": "of these each of these",
      "start": 2954.599,
      "duration": 5.641
    },
    {
      "text": "IDs will have a vector with basically 1",
      "start": 2956.64,
      "duration": 6.56
    },
    {
      "text": "2 3 4 five with five Dimensions so the",
      "start": 2960.24,
      "duration": 4.599
    },
    {
      "text": "ID number zero will have a vector",
      "start": 2963.2,
      "duration": 4.08
    },
    {
      "text": "embedding of five dimensions ID number",
      "start": 2964.839,
      "duration": 4.401
    },
    {
      "text": "one will have a vector embedding of five",
      "start": 2967.28,
      "duration": 5.039
    },
    {
      "text": "Dimensions Etc ID number two will have a",
      "start": 2969.24,
      "duration": 5.319
    },
    {
      "text": "vector embedding of five dimensions and",
      "start": 2972.319,
      "duration": 3.841
    },
    {
      "text": "ID number three will have Vector",
      "start": 2974.559,
      "duration": 3.121
    },
    {
      "text": "embedding of five",
      "start": 2976.16,
      "duration": 4.12
    },
    {
      "text": "Dimensions so the embedding Matrix which",
      "start": 2977.68,
      "duration": 5.28
    },
    {
      "text": "we create we pass in the first argument",
      "start": 2980.28,
      "duration": 4.319
    },
    {
      "text": "the first argument is the vocabulary",
      "start": 2982.96,
      "duration": 4.0
    },
    {
      "text": "size which is the four number of rows",
      "start": 2984.599,
      "duration": 4.281
    },
    {
      "text": "and then the second argument is the",
      "start": 2986.96,
      "duration": 4.08
    },
    {
      "text": "embedding Dimension so that is five",
      "start": 2988.88,
      "duration": 4.4
    },
    {
      "text": "right and so then if you print out the",
      "start": 2991.04,
      "duration": 4.68
    },
    {
      "text": "embedding weights you will see that and",
      "start": 2993.28,
      "duration": 4.44
    },
    {
      "text": "you can even try this in python the",
      "start": 2995.72,
      "duration": 3.8
    },
    {
      "text": "embedding weights will be these which",
      "start": 2997.72,
      "duration": 6.839
    },
    {
      "text": "are initialized to random values and",
      "start": 2999.52,
      "duration": 5.039
    },
    {
      "text": "U you'll see that there are four rows",
      "start": 3007.599,
      "duration": 4.881
    },
    {
      "text": "and five columns because we have four",
      "start": 3010.2,
      "duration": 4.72
    },
    {
      "text": "IDs in the vocabulary and each ID has",
      "start": 3012.48,
      "duration": 4.96
    },
    {
      "text": "five um has a vector with five",
      "start": 3014.92,
      "duration": 4.12
    },
    {
      "text": "Dimensions this is",
      "start": 3017.44,
      "duration": 4.76
    },
    {
      "text": "great then what we do is that we just",
      "start": 3019.04,
      "duration": 6.44
    },
    {
      "text": "retrieve those weight Vector weights",
      "start": 3022.2,
      "duration": 5.119
    },
    {
      "text": "which we need so then we passing this",
      "start": 3025.48,
      "duration": 5.2
    },
    {
      "text": "idx so we only need vectors for ID is 2",
      "start": 3027.319,
      "duration": 6.321
    },
    {
      "text": "3 1 we don't need the vector for ID Z so",
      "start": 3030.68,
      "duration": 5.96
    },
    {
      "text": "then these are the IDS which are passed",
      "start": 3033.64,
      "duration": 4.8
    },
    {
      "text": "and then we do the lookup operation and",
      "start": 3036.64,
      "duration": 4.76
    },
    {
      "text": "then this is the embedding table which",
      "start": 3038.44,
      "duration": 4.8
    },
    {
      "text": "is extracted or the embedding Matrix",
      "start": 3041.4,
      "duration": 5.28
    },
    {
      "text": "which is extracted for IDs 2 3 and 1 now",
      "start": 3043.24,
      "duration": 6.0
    },
    {
      "text": "one thing which I want to explain to you",
      "start": 3046.68,
      "duration": 4.72
    },
    {
      "text": "that this embedding layer is actually",
      "start": 3049.24,
      "duration": 4.28
    },
    {
      "text": "the same as a neural network linear",
      "start": 3051.4,
      "duration": 5.8
    },
    {
      "text": "layer so uh I'll try to explain this",
      "start": 3053.52,
      "duration": 5.319
    },
    {
      "text": "quickly because I don't want to divert",
      "start": 3057.2,
      "duration": 3.76
    },
    {
      "text": "you from the main purpose of the lecture",
      "start": 3058.839,
      "duration": 3.48
    },
    {
      "text": "but let's say if you have a neural",
      "start": 3060.96,
      "duration": 2.92
    },
    {
      "text": "network with four",
      "start": 3062.319,
      "duration": 4.52
    },
    {
      "text": "inputs uh four as the input Dimension",
      "start": 3063.88,
      "duration": 6.32
    },
    {
      "text": "and then you have three batches of",
      "start": 3066.839,
      "duration": 5.76
    },
    {
      "text": "inputs coming in the first batch is",
      "start": 3070.2,
      "duration": 4.399
    },
    {
      "text": "basically the first ID number which is",
      "start": 3072.599,
      "duration": 4.52
    },
    {
      "text": "two and encoded as a one hot",
      "start": 3074.599,
      "duration": 4.72
    },
    {
      "text": "representation the second batch is the",
      "start": 3077.119,
      "duration": 4.881
    },
    {
      "text": "ID number uh so let's see the IDS which",
      "start": 3079.319,
      "duration": 5.881
    },
    {
      "text": "we needed ID is 231 right so the second",
      "start": 3082.0,
      "duration": 4.799
    },
    {
      "text": "batch is the ID number three which is",
      "start": 3085.2,
      "duration": 4.04
    },
    {
      "text": "encoded as the one hot Vector",
      "start": 3086.799,
      "duration": 5.161
    },
    {
      "text": "001 and the third ID is the ID number",
      "start": 3089.24,
      "duration": 5.8
    },
    {
      "text": "one which is encoded as 0 1",
      "start": 3091.96,
      "duration": 6.2
    },
    {
      "text": "0 if these three are the input batches",
      "start": 3095.04,
      "duration": 4.84
    },
    {
      "text": "which are fed to this neural network and",
      "start": 3098.16,
      "duration": 4.28
    },
    {
      "text": "let's say there are five neurons here",
      "start": 3099.88,
      "duration": 5.0
    },
    {
      "text": "the output of this linear layer is X",
      "start": 3102.44,
      "duration": 3.679
    },
    {
      "text": "into W",
      "start": 3104.88,
      "duration": 4.64
    },
    {
      "text": "transpose so what is w every neuron here",
      "start": 3106.119,
      "duration": 5.361
    },
    {
      "text": "will have four weights associated with",
      "start": 3109.52,
      "duration": 4.039
    },
    {
      "text": "it because every input has four",
      "start": 3111.48,
      "duration": 3.56
    },
    {
      "text": "dimensions if you look at the first",
      "start": 3113.559,
      "duration": 5.601
    },
    {
      "text": "input it's zero 0 1 0 it has four uh",
      "start": 3115.04,
      "duration": 6.2
    },
    {
      "text": "four dimensions so every neuron here",
      "start": 3119.16,
      "duration": 4.399
    },
    {
      "text": "will have four weights associated with",
      "start": 3121.24,
      "duration": 4.28
    },
    {
      "text": "it so if you look at this weight",
      "start": 3123.559,
      "duration": 4.161
    },
    {
      "text": "transpose Matrix the First Column will",
      "start": 3125.52,
      "duration": 4.36
    },
    {
      "text": "be the weights of the first neuron the",
      "start": 3127.72,
      "duration": 3.72
    },
    {
      "text": "second column will be the four weights",
      "start": 3129.88,
      "duration": 3.8
    },
    {
      "text": "of the second neuron the third column",
      "start": 3131.44,
      "duration": 3.72
    },
    {
      "text": "will be the four weights of the third",
      "start": 3133.68,
      "duration": 3.96
    },
    {
      "text": "neuron fourth column will be the four",
      "start": 3135.16,
      "duration": 4.959
    },
    {
      "text": "weights of the fourth neuron and the",
      "start": 3137.64,
      "duration": 4.199
    },
    {
      "text": "last colum will be the four weights of",
      "start": 3140.119,
      "duration": 4.2
    },
    {
      "text": "the fifth neuron so let's say if x is",
      "start": 3141.839,
      "duration": 4.841
    },
    {
      "text": "the input Matrix W transpose is the",
      "start": 3144.319,
      "duration": 5.641
    },
    {
      "text": "weight Matrix when you do X into W",
      "start": 3146.68,
      "duration": 5.639
    },
    {
      "text": "transpose you will get essentially a",
      "start": 3149.96,
      "duration": 4.8
    },
    {
      "text": "matrix which has three",
      "start": 3152.319,
      "duration": 6.201
    },
    {
      "text": "rows and uh it has five columns why",
      "start": 3154.76,
      "duration": 5.799
    },
    {
      "text": "three rows because for every input we",
      "start": 3158.52,
      "duration": 5.039
    },
    {
      "text": "have three inputs input number input",
      "start": 3160.559,
      "duration": 4.961
    },
    {
      "text": "number one input number two and input",
      "start": 3163.559,
      "duration": 4.601
    },
    {
      "text": "number three and for each input we want",
      "start": 3165.52,
      "duration": 5.12
    },
    {
      "text": "a vector embedding with five Dimensions",
      "start": 3168.16,
      "duration": 4.399
    },
    {
      "text": "so there is three rows and there is five",
      "start": 3170.64,
      "duration": 4.56
    },
    {
      "text": "columns so each row corresponds to the",
      "start": 3172.559,
      "duration": 4.441
    },
    {
      "text": "vector embedding for that particular",
      "start": 3175.2,
      "duration": 4.919
    },
    {
      "text": "input now if you look at this if you",
      "start": 3177.0,
      "duration": 5.28
    },
    {
      "text": "look at this output look at the first",
      "start": 3180.119,
      "duration": 3.48
    },
    {
      "text": "row 0.",
      "start": 3182.28,
      "duration": 3.64
    },
    {
      "text": "6957 and if you look at this output",
      "start": 3183.599,
      "duration": 4.321
    },
    {
      "text": "you'll see that it's exactly",
      "start": 3185.92,
      "duration": 4.399
    },
    {
      "text": "similar in fact both these outputs are",
      "start": 3187.92,
      "duration": 5.439
    },
    {
      "text": "completely similar so what is exactly",
      "start": 3190.319,
      "duration": 5.721
    },
    {
      "text": "happening here what the embedding Matrix",
      "start": 3193.359,
      "duration": 4.921
    },
    {
      "text": "is actually doing underneath is that",
      "start": 3196.04,
      "duration": 3.92
    },
    {
      "text": "it's the same operation as a neural",
      "start": 3198.28,
      "duration": 4.12
    },
    {
      "text": "network linear layer so what an",
      "start": 3199.96,
      "duration": 4.399
    },
    {
      "text": "embedding does is actually is that you",
      "start": 3202.4,
      "duration": 4.8
    },
    {
      "text": "have inputs right so let's say we have",
      "start": 3204.359,
      "duration": 6.081
    },
    {
      "text": "three tokens three token IDs those are",
      "start": 3207.2,
      "duration": 5.44
    },
    {
      "text": "converted into one hot representations",
      "start": 3210.44,
      "duration": 3.84
    },
    {
      "text": "they are fed into a neural network with",
      "start": 3212.64,
      "duration": 4.719
    },
    {
      "text": "five neurons why five because the vector",
      "start": 3214.28,
      "duration": 5.0
    },
    {
      "text": "Dimension is five and then we have a",
      "start": 3217.359,
      "duration": 4.361
    },
    {
      "text": "linear layer whose output is X into W",
      "start": 3219.28,
      "duration": 5.4
    },
    {
      "text": "transpose this gives the same output as",
      "start": 3221.72,
      "duration": 5.32
    },
    {
      "text": "the embedding layer so earlier I showed",
      "start": 3224.68,
      "duration": 5.0
    },
    {
      "text": "you tor. nn. embedding right here I'm",
      "start": 3227.04,
      "duration": 5.039
    },
    {
      "text": "showing you tor. nn.",
      "start": 3229.68,
      "duration": 5.28
    },
    {
      "text": "linear then you might be thinking why is",
      "start": 3232.079,
      "duration": 5.72
    },
    {
      "text": "nn. linear not used used to define the",
      "start": 3234.96,
      "duration": 5.0
    },
    {
      "text": "embedding Matrix the reason is it's not",
      "start": 3237.799,
      "duration": 5.28
    },
    {
      "text": "used is because so both embedding layer",
      "start": 3239.96,
      "duration": 5.32
    },
    {
      "text": "and NN layer lead to the same output",
      "start": 3243.079,
      "duration": 4.0
    },
    {
      "text": "both embedding layer and the NN linear",
      "start": 3245.28,
      "duration": 3.76
    },
    {
      "text": "layer lead to the same output but",
      "start": 3247.079,
      "duration": 3.441
    },
    {
      "text": "embedding layer is much more",
      "start": 3249.04,
      "duration": 3.96
    },
    {
      "text": "computationally efficient because in the",
      "start": 3250.52,
      "duration": 4.92
    },
    {
      "text": "NN layer we have many unnecessarily",
      "start": 3253.0,
      "duration": 5.359
    },
    {
      "text": "multiplications with zero so you could",
      "start": 3255.44,
      "duration": 5.0
    },
    {
      "text": "just use NN do linear operation over",
      "start": 3258.359,
      "duration": 4.561
    },
    {
      "text": "here and do the X into W transpose but",
      "start": 3260.44,
      "duration": 3.879
    },
    {
      "text": "here you see you have to do one hot",
      "start": 3262.92,
      "duration": 3.6
    },
    {
      "text": "encoding so there are many zeros and a",
      "start": 3264.319,
      "duration": 3.921
    },
    {
      "text": "lot of unnecessary computations are",
      "start": 3266.52,
      "duration": 4.039
    },
    {
      "text": "there these unnecessary computations",
      "start": 3268.24,
      "duration": 4.839
    },
    {
      "text": "really scale up when we are dealing with",
      "start": 3270.559,
      "duration": 5.361
    },
    {
      "text": "vocabulary sizes of chat gp2",
      "start": 3273.079,
      "duration": 5.601
    },
    {
      "text": "gpt2 and that's why we use the embedding",
      "start": 3275.92,
      "duration": 5.439
    },
    {
      "text": "layer here so there is also the",
      "start": 3278.68,
      "duration": 5.0
    },
    {
      "text": "torch. nn.",
      "start": 3281.359,
      "duration": 4.921
    },
    {
      "text": "linear this layer is also there there",
      "start": 3283.68,
      "duration": 5.639
    },
    {
      "text": "are two layers and even you can use nn.",
      "start": 3286.28,
      "duration": 5.039
    },
    {
      "text": "linear to create the embedding Matrix",
      "start": 3289.319,
      "duration": 3.721
    },
    {
      "text": "but the reason it's not used is because",
      "start": 3291.319,
      "duration": 4.161
    },
    {
      "text": "it's not efficient so the embedding",
      "start": 3293.04,
      "duration": 5.16
    },
    {
      "text": "layer is much more preferred or the nn.",
      "start": 3295.48,
      "duration": 4.24
    },
    {
      "text": "linear layer when you create the",
      "start": 3298.2,
      "duration": 4.399
    },
    {
      "text": "embedding Matrix this is just a small",
      "start": 3299.72,
      "duration": 5.0
    },
    {
      "text": "takeaway for anyone who is familiar with",
      "start": 3302.599,
      "duration": 4.441
    },
    {
      "text": "neural networks but if you are not don't",
      "start": 3304.72,
      "duration": 4.0
    },
    {
      "text": "worry if you did not understand this",
      "start": 3307.04,
      "duration": 5.319
    },
    {
      "text": "part mostly I wanted to cover some other",
      "start": 3308.72,
      "duration": 6.0
    },
    {
      "text": "major points in today's lecture and some",
      "start": 3312.359,
      "duration": 4.801
    },
    {
      "text": "of them are first I wanted you to",
      "start": 3314.72,
      "duration": 3.96
    },
    {
      "text": "understand the conceptual understanding",
      "start": 3317.16,
      "duration": 4.04
    },
    {
      "text": "of why token embeddings are needed in",
      "start": 3318.68,
      "duration": 4.119
    },
    {
      "text": "convolutional neural networks we",
      "start": 3321.2,
      "duration": 3.52
    },
    {
      "text": "exploited the spatial features of an",
      "start": 3322.799,
      "duration": 4.121
    },
    {
      "text": "image before giving it as as input for",
      "start": 3324.72,
      "duration": 5.44
    },
    {
      "text": "training this is exactly what we do in",
      "start": 3326.92,
      "duration": 5.52
    },
    {
      "text": "uh token embeddings words can be",
      "start": 3330.16,
      "duration": 4.399
    },
    {
      "text": "represented as vectors and those vectors",
      "start": 3332.44,
      "duration": 4.28
    },
    {
      "text": "can carry meaning that is called as",
      "start": 3334.559,
      "duration": 4.24
    },
    {
      "text": "Vector embedding or token",
      "start": 3336.72,
      "duration": 4.52
    },
    {
      "text": "embedding and if the token embeddings",
      "start": 3338.799,
      "duration": 4.56
    },
    {
      "text": "are trained properly we show I showed",
      "start": 3341.24,
      "duration": 4.879
    },
    {
      "text": "you an example where the words the",
      "start": 3343.359,
      "duration": 5.24
    },
    {
      "text": "vectors can actually carry meaning so if",
      "start": 3346.119,
      "duration": 4.881
    },
    {
      "text": "you have Vector for King which encodes",
      "start": 3348.599,
      "duration": 4.76
    },
    {
      "text": "some masculinity if you have Vector for",
      "start": 3351.0,
      "duration": 4.52
    },
    {
      "text": "woman which encodes some femininity and",
      "start": 3353.359,
      "duration": 4.161
    },
    {
      "text": "we subtract the Vector for man which",
      "start": 3355.52,
      "duration": 4.519
    },
    {
      "text": "also encodes masculinity the answer is a",
      "start": 3357.52,
      "duration": 4.64
    },
    {
      "text": "vector which encodes femininity which is",
      "start": 3360.039,
      "duration": 5.08
    },
    {
      "text": "Queen what also we can do is that we can",
      "start": 3362.16,
      "duration": 6.36
    },
    {
      "text": "show that if you take two vectors of",
      "start": 3365.119,
      "duration": 5.121
    },
    {
      "text": "Words which are similar to each other",
      "start": 3368.52,
      "duration": 3.48
    },
    {
      "text": "and if you take the magnitude of the",
      "start": 3370.24,
      "duration": 3.879
    },
    {
      "text": "difference between those vectors that",
      "start": 3372.0,
      "duration": 4.24
    },
    {
      "text": "magnitude is much lesser than words",
      "start": 3374.119,
      "duration": 5.081
    },
    {
      "text": "which do not mean anything which means",
      "start": 3376.24,
      "duration": 5.48
    },
    {
      "text": "that if you if you have vectors for",
      "start": 3379.2,
      "duration": 4.04
    },
    {
      "text": "Words which are similar to each other",
      "start": 3381.72,
      "duration": 4.28
    },
    {
      "text": "they might be closer together in space",
      "start": 3383.24,
      "duration": 5.64
    },
    {
      "text": "so if embeddings are created nicely they",
      "start": 3386.0,
      "duration": 5.92
    },
    {
      "text": "can actually encode the meaning between",
      "start": 3388.88,
      "duration": 5.88
    },
    {
      "text": "words I found this concept very hard to",
      "start": 3391.92,
      "duration": 4.8
    },
    {
      "text": "understand so I wanted you to First",
      "start": 3394.76,
      "duration": 3.839
    },
    {
      "text": "understand that it is possible to have",
      "start": 3396.72,
      "duration": 3.52
    },
    {
      "text": "vectors in such a way that they encode",
      "start": 3398.599,
      "duration": 4.0
    },
    {
      "text": "meaning many people don't even",
      "start": 3400.24,
      "duration": 3.559
    },
    {
      "text": "understand what does it mean that",
      "start": 3402.599,
      "duration": 3.881
    },
    {
      "text": "vectors have meanings so the first two",
      "start": 3403.799,
      "duration": 4.32
    },
    {
      "text": "points in today's lecture were devoted",
      "start": 3406.48,
      "duration": 3.359
    },
    {
      "text": "for you to get an conceptual",
      "start": 3408.119,
      "duration": 3.281
    },
    {
      "text": "understanding of why token embeddings",
      "start": 3409.839,
      "duration": 2.24
    },
    {
      "text": "are",
      "start": 3411.4,
      "duration": 3.04
    },
    {
      "text": "needed then we looked at a practical",
      "start": 3412.079,
      "duration": 4.361
    },
    {
      "text": "aspect of how token embeddings are are",
      "start": 3414.44,
      "duration": 4.119
    },
    {
      "text": "created to create a token embedding",
      "start": 3416.44,
      "duration": 4.32
    },
    {
      "text": "Matrix you need two parameters you need",
      "start": 3418.559,
      "duration": 4.441
    },
    {
      "text": "your vocabulary size and you need the",
      "start": 3420.76,
      "duration": 5.4
    },
    {
      "text": "vector dimension for gpt2 the vocabulary",
      "start": 3423.0,
      "duration": 4.16
    },
    {
      "text": "size was",
      "start": 3426.16,
      "duration": 4.399
    },
    {
      "text": "50257 and the vector Dimension was 768",
      "start": 3427.16,
      "duration": 5.12
    },
    {
      "text": "so you essentially have an embedding",
      "start": 3430.559,
      "duration": 5.56
    },
    {
      "text": "weight Matrix which has 50257 rows and",
      "start": 3432.28,
      "duration": 7.0
    },
    {
      "text": "768 columns for each token ID in the",
      "start": 3436.119,
      "duration": 6.68
    },
    {
      "text": "vocabulary you have to construct a",
      "start": 3439.28,
      "duration": 6.88
    },
    {
      "text": "vector now how are these weights of the",
      "start": 3442.799,
      "duration": 5.121
    },
    {
      "text": "embedding Matrix determined they are",
      "start": 3446.16,
      "duration": 4.0
    },
    {
      "text": "initialized randomly these weights are",
      "start": 3447.92,
      "duration": 5.199
    },
    {
      "text": "initialized randomly and then the",
      "start": 3450.16,
      "duration": 5.04
    },
    {
      "text": "embedded weights are optimized as part",
      "start": 3453.119,
      "duration": 5.0
    },
    {
      "text": "of the llm training process that's very",
      "start": 3455.2,
      "duration": 5.08
    },
    {
      "text": "important uh so this this is how the",
      "start": 3458.119,
      "duration": 4.24
    },
    {
      "text": "embedding weight Matrix is created but",
      "start": 3460.28,
      "duration": 3.88
    },
    {
      "text": "what's also quite interesting is that at",
      "start": 3462.359,
      "duration": 3.361
    },
    {
      "text": "the heart of it the embedding weight",
      "start": 3464.16,
      "duration": 3.919
    },
    {
      "text": "Matrix is just a lookup operation which",
      "start": 3465.72,
      "duration": 4.44
    },
    {
      "text": "means that if you have a embedding",
      "start": 3468.079,
      "duration": 3.04
    },
    {
      "text": "weight",
      "start": 3470.16,
      "duration": 3.12
    },
    {
      "text": "Matrix if you have an embedding weight",
      "start": 3471.119,
      "duration": 5.081
    },
    {
      "text": "Matrix you can just pass in the input ID",
      "start": 3473.28,
      "duration": 4.759
    },
    {
      "text": "or the token ID for which you want the",
      "start": 3476.2,
      "duration": 3.48
    },
    {
      "text": "vector embedding and then you get it",
      "start": 3478.039,
      "duration": 4.04
    },
    {
      "text": "corresponding to the particular row you",
      "start": 3479.68,
      "duration": 4.639
    },
    {
      "text": "can even pass in a bunch of input IDs",
      "start": 3482.079,
      "duration": 3.681
    },
    {
      "text": "and then the embedding layer will just",
      "start": 3484.319,
      "duration": 3.681
    },
    {
      "text": "look for that the row corresponding to",
      "start": 3485.76,
      "duration": 5.12
    },
    {
      "text": "the input ID and retrieve the vector",
      "start": 3488.0,
      "duration": 6.039
    },
    {
      "text": "embedding for you so simple way to look",
      "start": 3490.88,
      "duration": 4.76
    },
    {
      "text": "at the embedding layer is that it's",
      "start": 3494.039,
      "duration": 3.52
    },
    {
      "text": "essentially just a lookup operation",
      "start": 3495.64,
      "duration": 2.88
    },
    {
      "text": "that's",
      "start": 3497.559,
      "duration": 3.921
    },
    {
      "text": "it now towards the end we also saw one",
      "start": 3498.52,
      "duration": 5.24
    },
    {
      "text": "more thing that whatever the embedding",
      "start": 3501.48,
      "duration": 4.24
    },
    {
      "text": "layer does can actually be done using",
      "start": 3503.76,
      "duration": 4.799
    },
    {
      "text": "the neur oral Network linear layer but",
      "start": 3505.72,
      "duration": 4.68
    },
    {
      "text": "the reason it's not preferred is because",
      "start": 3508.559,
      "duration": 3.401
    },
    {
      "text": "the embedding layer is much more",
      "start": 3510.4,
      "duration": 3.32
    },
    {
      "text": "computationally",
      "start": 3511.96,
      "duration": 4.159
    },
    {
      "text": "efficient awesome in this lecture I have",
      "start": 3513.72,
      "duration": 4.56
    },
    {
      "text": "not covered how to train the embeddings",
      "start": 3516.119,
      "duration": 4.24
    },
    {
      "text": "but I just wanted to give you an overall",
      "start": 3518.28,
      "duration": 3.48
    },
    {
      "text": "understanding of what token embeddings",
      "start": 3520.359,
      "duration": 3.44
    },
    {
      "text": "are what is the embedding layer weight",
      "start": 3521.76,
      "duration": 5.0
    },
    {
      "text": "Matrix but in in subsequent lectures we",
      "start": 3523.799,
      "duration": 4.76
    },
    {
      "text": "are also going to see how to train the",
      "start": 3526.76,
      "duration": 4.359
    },
    {
      "text": "embedding layer so in this example which",
      "start": 3528.559,
      "duration": 4.681
    },
    {
      "text": "we saw I directly use the pre-trained",
      "start": 3531.119,
      "duration": 4.601
    },
    {
      "text": "word to Google news right later we'll",
      "start": 3533.24,
      "duration": 4.76
    },
    {
      "text": "also see how this pre-training is done",
      "start": 3535.72,
      "duration": 5.599
    },
    {
      "text": "and how gpt2 gpt3 and gp4 did the",
      "start": 3538.0,
      "duration": 5.24
    },
    {
      "text": "pre-training for the for creating the",
      "start": 3541.319,
      "duration": 4.561
    },
    {
      "text": "vector embeddings in the next lecture we",
      "start": 3543.24,
      "duration": 4.319
    },
    {
      "text": "are going to look at another important",
      "start": 3545.88,
      "duration": 3.36
    },
    {
      "text": "concept which is called as positional",
      "start": 3547.559,
      "duration": 4.28
    },
    {
      "text": "embedding so until now we looked at how",
      "start": 3549.24,
      "duration": 5.16
    },
    {
      "text": "to connot words into vectors right but",
      "start": 3551.839,
      "duration": 4.76
    },
    {
      "text": "when sentences are given the positioning",
      "start": 3554.4,
      "duration": 4.88
    },
    {
      "text": "of the sentence also matters a lot uh",
      "start": 3556.599,
      "duration": 5.121
    },
    {
      "text": "the cat sits on the mat so cat and mat",
      "start": 3559.28,
      "duration": 4.759
    },
    {
      "text": "are close by but if mat is somewhere far",
      "start": 3561.72,
      "duration": 5.0
    },
    {
      "text": "away they are not related so position of",
      "start": 3564.039,
      "duration": 4.481
    },
    {
      "text": "the words also matter a lot apart from",
      "start": 3566.72,
      "duration": 4.399
    },
    {
      "text": "their semantic meaning up till now the",
      "start": 3568.52,
      "duration": 3.92
    },
    {
      "text": "vector embeddings which I have showed",
      "start": 3571.119,
      "duration": 3.281
    },
    {
      "text": "you do not encode the position for where",
      "start": 3572.44,
      "duration": 3.399
    },
    {
      "text": "the word comes in the particular",
      "start": 3574.4,
      "duration": 4.959
    },
    {
      "text": "sentence but that is another uh feature",
      "start": 3575.839,
      "duration": 5.28
    },
    {
      "text": "of words and sentences which we are",
      "start": 3579.359,
      "duration": 4.881
    },
    {
      "text": "going to exploit in images we exploited",
      "start": 3581.119,
      "duration": 5.561
    },
    {
      "text": "transational invariance and we also",
      "start": 3584.24,
      "duration": 4.48
    },
    {
      "text": "exploited spatial similarities between",
      "start": 3586.68,
      "duration": 4.48
    },
    {
      "text": "features in vctor embeddings we have",
      "start": 3588.72,
      "duration": 4.8
    },
    {
      "text": "already exploited the semantic",
      "start": 3591.16,
      "duration": 4.12
    },
    {
      "text": "relationship and the semantic meaning",
      "start": 3593.52,
      "duration": 4.2
    },
    {
      "text": "between words but we'll also see how to",
      "start": 3595.28,
      "duration": 4.96
    },
    {
      "text": "exploit the position and where the words",
      "start": 3597.72,
      "duration": 5.319
    },
    {
      "text": "are positioned in sentences and that",
      "start": 3600.24,
      "duration": 5.52
    },
    {
      "text": "will be the subject of the next lecture",
      "start": 3603.039,
      "duration": 4.04
    },
    {
      "text": "where we are going to learn about",
      "start": 3605.76,
      "duration": 3.68
    },
    {
      "text": "positional embeddings thank you so much",
      "start": 3607.079,
      "duration": 4.201
    },
    {
      "text": "everyone I know these lectures are",
      "start": 3609.44,
      "duration": 4.04
    },
    {
      "text": "becoming a bit long but I deliberately",
      "start": 3611.28,
      "duration": 3.92
    },
    {
      "text": "want to construct everything so that I",
      "start": 3613.48,
      "duration": 4.76
    },
    {
      "text": "show you a whiteboard approach um and I",
      "start": 3615.2,
      "duration": 5.159
    },
    {
      "text": "also show you presentations and I also",
      "start": 3618.24,
      "duration": 4.559
    },
    {
      "text": "show you the code files I'll be sharing",
      "start": 3620.359,
      "duration": 4.48
    },
    {
      "text": "this code file and also this code file",
      "start": 3622.799,
      "duration": 3.601
    },
    {
      "text": "with you so that you can play around",
      "start": 3624.839,
      "duration": 3.881
    },
    {
      "text": "with it have access to it please comment",
      "start": 3626.4,
      "duration": 3.719
    },
    {
      "text": "in the chat if you're liking these",
      "start": 3628.72,
      "duration": 3.399
    },
    {
      "text": "lectures because then I will modify",
      "start": 3630.119,
      "duration": 4.521
    },
    {
      "text": "adapt it accordingly and as I say many",
      "start": 3632.119,
      "duration": 4.0
    },
    {
      "text": "times the most important thing is",
      "start": 3634.64,
      "duration": 4.24
    },
    {
      "text": "showing up for these lectures uh don't",
      "start": 3636.119,
      "duration": 4.761
    },
    {
      "text": "lose interest don't lose motivation and",
      "start": 3638.88,
      "duration": 4.52
    },
    {
      "text": "keep on learning along with me thanks so",
      "start": 3640.88,
      "duration": 3.919
    },
    {
      "text": "much everyone and I look forward to",
      "start": 3643.4,
      "duration": 5.32
    },
    {
      "text": "seeing you in the next lecture",
      "start": 3644.799,
      "duration": 3.921
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch Series today I am going to cover a very very important topic and that topic is called token embeddings let me highlight this over here so today we are going to learn about this concept called token embeddings people also call these as vector embeddings or word embeddings calling them Vector embeddings is fine but word embeddings is not entirely accurate uh tokens Can Be words can be subwords or even can be characters and token is a more broader term and that's why I prefer to use the word token embeddings so what are token embeddings and why are they so important let's get started with today's lecture if you look at this workflow of how large language models actually work there is an input text and let's say the input text is this is an example what happens next is that the input text is broken down into tokens let's say this is the one token is is the second token as is the third token and example is the fourth token this is an example of a word based tokenizer so each token will be one word but the way tokenizers actually work in model such as GPT is that GPT uses a bite pair encoder as a tokenizer which is a subword tokenizer which means that even parts of words or even characters can be individual tokens but that's not the main focus of today's lecture if you want to understand about tokenizers in detail we have covered that in one of the previous lectures and we have also seen about bite pair encoding in one of the previous lectures so tokenizing the word is the step number one then comes step number two which is converting these tokens into token IDs so every token is converted into token IDs and then comes step number three we don't just stop at these token IDs token IDs are converted into something which is called as token embeddings and these token embeddings then serve as the input to training the large language model such as the GPT and and then there are number of postprocessing steps and then comes the final output so today we are going to look at step number three after you generate the tokens after you generate the token IDs in Step number one and two what are token embeddings why do you need them and why this third step is so important especially when dealing with language so as I have mentioned here today we are going to learn about step number three which is creating token embed ICS awesome so I have broken down today's lecture into 1 2 three um four to five different five to six different modules and we are going through we are going to go through some Jupiter notebooks some code and Ive also constructed some presentation specially for today's lecture the reason is that I want this lecture to be very comprehensive I have seen so much content out there which really does not motivate the concept of token embeddings it does not show small practical demonstrations all of this is going to be covered in today's lecture so we are going to start with a conceptual understanding of why token embeddings are important then we are going to see a small Hands-On demo where we'll play with token embeddings to give you an intuitive feeling and then we are going to look at how are token embeddings created for large language models so let's get started with today's lecture in which the first part which I'm going to cover is conceptual understanding of why token embeddings are needed so as I mentioned I've created a separate presentation in this build llms from scratch series and this is titled what are token embeddings and why do we really need them so uh okay let's get started with this problem of you have words right and you want these words to be input to the machine learning model or to the large language anguage model let's say but computers can't understand words right so you need to represent the words in the format of numbers so let's say we assign random numbers to each word so let's say cat is 34 book is 2.9 tablet is minus 20 kitten is -13 so let's say in the llm framework which we just saw we have already converted the tokens into token IDs right and we have maintained the vocabulary so let's say our vocabulary consists of all the tokens and a token ID corresponding to each token so then these token IDs itself can be the training input to the large language model so then why do we need this step number three so yeah words can be represented as numbers and I have already converted tokens into token IDs like this so why can't I just use token IDs as the input to the model there is a reason for this we cannot just use randomly assigned numbers and the main problem is that the beauty of language is that some words are related to other words for example cat and kitten cat and kitten they are related right um dog and puppy they're related words but just assigning random numbers or token IDs to each word does not really capture the semantic meaning between these individual words so cat and kitten are s ically related however the associated numbers 34 and minus33 does not capture this relation that's one of the major problem of just using token IDs and I want to explain this to you in another way uh when all of us look at this image we see that it's a cat but do you know why convolutional neural networks work so well because convolutional neural networks don't just use the pixel values and stretch it out as one input vector they actually encode the spatial relation between the pixels so the these two eyes are close to each other right uh the whiskers are closer to the eyes these two ears are close to each other that is an information which is contained in the image itself and we should exploit this information when we give in when we feed the input to the model if we don't exploit the information which is inherently present in the image we are not doing an optimal thing I could just take these pixels here convert them into numbers and feed them as input but then I won't extract the information which is already available to me in this image like which parts are closer to each other the ears are closed the eyes are closed the nose is closer to the eyes Etc that's why convolutional neural networks work so well because they exploit the spatial relation between pixels and they exploit this information inherently present in an image consider text when you look at sentences we as humans are able to understand what sentences mean because words carry meanings and there are some words which are closer in meaning to other words this is the inherent advantage in the text which we need to exploit we need to exploit the fact that cat and kitten are closer to each other dog and puppy are somehow closer to each other in meaning if we don't exploit this information we will train for a huge amount of time and we won't be doing an optimal uh machine learning training words are beautiful they carry meaning so then why not exploit the similarities in meaning between different words so then you might be thinking okay what about one hot encoding so I'll take every word so I'll first have a huge vocabulary of all possible words and then I'll assign one hot encoding so dog would be 0 00 0 let's say one and then rest will be zeros so to every word there will be all zeros but there will be only one one so let me do one hot encoding for every word so dog will be this puppy will be another one hot encoding but this also leads to a similar problem with random number assignment one hot encoding also fails to capture the semantic relationship between words let's say for example if you see dog and puppy how do you know from this one hot encoding that dog and puppy are more closer to each other there is we don't encode this information at all and again it leads to the same problem words have meaning and why don't we exploit this meaning when we construct uh the inputs to be given to the large language model so assigning random token IDs does not work one hot encoding does not work okay so then how do you encode semantic relationship or how do you encode the semantic meaning when you are going to convert these words to numbers then came the idea that what if every word was encoded as a vector this is going to be very important I'm going to take four words here dog cat apple and banana and I'm going to say why don't you encode every word as a vector then you might be saying what should be the dimension of this vector Vector is it a two dimensional Vector is it a three dimensional Vector is it a thousand Dimension Vector then I will say well what if the dimension is determined by features then you might ask okay which features then I'll say that okay let's take these four words dog cat apple and banana and I look at these five features has a tail is eatable has four legs makes sound is a pet and based on these questions so first I'll ask a question does it have a tail is it eatable does it have four legs does it make a sound is it a pet based on the answers if the answer is yes it has a tail the value of that feature will be very high if the answer is yes it's eatable the value of that feature will be very high this is how I will construct these vectors so let me show you what I mean in some detail so let's say you look at a dog right look at the values which are very high has a tail this value is very high has four legs its value is high makes sound its value is high is a pet its value is very high great look at cat has a tail it's again the value is very high has four legs makes sound and is a pet so now if you see dog and cat you will see that they are kind of closer to each other right because whichever values are high in a dog are also higher in a cat as well look at the first index has a tail it's higher in both dog as well as cat look at the second index it's low in both dog as well as cat look at the last index is a pet it's high in both dog as well as cat now let's look at the vector representation for apple and banana for apple and banana has a tail has four legs and is a pet are very low but what is high is is eatable makes sound is also very low so if you look at apple and banana you definitely see that they are closer to each other right because whatever is high for apple is also high for banana whatever is low for apple is also low for banana so the good thing is that if we represent words as vectors and if we construct these vectors in a smart manner vectors can capture semantic meaning which was not captured before when you did one hot encoding or when you did random number assignment the semantic meaning was not captured only when we represented words as these vectors was the semantic meaning between different words captured one more thing to note here is that dog and cat are similar or closer to each other than let's say dog and banana if you compare dog and banana you'll see that whatever is higher in the dog let's say has a tail is lower in a banana whatever is higher in banana let's say is eatable is lower in a dog So based on Vector representation you can group Words which are similar to each other and also see which words are farther away from each other isn't this awesome similar to how we encoded the information inherently presented present in an image while feeding the input to a convolutional neural network what we are doing here is that we are saying that what is the information inherently present in text and that information is that textual words have semantic meaning so then why don't we convert these words into vectors that can capture this meaning great so these are called as vector embeddings and they are also called as token embeddings because every token is converted into a vector embedding that's where the word or that's where the I would say phrase token embeddings actually comes into the picture so the first point to take away from this lecture is that vectors can definitely capture semantic meaning now the next question is how do you construct these vectors how do you make sure that okay how how do I make these vectors so that let's say dog and puppy are closer dog and cat are closer but dog and banana are farther apart how do I make these vectors and that's all I'm going to tell you in the next part of this lecture how do you come up with these vector embeddings or token embeddings and uh the answer here the real simple answer is that we have to train a neural network to create Vector embedding so for example we have information which are all the tokens and we have some output and uh based on this information and the output we have to train a neural network to make sure that the vector embedding is correct where does this information come from it's from text So based on the sentences in a textual document we know which words are closer to each other which words are similar to each other and those should have similar vectors that's the training data and we train a neural network to construct a vector embedding uh and I'll explain this to you in a bit more detail but just know this that creating these Vector embeddings is not easy because I'm just showing four words right now but imagine there is a vocabulary of 50,000 words when gpt2 was trained it had a vocabulary of 50,000 words and you have to create a vector embedding in for these many words remember how how computationally expensive that would be and that's why training GPT takes a huge amount of time okay so this brings me to the end of the presentation where hopefully I wanted to convey two points first is that words carry meaning and to give these words as input to the large language models we need to exploit this meaning if we just use random token IDs or if you use one hot encoding this cement IC relationship or meaning between words is not exploited but we saw a glimpse of if words are represented as vectors maybe we can incorporate this semantic relationship between the different words incorporating words or representing words as vectors so that the semantic relationship is preserved is called as Vector embedding and it's also called as token embedding in the last part of this PPT we saw that creating these vector or token embeddings is not easy because you need to train a neural network to make sure that the right Vector embedding is created great so now we have finished the the first aspect of our agenda today which was essentially to show you all a conceptual understanding of why token embeddings are needed now what we are going to do is that we are going to see a small Hands-On demo so that you improve your conceptual understanding of to embeddings this demo is not related to the main code file which we are developing for uh building a large language model but it's just a toy demo uh which I have constructed over here so uh let's get started with this demo many big companies like Google already have pre-trained token um embeddings which means that this word to W Google News 300 let's search about it a bit so uh Google so there is a Google News data set which has about 100 billion words so this word to W Google News 300 are already pre-trained vectors on this huge data set so Google has already trained uh or trained the neural network to create this Vector embeddings so what has been done is that we get the Google News data set with 100 billion words and then we do the training to map every word to a vector now what is this 300 the 300 is B basically the number of Dimensions when we create token embeddings or vector embeddings words are mapped into a large dimensional Vector space in the demonstration which you just saw this was a five dimensional Vector space but five dimensions are not really enough to capture all the meaning so here we are using a 300 Dimension word to W which means that every word is transformed into a 300 dimensional vector and then we train based on the underlying data so that the semantic meaning between the words is preserved this is already a pre-trained data set when GPT was built or when large language models are built they don't use a pre-train data set they train these embeddings uh along with training the large language model itself I'll come to that in a moment but for now for the sake of this demonstration just know that I'm already using pre-trained word to we which means that this model it can take any word as an input and convert it into vectors 300 dimensional Vector so what I'm going to do now is that I'm going to assign a dictionary which is word vectors and uh equal to model so model is basically the word to Vector embeddings in this word to Google News 300 and then word vectors is the dictionary how is it a dictionary basically it will be uh every there will be words in this dictionary and then every word will be assigned to a 300 dimensional Vector so let's see what the vector for computer looks like so if you print this out you will see this is the vector for computer it's a 300 Dimension Vector does not mean anything right now for now just know that it's 300 Dimension vector and if you print the vector shape for any word you'll see that it's 300 which means that every word is encoded into a 300 dimensional Vector that's fine what I want to show you now is I want to prove to you that well trained Vector embeddings actually the semantic meaning right so king plus woman minus man what do you think this should be just tell me the first thing which comes to your mind uh you can pause here right now and think about if words are actually if vectors are actually encoding the meaning between words and if I have a vector for King let's say if I have a vector for woman and if I have a vector for man and if I add the vector for King with the vector for woman and then I subtract the vector for man what should I be left with okay I hope all of you have got the answer so I should be left with something which resembles similar to a queen because king plus man in in ideally should be equal to Queen plus woman let's say queen plus king plus woman should be equal to man plus Queen so king plus woman minus man should ideally be Queen why because we take a masculine aspect we subtract another another masculine aspect so what should remain is only a feminine aspect and woman and a man are there so ideally the answer to this should be Queen right uh then we will be satisfied that the vectors are indeed encoding some meaning so let's try to do this so we are going to uh what we are going to do is that we are going to add King and woman and we are going to subtract man and then we are going to print out the words which are the most similar to the answer and when you print this you will see that indeed Queen is the answer of this and the vector uh and the probability of getting queen as the answer is around 71% which means that uh here is a list of top 10 answers and out of this queen is the most preferred answer so this is the first indication to all of you that if you convert words to vectors and then you do addition and subtraction of these vectors essentially you are encoding some meaning here so somehow the vectors have this information that the vector for King encodes some masculinity the vector for woman encodes some femininity the vector for man encodes some masculinity these vectors also have the meaning that somehow king and queen are closer to each other somehow man and woman are closer to each other isn't that amazing this really blew my mind when I knew about this for the first time and when I knew about this these things such as as one hot encoding just seemed so boring because in one hot encoding no information is captured no meaning is captured but if you actually convert words to vectors and preserve meaning you can get some you actually get the meaning preserved I was not really sure that the meaning will be preserved as vectors but it it is preserved we can also do couple of other things uh ideally woman and man should be closer to each other king and queen should be closer uncle and Aunt are related boy and girl are related nephew and niece are related paper and water are related what we can do is that we can now check whether the vector embeddings are also related to each other so what we do is that we convert these words into vectors and test the similarity between vectors the way it's done is by I think finding the distance between the vectors so what I do here is World vectors. similarity woman and man and I do the same for all these other words so if if you look at the answers you'll see that for the first five the similarity score is pretty high because woman man king queen uncle aunt boy girl nephew niece are closer to each other awesome right but if you look at the last two words paper and water they are not closer to each other they are not related at all and our vectors are capturing this meaning let's say there are there is a vector for paper somewhere there's a vector for water somewhere these vectors are so far apart that they are not related to each other and that's why the similarities score is very low in fact if you see woman and man king and queen uncle and Aunt boy and girl nephew and niece these vectors are closer to each other because they capture the meaning but paper and water are not close to each other at all and that's why the similarity score between these two vectors is low we can also do some cool things like we can find Words which are similar to a given word so if you look at Tower and then look at the vectors which are most similar to Tower so the answers are scraper Tower Spire uh Etc we can also see some other things like similarities between man woman semiconductor earthor nephew n Etc and here we can see that the magnitude of the difference between the man and woman so this is a vector difference and if you see np. lin. Norm so this is finding the norm of the difference in difference in these two vectors so you'll see that the magnitude of the difference between the man and woman is 1.73 the magnitude of the vector difference between nephew and N is 1.96 but the magnitude of the vector difference between semiconductor and earthor is 5.67 this is another indication that the vectors actually encode some meaning and if you find the magnitude of the difference between the vectors it's an indication of how closer in meaning the words are isn't that amazing let me repeat that again if you take two vectors and if you find the magnitude of the difference between the vectors that's an indication of how how close or how far the words are in their meaning so when you do Vector embedding or when you do token embedding the beautiful thing is that you actually retain the information or retain the meaning uh of words and then you feed these embeddings into the large language model and that makes a huge amount of difference instead of let's say just feeding word one hot encodings awesome I hope everyone is with me until now now I could have directly started with step number three but I wanted to show you this small Hands-On demo so that you get an intuitive feel that if Vector embeddings are trained nicely like they done in this word twek Google News model we can actually encode meanings in these vectors awesome so I hope in point number one and two I have been successful in making you understand what is the need for token embeddings and that if token embeddings are created successfully they can indeed encode some meaning great now let's come to the third point which is how are token embeddings created for large language models so the way this is done is that we start with the vocabulary we start with a vocabulary for large language models and then we have tokens in that vocabulary and then we have token IDs so let me show you so this is the vocabulary so the first step what is done is that we take the Vo vocabulary and we have token IDs every token ID is converted into embedding vectors so if you have uh if you see this is the output this is also called as the embedding M embedding weight Matrix don't worry about this right now uh there are two things you need before you construct this Matrix you need first of all the vocabulary size and second thing you need is the vector Dimension so you need how many Dimension vector uh is the embedding going to be so for example let me actually ask let me go to chat GPT and let me ask chat GPT what was the vector embedding dimension for training gpt2 also what was the vocabulary size vocabulary size means how many tokens were there and how many token IDs was there okay so remember this the vector embedding dimension for gpt2 was 768 and uh for the smallest model and for the largest model it was 160 so let's stick with 768 for now and the vocabulary size for gpt2 was 50257 so I'm going to go here right now and let's look at how the embedding Matrix was then constructed so the vocabulary size was 50257 right which means gpt2 had these many tokens those were subwords uh made through bite pair encoding so there are 50257 tokens and token IDs so token IDs went from 01 2 3 up to 50257 50257 awesome and then what we are going to do is that for each of these token IDs each of these token ID which corresponds to one token there would be a vector and the vector Dimension was 768 in this case so for the zero token ID there will be 768 a vector of 768 dimensions for the token ID of one there will be a vector of 768 Dimensions similarly for the token ID of 50257 there will be a vector of 768 Dimensions so for every token ID there will be a vector of 768 Dimensions so think of the size of this embedding layer weight Matrix right for let's say if you look at the first token ID there will be 768 weights because when you construct the vector it has 768 dimensions for the second token ID which is token ID 1 it also has 768 weights similarly if you reach to the end 50257 this token ID and the token corresponding with it has 768 weights so the number of tokens in this token in this embedding Matrix is 50257 into 76 so these are the weights okay and this is called as the embedding layer weight Matrix this is extremely important so once you get the token IDs so if I go to this flow map right now once you get the token IDs you convert these token IDs into an embedding layer weight Matrix and initially when this weight Matrix is initialized we do not know how the vectors are right what I showed you over here the word model it's a pre-trained model but now I'm going to tell you how is it actually trained so before so you just now know the size of this Matrix that if you are the person training gpt2 you know that okay I have to ultimately create an embedding layer weight Matrix which has 50257 rows and which has 768 columns but you you don't know what each weight value will be so then what do you do what you do is that you initialize the embedding weights with random values that's the First Step so all of these 50257 into 768 values will be initialized randomly step number one this initialization serves as the starting point for the llm learning process then what do you do these weights are then optimized as part of the llm training process this is extremely important when gpt2 was trained these values were not known before what were the ideal weight values a training process was implemented where we had these many parameters 5257 by 768 parameters were there and each of these weight parameters were optimized during the training process and that is how vector embeddings or token embeddings were created how was the optimization process done we had the training data and based on the data we knew which words were closer to each other which words were farther apart from each other for example when we looked at this word to the training data was Google news right and we had so the Google news of course if we had 300 billion words we have the information that king and queen are similar man and woman are similar so that training data is used as underlying information to modify each of these parameters so if you know about neural network training similarly back propagation is implemented here to optimize all of these weights of the embedding layer weight Matrix and that is how token IDs are converted into vector embeddings so if you think about it at the heart of it large language models are just giant neural networks right and one part of this giant neural networks is training and embedding layer weight Matrix so if you look at this uh uh this graphic here so token embeddings are fed as an input to to the training right that's what this graphic shows but during the training of the GPT while it's training to predict the next word we also train that embedding itself so the embedding neural n network is trained and then we also train the prediction for the next World so there are two trainings actually which are kind of going on here for now all you need to remember is that uh words so we start with a vocabulary such as for gpt2 we start with this vocabulary of 50257 tokens and then we have to decide that okay when I do the vector embedding what the dimension of the vector I want and then that's 768 then that decides the size of your embedding layer Matrix so we have 5 0257 rows and we have 768 column for each token ID we are going to have a vector with 768 values how are these values decided through training through back propagation we start out with initializing these values of the embedding layer in a random Manner and then all of these values are optimized during the training process uh I hope you have followed until this point because now I'm going to take you through code and we are actually going to learn a bit about these token embeddings and we are going to learn how to essentially create um this embedding layer Matrix which we just saw over here so please keep this image in mind and remember that there are two Dimensions which are important the vector Dimension which is the size of the each vector and the vocabulary size okay so let us illustrate how the token ID to the embedding Vector conversion works with a Hands-On example so let's say we have the four input tokens which are input number or ID number 2351 remember these are token IDs so every token ID is associated with a word uh to give you uh like a concrete feel for this let's say the example is uh so I'm going to look at 0 1 2 3 4 5 so I need six words actually um so here it's going to be let's say my sentence is quick uh my sentence is quick fox is in the house let's say this is my sentence so what I will do is first convert this into tokens so let's say quick and I'm just showing word tokens word based tokenizer for Simplicity let's say quick is one token Fox is one token is is is one token in is one token the is one token and house is the next token then we arrange these tokens in ascending order and then assign them token IDs so house will probably come first with a token ID zero then uh no I think Fox would come first with a token ID zero house would come second uh with the token ID one I hope I'm not Mak making any mistake here then in will come two is will come three quick will come four and then the will come five right and now what I want to do is I want to encode uh or I want to rather convert token ID 235 and 1 into embeddings or into Vector embeddings so what is ID number two is this word in 2 three is five and one so I want to convert these words in is the and house I want to convert them into embedding vectors 2 3 5 and 1 right so 2 3 5 and 1 correct so then my input IDs are tor. tensor 2351 remember we are using tensors here because ultimately we are going to use back propagation to optimize the embedding layer weights so it's much better to represent everything as tensors great so these are my input IDs and now for the sake of Simplicity we are going to use only small vocabulary of six words remember gpt2 had 50257 tokens in the vocabulary right now just for Simplicity we are just going to use a small vocabulary of only six tokens instead of the 5257 words in the BP tokenizer and let's say we want to create embeddings of size three uh so here see GPT to 768 right so here I told you two two Dimensions were important the size of the vocabulary which now we are assuming six and the vector Dimension so in this uh code file I'm assuming the vector Dimension three so what I'm going to do here is that for each of the words in my vocabulary which are these words so for now I'm starting with these six words for each of these six words in my vocabulary I will have a vector and that Vector will have three dimensions okay this is how I'm going to construct the vector embedding so the vocabulary size will be six what are the six words quick fox is in the house then output Dimension will be three which means that every token of this vocabulary will be converted into a vector of three dimensions how that is done in practice is that we create an embedding layer and then we use tor. nn. embedding and uh then what we do is we we pass in two arguments the vocabulary size which means the number of words which need to be converted into embeddings and the output Dimension which is the dimension of each Vector embedding uh awesome right so this is how we use the tor. nn. embedding let me show you this in Python right now so if you look at the embedding documentation you will see that it's a simple lookup table that stores embedding of a fixed dictionary and size what this means is that we need to have a vocabulary and we need to give the size of the embedding Vector that's it and then it creates a dictionary I'll show you why is it called a lookup table and I'll tell you okay why does the word lookup table come into to the picture for now all you need to remember is that the way we initialize these Vector embeddings is tor. nn. embedding and this initialize the weights of the embedding Matrix in a random manner right so here I showed you that every weight in this vocab in this embedding Matrix is initialized randomly so let me again show this to you here now you see I have six IDs right so 0 1 2 3 4 and five and each of these will have a three dimensional Vector associated with it 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 so I will have essentially six rows and I will have three columns this is my embedding layer weight Matrix and all of these values are initialized randomly how can you get this values you then just have to type embedding layer. weight What will what this will give you is it will give you all the weights which are initialized through the embedding layer so when you print this you'll get this and you'll see it's exactly the same size as what we had shown over here it has six rows and three columns here also you see we have six rows and three columns every row here corresponds to the vector associated with that token ID so this is the three-dimensional Vector with the zero token ID this is the threedimensional vector with the first token ID this is the threedimensional vector of the second token ID Etc uh so now you can see a tensor has been returned and these are the initial weights which need to be optimized so as we can see here the weight Matrix of the embedding layer consists of small random values initially and these are the values which are optimized during llm training as part of the llm optimization itself which we will see in further chapters so when the llm is optimized there are actually two broad level things which are optimized first is the embedding layer weights and second is actually the we uh which are needed later to also predict the next word I'll come to that in one of the upcoming chapters moreover we can see that the weight Matrix has six rows and three columns as we saw over here and there is one row for each of the six possible tokens in the vocabulary which we already discussed each row is essentially the vector embedding of each token or each token ID great now what we can do is that uh once this embedding layer has been created right uh what I want to show you is that how can we get the vectors for each ID and that's actually pretty simple because this this is the first row is the vector for the zeroth ID the second row is the vector for the first ID Etc so let's say uh if you want to get the vector for ID number three uh let me show this to you in code ID number three is is right and if you you want to get the vector for is how do you do it you first look at ID number three so ID number 0 1 2 3 so this this is that ID number right so then all you need to do is look at this corresponding Row in the embedding weight Matrix that's why it's called a lookup table to find the vector associated with a particular ID you just need to take this Matrix you look at the vector corresponding with that particular ID row number that's it this is exactly what we are going going to do over here uh we want to obtain the vector representation for ID number three right so that's what we are going to do we are going to access the embedding layer it's a lookup table and we are going to access the uh embedding Matrix for the token ID 3 and what will this be this will be the fourth row because the zero ID is the first row the first ID is the second row second ID is the third row and third ID is the fourth row that's it and then when you print this you will get this vector so this is the vector which is the vector embedding for that particular ID which is ID number three so I also written this over here if we compare the embedding Vector for token ID3 we see that it is identical to the fourth row so look at this this Vector it's exactly the same as the fourth row right uh in other words the embedding layer is essentially a lookup operation that retrieves rows from the embedding layers weight Matrix via a token ID let me explain this in simpler words the embedding layer is essentially just a lookup operation and what this lookup operation does is that if you give it an ID number it looks for that particular row and retrieves a vector for you so for example if you want the vector for ID number five all you will need to do is look at this particular row which is row number six and then it will retrieve or it will give you that particular Vector if you look at if you want the vector for ID number Z just look at row number one if you look at the vector for ID number or if you want the vector for ID number one just look at row number two so that's why you can so the embedding weight Matrix is of course a matrix of the weights for Vector embeddings but it's also a lookup table if you specify the ID number you can use the embedding weight Matrix to find the exact Vector representation for that particular ID so that's why if you look at the P documentation this embedding is also called as a simple lookup table I hope you have understood this right now okay so uh one major portion of this lecture was for you to understand the embedding weight Matrix and why it is actually considered to be a lookup table awesome now let's come to the next part so previously we have seen how to convert a single token ID into three dimensional embedding right we just gave a single token ID and converted it into an embedding vector but remember what we started from I wanted the vector representations for these four IDs so I wanted the vector representation for ID number uh one ID number five ID number two and ID number three so how can we give these four to the lookup table we just specify the particular array so here we have the input IDs right uh we have the input IDs for which we want the vector representation all we do is that we just use the embedding layer and pass in the input IDs so similar to what happened here what this operation does is that it first looks at the input IDs and it sees that there are actually Four values in the input ID then what it does it goes through each individual ID and looks up the embedding Vector for that ID that's it so when you pass in the input IDs it will first look at uh this thing and it will look at first it will look at row number three then row number four row number six and row number two two so it will look at row number four row number six row number three and row number two and then it will print out the answer so essentially each row in this output Matrix is the corresponding Vector embedding for that particular ID so the only thing which you have to remember right now is the embedding layer is a lookup Matrix and you can pass a single ID to this lookup Matrix you can even pass multiple IDs or a group of IDs and in just one line of command you can get all the vector embeddings for that particular token ID this is how embedding layer is actually implemented in practice right now uh small random values have been initiated but um we are going to train these values so that they actually capture the meaning and that is what we'll come to in one of the subsequent lectures okay so let's see how much of the lecture we have covered so far we covered this part with where we saw that uh we saw that the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer weight Matrix using a token ID here is an image which also explain this so let's say this is the weight Matrix uh embedding Matrix and let's say uh these are the token IDs which we want to embed or we want to find the vector representations so if you actually pass in these token IDs to the embedding layer what it will do is that it will first look at each particular ID so it will look at ID number two which means it will go to row number three which is highlighted in blue that will be the first answer of this lookup table then it will look at ID number three and that will mean row number four that is the second row of the final answer then ID number five which is essentially row number six and then uh it will give the vector corresponding to row number six and finally uh ID one which means row number two so then it will give the vector corresponding to row number two that's it so this is the embedding weight Matrix and then it just looks at the particular row based on these IDs and then it gives the vector embeddings for all the IDS which we asked for so here we asked for fox jumps over dog and then it gives the vector embeddings for all those IDs so if someone asks you what's an embedding layer you can say that it's a simple lookup operation that retrieves the vector for the particular token ID that's it now I want to show you one last thing actually uh it is a bit of a finer detail but I think it's important for you to also think about an embedding layer in some another dimension right so let's say we have the following three training examples let's say we are we want to have the embedding for ID number two ID number three and ID number one so let's say there are four words in our vocabulary and uh which means that 0 1 2 3 these are the IDS with these four words and we want to encode each of these IDs into a vector so uh let's say the embedding Dimension is five so each of these each of these IDs will have a vector with basically 1 2 3 4 five with five Dimensions so the ID number zero will have a vector embedding of five dimensions ID number one will have a vector embedding of five Dimensions Etc ID number two will have a vector embedding of five dimensions and ID number three will have Vector embedding of five Dimensions so the embedding Matrix which we create we pass in the first argument the first argument is the vocabulary size which is the four number of rows and then the second argument is the embedding Dimension so that is five right and so then if you print out the embedding weights you will see that and you can even try this in python the embedding weights will be these which are initialized to random values and U you'll see that there are four rows and five columns because we have four IDs in the vocabulary and each ID has five um has a vector with five Dimensions this is great then what we do is that we just retrieve those weight Vector weights which we need so then we passing this idx so we only need vectors for ID is 2 3 1 we don't need the vector for ID Z so then these are the IDS which are passed and then we do the lookup operation and then this is the embedding table which is extracted or the embedding Matrix which is extracted for IDs 2 3 and 1 now one thing which I want to explain to you that this embedding layer is actually the same as a neural network linear layer so uh I'll try to explain this quickly because I don't want to divert you from the main purpose of the lecture but let's say if you have a neural network with four inputs uh four as the input Dimension and then you have three batches of inputs coming in the first batch is basically the first ID number which is two and encoded as a one hot representation the second batch is the ID number uh so let's see the IDS which we needed ID is 231 right so the second batch is the ID number three which is encoded as the one hot Vector 001 and the third ID is the ID number one which is encoded as 0 1 0 if these three are the input batches which are fed to this neural network and let's say there are five neurons here the output of this linear layer is X into W transpose so what is w every neuron here will have four weights associated with it because every input has four dimensions if you look at the first input it's zero 0 1 0 it has four uh four dimensions so every neuron here will have four weights associated with it so if you look at this weight transpose Matrix the First Column will be the weights of the first neuron the second column will be the four weights of the second neuron the third column will be the four weights of the third neuron fourth column will be the four weights of the fourth neuron and the last colum will be the four weights of the fifth neuron so let's say if x is the input Matrix W transpose is the weight Matrix when you do X into W transpose you will get essentially a matrix which has three rows and uh it has five columns why three rows because for every input we have three inputs input number input number one input number two and input number three and for each input we want a vector embedding with five Dimensions so there is three rows and there is five columns so each row corresponds to the vector embedding for that particular input now if you look at this if you look at this output look at the first row 0. 6957 and if you look at this output you'll see that it's exactly similar in fact both these outputs are completely similar so what is exactly happening here what the embedding Matrix is actually doing underneath is that it's the same operation as a neural network linear layer so what an embedding does is actually is that you have inputs right so let's say we have three tokens three token IDs those are converted into one hot representations they are fed into a neural network with five neurons why five because the vector Dimension is five and then we have a linear layer whose output is X into W transpose this gives the same output as the embedding layer so earlier I showed you tor. nn. embedding right here I'm showing you tor. nn. linear then you might be thinking why is nn. linear not used used to define the embedding Matrix the reason is it's not used is because so both embedding layer and NN layer lead to the same output both embedding layer and the NN linear layer lead to the same output but embedding layer is much more computationally efficient because in the NN layer we have many unnecessarily multiplications with zero so you could just use NN do linear operation over here and do the X into W transpose but here you see you have to do one hot encoding so there are many zeros and a lot of unnecessary computations are there these unnecessary computations really scale up when we are dealing with vocabulary sizes of chat gp2 gpt2 and that's why we use the embedding layer here so there is also the torch. nn. linear this layer is also there there are two layers and even you can use nn. linear to create the embedding Matrix but the reason it's not used is because it's not efficient so the embedding layer is much more preferred or the nn. linear layer when you create the embedding Matrix this is just a small takeaway for anyone who is familiar with neural networks but if you are not don't worry if you did not understand this part mostly I wanted to cover some other major points in today's lecture and some of them are first I wanted you to understand the conceptual understanding of why token embeddings are needed in convolutional neural networks we exploited the spatial features of an image before giving it as as input for training this is exactly what we do in uh token embeddings words can be represented as vectors and those vectors can carry meaning that is called as Vector embedding or token embedding and if the token embeddings are trained properly we show I showed you an example where the words the vectors can actually carry meaning so if you have Vector for King which encodes some masculinity if you have Vector for woman which encodes some femininity and we subtract the Vector for man which also encodes masculinity the answer is a vector which encodes femininity which is Queen what also we can do is that we can show that if you take two vectors of Words which are similar to each other and if you take the magnitude of the difference between those vectors that magnitude is much lesser than words which do not mean anything which means that if you if you have vectors for Words which are similar to each other they might be closer together in space so if embeddings are created nicely they can actually encode the meaning between words I found this concept very hard to understand so I wanted you to First understand that it is possible to have vectors in such a way that they encode meaning many people don't even understand what does it mean that vectors have meanings so the first two points in today's lecture were devoted for you to get an conceptual understanding of why token embeddings are needed then we looked at a practical aspect of how token embeddings are are created to create a token embedding Matrix you need two parameters you need your vocabulary size and you need the vector dimension for gpt2 the vocabulary size was 50257 and the vector Dimension was 768 so you essentially have an embedding weight Matrix which has 50257 rows and 768 columns for each token ID in the vocabulary you have to construct a vector now how are these weights of the embedding Matrix determined they are initialized randomly these weights are initialized randomly and then the embedded weights are optimized as part of the llm training process that's very important uh so this this is how the embedding weight Matrix is created but what's also quite interesting is that at the heart of it the embedding weight Matrix is just a lookup operation which means that if you have a embedding weight Matrix if you have an embedding weight Matrix you can just pass in the input ID or the token ID for which you want the vector embedding and then you get it corresponding to the particular row you can even pass in a bunch of input IDs and then the embedding layer will just look for that the row corresponding to the input ID and retrieve the vector embedding for you so simple way to look at the embedding layer is that it's essentially just a lookup operation that's it now towards the end we also saw one more thing that whatever the embedding layer does can actually be done using the neur oral Network linear layer but the reason it's not preferred is because the embedding layer is much more computationally efficient awesome in this lecture I have not covered how to train the embeddings but I just wanted to give you an overall understanding of what token embeddings are what is the embedding layer weight Matrix but in in subsequent lectures we are also going to see how to train the embedding layer so in this example which we saw I directly use the pre-trained word to Google news right later we'll also see how this pre-training is done and how gpt2 gpt3 and gp4 did the pre-training for the for creating the vector embeddings in the next lecture we are going to look at another important concept which is called as positional embedding so until now we looked at how to connot words into vectors right but when sentences are given the positioning of the sentence also matters a lot uh the cat sits on the mat so cat and mat are close by but if mat is somewhere far away they are not related so position of the words also matter a lot apart from their semantic meaning up till now the vector embeddings which I have showed you do not encode the position for where the word comes in the particular sentence but that is another uh feature of words and sentences which we are going to exploit in images we exploited transational invariance and we also exploited spatial similarities between features in vctor embeddings we have already exploited the semantic relationship and the semantic meaning between words but we'll also see how to exploit the position and where the words are positioned in sentences and that will be the subject of the next lecture where we are going to learn about positional embeddings thank you so much everyone I know these lectures are becoming a bit long but I deliberately want to construct everything so that I show you a whiteboard approach um and I also show you presentations and I also show you the code files I'll be sharing this code file and also this code file with you so that you can play around with it have access to it please comment in the chat if you're liking these lectures because then I will modify adapt it accordingly and as I say many times the most important thing is showing up for these lectures uh don't lose interest don't lose motivation and keep on learning along with me thanks so much everyone and I look forward to seeing you in the next lecture"
}