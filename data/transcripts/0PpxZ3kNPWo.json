{
  "video": {
    "video_id": "0PpxZ3kNPWo",
    "title": "Coding a fine-tuned LLM spam classification model | From Scratch",
    "duration": 2979.0,
    "index": 35
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.88
    },
    {
      "text": "hello everyone and uh welcome to this",
      "start": 6.2,
      "duration": 4.6
    },
    {
      "text": "lecture in the build large language",
      "start": 8.88,
      "duration": 5.48
    },
    {
      "text": "models from scratch Series today we are",
      "start": 10.8,
      "duration": 7.6
    },
    {
      "text": "going to continue with the fine tuning",
      "start": 14.36,
      "duration": 6.679
    },
    {
      "text": "based classification project which we",
      "start": 18.4,
      "duration": 5.039
    },
    {
      "text": "have been working on for the past three",
      "start": 21.039,
      "duration": 4.801
    },
    {
      "text": "lectures today we are going to look at",
      "start": 23.439,
      "duration": 4.881
    },
    {
      "text": "calculating the classification loss and",
      "start": 25.84,
      "duration": 5.88
    },
    {
      "text": "the accuracy and we will also implement",
      "start": 28.32,
      "duration": 5.84
    },
    {
      "text": "the training Loop today the testing Loop",
      "start": 31.72,
      "duration": 4.4
    },
    {
      "text": "and essentially we'll complete we'll",
      "start": 34.16,
      "duration": 5.32
    },
    {
      "text": "complete the entire fine tuning project",
      "start": 36.12,
      "duration": 5.64
    },
    {
      "text": "so let's get started with today's",
      "start": 39.48,
      "duration": 4.88
    },
    {
      "text": "lecture first I want to recap what all",
      "start": 41.76,
      "duration": 4.76
    },
    {
      "text": "we have covered so far in this",
      "start": 44.36,
      "duration": 5.56
    },
    {
      "text": "classification based fine-tuning handson",
      "start": 46.52,
      "duration": 6.519
    },
    {
      "text": "project initially we downloaded the data",
      "start": 49.92,
      "duration": 5.319
    },
    {
      "text": "set then we preprocessed the data set",
      "start": 53.039,
      "duration": 4.68
    },
    {
      "text": "and created data loaders to give you a",
      "start": 55.239,
      "duration": 4.48
    },
    {
      "text": "sense of what the data set actually was",
      "start": 57.719,
      "duration": 5.121
    },
    {
      "text": "let me scroll up a bit in the code to",
      "start": 59.719,
      "duration": 5.08
    },
    {
      "text": "show you the data set and how it looked",
      "start": 62.84,
      "duration": 4.4
    },
    {
      "text": "like so essentially the data set was a",
      "start": 64.799,
      "duration": 5.401
    },
    {
      "text": "Spam no spam email classification so you",
      "start": 67.24,
      "duration": 6.48
    },
    {
      "text": "can even check this from the UCL machine",
      "start": 70.2,
      "duration": 5.4
    },
    {
      "text": "learning repository we downloaded the",
      "start": 73.72,
      "duration": 4.48
    },
    {
      "text": "SMS spam collection data set from here",
      "start": 75.6,
      "duration": 4.6
    },
    {
      "text": "which consists of text messages which",
      "start": 78.2,
      "duration": 4.72
    },
    {
      "text": "are either spam or not spam upon",
      "start": 80.2,
      "duration": 4.879
    },
    {
      "text": "downloading this data set we saw that it",
      "start": 82.92,
      "duration": 4.159
    },
    {
      "text": "was imbalanced so there were around",
      "start": 85.079,
      "duration": 6.121
    },
    {
      "text": "4,800 no spam messages and only 747 spam",
      "start": 87.079,
      "duration": 6.801
    },
    {
      "text": "messages so then we balance the data set",
      "start": 91.2,
      "duration": 5.12
    },
    {
      "text": "so that in the no spam category and in",
      "start": 93.88,
      "duration": 4.44
    },
    {
      "text": "the spam category both there were there",
      "start": 96.32,
      "duration": 5.6
    },
    {
      "text": "are 747 messages that's the First Data",
      "start": 98.32,
      "duration": 6.119
    },
    {
      "text": "pre-processing step we did after that we",
      "start": 101.92,
      "duration": 4.799
    },
    {
      "text": "implemented data loaders when we",
      "start": 104.439,
      "duration": 4.401
    },
    {
      "text": "implemented data loaders",
      "start": 106.719,
      "duration": 5.921
    },
    {
      "text": "the entire data set was batched into the",
      "start": 108.84,
      "duration": 8.639
    },
    {
      "text": "input and uh the target so basically",
      "start": 112.64,
      "duration": 6.68
    },
    {
      "text": "imagine the data set that's being split",
      "start": 117.479,
      "duration": 5.0
    },
    {
      "text": "into training testing and validation 70%",
      "start": 119.32,
      "duration": 6.68
    },
    {
      "text": "for training 20% for testing and 10% for",
      "start": 122.479,
      "duration": 6.041
    },
    {
      "text": "validation if you look at the 70%",
      "start": 126.0,
      "duration": 5.2
    },
    {
      "text": "training data right now due to the data",
      "start": 128.52,
      "duration": 4.719
    },
    {
      "text": "loaders that data will be split into",
      "start": 131.2,
      "duration": 4.92
    },
    {
      "text": "input and Target so in the input we have",
      "start": 133.239,
      "duration": 5.161
    },
    {
      "text": "defined batches each containing eight",
      "start": 136.12,
      "duration": 5.08
    },
    {
      "text": "samples so there are around 130 such",
      "start": 138.4,
      "duration": 5.52
    },
    {
      "text": "batches of the training data and the",
      "start": 141.2,
      "duration": 5.2
    },
    {
      "text": "number of columns is equal to 120 which",
      "start": 143.92,
      "duration": 4.16
    },
    {
      "text": "are the number of token IDs",
      "start": 146.4,
      "duration": 4.559
    },
    {
      "text": "corresponding to each text message",
      "start": 148.08,
      "duration": 4.84
    },
    {
      "text": "to make sure that all the text messages",
      "start": 150.959,
      "duration": 4.041
    },
    {
      "text": "have similar number or exact same token",
      "start": 152.92,
      "duration": 4.76
    },
    {
      "text": "IDs we have padded the smaller text",
      "start": 155.0,
      "duration": 4.319
    },
    {
      "text": "messages with this token",
      "start": 157.68,
      "duration": 3.639
    },
    {
      "text": "50256 which is the token ID",
      "start": 159.319,
      "duration": 5.081
    },
    {
      "text": "corresponding to the end of text token",
      "start": 161.319,
      "duration": 5.041
    },
    {
      "text": "so this is the input tensor and here",
      "start": 164.4,
      "duration": 4.08
    },
    {
      "text": "what I'm showing is the target tensor",
      "start": 166.36,
      "duration": 4.64
    },
    {
      "text": "this just has zeros and ones so zero",
      "start": 168.48,
      "duration": 5.839
    },
    {
      "text": "means no spam and one means spam so when",
      "start": 171.0,
      "duration": 5.0
    },
    {
      "text": "you implement the data loader for the",
      "start": 174.319,
      "duration": 3.28
    },
    {
      "text": "training data it looks something like",
      "start": 176.0,
      "duration": 4.56
    },
    {
      "text": "this and it has 130 batch es when you",
      "start": 177.599,
      "duration": 6.0
    },
    {
      "text": "implement data loader for the validation",
      "start": 180.56,
      "duration": 6.0
    },
    {
      "text": "and the testing data the data is batched",
      "start": 183.599,
      "duration": 6.761
    },
    {
      "text": "into similar batches so we have 130",
      "start": 186.56,
      "duration": 6.16
    },
    {
      "text": "training batches 19 validation batches",
      "start": 190.36,
      "duration": 4.68
    },
    {
      "text": "and 38 test",
      "start": 192.72,
      "duration": 5.72
    },
    {
      "text": "batches so that's what we implemented uh",
      "start": 195.04,
      "duration": 5.199
    },
    {
      "text": "in this first three steps which was",
      "start": 198.44,
      "duration": 4.799
    },
    {
      "text": "downloading the downloading the data set",
      "start": 200.239,
      "duration": 5.08
    },
    {
      "text": "pre-processing the data set and creating",
      "start": 203.239,
      "duration": 4.961
    },
    {
      "text": "data loaders in the next steps what we",
      "start": 205.319,
      "duration": 5.881
    },
    {
      "text": "did was we took our model architecture",
      "start": 208.2,
      "duration": 4.48
    },
    {
      "text": "and the model architecture looked",
      "start": 211.2,
      "duration": 2.8
    },
    {
      "text": "something like this",
      "start": 212.68,
      "duration": 3.72
    },
    {
      "text": "initially what we did was we looked at",
      "start": 214.0,
      "duration": 5.36
    },
    {
      "text": "the final output layer and initially",
      "start": 216.4,
      "duration": 4.72
    },
    {
      "text": "that output layer looked like this",
      "start": 219.36,
      "duration": 4.84
    },
    {
      "text": "neural network which took in the input",
      "start": 221.12,
      "duration": 4.64
    },
    {
      "text": "equal to the size of the embedding",
      "start": 224.2,
      "duration": 4.72
    },
    {
      "text": "Dimension that's 768 and the output was",
      "start": 225.76,
      "duration": 5.96
    },
    {
      "text": "50257 which was the vocabulary size",
      "start": 228.92,
      "duration": 5.12
    },
    {
      "text": "since we are doing a classification task",
      "start": 231.72,
      "duration": 4.12
    },
    {
      "text": "what we did is we replaced this neural",
      "start": 234.04,
      "duration": 4.199
    },
    {
      "text": "network with this kind of a",
      "start": 235.84,
      "duration": 4.44
    },
    {
      "text": "classification head as the output",
      "start": 238.239,
      "duration": 4.0
    },
    {
      "text": "where the number of inputs to the neural",
      "start": 240.28,
      "duration": 4.36
    },
    {
      "text": "network is 768 but the number of outputs",
      "start": 242.239,
      "duration": 6.881
    },
    {
      "text": "is equal to two spam or no",
      "start": 244.64,
      "duration": 4.48
    },
    {
      "text": "spam by the end of the last lecture what",
      "start": 250.92,
      "duration": 5.36
    },
    {
      "text": "we saw is that if we pass in any input",
      "start": 253.48,
      "duration": 5.039
    },
    {
      "text": "to this modified architecture so let's",
      "start": 256.28,
      "duration": 6.08
    },
    {
      "text": "say if we pass in an input such as",
      "start": 258.519,
      "duration": 6.68
    },
    {
      "text": "uh um let me show you let's say we pass",
      "start": 262.36,
      "duration": 5.839
    },
    {
      "text": "in an input such as do you have time and",
      "start": 265.199,
      "duration": 4.881
    },
    {
      "text": "we pass this input to this mod ified",
      "start": 268.199,
      "duration": 4.801
    },
    {
      "text": "architecture the output will look",
      "start": 270.08,
      "duration": 4.6
    },
    {
      "text": "something like this",
      "start": 273.0,
      "duration": 4.44
    },
    {
      "text": "do you",
      "start": 274.68,
      "duration": 6.4
    },
    {
      "text": "have time and then for each of these",
      "start": 277.44,
      "duration": 6.319
    },
    {
      "text": "token there will be two outputs",
      "start": 281.08,
      "duration": 6.36
    },
    {
      "text": "corresponding to spam or no",
      "start": 283.759,
      "duration": 3.681
    },
    {
      "text": "spam then what we saw is that instead of",
      "start": 288.039,
      "duration": 4.72
    },
    {
      "text": "looking at all of these four outputs we",
      "start": 290.6,
      "duration": 4.0
    },
    {
      "text": "only look at the output corresponding to",
      "start": 292.759,
      "duration": 3.961
    },
    {
      "text": "the last token which is time in this",
      "start": 294.6,
      "duration": 4.319
    },
    {
      "text": "case because this last token contains",
      "start": 296.72,
      "duration": 4.36
    },
    {
      "text": "information of all the other tokens",
      "start": 298.919,
      "duration": 4.361
    },
    {
      "text": "through its attention",
      "start": 301.08,
      "duration": 5.36
    },
    {
      "text": "weights um so we have reached until this",
      "start": 303.28,
      "duration": 5.12
    },
    {
      "text": "stage where we pass in this",
      "start": 306.44,
      "duration": 4.56
    },
    {
      "text": "input and we get an output which is a",
      "start": 308.4,
      "duration": 5.88
    },
    {
      "text": "tensor of 1x 4X two since this is one",
      "start": 311.0,
      "duration": 6.16
    },
    {
      "text": "batch uh so batch size so each batch has",
      "start": 314.28,
      "duration": 5.56
    },
    {
      "text": "only one sample so it's one four because",
      "start": 317.16,
      "duration": 5.8
    },
    {
      "text": "there are four tokens every do is the",
      "start": 319.84,
      "duration": 5.28
    },
    {
      "text": "first token you is the second token have",
      "start": 322.96,
      "duration": 3.88
    },
    {
      "text": "is the third token time is the fourth",
      "start": 325.12,
      "duration": 4.68
    },
    {
      "text": "token and two because every token as as",
      "start": 326.84,
      "duration": 4.6
    },
    {
      "text": "I showed you has two outputs",
      "start": 329.8,
      "duration": 5.8
    },
    {
      "text": "corresponding to uh has two outputs",
      "start": 331.44,
      "duration": 6.84
    },
    {
      "text": "corresponding to spam or no spam then",
      "start": 335.6,
      "duration": 4.2
    },
    {
      "text": "what we saw is that we are going to look",
      "start": 338.28,
      "duration": 4.479
    },
    {
      "text": "at the last output token um and the last",
      "start": 339.8,
      "duration": 4.959
    },
    {
      "text": "output token will give us two",
      "start": 342.759,
      "duration": 4.921
    },
    {
      "text": "values so this will be value number one",
      "start": 344.759,
      "duration": 4.921
    },
    {
      "text": "and this will be value number two we",
      "start": 347.68,
      "duration": 4.12
    },
    {
      "text": "have reached up till this stage now in",
      "start": 349.68,
      "duration": 3.799
    },
    {
      "text": "today's lecture what we are going to see",
      "start": 351.8,
      "duration": 4.16
    },
    {
      "text": "is that okay once you get the final two",
      "start": 353.479,
      "duration": 5.921
    },
    {
      "text": "outputs from the last token what will",
      "start": 355.96,
      "duration": 5.6
    },
    {
      "text": "you do with these",
      "start": 359.4,
      "duration": 4.84
    },
    {
      "text": "outputs so we will first Implement two",
      "start": 361.56,
      "duration": 5.16
    },
    {
      "text": "Matrix we'll get the accuracy we'll get",
      "start": 364.24,
      "duration": 4.56
    },
    {
      "text": "the loss function then we will Implement",
      "start": 366.72,
      "duration": 4.879
    },
    {
      "text": "a backward pass so that we can train our",
      "start": 368.8,
      "duration": 4.48
    },
    {
      "text": "architecture to minimize the loss",
      "start": 371.599,
      "duration": 4.081
    },
    {
      "text": "function we'll modify all the parameters",
      "start": 373.28,
      "duration": 4.479
    },
    {
      "text": "so that the loss is minimized and then",
      "start": 375.68,
      "duration": 4.4
    },
    {
      "text": "we will do the testing on some new data",
      "start": 377.759,
      "duration": 4.72
    },
    {
      "text": "which the model has not seen awesome so",
      "start": 380.08,
      "duration": 4.72
    },
    {
      "text": "let's get started the first thing which",
      "start": 382.479,
      "duration": 4.241
    },
    {
      "text": "we'll need to do as I've mentioned in",
      "start": 384.8,
      "duration": 3.16
    },
    {
      "text": "the code",
      "start": 386.72,
      "duration": 4.28
    },
    {
      "text": "also is that we we need to First discuss",
      "start": 387.96,
      "duration": 4.88
    },
    {
      "text": "how we can convert the model outputs",
      "start": 391.0,
      "duration": 4.56
    },
    {
      "text": "into class label predictions so let's",
      "start": 392.84,
      "duration": 5.44
    },
    {
      "text": "say if you have the input text message",
      "start": 395.56,
      "duration": 5.359
    },
    {
      "text": "as you won the lottery until now we have",
      "start": 398.28,
      "duration": 4.4
    },
    {
      "text": "seen that we can extract the outputs",
      "start": 400.919,
      "duration": 3.801
    },
    {
      "text": "corresponding to the last row right and",
      "start": 402.68,
      "duration": 4.239
    },
    {
      "text": "let's say the output look like this",
      "start": 404.72,
      "duration": 4.24
    },
    {
      "text": "based on this output how can we say that",
      "start": 406.919,
      "duration": 4.28
    },
    {
      "text": "whether it's a Spam or not a",
      "start": 408.96,
      "duration": 4.679
    },
    {
      "text": "Spam uh what we can do in practice is",
      "start": 411.199,
      "duration": 4.56
    },
    {
      "text": "that we can apply a soft Max function on",
      "start": 413.639,
      "duration": 3.881
    },
    {
      "text": "this so that these two outputs are",
      "start": 415.759,
      "duration": 4.201
    },
    {
      "text": "converted into a set of probabilities so",
      "start": 417.52,
      "duration": 4.84
    },
    {
      "text": "then the first value will be99 the",
      "start": 419.96,
      "duration": 3.6
    },
    {
      "text": "second will be",
      "start": 422.36,
      "duration": 4.279
    },
    {
      "text": "0.01 then we look at the index which has",
      "start": 423.56,
      "duration": 5.96
    },
    {
      "text": "the highest probability value so since",
      "start": 426.639,
      "duration": 5.28
    },
    {
      "text": "99 is the highest is a higher",
      "start": 429.52,
      "duration": 5.32
    },
    {
      "text": "probability than 0.01 which means index",
      "start": 431.919,
      "duration": 5.881
    },
    {
      "text": "number 0 is more likely to be the answer",
      "start": 434.84,
      "duration": 5.68
    },
    {
      "text": "and index number zero is no spam that's",
      "start": 437.8,
      "duration": 4.799
    },
    {
      "text": "why this text message will be classified",
      "start": 440.52,
      "duration": 4.64
    },
    {
      "text": "as no spam similarly if you have second",
      "start": 442.599,
      "duration": 5.28
    },
    {
      "text": "text messages do you have time if the",
      "start": 445.16,
      "duration": 4.56
    },
    {
      "text": "output corresponding to the last last",
      "start": 447.879,
      "duration": 4.401
    },
    {
      "text": "row are these two tokens we'll again",
      "start": 449.72,
      "duration": 4.64
    },
    {
      "text": "apply soft Max and then we'll have a",
      "start": 452.28,
      "duration": 5.319
    },
    {
      "text": "tensor of probability 0.01",
      "start": 454.36,
      "duration": 6.08
    },
    {
      "text": "and99 then index number one is higher",
      "start": 457.599,
      "duration": 5.16
    },
    {
      "text": "and so the output which our model will",
      "start": 460.44,
      "duration": 3.8
    },
    {
      "text": "predict will be one and that will be",
      "start": 462.759,
      "duration": 3.44
    },
    {
      "text": "spam so these are the steps we'll",
      "start": 464.24,
      "duration": 4.12
    },
    {
      "text": "Implement in the code you'll see later",
      "start": 466.199,
      "duration": 3.72
    },
    {
      "text": "that there is actually no need to even",
      "start": 468.36,
      "duration": 3.799
    },
    {
      "text": "Implement soft Max since we are only",
      "start": 469.919,
      "duration": 5.081
    },
    {
      "text": "seeing the index of the value Which is",
      "start": 472.159,
      "duration": 5.121
    },
    {
      "text": "higher so for example even if we look at",
      "start": 475.0,
      "duration": 5.08
    },
    {
      "text": "these values the index this index is",
      "start": 477.28,
      "duration": 4.28
    },
    {
      "text": "higher so index zero is higher so it",
      "start": 480.08,
      "duration": 3.76
    },
    {
      "text": "will be no spam if we look at these two",
      "start": 481.56,
      "duration": 3.919
    },
    {
      "text": "the index one will be higher so it will",
      "start": 483.84,
      "duration": 2.479
    },
    {
      "text": "be",
      "start": 485.479,
      "duration": 3.761
    },
    {
      "text": "spam so let's go to code right now and",
      "start": 486.319,
      "duration": 4.681
    },
    {
      "text": "discuss how we can convert the model",
      "start": 489.24,
      "duration": 4.04
    },
    {
      "text": "outputs into class label",
      "start": 491.0,
      "duration": 4.639
    },
    {
      "text": "predictions Okay so until now we let's",
      "start": 493.28,
      "duration": 4.199
    },
    {
      "text": "say have a last token output which looks",
      "start": 495.639,
      "duration": 5.52
    },
    {
      "text": "something like this minus 35983 and 3.99",
      "start": 497.479,
      "duration": 6.4
    },
    {
      "text": "02 as we discussed first we'll apply the",
      "start": 501.159,
      "duration": 4.641
    },
    {
      "text": "soft Max so that we'll convert this into",
      "start": 503.879,
      "duration": 4.28
    },
    {
      "text": "a set of probabilities so let's say we",
      "start": 505.8,
      "duration": 4.72
    },
    {
      "text": "apply soft Max to these outputs and let",
      "start": 508.159,
      "duration": 4.36
    },
    {
      "text": "me actually",
      "start": 510.52,
      "duration": 5.68
    },
    {
      "text": "print let me actually print the soft Max",
      "start": 512.519,
      "duration": 5.76
    },
    {
      "text": "values over here so you'll see that when",
      "start": 516.2,
      "duration": 5.199
    },
    {
      "text": "you apply softmax to these two the",
      "start": 518.279,
      "duration": 5.361
    },
    {
      "text": "output tensor has two values",
      "start": 521.399,
      "duration": 4.88
    },
    {
      "text": "0.005 and",
      "start": 523.64,
      "duration": 4.56
    },
    {
      "text": "0.9995 and since",
      "start": 526.279,
      "duration": 5.12
    },
    {
      "text": "0.9995 is higher what we then do is that",
      "start": 528.2,
      "duration": 5.56
    },
    {
      "text": "we actually look at the argmax which",
      "start": 531.399,
      "duration": 3.961
    },
    {
      "text": "means we look at the index which has a",
      "start": 533.76,
      "duration": 3.04
    },
    {
      "text": "higher value and that will be index",
      "start": 535.36,
      "duration": 3.84
    },
    {
      "text": "number one so our class label prediction",
      "start": 536.8,
      "duration": 4.76
    },
    {
      "text": "will be be number one so in this case",
      "start": 539.2,
      "duration": 4.0
    },
    {
      "text": "the code returns one meaning that the",
      "start": 541.56,
      "duration": 4.08
    },
    {
      "text": "model predicts that the input text is",
      "start": 543.2,
      "duration": 5.16
    },
    {
      "text": "Spam as I mentioned using the soft Max",
      "start": 545.64,
      "duration": 4.8
    },
    {
      "text": "is optional because the largest outputs",
      "start": 548.36,
      "duration": 3.52
    },
    {
      "text": "directly correspond to the highest",
      "start": 550.44,
      "duration": 3.8
    },
    {
      "text": "probability scores so we can just take a",
      "start": 551.88,
      "duration": 4.56
    },
    {
      "text": "look at these output and find the ARG",
      "start": 554.24,
      "duration": 4.56
    },
    {
      "text": "Max so that's what we are going to do",
      "start": 556.44,
      "duration": 4.44
    },
    {
      "text": "let's say we look at the final token and",
      "start": 558.8,
      "duration": 3.76
    },
    {
      "text": "we look at its outputs we are going to",
      "start": 560.88,
      "duration": 3.32
    },
    {
      "text": "take the ARG Max which will give me the",
      "start": 562.56,
      "duration": 3.719
    },
    {
      "text": "index with the higher value and that",
      "start": 564.2,
      "duration": 4.28
    },
    {
      "text": "will be index number one so my class",
      "start": 566.279,
      "duration": 3.721
    },
    {
      "text": "label will just be",
      "start": 568.48,
      "duration": 4.44
    },
    {
      "text": "that label do item and so we'll get it",
      "start": 570.0,
      "duration": 6.72
    },
    {
      "text": "the output as one so now this is my uh",
      "start": 572.92,
      "duration": 6.599
    },
    {
      "text": "classification accuracy which measures",
      "start": 576.72,
      "duration": 5.799
    },
    {
      "text": "the percentage of correct predictions",
      "start": 579.519,
      "duration": 6.681
    },
    {
      "text": "which are seen across a data set so if",
      "start": 582.519,
      "duration": 5.721
    },
    {
      "text": "let's say the correct answer is class",
      "start": 586.2,
      "duration": 4.48
    },
    {
      "text": "label one and if I get a class label one",
      "start": 588.24,
      "duration": 5.719
    },
    {
      "text": "it's awesome then my it will be good so",
      "start": 590.68,
      "duration": 5.24
    },
    {
      "text": "we'll actually compare our model",
      "start": 593.959,
      "duration": 3.641
    },
    {
      "text": "prediction and the correct label",
      "start": 595.92,
      "duration": 3.8
    },
    {
      "text": "prediction and then that will give me my",
      "start": 597.6,
      "duration": 4.08
    },
    {
      "text": "accuracy so to determine the",
      "start": 599.72,
      "duration": 3.72
    },
    {
      "text": "classification accuracy we apply the",
      "start": 601.68,
      "duration": 3.92
    },
    {
      "text": "argmax based prediction code to all",
      "start": 603.44,
      "duration": 4.48
    },
    {
      "text": "examples in the data set and then what",
      "start": 605.6,
      "duration": 3.96
    },
    {
      "text": "we are going to do is that we are going",
      "start": 607.92,
      "duration": 4.0
    },
    {
      "text": "to uh actually compare it with the",
      "start": 609.56,
      "duration": 5.2
    },
    {
      "text": "actual value in the data set and then we",
      "start": 611.92,
      "duration": 4.84
    },
    {
      "text": "are going to find the accuracy so to",
      "start": 614.76,
      "duration": 3.68
    },
    {
      "text": "illustrate this what we are going to do",
      "start": 616.76,
      "duration": 3.16
    },
    {
      "text": "is that let's say our batch looks",
      "start": 618.44,
      "duration": 3.639
    },
    {
      "text": "something like this which I told you",
      "start": 619.92,
      "duration": 4.359
    },
    {
      "text": "before let me rub this right now so",
      "start": 622.079,
      "duration": 4.0
    },
    {
      "text": "let's say our batch looks something like",
      "start": 624.279,
      "duration": 3.841
    },
    {
      "text": "this so what I'm going to do is that",
      "start": 626.079,
      "duration": 4.841
    },
    {
      "text": "let's say if this is my first input",
      "start": 628.12,
      "duration": 5.04
    },
    {
      "text": "right I will pass in through my model",
      "start": 630.92,
      "duration": 4.12
    },
    {
      "text": "and I will get those two logits then I",
      "start": 633.16,
      "duration": 4.16
    },
    {
      "text": "will apply the AR Max function and then",
      "start": 635.04,
      "duration": 3.919
    },
    {
      "text": "predict whether it's spam or no spam",
      "start": 637.32,
      "duration": 4.6
    },
    {
      "text": "let's say it's predicted spam so the so",
      "start": 638.959,
      "duration": 5.161
    },
    {
      "text": "similar to this output labels I'll have",
      "start": 641.92,
      "duration": 5.12
    },
    {
      "text": "another labels which are the predicted",
      "start": 644.12,
      "duration": 5.36
    },
    {
      "text": "labels so these output labels are also",
      "start": 647.04,
      "duration": 4.12
    },
    {
      "text": "called as the target labels which are my",
      "start": 649.48,
      "duration": 5.52
    },
    {
      "text": "true values and here I have my predicted",
      "start": 651.16,
      "duration": 6.32
    },
    {
      "text": "labels and then I'll just compare these",
      "start": 655.0,
      "duration": 5.8
    },
    {
      "text": "two and that way I'll get the ACC",
      "start": 657.48,
      "duration": 5.72
    },
    {
      "text": "score so this is exactly what we are",
      "start": 660.8,
      "duration": 4.839
    },
    {
      "text": "going to do in the code right now so",
      "start": 663.2,
      "duration": 5.36
    },
    {
      "text": "here you can see um we are going to",
      "start": 665.639,
      "duration": 5.281
    },
    {
      "text": "Define this calculate accuracy given a",
      "start": 668.56,
      "duration": 4.48
    },
    {
      "text": "loader so let's say if you are given a",
      "start": 670.92,
      "duration": 4.12
    },
    {
      "text": "training data loader what we are going",
      "start": 673.04,
      "duration": 4.28
    },
    {
      "text": "to first do is that if number of batches",
      "start": 675.04,
      "duration": 4.359
    },
    {
      "text": "is not specified we are just going to",
      "start": 677.32,
      "duration": 3.759
    },
    {
      "text": "use the length of the data loader as the",
      "start": 679.399,
      "duration": 4.12
    },
    {
      "text": "number of batches or the batch size so",
      "start": 681.079,
      "duration": 5.801
    },
    {
      "text": "here you can see each batch consists of",
      "start": 683.519,
      "duration": 5.44
    },
    {
      "text": "eight training examples and so the",
      "start": 686.88,
      "duration": 4.959
    },
    {
      "text": "number number of batches are equal to",
      "start": 688.959,
      "duration": 4.721
    },
    {
      "text": "130 like this for the training data",
      "start": 691.839,
      "duration": 4.0
    },
    {
      "text": "sample so the number of batches will be",
      "start": 693.68,
      "duration": 4.8
    },
    {
      "text": "130 if we have not specified it if we",
      "start": 695.839,
      "duration": 4.361
    },
    {
      "text": "have specified the number of batches",
      "start": 698.48,
      "duration": 3.68
    },
    {
      "text": "here then the number of batches will be",
      "start": 700.2,
      "duration": 3.68
    },
    {
      "text": "minimum of what we have specified here",
      "start": 702.16,
      "duration": 4.72
    },
    {
      "text": "let's say that's 50 and 130 so then it",
      "start": 703.88,
      "duration": 4.639
    },
    {
      "text": "we'll consider the number of batches to",
      "start": 706.88,
      "duration": 4.44
    },
    {
      "text": "be equal to 50 and only compute the",
      "start": 708.519,
      "duration": 4.641
    },
    {
      "text": "accuracy for those many number of",
      "start": 711.32,
      "duration": 4.68
    },
    {
      "text": "batches so let's say what will happen in",
      "start": 713.16,
      "duration": 4.52
    },
    {
      "text": "this code is that we'll look at each",
      "start": 716.0,
      "duration": 3.72
    },
    {
      "text": "batch in this data loader so let's say",
      "start": 717.68,
      "duration": 4.12
    },
    {
      "text": "we are looking at the first batch even",
      "start": 719.72,
      "duration": 3.72
    },
    {
      "text": "the first batch you can see has eight",
      "start": 721.8,
      "duration": 3.64
    },
    {
      "text": "samples right so when we are looking at",
      "start": 723.44,
      "duration": 4.639
    },
    {
      "text": "each batch so let's say we are going to",
      "start": 725.44,
      "duration": 4.8
    },
    {
      "text": "look at each batch in the data",
      "start": 728.079,
      "duration": 5.961
    },
    {
      "text": "loader and each batch has eight samples",
      "start": 730.24,
      "duration": 6.599
    },
    {
      "text": "so I'm going to uh pass in all the",
      "start": 734.04,
      "duration": 4.919
    },
    {
      "text": "samples of a batch and I'm going to find",
      "start": 736.839,
      "duration": 4.041
    },
    {
      "text": "the logits which are the two output",
      "start": 738.959,
      "duration": 4.041
    },
    {
      "text": "values the logits of the last output",
      "start": 740.88,
      "duration": 5.199
    },
    {
      "text": "token similar to this but now imagine",
      "start": 743.0,
      "duration": 4.92
    },
    {
      "text": "that one batch has eight samples so I",
      "start": 746.079,
      "duration": 4.041
    },
    {
      "text": "I'll have eight such tensor",
      "start": 747.92,
      "duration": 4.159
    },
    {
      "text": "and then what I'll be doing is that I'll",
      "start": 750.12,
      "duration": 4.04
    },
    {
      "text": "actually be finding the ARG Max which",
      "start": 752.079,
      "duration": 4.841
    },
    {
      "text": "are the values for that entire batch and",
      "start": 754.16,
      "duration": 4.32
    },
    {
      "text": "then what I'll be doing is that I'll",
      "start": 756.92,
      "duration": 3.32
    },
    {
      "text": "compare the predicted labels with the",
      "start": 758.48,
      "duration": 4.64
    },
    {
      "text": "target labels which is my actual answer",
      "start": 760.24,
      "duration": 6.399
    },
    {
      "text": "and if it's uh if it's a correct",
      "start": 763.12,
      "duration": 5.159
    },
    {
      "text": "prediction which means if they are equal",
      "start": 766.639,
      "duration": 3.401
    },
    {
      "text": "I'll update the correct predictions I'll",
      "start": 768.279,
      "duration": 2.92
    },
    {
      "text": "increase the number of correct",
      "start": 770.04,
      "duration": 4.359
    },
    {
      "text": "predictions by one number and as I'm",
      "start": 771.199,
      "duration": 5.76
    },
    {
      "text": "going through the examples I'll also uh",
      "start": 774.399,
      "duration": 3.921
    },
    {
      "text": "whenever I make a prediction I'll",
      "start": 776.959,
      "duration": 3.68
    },
    {
      "text": "increase the number of examples by one",
      "start": 778.32,
      "duration": 4.24
    },
    {
      "text": "so if I'm going through the first",
      "start": 780.639,
      "duration": 4.361
    },
    {
      "text": "example here and if I make a prediction",
      "start": 782.56,
      "duration": 3.719
    },
    {
      "text": "so if I'm going through the first",
      "start": 785.0,
      "duration": 3.44
    },
    {
      "text": "example here and if I make a prediction",
      "start": 786.279,
      "duration": 3.56
    },
    {
      "text": "here the number of",
      "start": 788.44,
      "duration": 4.56
    },
    {
      "text": "examples the number of examples will",
      "start": 789.839,
      "duration": 5.8
    },
    {
      "text": "increase by one number of examples",
      "start": 793.0,
      "duration": 4.279
    },
    {
      "text": "increases by",
      "start": 795.639,
      "duration": 4.361
    },
    {
      "text": "one so when I make the second prediction",
      "start": 797.279,
      "duration": 4.36
    },
    {
      "text": "it will again increase by one so I'm",
      "start": 800.0,
      "duration": 3.24
    },
    {
      "text": "just keeping a track of the number of",
      "start": 801.639,
      "duration": 4.521
    },
    {
      "text": "examples and correct predictions so",
      "start": 803.24,
      "duration": 4.719
    },
    {
      "text": "towards the end to find the accuracy",
      "start": 806.16,
      "duration": 3.2
    },
    {
      "text": "score I'll just take the correct correct",
      "start": 807.959,
      "duration": 3.041
    },
    {
      "text": "predictions and divide by the number of",
      "start": 809.36,
      "duration": 3.96
    },
    {
      "text": "examples so if the number of examples is",
      "start": 811.0,
      "duration": 4.0
    },
    {
      "text": "th and if the correct predictions are",
      "start": 813.32,
      "duration": 4.36
    },
    {
      "text": "600 my accuracy will be 600 divided by",
      "start": 815.0,
      "duration": 5.04
    },
    {
      "text": "th000 we are doing a very simple thing",
      "start": 817.68,
      "duration": 5.12
    },
    {
      "text": "here we are just calculating the",
      "start": 820.04,
      "duration": 4.88
    },
    {
      "text": "prediction from our model and we are",
      "start": 822.8,
      "duration": 3.92
    },
    {
      "text": "comparing it with the actual values and",
      "start": 824.92,
      "duration": 3.039
    },
    {
      "text": "then we are adding up how many",
      "start": 826.72,
      "duration": 3.32
    },
    {
      "text": "predictions we got correct that's the",
      "start": 827.959,
      "duration": 4.601
    },
    {
      "text": "simplest way to find the accuracy right",
      "start": 830.04,
      "duration": 4.799
    },
    {
      "text": "so this is the code calcul calculate",
      "start": 832.56,
      "duration": 4.56
    },
    {
      "text": "accuracy loader now what we are going to",
      "start": 834.839,
      "duration": 3.761
    },
    {
      "text": "do is that we are going to use this",
      "start": 837.12,
      "duration": 3.719
    },
    {
      "text": "function calculate accuracy loader and",
      "start": 838.6,
      "duration": 3.919
    },
    {
      "text": "I'm just going to specify the number of",
      "start": 840.839,
      "duration": 3.761
    },
    {
      "text": "batches equal to 10 for the sake of",
      "start": 842.519,
      "duration": 4.041
    },
    {
      "text": "Simplicity our training data loader",
      "start": 844.6,
      "duration": 4.159
    },
    {
      "text": "actually has 130 batches but I'm",
      "start": 846.56,
      "duration": 4.36
    },
    {
      "text": "specifying your number of batches equal",
      "start": 848.759,
      "duration": 4.281
    },
    {
      "text": "to 10 so that you can just see whether",
      "start": 850.92,
      "duration": 4.2
    },
    {
      "text": "we are able to calculate the training",
      "start": 853.04,
      "duration": 4.0
    },
    {
      "text": "the validation and the testing accuracy",
      "start": 855.12,
      "duration": 4.44
    },
    {
      "text": "on our entire data set of course nothing",
      "start": 857.04,
      "duration": 4.919
    },
    {
      "text": "is optimized here so our values will not",
      "start": 859.56,
      "duration": 3.48
    },
    {
      "text": "be",
      "start": 861.959,
      "duration": 3.961
    },
    {
      "text": "uh uh very good but I just want to show",
      "start": 863.04,
      "duration": 5.039
    },
    {
      "text": "you that this code indeed runs so you",
      "start": 865.92,
      "duration": 3.52
    },
    {
      "text": "have this function Cal calculate",
      "start": 868.079,
      "duration": 3.081
    },
    {
      "text": "accuracy loader and first you pass in",
      "start": 869.44,
      "duration": 4.28
    },
    {
      "text": "the training loader so that will have",
      "start": 871.16,
      "duration": 4.32
    },
    {
      "text": "data such as this from from the training",
      "start": 873.72,
      "duration": 4.6
    },
    {
      "text": "data set that 70% of our data then you",
      "start": 875.48,
      "duration": 5.0
    },
    {
      "text": "pass in the validation loader that's 10%",
      "start": 878.32,
      "duration": 3.72
    },
    {
      "text": "of your data and then you pass in the",
      "start": 880.48,
      "duration": 4.32
    },
    {
      "text": "test loader that's 20% of your data in",
      "start": 882.04,
      "duration": 4.479
    },
    {
      "text": "each case we specify the number of",
      "start": 884.8,
      "duration": 5.719
    },
    {
      "text": "batches equal to 10 right uh and then we",
      "start": 886.519,
      "duration": 5.521
    },
    {
      "text": "print out the training accuracy",
      "start": 890.519,
      "duration": 3.56
    },
    {
      "text": "validation accuracy and test accuracy",
      "start": 892.04,
      "duration": 3.84
    },
    {
      "text": "the model has not been optimized we have",
      "start": 894.079,
      "duration": 4.081
    },
    {
      "text": "not yet implemented back propagation so",
      "start": 895.88,
      "duration": 5.0
    },
    {
      "text": "these accuracy m won't be good but let's",
      "start": 898.16,
      "duration": 4.799
    },
    {
      "text": "just see what they are so when you print",
      "start": 900.88,
      "duration": 4.48
    },
    {
      "text": "out the training accuracy the validation",
      "start": 902.959,
      "duration": 4.721
    },
    {
      "text": "accuracy and the test accuracy you get",
      "start": 905.36,
      "duration": 4.52
    },
    {
      "text": "that the training accuracy is 46%",
      "start": 907.68,
      "duration": 4.64
    },
    {
      "text": "validation accuracy is 45% and test",
      "start": 909.88,
      "duration": 5.639
    },
    {
      "text": "accuracy is 48% it's pretty bad it's",
      "start": 912.32,
      "duration": 5.319
    },
    {
      "text": "even worse than a coin toss I could have",
      "start": 915.519,
      "duration": 3.76
    },
    {
      "text": "just done a coin toss and randomly",
      "start": 917.639,
      "duration": 3.601
    },
    {
      "text": "predicted values and I would have been",
      "start": 919.279,
      "duration": 3.721
    },
    {
      "text": "right 50% of the",
      "start": 921.24,
      "duration": 4.039
    },
    {
      "text": "time so to improve the prediction",
      "start": 923.0,
      "duration": 3.88
    },
    {
      "text": "accuracies we need to fine tune the",
      "start": 925.279,
      "duration": 4.56
    },
    {
      "text": "model right so remember what how do we F",
      "start": 926.88,
      "duration": 5.0
    },
    {
      "text": "tune or how do we optimize the model",
      "start": 929.839,
      "duration": 4.401
    },
    {
      "text": "parameters the way to optimize the model",
      "start": 931.88,
      "duration": 4.92
    },
    {
      "text": "parameters is that we now we can do two",
      "start": 934.24,
      "duration": 5.12
    },
    {
      "text": "things now we can we have the target",
      "start": 936.8,
      "duration": 6.12
    },
    {
      "text": "which is the true values and we have the",
      "start": 939.36,
      "duration": 6.959
    },
    {
      "text": "predicted values right now what we will",
      "start": 942.92,
      "duration": 5.039
    },
    {
      "text": "need to do is that based on the True",
      "start": 946.319,
      "duration": 3.241
    },
    {
      "text": "Values and the predicted values we'll",
      "start": 947.959,
      "duration": 4.601
    },
    {
      "text": "need to define a loss",
      "start": 949.56,
      "duration": 3.0
    },
    {
      "text": "function and once the loss function is",
      "start": 952.88,
      "duration": 4.319
    },
    {
      "text": "defined then what we'll do is that we'll",
      "start": 955.199,
      "duration": 3.56
    },
    {
      "text": "simply take the partial derivative of",
      "start": 957.199,
      "duration": 3.361
    },
    {
      "text": "the loss function with respect to all my",
      "start": 958.759,
      "duration": 3.241
    },
    {
      "text": "trainable",
      "start": 960.56,
      "duration": 3.839
    },
    {
      "text": "weights we'll calculate the gradient",
      "start": 962.0,
      "duration": 4.68
    },
    {
      "text": "with respect to the trainable weights",
      "start": 964.399,
      "duration": 6.88
    },
    {
      "text": "and then we'll just uh update so weight",
      "start": 966.68,
      "duration": 8.079
    },
    {
      "text": "new is equal to weight old minus the",
      "start": 971.279,
      "duration": 5.281
    },
    {
      "text": "partial derivative of loss with respect",
      "start": 974.759,
      "duration": 4.041
    },
    {
      "text": "to that weight so we'll use a variation",
      "start": 976.56,
      "duration": 3.839
    },
    {
      "text": "of this simple gradient descent called",
      "start": 978.8,
      "duration": 4.279
    },
    {
      "text": "Adam or Adam W and so then we'll just",
      "start": 980.399,
      "duration": 4.88
    },
    {
      "text": "continue updating these parameters until",
      "start": 983.079,
      "duration": 4.361
    },
    {
      "text": "the loss function is minimized So",
      "start": 985.279,
      "duration": 4.12
    },
    {
      "text": "currently so let's say the loss function",
      "start": 987.44,
      "duration": 3.959
    },
    {
      "text": "looks like this it of course won't be as",
      "start": 989.399,
      "duration": 3.92
    },
    {
      "text": "simple as this but I'm taking a",
      "start": 991.399,
      "duration": 4.44
    },
    {
      "text": "simplified example initially we start",
      "start": 993.319,
      "duration": 5.481
    },
    {
      "text": "out with this where the loss is not that",
      "start": 995.839,
      "duration": 5.961
    },
    {
      "text": "low and then we move down this loss",
      "start": 998.8,
      "duration": 4.56
    },
    {
      "text": "function and hopefully we'll Reach This",
      "start": 1001.8,
      "duration": 4.12
    },
    {
      "text": "Global Minima where the loss is",
      "start": 1003.36,
      "duration": 4.839
    },
    {
      "text": "minimized and once loss is minimized",
      "start": 1005.92,
      "duration": 3.76
    },
    {
      "text": "then we'll make sure that the accuracy",
      "start": 1008.199,
      "duration": 3.841
    },
    {
      "text": "is also higher automatically so then",
      "start": 1009.68,
      "duration": 4.079
    },
    {
      "text": "comes the question of how do you define",
      "start": 1012.04,
      "duration": 3.64
    },
    {
      "text": "the loss function and what loss function",
      "start": 1013.759,
      "duration": 4.121
    },
    {
      "text": "to use if you have studied neural",
      "start": 1015.68,
      "duration": 3.519
    },
    {
      "text": "networks and machine learning learning",
      "start": 1017.88,
      "duration": 5.6
    },
    {
      "text": "before we know that if we have uh if we",
      "start": 1019.199,
      "duration": 7.72
    },
    {
      "text": "have targets um which are",
      "start": 1023.48,
      "duration": 6.88
    },
    {
      "text": "Pi or let's say if the targets are",
      "start": 1026.919,
      "duration": 7.441
    },
    {
      "text": "Yi and if my predictions are Pi then the",
      "start": 1030.36,
      "duration": 5.839
    },
    {
      "text": "loss function which is used in this case",
      "start": 1034.36,
      "duration": 3.8
    },
    {
      "text": "is the categorical cross entropy loss",
      "start": 1036.199,
      "duration": 4.201
    },
    {
      "text": "and is defined by negative of Sigma",
      "start": 1038.16,
      "duration": 4.32
    },
    {
      "text": "which is adding over all the class",
      "start": 1040.4,
      "duration": 7.159
    },
    {
      "text": "labels and minus Yi into log of",
      "start": 1042.48,
      "duration": 7.68
    },
    {
      "text": "Pi let me illustrate with a simple",
      "start": 1047.559,
      "duration": 4.801
    },
    {
      "text": "example here let's say if we have a text",
      "start": 1050.16,
      "duration": 4.48
    },
    {
      "text": "Data whose True Value is that it's not a",
      "start": 1052.36,
      "duration": 4.559
    },
    {
      "text": "Spam which means that it's one hot",
      "start": 1054.64,
      "duration": 4.68
    },
    {
      "text": "encoding is one and zero so let's say",
      "start": 1056.919,
      "duration": 5.0
    },
    {
      "text": "this is not a Spam but our predicted",
      "start": 1059.32,
      "duration": 5.76
    },
    {
      "text": "values our predicted values after or",
      "start": 1061.919,
      "duration": 6.961
    },
    {
      "text": "predicted values here are 08 and 02 then",
      "start": 1065.08,
      "duration": 6.08
    },
    {
      "text": "the cross entropy loss is negative of",
      "start": 1068.88,
      "duration": 4.039
    },
    {
      "text": "we'll need to sum over all the classes",
      "start": 1071.16,
      "duration": 5.72
    },
    {
      "text": "Yi into log of Pi Yi is the true value",
      "start": 1072.919,
      "duration": 7.081
    },
    {
      "text": "Yi is the true value and Pi is the",
      "start": 1076.88,
      "duration": 5.76
    },
    {
      "text": "predicted value",
      "start": 1080.0,
      "duration": 5.08
    },
    {
      "text": "right so let's multiply so we'll",
      "start": 1082.64,
      "duration": 4.279
    },
    {
      "text": "multiply one which is the true value",
      "start": 1085.08,
      "duration": 5.36
    },
    {
      "text": "multiplied by log of8 so 1 will be",
      "start": 1086.919,
      "duration": 5.76
    },
    {
      "text": "multiplied by log of8 and 0 will be",
      "start": 1090.44,
      "duration": 4.92
    },
    {
      "text": "multiplied by log of0 2 and we'll take",
      "start": 1092.679,
      "duration": 6.0
    },
    {
      "text": "the negative sign of this so 0 * log",
      "start": 1095.36,
      "duration": 6.52
    },
    {
      "text": "point2 is 0 and then 1 * log point8 if",
      "start": 1098.679,
      "duration": 6.561
    },
    {
      "text": "you take the negative that's 2 2231 why",
      "start": 1101.88,
      "duration": 5.56
    },
    {
      "text": "is this a good measure of loss because",
      "start": 1105.24,
      "duration": 4.96
    },
    {
      "text": "if our predicted value was 1 and zero",
      "start": 1107.44,
      "duration": 4.64
    },
    {
      "text": "which is exactly equal to true this",
      "start": 1110.2,
      "duration": 4.88
    },
    {
      "text": "second will anyway be zero but the first",
      "start": 1112.08,
      "duration": 7.16
    },
    {
      "text": "but the first entry will be 1 into log 1",
      "start": 1115.08,
      "duration": 5.8
    },
    {
      "text": "which will be equal to",
      "start": 1119.24,
      "duration": 4.559
    },
    {
      "text": "zero so if the predicted value equals to",
      "start": 1120.88,
      "duration": 4.44
    },
    {
      "text": "the True Value then our loss will be",
      "start": 1123.799,
      "duration": 3.841
    },
    {
      "text": "zero which is exactly what we want so",
      "start": 1125.32,
      "duration": 6.16
    },
    {
      "text": "this negative of Y log Pi is a very good",
      "start": 1127.64,
      "duration": 6.76
    },
    {
      "text": "loss function to be to calculate the",
      "start": 1131.48,
      "duration": 5.4
    },
    {
      "text": "loss in the case of this categorical",
      "start": 1134.4,
      "duration": 5.399
    },
    {
      "text": "predictions in the in classification",
      "start": 1136.88,
      "duration": 6.44
    },
    {
      "text": "tasks uh so to give you just a",
      "start": 1139.799,
      "duration": 7.201
    },
    {
      "text": "brief visual flavor negative of log",
      "start": 1143.32,
      "duration": 5.4
    },
    {
      "text": "negative of log of x looks something",
      "start": 1147.0,
      "duration": 3.24
    },
    {
      "text": "like this so this is X and this is",
      "start": 1148.72,
      "duration": 5.4
    },
    {
      "text": "negative of log of x and we want X to be",
      "start": 1150.24,
      "duration": 5.64
    },
    {
      "text": "as close to one as possible which is",
      "start": 1154.12,
      "duration": 4.28
    },
    {
      "text": "this probability of the correct class So",
      "start": 1155.88,
      "duration": 4.24
    },
    {
      "text": "eventually we'll start out from some",
      "start": 1158.4,
      "duration": 4.44
    },
    {
      "text": "high loss and our goal is to make the",
      "start": 1160.12,
      "duration": 5.4
    },
    {
      "text": "loss as close to zero as possible",
      "start": 1162.84,
      "duration": 4.52
    },
    {
      "text": "another advantage of the Cross entropy",
      "start": 1165.52,
      "duration": 3.88
    },
    {
      "text": "loss is that it's differentiable so it's",
      "start": 1167.36,
      "duration": 3.88
    },
    {
      "text": "very useful for us in the case of back",
      "start": 1169.4,
      "duration": 4.8
    },
    {
      "text": "propagation right um okay so let's",
      "start": 1171.24,
      "duration": 4.84
    },
    {
      "text": "actually Define the cross entropy loss",
      "start": 1174.2,
      "duration": 5.479
    },
    {
      "text": "now along with this uh calculation of",
      "start": 1176.08,
      "duration": 5.88
    },
    {
      "text": "accuracy loader what we are also going",
      "start": 1179.679,
      "duration": 4.36
    },
    {
      "text": "to do is that we are going to define a",
      "start": 1181.96,
      "duration": 3.92
    },
    {
      "text": "loss function which is the cross entropy",
      "start": 1184.039,
      "duration": 3.921
    },
    {
      "text": "loss why can't we use just",
      "start": 1185.88,
      "duration": 3.799
    },
    {
      "text": "classification accuracy and take the",
      "start": 1187.96,
      "duration": 3.52
    },
    {
      "text": "inverse of that accuracy maybe to get",
      "start": 1189.679,
      "duration": 3.841
    },
    {
      "text": "the loss it's because classification",
      "start": 1191.48,
      "duration": 3.6
    },
    {
      "text": "accuracy is not a differentiable",
      "start": 1193.52,
      "duration": 3.639
    },
    {
      "text": "function so we will use the cross",
      "start": 1195.08,
      "duration": 4.36
    },
    {
      "text": "entropy loss as a proxy to maximize the",
      "start": 1197.159,
      "duration": 4.481
    },
    {
      "text": "accuracy this is the same as the cross",
      "start": 1199.44,
      "duration": 3.96
    },
    {
      "text": "entropy loss which we use to pre-train",
      "start": 1201.64,
      "duration": 3.56
    },
    {
      "text": "the large language",
      "start": 1203.4,
      "duration": 5.36
    },
    {
      "text": "model so uh okay so what we are going to",
      "start": 1205.2,
      "duration": 5.839
    },
    {
      "text": "do now is that let's say we get an input",
      "start": 1208.76,
      "duration": 4.159
    },
    {
      "text": "batch and a Target batch always when an",
      "start": 1211.039,
      "duration": 4.161
    },
    {
      "text": "input and Target batch is given your",
      "start": 1212.919,
      "duration": 4.0
    },
    {
      "text": "visual mind should take you to this",
      "start": 1215.2,
      "duration": 3.44
    },
    {
      "text": "figure where we have an input batch and",
      "start": 1216.919,
      "duration": 4.841
    },
    {
      "text": "we have a Target batch so what what has",
      "start": 1218.64,
      "duration": 5.44
    },
    {
      "text": "to be done here is that once you get the",
      "start": 1221.76,
      "duration": 3.799
    },
    {
      "text": "input batch you pass in through the",
      "start": 1224.08,
      "duration": 3.2
    },
    {
      "text": "model and then you only look at the",
      "start": 1225.559,
      "duration": 4.24
    },
    {
      "text": "Logics of the last output token because",
      "start": 1227.28,
      "duration": 4.879
    },
    {
      "text": "that contains the most information and",
      "start": 1229.799,
      "duration": 4.081
    },
    {
      "text": "then you find the categorical cross",
      "start": 1232.159,
      "duration": 4.561
    },
    {
      "text": "entropy loss between this logic sensor",
      "start": 1233.88,
      "duration": 5.0
    },
    {
      "text": "which is the output of the last token",
      "start": 1236.72,
      "duration": 5.6
    },
    {
      "text": "and the target batch so the logic sensor",
      "start": 1238.88,
      "duration": 7.24
    },
    {
      "text": "output can be something like uh8 and 02",
      "start": 1242.32,
      "duration": 6.4
    },
    {
      "text": "and the target out is one Z so when you",
      "start": 1246.12,
      "duration": 4.4
    },
    {
      "text": "calculate the cross entropy loss you'll",
      "start": 1248.72,
      "duration": 4.52
    },
    {
      "text": "get some value of the loss function so",
      "start": 1250.52,
      "duration": 5.96
    },
    {
      "text": "I'll just show you here torch.nn do",
      "start": 1253.24,
      "duration": 5.96
    },
    {
      "text": "functional cross entropy this is the P",
      "start": 1256.48,
      "duration": 4.48
    },
    {
      "text": "torch functionality which we are using",
      "start": 1259.2,
      "duration": 4.44
    },
    {
      "text": "over here to find the cross entropy",
      "start": 1260.96,
      "duration": 5.24
    },
    {
      "text": "loss awesome and this is differentiable",
      "start": 1263.64,
      "duration": 4.32
    },
    {
      "text": "so it will be very useful for us when we",
      "start": 1266.2,
      "duration": 3.44
    },
    {
      "text": "do the back",
      "start": 1267.96,
      "duration": 3.959
    },
    {
      "text": "propagation okay so we will use this",
      "start": 1269.64,
      "duration": 4.399
    },
    {
      "text": "calculate loss batch function to compute",
      "start": 1271.919,
      "duration": 4.441
    },
    {
      "text": "the loss for a single batch and we can",
      "start": 1274.039,
      "duration": 4.401
    },
    {
      "text": "also use it to calculate the loss for a",
      "start": 1276.36,
      "duration": 4.84
    },
    {
      "text": "multiple set of batches so for to",
      "start": 1278.44,
      "duration": 6.16
    },
    {
      "text": "calculate the LW for multiple batches we",
      "start": 1281.2,
      "duration": 5.76
    },
    {
      "text": "have to use similar code lines as we",
      "start": 1284.6,
      "duration": 4.679
    },
    {
      "text": "used over here so so if number of",
      "start": 1286.96,
      "duration": 5.04
    },
    {
      "text": "batches is not specified then we take",
      "start": 1289.279,
      "duration": 4.361
    },
    {
      "text": "the number of batches to be equal to the",
      "start": 1292.0,
      "duration": 4.0
    },
    {
      "text": "length of the data loader if number of",
      "start": 1293.64,
      "duration": 4.24
    },
    {
      "text": "batches is specified then it's equal to",
      "start": 1296.0,
      "duration": 3.6
    },
    {
      "text": "the minimum of the number of batches",
      "start": 1297.88,
      "duration": 3.6
    },
    {
      "text": "specified and what is the length of the",
      "start": 1299.6,
      "duration": 3.959
    },
    {
      "text": "data loader very similar to the accuracy",
      "start": 1301.48,
      "duration": 4.36
    },
    {
      "text": "classification code which we saw then",
      "start": 1303.559,
      "duration": 3.561
    },
    {
      "text": "what we are going to do is that we are",
      "start": 1305.84,
      "duration": 3.24
    },
    {
      "text": "going to take one input batch one target",
      "start": 1307.12,
      "duration": 4.08
    },
    {
      "text": "batch calculate the loss between all the",
      "start": 1309.08,
      "duration": 3.68
    },
    {
      "text": "samples of the input batch and the",
      "start": 1311.2,
      "duration": 3.68
    },
    {
      "text": "target batch using this calculate loss",
      "start": 1312.76,
      "duration": 3.799
    },
    {
      "text": "batch which will implement the",
      "start": 1314.88,
      "duration": 3.84
    },
    {
      "text": "categorical cross entropy we are going",
      "start": 1316.559,
      "duration": 5.0
    },
    {
      "text": "to add the loss every time we get a loss",
      "start": 1318.72,
      "duration": 4.6
    },
    {
      "text": "we are going to add the loss and then",
      "start": 1321.559,
      "duration": 4.72
    },
    {
      "text": "that is in the total",
      "start": 1323.32,
      "duration": 5.479
    },
    {
      "text": "loss um awesome and then what we are",
      "start": 1326.279,
      "duration": 3.961
    },
    {
      "text": "ultimately going to do is that we'll",
      "start": 1328.799,
      "duration": 3.12
    },
    {
      "text": "divide the total loss with the number of",
      "start": 1330.24,
      "duration": 3.28
    },
    {
      "text": "batches so that will kind of give us an",
      "start": 1331.919,
      "duration": 4.12
    },
    {
      "text": "average loss per batch and this is the",
      "start": 1333.52,
      "duration": 4.159
    },
    {
      "text": "loss which we will eventually try to",
      "start": 1336.039,
      "duration": 3.961
    },
    {
      "text": "minimize using back propagation that is",
      "start": 1337.679,
      "duration": 5.0
    },
    {
      "text": "the whole workflow which we are going to",
      "start": 1340.0,
      "duration": 5.2
    },
    {
      "text": "follow so now what we can do is that we",
      "start": 1342.679,
      "duration": 4.801
    },
    {
      "text": "can implement this loss function on our",
      "start": 1345.2,
      "duration": 4.12
    },
    {
      "text": "data set again we have not implemented",
      "start": 1347.48,
      "duration": 3.96
    },
    {
      "text": "back propagation so the loss will be",
      "start": 1349.32,
      "duration": 3.76
    },
    {
      "text": "very high but I just want to show you",
      "start": 1351.44,
      "duration": 4.119
    },
    {
      "text": "the initial values of the training loss",
      "start": 1353.08,
      "duration": 4.88
    },
    {
      "text": "the validation loss and the test loss so",
      "start": 1355.559,
      "duration": 3.801
    },
    {
      "text": "here again I'm setting the number of",
      "start": 1357.96,
      "duration": 3.959
    },
    {
      "text": "batches equal to five U because actually",
      "start": 1359.36,
      "duration": 5.4
    },
    {
      "text": "the train data loader has 130 batches I",
      "start": 1361.919,
      "duration": 4.921
    },
    {
      "text": "think so that will take a long time to",
      "start": 1364.76,
      "duration": 3.84
    },
    {
      "text": "calculate and anyway we have not done",
      "start": 1366.84,
      "duration": 3.439
    },
    {
      "text": "the training here so I just want to",
      "start": 1368.6,
      "duration": 4.319
    },
    {
      "text": "illustrate that the loss can be found on",
      "start": 1370.279,
      "duration": 5.441
    },
    {
      "text": "five batches like this so you you",
      "start": 1372.919,
      "duration": 4.88
    },
    {
      "text": "implement the Cal Closs loader function",
      "start": 1375.72,
      "duration": 4.04
    },
    {
      "text": "and you pass in train loader then",
      "start": 1377.799,
      "duration": 4.24
    },
    {
      "text": "validation loader and the test loader",
      "start": 1379.76,
      "duration": 3.84
    },
    {
      "text": "and then you also pass in the number of",
      "start": 1382.039,
      "duration": 3.681
    },
    {
      "text": "batches so then you can print out the",
      "start": 1383.6,
      "duration": 3.679
    },
    {
      "text": "training loss you can print out the",
      "start": 1385.72,
      "duration": 3.12
    },
    {
      "text": "validation loss and you can print out",
      "start": 1387.279,
      "duration": 3.681
    },
    {
      "text": "the test loss and you can see these are",
      "start": 1388.84,
      "duration": 3.719
    },
    {
      "text": "the high these are the values which are",
      "start": 1390.96,
      "duration": 4.44
    },
    {
      "text": "pretty high it's again if you in the",
      "start": 1392.559,
      "duration": 4.561
    },
    {
      "text": "accuracy we saw that the accuracy was",
      "start": 1395.4,
      "duration": 3.32
    },
    {
      "text": "very bad and that is reflected in the",
      "start": 1397.12,
      "duration": 4.0
    },
    {
      "text": "loss values as well now we will",
      "start": 1398.72,
      "duration": 4.559
    },
    {
      "text": "Implement a training function to fine",
      "start": 1401.12,
      "duration": 3.799
    },
    {
      "text": "tune the model which means that we'll",
      "start": 1403.279,
      "duration": 3.801
    },
    {
      "text": "adjust the parameters to minimize the",
      "start": 1404.919,
      "duration": 4.24
    },
    {
      "text": "training loss and and then we'll also",
      "start": 1407.08,
      "duration": 4.56
    },
    {
      "text": "print out the validation loss and we'll",
      "start": 1409.159,
      "duration": 4.201
    },
    {
      "text": "print out the test",
      "start": 1411.64,
      "duration": 4.36
    },
    {
      "text": "loss so let's start looking at that part",
      "start": 1413.36,
      "duration": 4.799
    },
    {
      "text": "of the code right now so until now we",
      "start": 1416.0,
      "duration": 4.44
    },
    {
      "text": "have finished a number of steps here we",
      "start": 1418.159,
      "duration": 5.081
    },
    {
      "text": "have finished uh let's see we have",
      "start": 1420.44,
      "duration": 4.28
    },
    {
      "text": "finished downloading the data set",
      "start": 1423.24,
      "duration": 4.039
    },
    {
      "text": "pre-processing the data set create data",
      "start": 1424.72,
      "duration": 5.04
    },
    {
      "text": "loaders initialize model load pre-train",
      "start": 1427.279,
      "duration": 5.441
    },
    {
      "text": "weights modify model for fine tuning",
      "start": 1429.76,
      "duration": 4.96
    },
    {
      "text": "Implement evaluation utilities which is",
      "start": 1432.72,
      "duration": 4.199
    },
    {
      "text": "the loss and the accuracy",
      "start": 1434.72,
      "duration": 4.199
    },
    {
      "text": "basically and now we we are at this",
      "start": 1436.919,
      "duration": 3.721
    },
    {
      "text": "stage where we will actually fine tune",
      "start": 1438.919,
      "duration": 3.441
    },
    {
      "text": "the model which means that we'll Define",
      "start": 1440.64,
      "duration": 3.36
    },
    {
      "text": "the training Loop and we'll Implement",
      "start": 1442.36,
      "duration": 3.04
    },
    {
      "text": "back",
      "start": 1444.0,
      "duration": 3.64
    },
    {
      "text": "propagation so this is the training Loop",
      "start": 1445.4,
      "duration": 4.56
    },
    {
      "text": "which we are going to Define first we'll",
      "start": 1447.64,
      "duration": 4.76
    },
    {
      "text": "have the EPO which means one Epoch is",
      "start": 1449.96,
      "duration": 4.28
    },
    {
      "text": "going through the entire data set once",
      "start": 1452.4,
      "duration": 5.24
    },
    {
      "text": "right so let's say if you if you are",
      "start": 1454.24,
      "duration": 5.039
    },
    {
      "text": "running in one particular Epoch the",
      "start": 1457.64,
      "duration": 3.2
    },
    {
      "text": "second Loop is that you have to go",
      "start": 1459.279,
      "duration": 4.441
    },
    {
      "text": "within each batch so each batch has",
      "start": 1460.84,
      "duration": 4.88
    },
    {
      "text": "eight samples at least that's how we",
      "start": 1463.72,
      "duration": 4.72
    },
    {
      "text": "Define the training data loader to be so",
      "start": 1465.72,
      "duration": 4.24
    },
    {
      "text": "then we'll look at each particular",
      "start": 1468.44,
      "duration": 4.119
    },
    {
      "text": "sample and then we'll calculate the loss",
      "start": 1469.96,
      "duration": 6.0
    },
    {
      "text": "on the current batch uh and uh we'll",
      "start": 1472.559,
      "duration": 5.521
    },
    {
      "text": "Implement a backward pass to calculate",
      "start": 1475.96,
      "duration": 4.36
    },
    {
      "text": "the loss gradients and then we'll update",
      "start": 1478.08,
      "duration": 3.52
    },
    {
      "text": "the model weights using the loss",
      "start": 1480.32,
      "duration": 2.92
    },
    {
      "text": "gradient so here what we are doing is",
      "start": 1481.6,
      "duration": 7.679
    },
    {
      "text": "that W new is equal to W old",
      "start": 1483.24,
      "duration": 9.919
    },
    {
      "text": "minus Alpha * the partial derivatives",
      "start": 1489.279,
      "duration": 5.681
    },
    {
      "text": "this is exactly what we written over we",
      "start": 1493.159,
      "duration": 3.681
    },
    {
      "text": "wrote over here",
      "start": 1494.96,
      "duration": 5.16
    },
    {
      "text": "also uh and then once the weights are",
      "start": 1496.84,
      "duration": 4.92
    },
    {
      "text": "updated we print the training and the",
      "start": 1500.12,
      "duration": 4.0
    },
    {
      "text": "validation loss and then we keep on",
      "start": 1501.76,
      "duration": 4.36
    },
    {
      "text": "doing the same thing for multiple number",
      "start": 1504.12,
      "duration": 4.0
    },
    {
      "text": "of epox so that the parameters are",
      "start": 1506.12,
      "duration": 4.36
    },
    {
      "text": "getting updated so the simplest way to",
      "start": 1508.12,
      "duration": 3.96
    },
    {
      "text": "think about this is that the most",
      "start": 1510.48,
      "duration": 4.04
    },
    {
      "text": "important step is this backward pass",
      "start": 1512.08,
      "duration": 4.88
    },
    {
      "text": "once we do the backward pass we get the",
      "start": 1514.52,
      "duration": 4.159
    },
    {
      "text": "loss gradients that's why we needed the",
      "start": 1516.96,
      "duration": 4.16
    },
    {
      "text": "loss function to be differentiable once",
      "start": 1518.679,
      "duration": 4.12
    },
    {
      "text": "we get the loss gradients with respect",
      "start": 1521.12,
      "duration": 3.48
    },
    {
      "text": "to the parameters we can actually update",
      "start": 1522.799,
      "duration": 4.12
    },
    {
      "text": "the parameters and once we do this",
      "start": 1524.6,
      "duration": 3.72
    },
    {
      "text": "enough number of times the par",
      "start": 1526.919,
      "duration": 2.76
    },
    {
      "text": "parameters will get updated and",
      "start": 1528.32,
      "duration": 3.92
    },
    {
      "text": "hopefully we'll reach a value of the",
      "start": 1529.679,
      "duration": 4.761
    },
    {
      "text": "loss where the loss function is",
      "start": 1532.24,
      "duration": 4.36
    },
    {
      "text": "minimized this is the exact same",
      "start": 1534.44,
      "duration": 3.719
    },
    {
      "text": "training function which we had",
      "start": 1536.6,
      "duration": 5.199
    },
    {
      "text": "implemented to pre-train the llm and",
      "start": 1538.159,
      "duration": 6.12
    },
    {
      "text": "here's what I'm what I want to show you",
      "start": 1541.799,
      "duration": 4.801
    },
    {
      "text": "is that when we finetune the model on",
      "start": 1544.279,
      "duration": 4.52
    },
    {
      "text": "supervised data which means data set",
      "start": 1546.6,
      "duration": 4.559
    },
    {
      "text": "such as the spam no spam label I showed",
      "start": 1548.799,
      "duration": 4.801
    },
    {
      "text": "you we need to again train the model so",
      "start": 1551.159,
      "duration": 4.24
    },
    {
      "text": "there is training process involved in",
      "start": 1553.6,
      "duration": 3.64
    },
    {
      "text": "pre-training and there is training",
      "start": 1555.399,
      "duration": 3.801
    },
    {
      "text": "process involved in fin tuning that's",
      "start": 1557.24,
      "duration": 3.52
    },
    {
      "text": "why it's called pre-training actually",
      "start": 1559.2,
      "duration": 3.4
    },
    {
      "text": "because it's before this second training",
      "start": 1560.76,
      "duration": 4.0
    },
    {
      "text": "process which needs to be",
      "start": 1562.6,
      "duration": 4.04
    },
    {
      "text": "implemented so let's see how the",
      "start": 1564.76,
      "duration": 3.84
    },
    {
      "text": "training process is implemented in code",
      "start": 1566.64,
      "duration": 4.159
    },
    {
      "text": "right now so this section I have named",
      "start": 1568.6,
      "duration": 4.48
    },
    {
      "text": "as finetuning the model on supervised",
      "start": 1570.799,
      "duration": 5.0
    },
    {
      "text": "data so until now we have actually not",
      "start": 1573.08,
      "duration": 5.079
    },
    {
      "text": "trained the model on the data set at all",
      "start": 1575.799,
      "duration": 3.561
    },
    {
      "text": "which means that that's why the",
      "start": 1578.159,
      "duration": 3.88
    },
    {
      "text": "parameters are not optimized so in this",
      "start": 1579.36,
      "duration": 4.16
    },
    {
      "text": "section we'll Define and use the",
      "start": 1582.039,
      "duration": 3.36
    },
    {
      "text": "training function to fine tune the",
      "start": 1583.52,
      "duration": 3.879
    },
    {
      "text": "pre-trained llm and improve its spam",
      "start": 1585.399,
      "duration": 3.561
    },
    {
      "text": "classification accur",
      "start": 1587.399,
      "duration": 3.88
    },
    {
      "text": "accuracy uh a note here is that if you",
      "start": 1588.96,
      "duration": 4.0
    },
    {
      "text": "have followed these lectures you'll see",
      "start": 1591.279,
      "duration": 3.361
    },
    {
      "text": "that the training function is very close",
      "start": 1592.96,
      "duration": 4.12
    },
    {
      "text": "to the train model simple function which",
      "start": 1594.64,
      "duration": 4.56
    },
    {
      "text": "we used for pre-training earlier the",
      "start": 1597.08,
      "duration": 3.88
    },
    {
      "text": "only distinction is that we are tracking",
      "start": 1599.2,
      "duration": 3.52
    },
    {
      "text": "the number of examples here the number",
      "start": 1600.96,
      "duration": 4.199
    },
    {
      "text": "of text samples instead of tracking the",
      "start": 1602.72,
      "duration": 4.64
    },
    {
      "text": "number of tokens which we had calculated",
      "start": 1605.159,
      "duration": 4.481
    },
    {
      "text": "earlier so in the code what we are going",
      "start": 1607.36,
      "duration": 4.76
    },
    {
      "text": "to do is that there are seven steps the",
      "start": 1609.64,
      "duration": 4.039
    },
    {
      "text": "first step is that we have to set the",
      "start": 1612.12,
      "duration": 3.12
    },
    {
      "text": "model to training",
      "start": 1613.679,
      "duration": 4.201
    },
    {
      "text": "mode uh so here you see we set the model",
      "start": 1615.24,
      "duration": 4.12
    },
    {
      "text": "to train training mode that's the first",
      "start": 1617.88,
      "duration": 3.799
    },
    {
      "text": "step the second step is reset the loss",
      "start": 1619.36,
      "duration": 4.799
    },
    {
      "text": "gradients from previous batch so when we",
      "start": 1621.679,
      "duration": 4.681
    },
    {
      "text": "look at each every batch we have to",
      "start": 1624.159,
      "duration": 4.681
    },
    {
      "text": "reset the loss gradients again so let's",
      "start": 1626.36,
      "duration": 4.12
    },
    {
      "text": "say we are looking at one batch right",
      "start": 1628.84,
      "duration": 4.959
    },
    {
      "text": "now uh we reset the loss gradients from",
      "start": 1630.48,
      "duration": 5.64
    },
    {
      "text": "the previous batch iteration then the",
      "start": 1633.799,
      "duration": 4.041
    },
    {
      "text": "third step is calculating the loss",
      "start": 1636.12,
      "duration": 3.76
    },
    {
      "text": "gradients and updating model weights",
      "start": 1637.84,
      "duration": 4.24
    },
    {
      "text": "these are the most important step so",
      "start": 1639.88,
      "duration": 4.44
    },
    {
      "text": "then what you do is you find the loss in",
      "start": 1642.08,
      "duration": 4.079
    },
    {
      "text": "that batch and then you calculate the",
      "start": 1644.32,
      "duration": 3.359
    },
    {
      "text": "loss gradients through a backward",
      "start": 1646.159,
      "duration": 3.88
    },
    {
      "text": "propagation then you do Optimizer do",
      "start": 1647.679,
      "duration": 4.161
    },
    {
      "text": "step this is where the optimizer comes",
      "start": 1650.039,
      "duration": 4.281
    },
    {
      "text": "into the picture in on the Whiteboard I",
      "start": 1651.84,
      "duration": 4.16
    },
    {
      "text": "showed you simple vanilla gradient",
      "start": 1654.32,
      "duration": 3.88
    },
    {
      "text": "desent over here but in practice we'll",
      "start": 1656.0,
      "duration": 5.32
    },
    {
      "text": "use a a more complicated optimization",
      "start": 1658.2,
      "duration": 4.92
    },
    {
      "text": "algorithm which keeps track of the",
      "start": 1661.32,
      "duration": 3.88
    },
    {
      "text": "previous gradient which keeps track of",
      "start": 1663.12,
      "duration": 5.24
    },
    {
      "text": "the previous gradient Square Etc so that",
      "start": 1665.2,
      "duration": 5.8
    },
    {
      "text": "the optimization is done in a in a",
      "start": 1668.36,
      "duration": 5.159
    },
    {
      "text": "better Manner and so that the model does",
      "start": 1671.0,
      "duration": 4.48
    },
    {
      "text": "not get stuck in local",
      "start": 1673.519,
      "duration": 4.76
    },
    {
      "text": "Minima then the next step is that we",
      "start": 1675.48,
      "duration": 4.64
    },
    {
      "text": "keeping track of the number of examples",
      "start": 1678.279,
      "duration": 3.64
    },
    {
      "text": "so we just keep track of the number of",
      "start": 1680.12,
      "duration": 4.159
    },
    {
      "text": "examples which we are seeing so input",
      "start": 1681.919,
      "duration": 5.24
    },
    {
      "text": "batch. shape zero is that let's say if",
      "start": 1684.279,
      "duration": 5.201
    },
    {
      "text": "each batch has eight samples when you",
      "start": 1687.159,
      "duration": 5.281
    },
    {
      "text": "look at the first shape uh first value",
      "start": 1689.48,
      "duration": 4.679
    },
    {
      "text": "of the batch shape it will give us the",
      "start": 1692.44,
      "duration": 3.76
    },
    {
      "text": "number of samples in the batch so for",
      "start": 1694.159,
      "duration": 5.441
    },
    {
      "text": "example if the batch has eight uh eight",
      "start": 1696.2,
      "duration": 5.959
    },
    {
      "text": "samples and the number of tokens is",
      "start": 1699.6,
      "duration": 5.28
    },
    {
      "text": "120 so then we'll get eight here input",
      "start": 1702.159,
      "duration": 4.721
    },
    {
      "text": "badge. shape zero which will give us the",
      "start": 1704.88,
      "duration": 5.24
    },
    {
      "text": "number of samples over here",
      "start": 1706.88,
      "duration": 4.799
    },
    {
      "text": "so then we keep track of the number of",
      "start": 1710.12,
      "duration": 3.36
    },
    {
      "text": "example scen so you can just think of",
      "start": 1711.679,
      "duration": 3.921
    },
    {
      "text": "this example scene as when you look at",
      "start": 1713.48,
      "duration": 4.28
    },
    {
      "text": "one text message that's one example seen",
      "start": 1715.6,
      "duration": 3.88
    },
    {
      "text": "when you look at second text message you",
      "start": 1717.76,
      "duration": 3.799
    },
    {
      "text": "increment the number of example seen by",
      "start": 1719.48,
      "duration": 4.72
    },
    {
      "text": "one whenever you go through a full batch",
      "start": 1721.559,
      "duration": 5.201
    },
    {
      "text": "you increase the global step by one",
      "start": 1724.2,
      "duration": 5.76
    },
    {
      "text": "right awesome now here we have that if",
      "start": 1726.76,
      "duration": 6.44
    },
    {
      "text": "Global step percentage of evaluation",
      "start": 1729.96,
      "duration": 4.92
    },
    {
      "text": "frequency equal to zero so we have to",
      "start": 1733.2,
      "duration": 4.599
    },
    {
      "text": "specify an evaluation frequency now if",
      "start": 1734.88,
      "duration": 6.0
    },
    {
      "text": "the training batch has 130 if the",
      "start": 1737.799,
      "duration": 6.081
    },
    {
      "text": "training uh data loader has 130 batches",
      "start": 1740.88,
      "duration": 4.399
    },
    {
      "text": "in",
      "start": 1743.88,
      "duration": 4.56
    },
    {
      "text": "training and if the evaluation frequency",
      "start": 1745.279,
      "duration": 5.24
    },
    {
      "text": "is",
      "start": 1748.44,
      "duration": 5.479
    },
    {
      "text": "50 it means that for after 50 batches",
      "start": 1750.519,
      "duration": 8.4
    },
    {
      "text": "are processed after 50 batches are",
      "start": 1753.919,
      "duration": 5.0
    },
    {
      "text": "processed for each Epoch after 50",
      "start": 1759.48,
      "duration": 5.799
    },
    {
      "text": "batches are processed in each Epoch we",
      "start": 1762.399,
      "duration": 5.241
    },
    {
      "text": "print and what are we going to print we",
      "start": 1765.279,
      "duration": 4.041
    },
    {
      "text": "are going to print the training loss and",
      "start": 1767.64,
      "duration": 5.08
    },
    {
      "text": "we are going to print the validation",
      "start": 1769.32,
      "duration": 3.4
    },
    {
      "text": "loss so this evaluation frequency just",
      "start": 1772.88,
      "duration": 5.88
    },
    {
      "text": "specifies how after how many batches are",
      "start": 1776.159,
      "duration": 5.4
    },
    {
      "text": "completed we print the training and the",
      "start": 1778.76,
      "duration": 5.56
    },
    {
      "text": "validation loss so here later we are",
      "start": 1781.559,
      "duration": 4.521
    },
    {
      "text": "going to set the evaluation frequency to",
      "start": 1784.32,
      "duration": 3.959
    },
    {
      "text": "50 which means that after 50 batches are",
      "start": 1786.08,
      "duration": 4.479
    },
    {
      "text": "processed in each Epoch we are going to",
      "start": 1788.279,
      "duration": 4.161
    },
    {
      "text": "print so in every Epoch we are going to",
      "start": 1790.559,
      "duration": 3.641
    },
    {
      "text": "print on an average of two times because",
      "start": 1792.44,
      "duration": 4.959
    },
    {
      "text": "130 divided 50 is around 2.6 so we are",
      "start": 1794.2,
      "duration": 5.719
    },
    {
      "text": "going to print 2 two times in every",
      "start": 1797.399,
      "duration": 5.801
    },
    {
      "text": "Epoch okay awesome so now to print the",
      "start": 1799.919,
      "duration": 5.041
    },
    {
      "text": "training loss and the validation loss we",
      "start": 1803.2,
      "duration": 3.479
    },
    {
      "text": "are going to calculate the evaluate",
      "start": 1804.96,
      "duration": 4.36
    },
    {
      "text": "model so evaluate model gives you an",
      "start": 1806.679,
      "duration": 4.48
    },
    {
      "text": "option to specify the evaluation",
      "start": 1809.32,
      "duration": 3.88
    },
    {
      "text": "iteration which means that the number of",
      "start": 1811.159,
      "duration": 5.161
    },
    {
      "text": "batches you want to use for evaluation",
      "start": 1813.2,
      "duration": 4.479
    },
    {
      "text": "sometimes if you want to show quick",
      "start": 1816.32,
      "duration": 3.16
    },
    {
      "text": "evaluation on a sample data set you",
      "start": 1817.679,
      "duration": 4.161
    },
    {
      "text": "don't want to use all the batches so",
      "start": 1819.48,
      "duration": 4.12
    },
    {
      "text": "here you can just set the number of",
      "start": 1821.84,
      "duration": 4.12
    },
    {
      "text": "evaluation iterations to be five or 10",
      "start": 1823.6,
      "duration": 4.4
    },
    {
      "text": "since the number of batches is 130 this",
      "start": 1825.96,
      "duration": 3.4
    },
    {
      "text": "this will really save us time when we",
      "start": 1828.0,
      "duration": 2.72
    },
    {
      "text": "print out the train loss and the",
      "start": 1829.36,
      "duration": 3.72
    },
    {
      "text": "validation loss so this actually",
      "start": 1830.72,
      "duration": 4.799
    },
    {
      "text": "evaluation step is optional but when we",
      "start": 1833.08,
      "duration": 3.959
    },
    {
      "text": "do the training you'll see that the",
      "start": 1835.519,
      "duration": 2.921
    },
    {
      "text": "train loss and validation loss are",
      "start": 1837.039,
      "duration": 4.281
    },
    {
      "text": "printed after every 50 batches due to",
      "start": 1838.44,
      "duration": 4.16
    },
    {
      "text": "this evaluation",
      "start": 1841.32,
      "duration": 3.44
    },
    {
      "text": "step then what we are going to do is",
      "start": 1842.6,
      "duration": 4.12
    },
    {
      "text": "that after every Epoch we are going to",
      "start": 1844.76,
      "duration": 3.68
    },
    {
      "text": "calculate the training accuracy and",
      "start": 1846.72,
      "duration": 3.4
    },
    {
      "text": "validation accuracy and we are going to",
      "start": 1848.44,
      "duration": 4.16
    },
    {
      "text": "print it out so after every Epoch what",
      "start": 1850.12,
      "duration": 3.88
    },
    {
      "text": "we are going to do is that we are going",
      "start": 1852.6,
      "duration": 3.199
    },
    {
      "text": "to print the training and the validation",
      "start": 1854.0,
      "duration": 4.799
    },
    {
      "text": "accuracy and after every batches we are",
      "start": 1855.799,
      "duration": 4.72
    },
    {
      "text": "going to print the training loss and the",
      "start": 1858.799,
      "duration": 4.401
    },
    {
      "text": "validation loss so let's do the training",
      "start": 1860.519,
      "duration": 5.16
    },
    {
      "text": "process now for me this training process",
      "start": 1863.2,
      "duration": 5.4
    },
    {
      "text": "took uh around 8.8 minutes and I have a",
      "start": 1865.679,
      "duration": 6.041
    },
    {
      "text": "MacBook Air 2020 um it does not have",
      "start": 1868.6,
      "duration": 4.919
    },
    {
      "text": "very high end configurations but it's a",
      "start": 1871.72,
      "duration": 4.52
    },
    {
      "text": "good laptop if you have an i5 or i7",
      "start": 1873.519,
      "duration": 5.081
    },
    {
      "text": "laptop or a Macbook this training should",
      "start": 1876.24,
      "duration": 5.0
    },
    {
      "text": "take only 7 to 10 minutes for you so",
      "start": 1878.6,
      "duration": 4.4
    },
    {
      "text": "here you can see that this is the main",
      "start": 1881.24,
      "duration": 4.559
    },
    {
      "text": "code where we write about the training",
      "start": 1883.0,
      "duration": 5.039
    },
    {
      "text": "so we are going to use adamw optimal",
      "start": 1885.799,
      "duration": 4.681
    },
    {
      "text": "izer let me show you a bit about this t.",
      "start": 1888.039,
      "duration": 5.401
    },
    {
      "text": "optim adamw it's a modification of the",
      "start": 1890.48,
      "duration": 6.799
    },
    {
      "text": "Adam Optimizer with weight DEC so it's",
      "start": 1893.44,
      "duration": 6.839
    },
    {
      "text": "very good to avoid local Minima this",
      "start": 1897.279,
      "duration": 5.441
    },
    {
      "text": "algorithm converges in a smooth Manner",
      "start": 1900.279,
      "duration": 4.76
    },
    {
      "text": "and it also leads to faster convergence",
      "start": 1902.72,
      "duration": 4.16
    },
    {
      "text": "you can try various things here you can",
      "start": 1905.039,
      "duration": 4.281
    },
    {
      "text": "try Adam you can try to change the",
      "start": 1906.88,
      "duration": 4.679
    },
    {
      "text": "learning rate weight Decay so this is",
      "start": 1909.32,
      "duration": 4.479
    },
    {
      "text": "why this kind of code opens the door for",
      "start": 1911.559,
      "duration": 5.0
    },
    {
      "text": "research if you just use chat GPT you",
      "start": 1913.799,
      "duration": 4.681
    },
    {
      "text": "will never get to change",
      "start": 1916.559,
      "duration": 3.321
    },
    {
      "text": "all of these things which are happening",
      "start": 1918.48,
      "duration": 3.48
    },
    {
      "text": "under the hood but once I share this",
      "start": 1919.88,
      "duration": 4.12
    },
    {
      "text": "code with you you can try playing around",
      "start": 1921.96,
      "duration": 3.679
    },
    {
      "text": "with various parameters and try seeing",
      "start": 1924.0,
      "duration": 4.039
    },
    {
      "text": "the effect on the loss function on the",
      "start": 1925.639,
      "duration": 3.52
    },
    {
      "text": "accuracy",
      "start": 1928.039,
      "duration": 3.52
    },
    {
      "text": "Etc so this is the optimizer which we",
      "start": 1929.159,
      "duration": 4.441
    },
    {
      "text": "have defined right now and then what we",
      "start": 1931.559,
      "duration": 3.521
    },
    {
      "text": "are going to do is that we're going to",
      "start": 1933.6,
      "duration": 4.72
    },
    {
      "text": "call this train classifier simple so I'm",
      "start": 1935.08,
      "duration": 5.0
    },
    {
      "text": "calling this train classifier simple",
      "start": 1938.32,
      "duration": 3.92
    },
    {
      "text": "function and I have to I have to pass",
      "start": 1940.08,
      "duration": 4.719
    },
    {
      "text": "the model so the model which I'm passing",
      "start": 1942.24,
      "duration": 6.2
    },
    {
      "text": "in is the GPT model class which we have",
      "start": 1944.799,
      "duration": 6.76
    },
    {
      "text": "created with the modified architecture",
      "start": 1948.44,
      "duration": 5.16
    },
    {
      "text": "so the modified architecture is this",
      "start": 1951.559,
      "duration": 3.96
    },
    {
      "text": "where the architecture has a",
      "start": 1953.6,
      "duration": 3.88
    },
    {
      "text": "classification head on top of",
      "start": 1955.519,
      "duration": 4.561
    },
    {
      "text": "it let me show you yeah this is the",
      "start": 1957.48,
      "duration": 4.48
    },
    {
      "text": "modified architecture which has this",
      "start": 1960.08,
      "duration": 4.64
    },
    {
      "text": "classification head on top of it this is",
      "start": 1961.96,
      "duration": 5.8
    },
    {
      "text": "the model which we are passing",
      "start": 1964.72,
      "duration": 5.88
    },
    {
      "text": "in and then we pass the train loader the",
      "start": 1967.76,
      "duration": 4.799
    },
    {
      "text": "validation loader the optimizer which is",
      "start": 1970.6,
      "duration": 5.199
    },
    {
      "text": "the admw uh number of epo evaluation",
      "start": 1972.559,
      "duration": 5.441
    },
    {
      "text": "frequency so this evaluation frequency",
      "start": 1975.799,
      "duration": 4.161
    },
    {
      "text": "as I mentioned here is after 50 batches",
      "start": 1978.0,
      "duration": 3.639
    },
    {
      "text": "we print the train loss and validation",
      "start": 1979.96,
      "duration": 4.0
    },
    {
      "text": "loss and evaluation iteration is",
      "start": 1981.639,
      "duration": 4.4
    },
    {
      "text": "basically when you print this train loss",
      "start": 1983.96,
      "duration": 4.199
    },
    {
      "text": "and validation loss how many batches you",
      "start": 1986.039,
      "duration": 5.041
    },
    {
      "text": "want to evaluate so I'm just doing five",
      "start": 1988.159,
      "duration": 4.48
    },
    {
      "text": "batches here so that the calculations",
      "start": 1991.08,
      "duration": 3.36
    },
    {
      "text": "would be quick if you do evaluation",
      "start": 1992.639,
      "duration": 3.801
    },
    {
      "text": "iteration equal to 50 batches or 100",
      "start": 1994.44,
      "duration": 3.719
    },
    {
      "text": "batches it will just take more time to",
      "start": 1996.44,
      "duration": 4.239
    },
    {
      "text": "do the evaluation of course this is not",
      "start": 1998.159,
      "duration": 4.601
    },
    {
      "text": "the best way to evaluate evaluate",
      "start": 2000.679,
      "duration": 4.24
    },
    {
      "text": "because we are only evaluating on five",
      "start": 2002.76,
      "duration": 4.639
    },
    {
      "text": "later in I have a code where we actually",
      "start": 2004.919,
      "duration": 5.321
    },
    {
      "text": "evalate on the entire data set for now",
      "start": 2007.399,
      "duration": 4.481
    },
    {
      "text": "this gives us a good sense at every",
      "start": 2010.24,
      "duration": 3.48
    },
    {
      "text": "iteration how the training loss and",
      "start": 2011.88,
      "duration": 4.48
    },
    {
      "text": "validation loss is progressing awesome",
      "start": 2013.72,
      "duration": 4.36
    },
    {
      "text": "so after I run this code you can see",
      "start": 2016.36,
      "duration": 4.12
    },
    {
      "text": "that I've already run it and it's 8.83",
      "start": 2018.08,
      "duration": 4.4
    },
    {
      "text": "minutes so if you look at the training",
      "start": 2020.48,
      "duration": 4.76
    },
    {
      "text": "loss the training loss goes down to",
      "start": 2022.48,
      "duration": 5.319
    },
    {
      "text": "0.083 and the validation loss goes down",
      "start": 2025.24,
      "duration": 4.399
    },
    {
      "text": "to",
      "start": 2027.799,
      "duration": 4.441
    },
    {
      "text": "0.074 training accuracy improves to",
      "start": 2029.639,
      "duration": 5.92
    },
    {
      "text": "around 100% And validation accuracy is",
      "start": 2032.24,
      "duration": 6.84
    },
    {
      "text": "97.5% you can even print the",
      "start": 2035.559,
      "duration": 5.36
    },
    {
      "text": "training loss and validation loss and",
      "start": 2039.08,
      "duration": 3.559
    },
    {
      "text": "along with it you can also print the",
      "start": 2040.919,
      "duration": 4.441
    },
    {
      "text": "example scene because then you can see",
      "start": 2042.639,
      "duration": 4.92
    },
    {
      "text": "the more examples the model sees the",
      "start": 2045.36,
      "duration": 3.92
    },
    {
      "text": "more text messages you can see that the",
      "start": 2047.559,
      "duration": 3.84
    },
    {
      "text": "training loss goes down as indicated by",
      "start": 2049.28,
      "duration": 4.839
    },
    {
      "text": "the blue line and the validation loss",
      "start": 2051.399,
      "duration": 4.28
    },
    {
      "text": "also goes down as indicated by the",
      "start": 2054.119,
      "duration": 3.56
    },
    {
      "text": "Orange Line This is actually perfect",
      "start": 2055.679,
      "duration": 3.601
    },
    {
      "text": "training because training loss is very",
      "start": 2057.679,
      "duration": 3.72
    },
    {
      "text": "low validation loss is also very low",
      "start": 2059.28,
      "duration": 3.839
    },
    {
      "text": "that's awesome that indicates that there",
      "start": 2061.399,
      "duration": 5.0
    },
    {
      "text": "is not too much overfitting here so as",
      "start": 2063.119,
      "duration": 5.04
    },
    {
      "text": "we can see based on the sharp downward",
      "start": 2066.399,
      "duration": 3.601
    },
    {
      "text": "slope the model is learning well from",
      "start": 2068.159,
      "duration": 4.2
    },
    {
      "text": "the training data and there is little to",
      "start": 2070.0,
      "duration": 4.56
    },
    {
      "text": "no indication of overfitting that is",
      "start": 2072.359,
      "duration": 4.0
    },
    {
      "text": "there is no noticeable gap between the",
      "start": 2074.56,
      "duration": 3.96
    },
    {
      "text": "training and the validation set",
      "start": 2076.359,
      "duration": 5.121
    },
    {
      "text": "losses that is exactly what we wanted if",
      "start": 2078.52,
      "duration": 4.68
    },
    {
      "text": "the validation loss is much higher than",
      "start": 2081.48,
      "duration": 2.84
    },
    {
      "text": "training loss let's say if the",
      "start": 2083.2,
      "duration": 3.32
    },
    {
      "text": "validation loss is somewhere here that",
      "start": 2084.32,
      "duration": 3.599
    },
    {
      "text": "is a sign of",
      "start": 2086.52,
      "duration": 3.76
    },
    {
      "text": "overfitting Now using the same plot we",
      "start": 2087.919,
      "duration": 3.96
    },
    {
      "text": "can also plot the classification",
      "start": 2090.28,
      "duration": 3.639
    },
    {
      "text": "accuracies so as the loss is decreasing",
      "start": 2091.879,
      "duration": 3.72
    },
    {
      "text": "the training and the validation loss you",
      "start": 2093.919,
      "duration": 3.44
    },
    {
      "text": "can also see that the training accuracy",
      "start": 2095.599,
      "duration": 3.921
    },
    {
      "text": "has shown by the blue line is increasing",
      "start": 2097.359,
      "duration": 4.521
    },
    {
      "text": "and then it reaches one the validation",
      "start": 2099.52,
      "duration": 4.2
    },
    {
      "text": "accuracy also increases and it reaches",
      "start": 2101.88,
      "duration": 3.88
    },
    {
      "text": "around 97 and",
      "start": 2103.72,
      "duration": 5.08
    },
    {
      "text": "plate uh one thing to note is that it's",
      "start": 2105.76,
      "duration": 4.52
    },
    {
      "text": "important to note that we have set",
      "start": 2108.8,
      "duration": 3.72
    },
    {
      "text": "evaluation iteration to be equal to five",
      "start": 2110.28,
      "duration": 4.68
    },
    {
      "text": "as I mentioned over here we have set the",
      "start": 2112.52,
      "duration": 4.96
    },
    {
      "text": "evaluation iteration to be equal to five",
      "start": 2114.96,
      "duration": 4.76
    },
    {
      "text": "so that's not so the values which we are",
      "start": 2117.48,
      "duration": 3.879
    },
    {
      "text": "seeing here of the accuracy are not",
      "start": 2119.72,
      "duration": 3.76
    },
    {
      "text": "representative of the accuracy on the",
      "start": 2121.359,
      "duration": 4.601
    },
    {
      "text": "entire data set since we only evaluate",
      "start": 2123.48,
      "duration": 4.76
    },
    {
      "text": "on five batches so this this means that",
      "start": 2125.96,
      "duration": 4.159
    },
    {
      "text": "our training and validation performance",
      "start": 2128.24,
      "duration": 3.68
    },
    {
      "text": "were based on only five batches for",
      "start": 2130.119,
      "duration": 4.561
    },
    {
      "text": "efficiency during training to calculate",
      "start": 2131.92,
      "duration": 4.48
    },
    {
      "text": "the performance matrics for the training",
      "start": 2134.68,
      "duration": 4.0
    },
    {
      "text": "validation and entire testing set for",
      "start": 2136.4,
      "duration": 4.08
    },
    {
      "text": "the full data",
      "start": 2138.68,
      "duration": 4.439
    },
    {
      "text": "set uh we can also do that so all we",
      "start": 2140.48,
      "duration": 4.2
    },
    {
      "text": "need to do is that then we have to run",
      "start": 2143.119,
      "duration": 4.0
    },
    {
      "text": "the calculation accuracy loader and then",
      "start": 2144.68,
      "duration": 4.12
    },
    {
      "text": "we have to pass in the train loader we",
      "start": 2147.119,
      "duration": 3.401
    },
    {
      "text": "have to pass in the model and we have to",
      "start": 2148.8,
      "duration": 4.44
    },
    {
      "text": "pass in the device either it's a CPU or",
      "start": 2150.52,
      "duration": 5.559
    },
    {
      "text": "a GPU so what this calculation accuracy",
      "start": 2153.24,
      "duration": 4.4
    },
    {
      "text": "loader will do as we have already",
      "start": 2156.079,
      "duration": 5.561
    },
    {
      "text": "defined earlier uh this calcul calculate",
      "start": 2157.64,
      "duration": 6.84
    },
    {
      "text": "uh this calculate accuracy loader will",
      "start": 2161.64,
      "duration": 5.0
    },
    {
      "text": "take in our model and then it will do",
      "start": 2164.48,
      "duration": 3.839
    },
    {
      "text": "the prediction it will compare it with",
      "start": 2166.64,
      "duration": 3.439
    },
    {
      "text": "the actual value then it will print out",
      "start": 2168.319,
      "duration": 3.721
    },
    {
      "text": "the accuracy and it will do this for all",
      "start": 2170.079,
      "duration": 3.921
    },
    {
      "text": "the batches in the training set so it's",
      "start": 2172.04,
      "duration": 6.44
    },
    {
      "text": "not only five batches so this um this",
      "start": 2174.0,
      "duration": 6.119
    },
    {
      "text": "accuracy measure for the training",
      "start": 2178.48,
      "duration": 3.56
    },
    {
      "text": "testing and validation data set is a",
      "start": 2180.119,
      "duration": 4.681
    },
    {
      "text": "much better representative than these",
      "start": 2182.04,
      "duration": 4.92
    },
    {
      "text": "plots because these plots are only for",
      "start": 2184.8,
      "duration": 4.08
    },
    {
      "text": "evaluation iteration which was set to be",
      "start": 2186.96,
      "duration": 3.0
    },
    {
      "text": "equal to",
      "start": 2188.88,
      "duration": 4.6
    },
    {
      "text": "five so let's print out these train",
      "start": 2189.96,
      "duration": 5.359
    },
    {
      "text": "accuracy validation accuracy and test",
      "start": 2193.48,
      "duration": 4.0
    },
    {
      "text": "accuracy on the entire data set so when",
      "start": 2195.319,
      "duration": 3.641
    },
    {
      "text": "you print out these you will see that",
      "start": 2197.48,
      "duration": 3.24
    },
    {
      "text": "the training accuracy is",
      "start": 2198.96,
      "duration": 5.159
    },
    {
      "text": "97% the validation accuracy is also 97%",
      "start": 2200.72,
      "duration": 5.24
    },
    {
      "text": "and the test accuracy is",
      "start": 2204.119,
      "duration": 4.521
    },
    {
      "text": "95% so the training and the test set",
      "start": 2205.96,
      "duration": 5.159
    },
    {
      "text": "performances are almost identical a",
      "start": 2208.64,
      "duration": 4.439
    },
    {
      "text": "slight discrepancy between the training",
      "start": 2211.119,
      "duration": 4.601
    },
    {
      "text": "and the test set accuracy so the test",
      "start": 2213.079,
      "duration": 4.121
    },
    {
      "text": "set accuracy slightly less right",
      "start": 2215.72,
      "duration": 3.2
    },
    {
      "text": "compared to the training it suggests",
      "start": 2217.2,
      "duration": 3.159
    },
    {
      "text": "that there is small amount of",
      "start": 2218.92,
      "duration": 3.88
    },
    {
      "text": "overfitting although there's small only",
      "start": 2220.359,
      "duration": 4.281
    },
    {
      "text": "2% difference is there but it still",
      "start": 2222.8,
      "duration": 3.2
    },
    {
      "text": "indicates that slight amount of",
      "start": 2224.64,
      "duration": 3.04
    },
    {
      "text": "overfitting is there on the training",
      "start": 2226.0,
      "duration": 4.119
    },
    {
      "text": "data typically the validation set",
      "start": 2227.68,
      "duration": 3.919
    },
    {
      "text": "accuracy is somewhat higher than the",
      "start": 2230.119,
      "duration": 3.841
    },
    {
      "text": "test set accuracy because the model",
      "start": 2231.599,
      "duration": 4.321
    },
    {
      "text": "development often involves fine-tuning",
      "start": 2233.96,
      "duration": 4.6
    },
    {
      "text": "parameters on the validation",
      "start": 2235.92,
      "duration": 5.32
    },
    {
      "text": "set this situation is common but the Gap",
      "start": 2238.56,
      "duration": 4.32
    },
    {
      "text": "could potentially be minimized by",
      "start": 2241.24,
      "duration": 3.68
    },
    {
      "text": "adjusting the model settings such as",
      "start": 2242.88,
      "duration": 4.08
    },
    {
      "text": "increasing the dropout rate or the",
      "start": 2244.92,
      "duration": 4.0
    },
    {
      "text": "weight Decap parameter in the optimizer",
      "start": 2246.96,
      "duration": 4.159
    },
    {
      "text": "configuration as I mentioned before once",
      "start": 2248.92,
      "duration": 3.96
    },
    {
      "text": "I share this notebook with you you will",
      "start": 2251.119,
      "duration": 4.681
    },
    {
      "text": "have a lot of scope to experiment so you",
      "start": 2252.88,
      "duration": 4.76
    },
    {
      "text": "can experiment with dropout rate in the",
      "start": 2255.8,
      "duration": 3.36
    },
    {
      "text": "model architecture you can even",
      "start": 2257.64,
      "duration": 3.32
    },
    {
      "text": "experiment with learning rate parameter",
      "start": 2259.16,
      "duration": 3.8
    },
    {
      "text": "weight DK parameter in the",
      "start": 2260.96,
      "duration": 5.04
    },
    {
      "text": "optimizer um you can also experiment",
      "start": 2262.96,
      "duration": 4.76
    },
    {
      "text": "with things like unfreezing certain",
      "start": 2266.0,
      "duration": 3.68
    },
    {
      "text": "parameters so if if you remember from",
      "start": 2267.72,
      "duration": 4.52
    },
    {
      "text": "our previous lecture the only parameters",
      "start": 2269.68,
      "duration": 4.76
    },
    {
      "text": "which are being trained here is of",
      "start": 2272.24,
      "duration": 4.0
    },
    {
      "text": "course the output classification head",
      "start": 2274.44,
      "duration": 3.48
    },
    {
      "text": "and along with that we are also training",
      "start": 2276.24,
      "duration": 3.72
    },
    {
      "text": "the last Transformer block the 12th",
      "start": 2277.92,
      "duration": 4.04
    },
    {
      "text": "Transformer block and the final",
      "start": 2279.96,
      "duration": 4.08
    },
    {
      "text": "normalization layer you can do some",
      "start": 2281.96,
      "duration": 3.879
    },
    {
      "text": "changes here so you can make sure that",
      "start": 2284.04,
      "duration": 3.76
    },
    {
      "text": "the last three Transformer blocks are",
      "start": 2285.839,
      "duration": 5.841
    },
    {
      "text": "trained Etc you can make sure that maybe",
      "start": 2287.8,
      "duration": 5.72
    },
    {
      "text": "this is false and that leads to better",
      "start": 2291.68,
      "duration": 4.96
    },
    {
      "text": "answers who knows so this kind of",
      "start": 2293.52,
      "duration": 5.72
    },
    {
      "text": "experimentation is open and I'll be very",
      "start": 2296.64,
      "duration": 4.439
    },
    {
      "text": "happy if you experiment with various",
      "start": 2299.24,
      "duration": 3.839
    },
    {
      "text": "options that will even improve your",
      "start": 2301.079,
      "duration": 4.201
    },
    {
      "text": "understanding further and try to see if",
      "start": 2303.079,
      "duration": 5.0
    },
    {
      "text": "you can increase the test accur further",
      "start": 2305.28,
      "duration": 5.72
    },
    {
      "text": "to match that of the training",
      "start": 2308.079,
      "duration": 5.401
    },
    {
      "text": "accuracy awesome so until now what we",
      "start": 2311.0,
      "duration": 4.599
    },
    {
      "text": "have done is that we have",
      "start": 2313.48,
      "duration": 6.119
    },
    {
      "text": "uh um let's see what all we have done we",
      "start": 2315.599,
      "duration": 7.281
    },
    {
      "text": "have fine tuned on the supervised data",
      "start": 2319.599,
      "duration": 5.161
    },
    {
      "text": "and we have even plotted the training",
      "start": 2322.88,
      "duration": 4.04
    },
    {
      "text": "and the validation loss now the last",
      "start": 2324.76,
      "duration": 3.88
    },
    {
      "text": "step is remaining which is using model",
      "start": 2326.92,
      "duration": 4.199
    },
    {
      "text": "on new data so whatever is shown in the",
      "start": 2328.64,
      "duration": 4.439
    },
    {
      "text": "tick mark here downloading the data set",
      "start": 2331.119,
      "duration": 3.841
    },
    {
      "text": "pre-processing the data set creating",
      "start": 2333.079,
      "duration": 4.641
    },
    {
      "text": "data loaders initializing the model load",
      "start": 2334.96,
      "duration": 5.28
    },
    {
      "text": "pre-train weights modify model for fine",
      "start": 2337.72,
      "duration": 4.639
    },
    {
      "text": "tuning Implement loss and accuracy",
      "start": 2340.24,
      "duration": 4.879
    },
    {
      "text": "functions then actually doing the backo",
      "start": 2342.359,
      "duration": 4.72
    },
    {
      "text": "pass and fine tuning the model and",
      "start": 2345.119,
      "duration": 3.96
    },
    {
      "text": "training and validating the model these",
      "start": 2347.079,
      "duration": 4.24
    },
    {
      "text": "nine steps we have done now what we have",
      "start": 2349.079,
      "duration": 3.961
    },
    {
      "text": "to do is that we have to use the model",
      "start": 2351.319,
      "duration": 4.121
    },
    {
      "text": "on new data which the model has not seen",
      "start": 2353.04,
      "duration": 4.559
    },
    {
      "text": "before so that is the real test whether",
      "start": 2355.44,
      "duration": 4.679
    },
    {
      "text": "our model our large language model how",
      "start": 2357.599,
      "duration": 4.921
    },
    {
      "text": "its performance is as a Spam",
      "start": 2360.119,
      "duration": 4.561
    },
    {
      "text": "classifier so let's go to the last",
      "start": 2362.52,
      "duration": 4.64
    },
    {
      "text": "section of this project right now and uh",
      "start": 2364.68,
      "duration": 4.28
    },
    {
      "text": "let's see whether our model is actually",
      "start": 2367.16,
      "duration": 4.28
    },
    {
      "text": "performing well on data which it has not",
      "start": 2368.96,
      "duration": 5.32
    },
    {
      "text": "seen so after fine-tuning and evaluating",
      "start": 2371.44,
      "duration": 4.56
    },
    {
      "text": "the model in the previous sections we",
      "start": 2374.28,
      "duration": 3.36
    },
    {
      "text": "are now in the final stage of this",
      "start": 2376.0,
      "duration": 3.64
    },
    {
      "text": "chapter where we will use the model to",
      "start": 2377.64,
      "duration": 5.76
    },
    {
      "text": "classify spam messages right so finally",
      "start": 2379.64,
      "duration": 6.28
    },
    {
      "text": "let's use the fine tuned GPT based spam",
      "start": 2383.4,
      "duration": 5.08
    },
    {
      "text": "spam classification model we'll need to",
      "start": 2385.92,
      "duration": 4.159
    },
    {
      "text": "define a function first we'll need to",
      "start": 2388.48,
      "duration": 3.8
    },
    {
      "text": "define a function called classify review",
      "start": 2390.079,
      "duration": 3.961
    },
    {
      "text": "which will take in any text and it will",
      "start": 2392.28,
      "duration": 4.44
    },
    {
      "text": "predict whether it's a Spam or not and",
      "start": 2394.04,
      "duration": 4.16
    },
    {
      "text": "what this function will do is that it",
      "start": 2396.72,
      "duration": 4.119
    },
    {
      "text": "will do a number of things first it will",
      "start": 2398.2,
      "duration": 4.28
    },
    {
      "text": "uh and let me actually write this down",
      "start": 2400.839,
      "duration": 3.721
    },
    {
      "text": "in description so let's say a text is",
      "start": 2402.48,
      "duration": 5.16
    },
    {
      "text": "given such as",
      "start": 2404.56,
      "duration": 5.559
    },
    {
      "text": "you let's say a text is given such as",
      "start": 2407.64,
      "duration": 5.76
    },
    {
      "text": "you on a lottery right if a text is",
      "start": 2410.119,
      "duration": 5.161
    },
    {
      "text": "given the first thing which we will do",
      "start": 2413.4,
      "duration": 3.48
    },
    {
      "text": "is that we'll convert this text into",
      "start": 2415.28,
      "duration": 2.64
    },
    {
      "text": "token",
      "start": 2416.88,
      "duration": 3.56
    },
    {
      "text": "IDs we'll convert this text into token",
      "start": 2417.92,
      "duration": 3.8
    },
    {
      "text": "IDs actually there is a nice",
      "start": 2420.44,
      "duration": 2.52
    },
    {
      "text": "representation of the data",
      "start": 2421.72,
      "duration": 2.8
    },
    {
      "text": "pre-processing which we had looked at",
      "start": 2422.96,
      "duration": 3.96
    },
    {
      "text": "before I'm just I'll just take you to",
      "start": 2424.52,
      "duration": 4.72
    },
    {
      "text": "that part so that you can see how this",
      "start": 2426.92,
      "duration": 5.04
    },
    {
      "text": "yeah so if a new text is given we'll",
      "start": 2429.24,
      "duration": 4.879
    },
    {
      "text": "first convert the text into token IDs",
      "start": 2431.96,
      "duration": 3.8
    },
    {
      "text": "something like this and that's the first",
      "start": 2434.119,
      "duration": 3.561
    },
    {
      "text": "thing which we have written in the code",
      "start": 2435.76,
      "duration": 4.52
    },
    {
      "text": "we'll first use tokenizer in code so",
      "start": 2437.68,
      "duration": 4.36
    },
    {
      "text": "this is the tick",
      "start": 2440.28,
      "duration": 4.36
    },
    {
      "text": "token this is the tick token tokenizer",
      "start": 2442.04,
      "duration": 3.96
    },
    {
      "text": "which we are going to use it's a bite",
      "start": 2444.64,
      "duration": 3.679
    },
    {
      "text": "pair encoder it takes in any sentence",
      "start": 2446.0,
      "duration": 4.16
    },
    {
      "text": "and converts it into a bunch of tokens",
      "start": 2448.319,
      "duration": 4.561
    },
    {
      "text": "right then we will uh we'll look at the",
      "start": 2450.16,
      "duration": 4.84
    },
    {
      "text": "supported context length and that's",
      "start": 2452.88,
      "duration": 5.28
    },
    {
      "text": "equal to uh 1024 in this this case",
      "start": 2455.0,
      "duration": 6.839
    },
    {
      "text": "because the uh so model. positional",
      "start": 2458.16,
      "duration": 6.56
    },
    {
      "text": "embedding weight shape that is a shape",
      "start": 2461.839,
      "duration": 5.48
    },
    {
      "text": "of the embedding weight Matrix and to",
      "start": 2464.72,
      "duration": 4.32
    },
    {
      "text": "give you an idea of what the shape size",
      "start": 2467.319,
      "duration": 4.081
    },
    {
      "text": "is it has the number of rows equal to",
      "start": 2469.04,
      "duration": 4.559
    },
    {
      "text": "the context length and it has number of",
      "start": 2471.4,
      "duration": 4.32
    },
    {
      "text": "columns equal to the embedding",
      "start": 2473.599,
      "duration": 4.641
    },
    {
      "text": "Dimension so the number of rows will",
      "start": 2475.72,
      "duration": 4.119
    },
    {
      "text": "give us the context length and that's",
      "start": 2478.24,
      "duration": 3.599
    },
    {
      "text": "why we are using the embedding shape",
      "start": 2479.839,
      "duration": 4.641
    },
    {
      "text": "zero to find the context",
      "start": 2481.839,
      "duration": 4.841
    },
    {
      "text": "length so the reason we find this",
      "start": 2484.48,
      "duration": 3.8
    },
    {
      "text": "context length is that we are going to",
      "start": 2486.68,
      "duration": 3.6
    },
    {
      "text": "compare it with the maximum length so",
      "start": 2488.28,
      "duration": 3.799
    },
    {
      "text": "what we did here is that we have we have",
      "start": 2490.28,
      "duration": 4.319
    },
    {
      "text": "found the maximum token length token ID",
      "start": 2492.079,
      "duration": 4.52
    },
    {
      "text": "length from the training set which means",
      "start": 2494.599,
      "duration": 3.52
    },
    {
      "text": "which is the text message which is the",
      "start": 2496.599,
      "duration": 3.801
    },
    {
      "text": "longest and we have got that length",
      "start": 2498.119,
      "duration": 5.841
    },
    {
      "text": "let's say that length is equal to 120 so",
      "start": 2500.4,
      "duration": 6.199
    },
    {
      "text": "uh if that length is equal to if that",
      "start": 2503.96,
      "duration": 4.639
    },
    {
      "text": "length is actually higher than the",
      "start": 2506.599,
      "duration": 4.401
    },
    {
      "text": "context length then we have to trunet",
      "start": 2508.599,
      "duration": 4.76
    },
    {
      "text": "everything down to the context length so",
      "start": 2511.0,
      "duration": 5.079
    },
    {
      "text": "sequences which are way higher than the",
      "start": 2513.359,
      "duration": 4.401
    },
    {
      "text": "maximum length we have to find the",
      "start": 2516.079,
      "duration": 3.481
    },
    {
      "text": "minimum of the maximum length and the",
      "start": 2517.76,
      "duration": 3.599
    },
    {
      "text": "supported context length so if the",
      "start": 2519.56,
      "duration": 3.799
    },
    {
      "text": "maximum length is actually higher than",
      "start": 2521.359,
      "duration": 5.441
    },
    {
      "text": "1024 then we are going to take the",
      "start": 2523.359,
      "duration": 6.161
    },
    {
      "text": "context length and truncate all the",
      "start": 2526.8,
      "duration": 4.44
    },
    {
      "text": "sequences to be equal to the context",
      "start": 2529.52,
      "duration": 3.799
    },
    {
      "text": "length in the cases where this does not",
      "start": 2531.24,
      "duration": 4.4
    },
    {
      "text": "happen our maximum length will be used",
      "start": 2533.319,
      "duration": 5.28
    },
    {
      "text": "and then all the input text will will",
      "start": 2535.64,
      "duration": 5.199
    },
    {
      "text": "have those many token IDs so let's say",
      "start": 2538.599,
      "duration": 2.921
    },
    {
      "text": "if",
      "start": 2540.839,
      "duration": 4.121
    },
    {
      "text": "the uh maximum length is 120 and you",
      "start": 2541.52,
      "duration": 6.16
    },
    {
      "text": "have received a text message such as",
      "start": 2544.96,
      "duration": 5.48
    },
    {
      "text": "uh you have won a",
      "start": 2547.68,
      "duration": 2.76
    },
    {
      "text": "lottery let's say you have received this",
      "start": 2551.28,
      "duration": 4.16
    },
    {
      "text": "text message and when you convert it",
      "start": 2553.4,
      "duration": 4.36
    },
    {
      "text": "into token IDs you you have seen that",
      "start": 2555.44,
      "duration": 5.0
    },
    {
      "text": "the length is only 50 so what you will",
      "start": 2557.76,
      "duration": 4.68
    },
    {
      "text": "do is that you have to extend this to",
      "start": 2560.44,
      "duration": 5.0
    },
    {
      "text": "120 by adding some end of text",
      "start": 2562.44,
      "duration": 6.08
    },
    {
      "text": "tokens so you add 70 end of text tokens",
      "start": 2565.44,
      "duration": 4.36
    },
    {
      "text": "here which are this",
      "start": 2568.52,
      "duration": 3.799
    },
    {
      "text": "50256 and you make sure that the length",
      "start": 2569.8,
      "duration": 5.0
    },
    {
      "text": "of the uh text is equal to the maximum",
      "start": 2572.319,
      "duration": 4.401
    },
    {
      "text": "length this is very important because",
      "start": 2574.8,
      "duration": 4.44
    },
    {
      "text": "when you batch it every sentence needs",
      "start": 2576.72,
      "duration": 5.16
    },
    {
      "text": "to have the same number of token IDs so",
      "start": 2579.24,
      "duration": 4.72
    },
    {
      "text": "you have to pad this you have to pad",
      "start": 2581.88,
      "duration": 4.28
    },
    {
      "text": "every input sequence to match the",
      "start": 2583.96,
      "duration": 4.76
    },
    {
      "text": "maximum length so the maximum length",
      "start": 2586.16,
      "duration": 5.04
    },
    {
      "text": "ideally is the length which we have got",
      "start": 2588.72,
      "duration": 4.48
    },
    {
      "text": "from our training data set so what's the",
      "start": 2591.2,
      "duration": 3.48
    },
    {
      "text": "maximum email length in the training",
      "start": 2593.2,
      "duration": 3.32
    },
    {
      "text": "data set but if it's higher than the",
      "start": 2594.68,
      "duration": 3.52
    },
    {
      "text": "context length the maximum length will",
      "start": 2596.52,
      "duration": 4.2
    },
    {
      "text": "be set equal to the context length so",
      "start": 2598.2,
      "duration": 4.6
    },
    {
      "text": "whenever you give a new test input it's",
      "start": 2600.72,
      "duration": 4.28
    },
    {
      "text": "first converted into token IDs and then",
      "start": 2602.8,
      "duration": 4.519
    },
    {
      "text": "it's padded with this end of text token",
      "start": 2605.0,
      "duration": 4.44
    },
    {
      "text": "which is 50256 so that the length is",
      "start": 2607.319,
      "duration": 6.121
    },
    {
      "text": "equal to the uh length is equal to the",
      "start": 2609.44,
      "duration": 6.52
    },
    {
      "text": "maximum length then we convert it into a",
      "start": 2613.44,
      "duration": 5.639
    },
    {
      "text": "tensor to add the batch Dimension uh and",
      "start": 2615.96,
      "duration": 4.92
    },
    {
      "text": "then we perform the model inference so",
      "start": 2619.079,
      "duration": 3.721
    },
    {
      "text": "we first calculate the prediction so we",
      "start": 2620.88,
      "duration": 3.84
    },
    {
      "text": "get the logit tensor which is the logits",
      "start": 2622.8,
      "duration": 4.279
    },
    {
      "text": "of the last output token and then we",
      "start": 2624.72,
      "duration": 5.879
    },
    {
      "text": "apply torch. arac so we have seen this",
      "start": 2627.079,
      "duration": 6.561
    },
    {
      "text": "implementation um let me recap your",
      "start": 2630.599,
      "duration": 3.881
    },
    {
      "text": "understanding we have seen this",
      "start": 2633.64,
      "duration": 2.56
    },
    {
      "text": "implementation in this part of the code",
      "start": 2634.48,
      "duration": 3.839
    },
    {
      "text": "right where we take the AR Max and this",
      "start": 2636.2,
      "duration": 3.399
    },
    {
      "text": "gives us the prediction whether it's",
      "start": 2638.319,
      "duration": 4.0
    },
    {
      "text": "spam or not a Spam and then that is our",
      "start": 2639.599,
      "duration": 5.081
    },
    {
      "text": "final answer so this model this model",
      "start": 2642.319,
      "duration": 4.641
    },
    {
      "text": "here is our train model which we are",
      "start": 2644.68,
      "duration": 4.96
    },
    {
      "text": "using now for inference for inference on",
      "start": 2646.96,
      "duration": 6.24
    },
    {
      "text": "any new text message so the main magic",
      "start": 2649.64,
      "duration": 5.439
    },
    {
      "text": "happens in this line where our input",
      "start": 2653.2,
      "duration": 4.04
    },
    {
      "text": "tensor is passed through this model and",
      "start": 2655.079,
      "duration": 5.0
    },
    {
      "text": "then we predict the label but before",
      "start": 2657.24,
      "duration": 4.64
    },
    {
      "text": "that we have to make sure that the token",
      "start": 2660.079,
      "duration": 4.76
    },
    {
      "text": "IDs are equal to the maximum length now",
      "start": 2661.88,
      "duration": 4.64
    },
    {
      "text": "what now let us actually take two",
      "start": 2664.839,
      "duration": 3.601
    },
    {
      "text": "sentences and let us pass them through",
      "start": 2666.52,
      "duration": 4.16
    },
    {
      "text": "our classify review function and let's",
      "start": 2668.44,
      "duration": 4.159
    },
    {
      "text": "see whether our model predicts them as",
      "start": 2670.68,
      "duration": 4.56
    },
    {
      "text": "as spam or no spam so the first sentence",
      "start": 2672.599,
      "duration": 4.72
    },
    {
      "text": "I'm taking is you are a winner you have",
      "start": 2675.24,
      "duration": 4.56
    },
    {
      "text": "been specially selected to receive ,000",
      "start": 2677.319,
      "duration": 5.321
    },
    {
      "text": "cash or $2,000 reward clearly it looks",
      "start": 2679.8,
      "duration": 4.799
    },
    {
      "text": "like a Spam rate and this is from a",
      "start": 2682.64,
      "duration": 3.84
    },
    {
      "text": "testing set my model has not seen it in",
      "start": 2684.599,
      "duration": 4.081
    },
    {
      "text": "the training data I'm going to pass it",
      "start": 2686.48,
      "duration": 4.28
    },
    {
      "text": "through the classifier review function",
      "start": 2688.68,
      "duration": 4.08
    },
    {
      "text": "and let me print out the output and our",
      "start": 2690.76,
      "duration": 4.2
    },
    {
      "text": "model is clearly recognizing the output",
      "start": 2692.76,
      "duration": 4.72
    },
    {
      "text": "to be that this is a spam then let's",
      "start": 2694.96,
      "duration": 4.48
    },
    {
      "text": "take a second sentence hey just wanted",
      "start": 2697.48,
      "duration": 3.879
    },
    {
      "text": "to check if we are still on for dinner",
      "start": 2699.44,
      "duration": 4.44
    },
    {
      "text": "tonight let me know I'll again pass it",
      "start": 2701.359,
      "duration": 4.121
    },
    {
      "text": "through the model and I'll check whether",
      "start": 2703.88,
      "duration": 3.479
    },
    {
      "text": "it's spam or no spam this looks like a",
      "start": 2705.48,
      "duration": 3.52
    },
    {
      "text": "very legitimate message right and it's",
      "start": 2707.359,
      "duration": 4.2
    },
    {
      "text": "clearly not a Spam and model makes a",
      "start": 2709.0,
      "duration": 4.599
    },
    {
      "text": "correct prediction that it's not a",
      "start": 2711.559,
      "duration": 4.681
    },
    {
      "text": "Spam so this seems that our model is",
      "start": 2713.599,
      "duration": 4.881
    },
    {
      "text": "doing an amazing job it's actually",
      "start": 2716.24,
      "duration": 4.96
    },
    {
      "text": "recognizing spam as spam and not a Spam",
      "start": 2718.48,
      "duration": 5.28
    },
    {
      "text": "as not a Spam when I share this code",
      "start": 2721.2,
      "duration": 4.32
    },
    {
      "text": "with you I actually encourage you to",
      "start": 2723.76,
      "duration": 3.76
    },
    {
      "text": "play around with several different text",
      "start": 2725.52,
      "duration": 4.079
    },
    {
      "text": "messages and check how the large",
      "start": 2727.52,
      "duration": 4.92
    },
    {
      "text": "language based model is doing but this",
      "start": 2729.599,
      "duration": 4.281
    },
    {
      "text": "is an awesome example which we have",
      "start": 2732.44,
      "duration": 3.399
    },
    {
      "text": "finished I never thought an llm could be",
      "start": 2733.88,
      "duration": 4.76
    },
    {
      "text": "used for classification task but this",
      "start": 2735.839,
      "duration": 4.441
    },
    {
      "text": "kind of an architecture when I saw",
      "start": 2738.64,
      "duration": 3.4
    },
    {
      "text": "attaching a classification head on top",
      "start": 2740.28,
      "duration": 3.839
    },
    {
      "text": "of the GPT architecture it really blew",
      "start": 2742.04,
      "duration": 4.559
    },
    {
      "text": "my mind it's awesome and it really works",
      "start": 2744.119,
      "duration": 3.841
    },
    {
      "text": "we have brought down the loss we have",
      "start": 2746.599,
      "duration": 3.361
    },
    {
      "text": "increased the accuracy and we have",
      "start": 2747.96,
      "duration": 4.96
    },
    {
      "text": "tested this model on new text samples",
      "start": 2749.96,
      "duration": 5.72
    },
    {
      "text": "and it seems to be performing well um",
      "start": 2752.92,
      "duration": 4.48
    },
    {
      "text": "this is pretty awesome right and through",
      "start": 2755.68,
      "duration": 3.32
    },
    {
      "text": "this I hope you also understood the",
      "start": 2757.4,
      "duration": 3.88
    },
    {
      "text": "concept of fine tuning remember we have",
      "start": 2759.0,
      "duration": 5.079
    },
    {
      "text": "used pre-trained weights from gpt2 but",
      "start": 2761.28,
      "duration": 4.76
    },
    {
      "text": "we needed to do the training procedure",
      "start": 2764.079,
      "duration": 5.161
    },
    {
      "text": "once more so that is one disadvantage",
      "start": 2766.04,
      "duration": 5.24
    },
    {
      "text": "you might say of fine tuning that you",
      "start": 2769.24,
      "duration": 3.72
    },
    {
      "text": "need to spend more time on doing",
      "start": 2771.28,
      "duration": 4.039
    },
    {
      "text": "additional training on specific data set",
      "start": 2772.96,
      "duration": 4.08
    },
    {
      "text": "what is the specific data set which we",
      "start": 2775.319,
      "duration": 4.0
    },
    {
      "text": "are using here it's the spam",
      "start": 2777.04,
      "duration": 5.039
    },
    {
      "text": "collection uh but this additional tuning",
      "start": 2779.319,
      "duration": 4.681
    },
    {
      "text": "also gives us an advantage that now our",
      "start": 2782.079,
      "duration": 3.76
    },
    {
      "text": "model is specifically working very well",
      "start": 2784.0,
      "duration": 4.559
    },
    {
      "text": "to this data set and it can act as a",
      "start": 2785.839,
      "duration": 4.0
    },
    {
      "text": "Spam",
      "start": 2788.559,
      "duration": 3.721
    },
    {
      "text": "classifier we can even go ahead and save",
      "start": 2789.839,
      "duration": 4.24
    },
    {
      "text": "the model in case we want to reuse the",
      "start": 2792.28,
      "duration": 4.52
    },
    {
      "text": "model later and please keep this trick",
      "start": 2794.079,
      "duration": 4.401
    },
    {
      "text": "in mind because if you do not save the",
      "start": 2796.8,
      "duration": 4.559
    },
    {
      "text": "model you'll need to train it again so",
      "start": 2798.48,
      "duration": 4.879
    },
    {
      "text": "just tor. save it's an awesome",
      "start": 2801.359,
      "duration": 4.041
    },
    {
      "text": "functionality implemented by toor P",
      "start": 2803.359,
      "duration": 4.801
    },
    {
      "text": "torch and I I'll share the link to this",
      "start": 2805.4,
      "duration": 5.76
    },
    {
      "text": "also tor. save allows you to save the",
      "start": 2808.16,
      "duration": 4.919
    },
    {
      "text": "model parameter so that you can just use",
      "start": 2811.16,
      "duration": 2.8
    },
    {
      "text": "them",
      "start": 2813.079,
      "duration": 3.801
    },
    {
      "text": "later uh and then you can load the same",
      "start": 2813.96,
      "duration": 5.159
    },
    {
      "text": "model parameters using tor. load and you",
      "start": 2816.88,
      "duration": 3.8
    },
    {
      "text": "specify the path where you saved the",
      "start": 2819.119,
      "duration": 3.601
    },
    {
      "text": "model parameters and then you can",
      "start": 2820.68,
      "duration": 3.879
    },
    {
      "text": "directly use the loaded parameters to do",
      "start": 2822.72,
      "duration": 3.76
    },
    {
      "text": "inference or to do further fine tuning",
      "start": 2824.559,
      "duration": 3.841
    },
    {
      "text": "Etc that will save a lot of time and",
      "start": 2826.48,
      "duration": 4.52
    },
    {
      "text": "effort for you this brings us to the end",
      "start": 2828.4,
      "duration": 3.76
    },
    {
      "text": "of this lecture where we have",
      "start": 2831.0,
      "duration": 4.88
    },
    {
      "text": "successfully implemented uh llm spam",
      "start": 2832.16,
      "duration": 7.64
    },
    {
      "text": "classifier project and uh this project",
      "start": 2835.88,
      "duration": 7.6
    },
    {
      "text": "showed you how to combine fine tuning",
      "start": 2839.8,
      "duration": 6.039
    },
    {
      "text": "with pre-training on a very specific",
      "start": 2843.48,
      "duration": 4.68
    },
    {
      "text": "data set I I hope you understood why it",
      "start": 2845.839,
      "duration": 4.121
    },
    {
      "text": "is called pre-training and fine tuning",
      "start": 2848.16,
      "duration": 4.12
    },
    {
      "text": "and why we need fine tuning if we did",
      "start": 2849.96,
      "duration": 4.8
    },
    {
      "text": "not do fine tuning Our model was having",
      "start": 2852.28,
      "duration": 4.36
    },
    {
      "text": "a very bad prediction so if you see",
      "start": 2854.76,
      "duration": 4.799
    },
    {
      "text": "above we had a special section where we",
      "start": 2856.64,
      "duration": 5.56
    },
    {
      "text": "had displayed the model prediction yeah",
      "start": 2859.559,
      "duration": 4.481
    },
    {
      "text": "so if we did not fine tune and if you",
      "start": 2862.2,
      "duration": 3.68
    },
    {
      "text": "give something in the prompt itself like",
      "start": 2864.04,
      "duration": 3.76
    },
    {
      "text": "is the following text spam answer with a",
      "start": 2865.88,
      "duration": 3.959
    },
    {
      "text": "yes or no the model could not answer",
      "start": 2867.8,
      "duration": 4.48
    },
    {
      "text": "correctly that's why you need to fine",
      "start": 2869.839,
      "duration": 4.52
    },
    {
      "text": "tune you need to change the GPT model",
      "start": 2872.28,
      "duration": 4.2
    },
    {
      "text": "architecture so that the model starts",
      "start": 2874.359,
      "duration": 3.72
    },
    {
      "text": "answering better and its accuracy is",
      "start": 2876.48,
      "duration": 3.8
    },
    {
      "text": "improved the same thing what you learned",
      "start": 2878.079,
      "duration": 3.881
    },
    {
      "text": "right now the same code can be applied",
      "start": 2880.28,
      "duration": 3.72
    },
    {
      "text": "to a wide range of classification tasks",
      "start": 2881.96,
      "duration": 3.96
    },
    {
      "text": "with different range of different data",
      "start": 2884.0,
      "duration": 4.28
    },
    {
      "text": "sets and I encourage you to explore with",
      "start": 2885.92,
      "duration": 4.679
    },
    {
      "text": "different data sets that will not only",
      "start": 2888.28,
      "duration": 3.96
    },
    {
      "text": "improve your understanding but it will",
      "start": 2890.599,
      "duration": 3.601
    },
    {
      "text": "make you much more confident as an llm",
      "start": 2892.24,
      "duration": 4.16
    },
    {
      "text": "engineer now I have taught you the nuts",
      "start": 2894.2,
      "duration": 4.359
    },
    {
      "text": "and bolts of how to do fine tuning so",
      "start": 2896.4,
      "duration": 3.959
    },
    {
      "text": "you should not be scared of when people",
      "start": 2898.559,
      "duration": 3.841
    },
    {
      "text": "say the word fine tuning it just",
      "start": 2900.359,
      "duration": 4.081
    },
    {
      "text": "changing the model parameters training",
      "start": 2902.4,
      "duration": 4.4
    },
    {
      "text": "it again on specific data so that it",
      "start": 2904.44,
      "duration": 5.679
    },
    {
      "text": "performs well on that data set in the",
      "start": 2906.8,
      "duration": 4.84
    },
    {
      "text": "next set of lectures we are going to",
      "start": 2910.119,
      "duration": 4.321
    },
    {
      "text": "look at instruction fine tuning so until",
      "start": 2911.64,
      "duration": 4.4
    },
    {
      "text": "now we have looked at classification",
      "start": 2914.44,
      "duration": 4.28
    },
    {
      "text": "fine tuning right which is just one one",
      "start": 2916.04,
      "duration": 5.0
    },
    {
      "text": "category of fine tuning but another",
      "start": 2918.72,
      "duration": 4.04
    },
    {
      "text": "major category is instruction fine",
      "start": 2921.04,
      "duration": 3.6
    },
    {
      "text": "tuning so we'll actually be building our",
      "start": 2922.76,
      "duration": 4.559
    },
    {
      "text": "own chat bot which can answer specific",
      "start": 2924.64,
      "duration": 4.36
    },
    {
      "text": "which can answer or reply to Specific",
      "start": 2927.319,
      "duration": 3.721
    },
    {
      "text": "Instructions so we'll cover that in the",
      "start": 2929.0,
      "duration": 4.24
    },
    {
      "text": "subsequent set of lectures thanks",
      "start": 2931.04,
      "duration": 3.92
    },
    {
      "text": "everyone I'm I hope you are enjoying",
      "start": 2933.24,
      "duration": 3.8
    },
    {
      "text": "this whiteboard approach Plus this",
      "start": 2934.96,
      "duration": 4.56
    },
    {
      "text": "coding approach as you are following",
      "start": 2937.04,
      "duration": 4.64
    },
    {
      "text": "please keep a track of the notes please",
      "start": 2939.52,
      "duration": 3.799
    },
    {
      "text": "make your own notes and run your own",
      "start": 2941.68,
      "duration": 4.32
    },
    {
      "text": "code ask questions uh discuss with each",
      "start": 2943.319,
      "duration": 4.161
    },
    {
      "text": "other so that your understanding is",
      "start": 2946.0,
      "duration": 3.799
    },
    {
      "text": "improved maybe change the data set",
      "start": 2947.48,
      "duration": 4.16
    },
    {
      "text": "instead of spam collection maybe use a",
      "start": 2949.799,
      "duration": 3.641
    },
    {
      "text": "heart disease data set and run the same",
      "start": 2951.64,
      "duration": 3.88
    },
    {
      "text": "code who knows you'll develop an awesome",
      "start": 2953.44,
      "duration": 4.6
    },
    {
      "text": "model this opens a lot of research",
      "start": 2955.52,
      "duration": 4.279
    },
    {
      "text": "opportunities not only with respect to",
      "start": 2958.04,
      "duration": 4.16
    },
    {
      "text": "llm architecture changing and testing",
      "start": 2959.799,
      "duration": 4.76
    },
    {
      "text": "various llm architecture but also with",
      "start": 2962.2,
      "duration": 4.0
    },
    {
      "text": "respect to applying this architecture on",
      "start": 2964.559,
      "duration": 3.76
    },
    {
      "text": "various CL classification projects",
      "start": 2966.2,
      "duration": 4.2
    },
    {
      "text": "thanks so much everyone I look forward",
      "start": 2968.319,
      "duration": 6.081
    },
    {
      "text": "to seeing you in the next lecture",
      "start": 2970.4,
      "duration": 4.0
    }
  ],
  "full_text": "[Music] hello everyone and uh welcome to this lecture in the build large language models from scratch Series today we are going to continue with the fine tuning based classification project which we have been working on for the past three lectures today we are going to look at calculating the classification loss and the accuracy and we will also implement the training Loop today the testing Loop and essentially we'll complete we'll complete the entire fine tuning project so let's get started with today's lecture first I want to recap what all we have covered so far in this classification based fine-tuning handson project initially we downloaded the data set then we preprocessed the data set and created data loaders to give you a sense of what the data set actually was let me scroll up a bit in the code to show you the data set and how it looked like so essentially the data set was a Spam no spam email classification so you can even check this from the UCL machine learning repository we downloaded the SMS spam collection data set from here which consists of text messages which are either spam or not spam upon downloading this data set we saw that it was imbalanced so there were around 4,800 no spam messages and only 747 spam messages so then we balance the data set so that in the no spam category and in the spam category both there were there are 747 messages that's the First Data pre-processing step we did after that we implemented data loaders when we implemented data loaders the entire data set was batched into the input and uh the target so basically imagine the data set that's being split into training testing and validation 70% for training 20% for testing and 10% for validation if you look at the 70% training data right now due to the data loaders that data will be split into input and Target so in the input we have defined batches each containing eight samples so there are around 130 such batches of the training data and the number of columns is equal to 120 which are the number of token IDs corresponding to each text message to make sure that all the text messages have similar number or exact same token IDs we have padded the smaller text messages with this token 50256 which is the token ID corresponding to the end of text token so this is the input tensor and here what I'm showing is the target tensor this just has zeros and ones so zero means no spam and one means spam so when you implement the data loader for the training data it looks something like this and it has 130 batch es when you implement data loader for the validation and the testing data the data is batched into similar batches so we have 130 training batches 19 validation batches and 38 test batches so that's what we implemented uh in this first three steps which was downloading the downloading the data set pre-processing the data set and creating data loaders in the next steps what we did was we took our model architecture and the model architecture looked something like this initially what we did was we looked at the final output layer and initially that output layer looked like this neural network which took in the input equal to the size of the embedding Dimension that's 768 and the output was 50257 which was the vocabulary size since we are doing a classification task what we did is we replaced this neural network with this kind of a classification head as the output where the number of inputs to the neural network is 768 but the number of outputs is equal to two spam or no spam by the end of the last lecture what we saw is that if we pass in any input to this modified architecture so let's say if we pass in an input such as uh um let me show you let's say we pass in an input such as do you have time and we pass this input to this mod ified architecture the output will look something like this do you have time and then for each of these token there will be two outputs corresponding to spam or no spam then what we saw is that instead of looking at all of these four outputs we only look at the output corresponding to the last token which is time in this case because this last token contains information of all the other tokens through its attention weights um so we have reached until this stage where we pass in this input and we get an output which is a tensor of 1x 4X two since this is one batch uh so batch size so each batch has only one sample so it's one four because there are four tokens every do is the first token you is the second token have is the third token time is the fourth token and two because every token as as I showed you has two outputs corresponding to uh has two outputs corresponding to spam or no spam then what we saw is that we are going to look at the last output token um and the last output token will give us two values so this will be value number one and this will be value number two we have reached up till this stage now in today's lecture what we are going to see is that okay once you get the final two outputs from the last token what will you do with these outputs so we will first Implement two Matrix we'll get the accuracy we'll get the loss function then we will Implement a backward pass so that we can train our architecture to minimize the loss function we'll modify all the parameters so that the loss is minimized and then we will do the testing on some new data which the model has not seen awesome so let's get started the first thing which we'll need to do as I've mentioned in the code also is that we we need to First discuss how we can convert the model outputs into class label predictions so let's say if you have the input text message as you won the lottery until now we have seen that we can extract the outputs corresponding to the last row right and let's say the output look like this based on this output how can we say that whether it's a Spam or not a Spam uh what we can do in practice is that we can apply a soft Max function on this so that these two outputs are converted into a set of probabilities so then the first value will be99 the second will be 0.01 then we look at the index which has the highest probability value so since 99 is the highest is a higher probability than 0.01 which means index number 0 is more likely to be the answer and index number zero is no spam that's why this text message will be classified as no spam similarly if you have second text messages do you have time if the output corresponding to the last last row are these two tokens we'll again apply soft Max and then we'll have a tensor of probability 0.01 and99 then index number one is higher and so the output which our model will predict will be one and that will be spam so these are the steps we'll Implement in the code you'll see later that there is actually no need to even Implement soft Max since we are only seeing the index of the value Which is higher so for example even if we look at these values the index this index is higher so index zero is higher so it will be no spam if we look at these two the index one will be higher so it will be spam so let's go to code right now and discuss how we can convert the model outputs into class label predictions Okay so until now we let's say have a last token output which looks something like this minus 35983 and 3.99 02 as we discussed first we'll apply the soft Max so that we'll convert this into a set of probabilities so let's say we apply soft Max to these outputs and let me actually print let me actually print the soft Max values over here so you'll see that when you apply softmax to these two the output tensor has two values 0.005 and 0.9995 and since 0.9995 is higher what we then do is that we actually look at the argmax which means we look at the index which has a higher value and that will be index number one so our class label prediction will be be number one so in this case the code returns one meaning that the model predicts that the input text is Spam as I mentioned using the soft Max is optional because the largest outputs directly correspond to the highest probability scores so we can just take a look at these output and find the ARG Max so that's what we are going to do let's say we look at the final token and we look at its outputs we are going to take the ARG Max which will give me the index with the higher value and that will be index number one so my class label will just be that label do item and so we'll get it the output as one so now this is my uh classification accuracy which measures the percentage of correct predictions which are seen across a data set so if let's say the correct answer is class label one and if I get a class label one it's awesome then my it will be good so we'll actually compare our model prediction and the correct label prediction and then that will give me my accuracy so to determine the classification accuracy we apply the argmax based prediction code to all examples in the data set and then what we are going to do is that we are going to uh actually compare it with the actual value in the data set and then we are going to find the accuracy so to illustrate this what we are going to do is that let's say our batch looks something like this which I told you before let me rub this right now so let's say our batch looks something like this so what I'm going to do is that let's say if this is my first input right I will pass in through my model and I will get those two logits then I will apply the AR Max function and then predict whether it's spam or no spam let's say it's predicted spam so the so similar to this output labels I'll have another labels which are the predicted labels so these output labels are also called as the target labels which are my true values and here I have my predicted labels and then I'll just compare these two and that way I'll get the ACC score so this is exactly what we are going to do in the code right now so here you can see um we are going to Define this calculate accuracy given a loader so let's say if you are given a training data loader what we are going to first do is that if number of batches is not specified we are just going to use the length of the data loader as the number of batches or the batch size so here you can see each batch consists of eight training examples and so the number number of batches are equal to 130 like this for the training data sample so the number of batches will be 130 if we have not specified it if we have specified the number of batches here then the number of batches will be minimum of what we have specified here let's say that's 50 and 130 so then it we'll consider the number of batches to be equal to 50 and only compute the accuracy for those many number of batches so let's say what will happen in this code is that we'll look at each batch in this data loader so let's say we are looking at the first batch even the first batch you can see has eight samples right so when we are looking at each batch so let's say we are going to look at each batch in the data loader and each batch has eight samples so I'm going to uh pass in all the samples of a batch and I'm going to find the logits which are the two output values the logits of the last output token similar to this but now imagine that one batch has eight samples so I I'll have eight such tensor and then what I'll be doing is that I'll actually be finding the ARG Max which are the values for that entire batch and then what I'll be doing is that I'll compare the predicted labels with the target labels which is my actual answer and if it's uh if it's a correct prediction which means if they are equal I'll update the correct predictions I'll increase the number of correct predictions by one number and as I'm going through the examples I'll also uh whenever I make a prediction I'll increase the number of examples by one so if I'm going through the first example here and if I make a prediction so if I'm going through the first example here and if I make a prediction here the number of examples the number of examples will increase by one number of examples increases by one so when I make the second prediction it will again increase by one so I'm just keeping a track of the number of examples and correct predictions so towards the end to find the accuracy score I'll just take the correct correct predictions and divide by the number of examples so if the number of examples is th and if the correct predictions are 600 my accuracy will be 600 divided by th000 we are doing a very simple thing here we are just calculating the prediction from our model and we are comparing it with the actual values and then we are adding up how many predictions we got correct that's the simplest way to find the accuracy right so this is the code calcul calculate accuracy loader now what we are going to do is that we are going to use this function calculate accuracy loader and I'm just going to specify the number of batches equal to 10 for the sake of Simplicity our training data loader actually has 130 batches but I'm specifying your number of batches equal to 10 so that you can just see whether we are able to calculate the training the validation and the testing accuracy on our entire data set of course nothing is optimized here so our values will not be uh uh very good but I just want to show you that this code indeed runs so you have this function Cal calculate accuracy loader and first you pass in the training loader so that will have data such as this from from the training data set that 70% of our data then you pass in the validation loader that's 10% of your data and then you pass in the test loader that's 20% of your data in each case we specify the number of batches equal to 10 right uh and then we print out the training accuracy validation accuracy and test accuracy the model has not been optimized we have not yet implemented back propagation so these accuracy m won't be good but let's just see what they are so when you print out the training accuracy the validation accuracy and the test accuracy you get that the training accuracy is 46% validation accuracy is 45% and test accuracy is 48% it's pretty bad it's even worse than a coin toss I could have just done a coin toss and randomly predicted values and I would have been right 50% of the time so to improve the prediction accuracies we need to fine tune the model right so remember what how do we F tune or how do we optimize the model parameters the way to optimize the model parameters is that we now we can do two things now we can we have the target which is the true values and we have the predicted values right now what we will need to do is that based on the True Values and the predicted values we'll need to define a loss function and once the loss function is defined then what we'll do is that we'll simply take the partial derivative of the loss function with respect to all my trainable weights we'll calculate the gradient with respect to the trainable weights and then we'll just uh update so weight new is equal to weight old minus the partial derivative of loss with respect to that weight so we'll use a variation of this simple gradient descent called Adam or Adam W and so then we'll just continue updating these parameters until the loss function is minimized So currently so let's say the loss function looks like this it of course won't be as simple as this but I'm taking a simplified example initially we start out with this where the loss is not that low and then we move down this loss function and hopefully we'll Reach This Global Minima where the loss is minimized and once loss is minimized then we'll make sure that the accuracy is also higher automatically so then comes the question of how do you define the loss function and what loss function to use if you have studied neural networks and machine learning learning before we know that if we have uh if we have targets um which are Pi or let's say if the targets are Yi and if my predictions are Pi then the loss function which is used in this case is the categorical cross entropy loss and is defined by negative of Sigma which is adding over all the class labels and minus Yi into log of Pi let me illustrate with a simple example here let's say if we have a text Data whose True Value is that it's not a Spam which means that it's one hot encoding is one and zero so let's say this is not a Spam but our predicted values our predicted values after or predicted values here are 08 and 02 then the cross entropy loss is negative of we'll need to sum over all the classes Yi into log of Pi Yi is the true value Yi is the true value and Pi is the predicted value right so let's multiply so we'll multiply one which is the true value multiplied by log of8 so 1 will be multiplied by log of8 and 0 will be multiplied by log of0 2 and we'll take the negative sign of this so 0 * log point2 is 0 and then 1 * log point8 if you take the negative that's 2 2231 why is this a good measure of loss because if our predicted value was 1 and zero which is exactly equal to true this second will anyway be zero but the first but the first entry will be 1 into log 1 which will be equal to zero so if the predicted value equals to the True Value then our loss will be zero which is exactly what we want so this negative of Y log Pi is a very good loss function to be to calculate the loss in the case of this categorical predictions in the in classification tasks uh so to give you just a brief visual flavor negative of log negative of log of x looks something like this so this is X and this is negative of log of x and we want X to be as close to one as possible which is this probability of the correct class So eventually we'll start out from some high loss and our goal is to make the loss as close to zero as possible another advantage of the Cross entropy loss is that it's differentiable so it's very useful for us in the case of back propagation right um okay so let's actually Define the cross entropy loss now along with this uh calculation of accuracy loader what we are also going to do is that we are going to define a loss function which is the cross entropy loss why can't we use just classification accuracy and take the inverse of that accuracy maybe to get the loss it's because classification accuracy is not a differentiable function so we will use the cross entropy loss as a proxy to maximize the accuracy this is the same as the cross entropy loss which we use to pre-train the large language model so uh okay so what we are going to do now is that let's say we get an input batch and a Target batch always when an input and Target batch is given your visual mind should take you to this figure where we have an input batch and we have a Target batch so what what has to be done here is that once you get the input batch you pass in through the model and then you only look at the Logics of the last output token because that contains the most information and then you find the categorical cross entropy loss between this logic sensor which is the output of the last token and the target batch so the logic sensor output can be something like uh8 and 02 and the target out is one Z so when you calculate the cross entropy loss you'll get some value of the loss function so I'll just show you here torch.nn do functional cross entropy this is the P torch functionality which we are using over here to find the cross entropy loss awesome and this is differentiable so it will be very useful for us when we do the back propagation okay so we will use this calculate loss batch function to compute the loss for a single batch and we can also use it to calculate the loss for a multiple set of batches so for to calculate the LW for multiple batches we have to use similar code lines as we used over here so so if number of batches is not specified then we take the number of batches to be equal to the length of the data loader if number of batches is specified then it's equal to the minimum of the number of batches specified and what is the length of the data loader very similar to the accuracy classification code which we saw then what we are going to do is that we are going to take one input batch one target batch calculate the loss between all the samples of the input batch and the target batch using this calculate loss batch which will implement the categorical cross entropy we are going to add the loss every time we get a loss we are going to add the loss and then that is in the total loss um awesome and then what we are ultimately going to do is that we'll divide the total loss with the number of batches so that will kind of give us an average loss per batch and this is the loss which we will eventually try to minimize using back propagation that is the whole workflow which we are going to follow so now what we can do is that we can implement this loss function on our data set again we have not implemented back propagation so the loss will be very high but I just want to show you the initial values of the training loss the validation loss and the test loss so here again I'm setting the number of batches equal to five U because actually the train data loader has 130 batches I think so that will take a long time to calculate and anyway we have not done the training here so I just want to illustrate that the loss can be found on five batches like this so you you implement the Cal Closs loader function and you pass in train loader then validation loader and the test loader and then you also pass in the number of batches so then you can print out the training loss you can print out the validation loss and you can print out the test loss and you can see these are the high these are the values which are pretty high it's again if you in the accuracy we saw that the accuracy was very bad and that is reflected in the loss values as well now we will Implement a training function to fine tune the model which means that we'll adjust the parameters to minimize the training loss and and then we'll also print out the validation loss and we'll print out the test loss so let's start looking at that part of the code right now so until now we have finished a number of steps here we have finished uh let's see we have finished downloading the data set pre-processing the data set create data loaders initialize model load pre-train weights modify model for fine tuning Implement evaluation utilities which is the loss and the accuracy basically and now we we are at this stage where we will actually fine tune the model which means that we'll Define the training Loop and we'll Implement back propagation so this is the training Loop which we are going to Define first we'll have the EPO which means one Epoch is going through the entire data set once right so let's say if you if you are running in one particular Epoch the second Loop is that you have to go within each batch so each batch has eight samples at least that's how we Define the training data loader to be so then we'll look at each particular sample and then we'll calculate the loss on the current batch uh and uh we'll Implement a backward pass to calculate the loss gradients and then we'll update the model weights using the loss gradient so here what we are doing is that W new is equal to W old minus Alpha * the partial derivatives this is exactly what we written over we wrote over here also uh and then once the weights are updated we print the training and the validation loss and then we keep on doing the same thing for multiple number of epox so that the parameters are getting updated so the simplest way to think about this is that the most important step is this backward pass once we do the backward pass we get the loss gradients that's why we needed the loss function to be differentiable once we get the loss gradients with respect to the parameters we can actually update the parameters and once we do this enough number of times the par parameters will get updated and hopefully we'll reach a value of the loss where the loss function is minimized this is the exact same training function which we had implemented to pre-train the llm and here's what I'm what I want to show you is that when we finetune the model on supervised data which means data set such as the spam no spam label I showed you we need to again train the model so there is training process involved in pre-training and there is training process involved in fin tuning that's why it's called pre-training actually because it's before this second training process which needs to be implemented so let's see how the training process is implemented in code right now so this section I have named as finetuning the model on supervised data so until now we have actually not trained the model on the data set at all which means that that's why the parameters are not optimized so in this section we'll Define and use the training function to fine tune the pre-trained llm and improve its spam classification accur accuracy uh a note here is that if you have followed these lectures you'll see that the training function is very close to the train model simple function which we used for pre-training earlier the only distinction is that we are tracking the number of examples here the number of text samples instead of tracking the number of tokens which we had calculated earlier so in the code what we are going to do is that there are seven steps the first step is that we have to set the model to training mode uh so here you see we set the model to train training mode that's the first step the second step is reset the loss gradients from previous batch so when we look at each every batch we have to reset the loss gradients again so let's say we are looking at one batch right now uh we reset the loss gradients from the previous batch iteration then the third step is calculating the loss gradients and updating model weights these are the most important step so then what you do is you find the loss in that batch and then you calculate the loss gradients through a backward propagation then you do Optimizer do step this is where the optimizer comes into the picture in on the Whiteboard I showed you simple vanilla gradient desent over here but in practice we'll use a a more complicated optimization algorithm which keeps track of the previous gradient which keeps track of the previous gradient Square Etc so that the optimization is done in a in a better Manner and so that the model does not get stuck in local Minima then the next step is that we keeping track of the number of examples so we just keep track of the number of examples which we are seeing so input batch. shape zero is that let's say if each batch has eight samples when you look at the first shape uh first value of the batch shape it will give us the number of samples in the batch so for example if the batch has eight uh eight samples and the number of tokens is 120 so then we'll get eight here input badge. shape zero which will give us the number of samples over here so then we keep track of the number of example scen so you can just think of this example scene as when you look at one text message that's one example seen when you look at second text message you increment the number of example seen by one whenever you go through a full batch you increase the global step by one right awesome now here we have that if Global step percentage of evaluation frequency equal to zero so we have to specify an evaluation frequency now if the training batch has 130 if the training uh data loader has 130 batches in training and if the evaluation frequency is 50 it means that for after 50 batches are processed after 50 batches are processed for each Epoch after 50 batches are processed in each Epoch we print and what are we going to print we are going to print the training loss and we are going to print the validation loss so this evaluation frequency just specifies how after how many batches are completed we print the training and the validation loss so here later we are going to set the evaluation frequency to 50 which means that after 50 batches are processed in each Epoch we are going to print so in every Epoch we are going to print on an average of two times because 130 divided 50 is around 2.6 so we are going to print 2 two times in every Epoch okay awesome so now to print the training loss and the validation loss we are going to calculate the evaluate model so evaluate model gives you an option to specify the evaluation iteration which means that the number of batches you want to use for evaluation sometimes if you want to show quick evaluation on a sample data set you don't want to use all the batches so here you can just set the number of evaluation iterations to be five or 10 since the number of batches is 130 this this will really save us time when we print out the train loss and the validation loss so this actually evaluation step is optional but when we do the training you'll see that the train loss and validation loss are printed after every 50 batches due to this evaluation step then what we are going to do is that after every Epoch we are going to calculate the training accuracy and validation accuracy and we are going to print it out so after every Epoch what we are going to do is that we are going to print the training and the validation accuracy and after every batches we are going to print the training loss and the validation loss so let's do the training process now for me this training process took uh around 8.8 minutes and I have a MacBook Air 2020 um it does not have very high end configurations but it's a good laptop if you have an i5 or i7 laptop or a Macbook this training should take only 7 to 10 minutes for you so here you can see that this is the main code where we write about the training so we are going to use adamw optimal izer let me show you a bit about this t. optim adamw it's a modification of the Adam Optimizer with weight DEC so it's very good to avoid local Minima this algorithm converges in a smooth Manner and it also leads to faster convergence you can try various things here you can try Adam you can try to change the learning rate weight Decay so this is why this kind of code opens the door for research if you just use chat GPT you will never get to change all of these things which are happening under the hood but once I share this code with you you can try playing around with various parameters and try seeing the effect on the loss function on the accuracy Etc so this is the optimizer which we have defined right now and then what we are going to do is that we're going to call this train classifier simple so I'm calling this train classifier simple function and I have to I have to pass the model so the model which I'm passing in is the GPT model class which we have created with the modified architecture so the modified architecture is this where the architecture has a classification head on top of it let me show you yeah this is the modified architecture which has this classification head on top of it this is the model which we are passing in and then we pass the train loader the validation loader the optimizer which is the admw uh number of epo evaluation frequency so this evaluation frequency as I mentioned here is after 50 batches we print the train loss and validation loss and evaluation iteration is basically when you print this train loss and validation loss how many batches you want to evaluate so I'm just doing five batches here so that the calculations would be quick if you do evaluation iteration equal to 50 batches or 100 batches it will just take more time to do the evaluation of course this is not the best way to evaluate evaluate because we are only evaluating on five later in I have a code where we actually evalate on the entire data set for now this gives us a good sense at every iteration how the training loss and validation loss is progressing awesome so after I run this code you can see that I've already run it and it's 8.83 minutes so if you look at the training loss the training loss goes down to 0.083 and the validation loss goes down to 0.074 training accuracy improves to around 100% And validation accuracy is 97.5% you can even print the training loss and validation loss and along with it you can also print the example scene because then you can see the more examples the model sees the more text messages you can see that the training loss goes down as indicated by the blue line and the validation loss also goes down as indicated by the Orange Line This is actually perfect training because training loss is very low validation loss is also very low that's awesome that indicates that there is not too much overfitting here so as we can see based on the sharp downward slope the model is learning well from the training data and there is little to no indication of overfitting that is there is no noticeable gap between the training and the validation set losses that is exactly what we wanted if the validation loss is much higher than training loss let's say if the validation loss is somewhere here that is a sign of overfitting Now using the same plot we can also plot the classification accuracies so as the loss is decreasing the training and the validation loss you can also see that the training accuracy has shown by the blue line is increasing and then it reaches one the validation accuracy also increases and it reaches around 97 and plate uh one thing to note is that it's important to note that we have set evaluation iteration to be equal to five as I mentioned over here we have set the evaluation iteration to be equal to five so that's not so the values which we are seeing here of the accuracy are not representative of the accuracy on the entire data set since we only evaluate on five batches so this this means that our training and validation performance were based on only five batches for efficiency during training to calculate the performance matrics for the training validation and entire testing set for the full data set uh we can also do that so all we need to do is that then we have to run the calculation accuracy loader and then we have to pass in the train loader we have to pass in the model and we have to pass in the device either it's a CPU or a GPU so what this calculation accuracy loader will do as we have already defined earlier uh this calcul calculate uh this calculate accuracy loader will take in our model and then it will do the prediction it will compare it with the actual value then it will print out the accuracy and it will do this for all the batches in the training set so it's not only five batches so this um this accuracy measure for the training testing and validation data set is a much better representative than these plots because these plots are only for evaluation iteration which was set to be equal to five so let's print out these train accuracy validation accuracy and test accuracy on the entire data set so when you print out these you will see that the training accuracy is 97% the validation accuracy is also 97% and the test accuracy is 95% so the training and the test set performances are almost identical a slight discrepancy between the training and the test set accuracy so the test set accuracy slightly less right compared to the training it suggests that there is small amount of overfitting although there's small only 2% difference is there but it still indicates that slight amount of overfitting is there on the training data typically the validation set accuracy is somewhat higher than the test set accuracy because the model development often involves fine-tuning parameters on the validation set this situation is common but the Gap could potentially be minimized by adjusting the model settings such as increasing the dropout rate or the weight Decap parameter in the optimizer configuration as I mentioned before once I share this notebook with you you will have a lot of scope to experiment so you can experiment with dropout rate in the model architecture you can even experiment with learning rate parameter weight DK parameter in the optimizer um you can also experiment with things like unfreezing certain parameters so if if you remember from our previous lecture the only parameters which are being trained here is of course the output classification head and along with that we are also training the last Transformer block the 12th Transformer block and the final normalization layer you can do some changes here so you can make sure that the last three Transformer blocks are trained Etc you can make sure that maybe this is false and that leads to better answers who knows so this kind of experimentation is open and I'll be very happy if you experiment with various options that will even improve your understanding further and try to see if you can increase the test accur further to match that of the training accuracy awesome so until now what we have done is that we have uh um let's see what all we have done we have fine tuned on the supervised data and we have even plotted the training and the validation loss now the last step is remaining which is using model on new data so whatever is shown in the tick mark here downloading the data set pre-processing the data set creating data loaders initializing the model load pre-train weights modify model for fine tuning Implement loss and accuracy functions then actually doing the backo pass and fine tuning the model and training and validating the model these nine steps we have done now what we have to do is that we have to use the model on new data which the model has not seen before so that is the real test whether our model our large language model how its performance is as a Spam classifier so let's go to the last section of this project right now and uh let's see whether our model is actually performing well on data which it has not seen so after fine-tuning and evaluating the model in the previous sections we are now in the final stage of this chapter where we will use the model to classify spam messages right so finally let's use the fine tuned GPT based spam spam classification model we'll need to define a function first we'll need to define a function called classify review which will take in any text and it will predict whether it's a Spam or not and what this function will do is that it will do a number of things first it will uh and let me actually write this down in description so let's say a text is given such as you let's say a text is given such as you on a lottery right if a text is given the first thing which we will do is that we'll convert this text into token IDs we'll convert this text into token IDs actually there is a nice representation of the data pre-processing which we had looked at before I'm just I'll just take you to that part so that you can see how this yeah so if a new text is given we'll first convert the text into token IDs something like this and that's the first thing which we have written in the code we'll first use tokenizer in code so this is the tick token this is the tick token tokenizer which we are going to use it's a bite pair encoder it takes in any sentence and converts it into a bunch of tokens right then we will uh we'll look at the supported context length and that's equal to uh 1024 in this this case because the uh so model. positional embedding weight shape that is a shape of the embedding weight Matrix and to give you an idea of what the shape size is it has the number of rows equal to the context length and it has number of columns equal to the embedding Dimension so the number of rows will give us the context length and that's why we are using the embedding shape zero to find the context length so the reason we find this context length is that we are going to compare it with the maximum length so what we did here is that we have we have found the maximum token length token ID length from the training set which means which is the text message which is the longest and we have got that length let's say that length is equal to 120 so uh if that length is equal to if that length is actually higher than the context length then we have to trunet everything down to the context length so sequences which are way higher than the maximum length we have to find the minimum of the maximum length and the supported context length so if the maximum length is actually higher than 1024 then we are going to take the context length and truncate all the sequences to be equal to the context length in the cases where this does not happen our maximum length will be used and then all the input text will will have those many token IDs so let's say if the uh maximum length is 120 and you have received a text message such as uh you have won a lottery let's say you have received this text message and when you convert it into token IDs you you have seen that the length is only 50 so what you will do is that you have to extend this to 120 by adding some end of text tokens so you add 70 end of text tokens here which are this 50256 and you make sure that the length of the uh text is equal to the maximum length this is very important because when you batch it every sentence needs to have the same number of token IDs so you have to pad this you have to pad every input sequence to match the maximum length so the maximum length ideally is the length which we have got from our training data set so what's the maximum email length in the training data set but if it's higher than the context length the maximum length will be set equal to the context length so whenever you give a new test input it's first converted into token IDs and then it's padded with this end of text token which is 50256 so that the length is equal to the uh length is equal to the maximum length then we convert it into a tensor to add the batch Dimension uh and then we perform the model inference so we first calculate the prediction so we get the logit tensor which is the logits of the last output token and then we apply torch. arac so we have seen this implementation um let me recap your understanding we have seen this implementation in this part of the code right where we take the AR Max and this gives us the prediction whether it's spam or not a Spam and then that is our final answer so this model this model here is our train model which we are using now for inference for inference on any new text message so the main magic happens in this line where our input tensor is passed through this model and then we predict the label but before that we have to make sure that the token IDs are equal to the maximum length now what now let us actually take two sentences and let us pass them through our classify review function and let's see whether our model predicts them as as spam or no spam so the first sentence I'm taking is you are a winner you have been specially selected to receive ,000 cash or $2,000 reward clearly it looks like a Spam rate and this is from a testing set my model has not seen it in the training data I'm going to pass it through the classifier review function and let me print out the output and our model is clearly recognizing the output to be that this is a spam then let's take a second sentence hey just wanted to check if we are still on for dinner tonight let me know I'll again pass it through the model and I'll check whether it's spam or no spam this looks like a very legitimate message right and it's clearly not a Spam and model makes a correct prediction that it's not a Spam so this seems that our model is doing an amazing job it's actually recognizing spam as spam and not a Spam as not a Spam when I share this code with you I actually encourage you to play around with several different text messages and check how the large language based model is doing but this is an awesome example which we have finished I never thought an llm could be used for classification task but this kind of an architecture when I saw attaching a classification head on top of the GPT architecture it really blew my mind it's awesome and it really works we have brought down the loss we have increased the accuracy and we have tested this model on new text samples and it seems to be performing well um this is pretty awesome right and through this I hope you also understood the concept of fine tuning remember we have used pre-trained weights from gpt2 but we needed to do the training procedure once more so that is one disadvantage you might say of fine tuning that you need to spend more time on doing additional training on specific data set what is the specific data set which we are using here it's the spam collection uh but this additional tuning also gives us an advantage that now our model is specifically working very well to this data set and it can act as a Spam classifier we can even go ahead and save the model in case we want to reuse the model later and please keep this trick in mind because if you do not save the model you'll need to train it again so just tor. save it's an awesome functionality implemented by toor P torch and I I'll share the link to this also tor. save allows you to save the model parameter so that you can just use them later uh and then you can load the same model parameters using tor. load and you specify the path where you saved the model parameters and then you can directly use the loaded parameters to do inference or to do further fine tuning Etc that will save a lot of time and effort for you this brings us to the end of this lecture where we have successfully implemented uh llm spam classifier project and uh this project showed you how to combine fine tuning with pre-training on a very specific data set I I hope you understood why it is called pre-training and fine tuning and why we need fine tuning if we did not do fine tuning Our model was having a very bad prediction so if you see above we had a special section where we had displayed the model prediction yeah so if we did not fine tune and if you give something in the prompt itself like is the following text spam answer with a yes or no the model could not answer correctly that's why you need to fine tune you need to change the GPT model architecture so that the model starts answering better and its accuracy is improved the same thing what you learned right now the same code can be applied to a wide range of classification tasks with different range of different data sets and I encourage you to explore with different data sets that will not only improve your understanding but it will make you much more confident as an llm engineer now I have taught you the nuts and bolts of how to do fine tuning so you should not be scared of when people say the word fine tuning it just changing the model parameters training it again on specific data so that it performs well on that data set in the next set of lectures we are going to look at instruction fine tuning so until now we have looked at classification fine tuning right which is just one one category of fine tuning but another major category is instruction fine tuning so we'll actually be building our own chat bot which can answer specific which can answer or reply to Specific Instructions so we'll cover that in the subsequent set of lectures thanks everyone I'm I hope you are enjoying this whiteboard approach Plus this coding approach as you are following please keep a track of the notes please make your own notes and run your own code ask questions uh discuss with each other so that your understanding is improved maybe change the data set instead of spam collection maybe use a heart disease data set and run the same code who knows you'll develop an awesome model this opens a lot of research opportunities not only with respect to llm architecture changing and testing various llm architecture but also with respect to applying this architecture on various CL classification projects thanks so much everyone I look forward to seeing you in the next lecture"
}