{
  "video": {
    "video_id": "fKd8s29e-l4",
    "title": "Lecture 8: The GPT Tokenizer: Byte Pair Encoding",
    "duration": 3215.0,
    "index": 7
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.679
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.359,
      "duration": 5.36
    },
    {
      "text": "in the large language models from",
      "start": 8.679,
      "duration": 3.481
    },
    {
      "text": "scratch",
      "start": 10.719,
      "duration": 4.88
    },
    {
      "text": "Series today we are going to learn about",
      "start": 12.16,
      "duration": 5.76
    },
    {
      "text": "a very important topic which is called",
      "start": 15.599,
      "duration": 5.961
    },
    {
      "text": "as bite pair encoding in many lectures",
      "start": 17.92,
      "duration": 5.679
    },
    {
      "text": "or even video content which you see on",
      "start": 21.56,
      "duration": 5.2
    },
    {
      "text": "large language models when tokenization",
      "start": 23.599,
      "duration": 5.6
    },
    {
      "text": "is covered the concept of bite pair",
      "start": 26.76,
      "duration": 6.319
    },
    {
      "text": "encoding is rarely explained however",
      "start": 29.199,
      "duration": 7.121
    },
    {
      "text": "behind modern algorithms such as gpt2",
      "start": 33.079,
      "duration": 7.921
    },
    {
      "text": "gpt3 Etc the tokenizer which these",
      "start": 36.32,
      "duration": 9.32
    },
    {
      "text": "modern llms use is usually bite pair",
      "start": 41.0,
      "duration": 7.0
    },
    {
      "text": "encoding so you really need to",
      "start": 45.64,
      "duration": 4.36
    },
    {
      "text": "understand what this means and how",
      "start": 48.0,
      "duration": 4.52
    },
    {
      "text": "encoding how this encoding is done for",
      "start": 50.0,
      "duration": 5.0
    },
    {
      "text": "modern L large language",
      "start": 52.52,
      "duration": 5.08
    },
    {
      "text": "models so let's get started with today's",
      "start": 55.0,
      "duration": 6.96
    },
    {
      "text": "lecture first let me take you to the",
      "start": 57.6,
      "duration": 6.12
    },
    {
      "text": "Google collab",
      "start": 61.96,
      "duration": 4.76
    },
    {
      "text": "notebook um until now if you have",
      "start": 63.72,
      "duration": 4.96
    },
    {
      "text": "followed the previous lecture what we",
      "start": 66.72,
      "duration": 4.2
    },
    {
      "text": "did in the previous lecture is that we",
      "start": 68.68,
      "duration": 5.72
    },
    {
      "text": "implemented a simple tokenization scheme",
      "start": 70.92,
      "duration": 5.28
    },
    {
      "text": "if you remember what we covered in the",
      "start": 74.4,
      "duration": 4.84
    },
    {
      "text": "previous lecture let me just uh take you",
      "start": 76.2,
      "duration": 5.239
    },
    {
      "text": "quickly through that in the previous",
      "start": 79.24,
      "duration": 4.4
    },
    {
      "text": "lecture what we did was we basically",
      "start": 81.439,
      "duration": 4.961
    },
    {
      "text": "took sentences we converted them into",
      "start": 83.64,
      "duration": 5.88
    },
    {
      "text": "tokens and then we converted this into",
      "start": 86.4,
      "duration": 4.039
    },
    {
      "text": "vocabul",
      "start": 89.52,
      "duration": 3.599
    },
    {
      "text": "so then every word of the token was",
      "start": 90.439,
      "duration": 5.241
    },
    {
      "text": "arranged in an ascending order and then",
      "start": 93.119,
      "duration": 5.801
    },
    {
      "text": "a ID or a numerical ID or a token ID was",
      "start": 95.68,
      "duration": 4.96
    },
    {
      "text": "assigned to each",
      "start": 98.92,
      "duration": 5.64
    },
    {
      "text": "token so here every word was a unique",
      "start": 100.64,
      "duration": 6.119
    },
    {
      "text": "token along with special characters like",
      "start": 104.56,
      "duration": 5.919
    },
    {
      "text": "comma full stop exclamation mark",
      "start": 106.759,
      "duration": 7.561
    },
    {
      "text": "Etc um now in today's lecture we are",
      "start": 110.479,
      "duration": 6.32
    },
    {
      "text": "going to cover a much more sophisticated",
      "start": 114.32,
      "duration": 5.079
    },
    {
      "text": "tokenizing tokenization scheme why",
      "start": 116.799,
      "duration": 4.6
    },
    {
      "text": "sophistic ated I'll come to that in a",
      "start": 119.399,
      "duration": 4.32
    },
    {
      "text": "moment but just remember that today we",
      "start": 121.399,
      "duration": 4.32
    },
    {
      "text": "are going to learn about the bite pair",
      "start": 123.719,
      "duration": 4.32
    },
    {
      "text": "encoding or BP",
      "start": 125.719,
      "duration": 5.121
    },
    {
      "text": "tokenizer this BP tokenizer which we are",
      "start": 128.039,
      "duration": 5.081
    },
    {
      "text": "going to cover today was used to train",
      "start": 130.84,
      "duration": 6.36
    },
    {
      "text": "large language models such as gpt2 gpt3",
      "start": 133.12,
      "duration": 6.759
    },
    {
      "text": "and the original model used in Chad",
      "start": 137.2,
      "duration": 5.16
    },
    {
      "text": "GPT if you have not seen the previous",
      "start": 139.879,
      "duration": 4.681
    },
    {
      "text": "lecture on tokenization I highly",
      "start": 142.36,
      "duration": 4.599
    },
    {
      "text": "encourage you to watch that so that this",
      "start": 144.56,
      "duration": 4.8
    },
    {
      "text": "lecture will become very clear to you",
      "start": 146.959,
      "duration": 4.041
    },
    {
      "text": "because in the previous lecture we",
      "start": 149.36,
      "duration": 4.36
    },
    {
      "text": "implemented our own tokenizer completely",
      "start": 151.0,
      "duration": 5.4
    },
    {
      "text": "from scratch and today we are going to",
      "start": 153.72,
      "duration": 5.48
    },
    {
      "text": "learn about a bit more advanced concept",
      "start": 156.4,
      "duration": 6.04
    },
    {
      "text": "and this scheme this bpe scheme is used",
      "start": 159.2,
      "duration": 6.319
    },
    {
      "text": "in all modern llms and so it's very",
      "start": 162.44,
      "duration": 5.96
    },
    {
      "text": "important for us to learn so let me take",
      "start": 165.519,
      "duration": 4.72
    },
    {
      "text": "you to the Whiteboard right now and",
      "start": 168.4,
      "duration": 3.8
    },
    {
      "text": "explain the concept of bite pair",
      "start": 170.239,
      "duration": 4.161
    },
    {
      "text": "tokenizer and why do we even need it in",
      "start": 172.2,
      "duration": 5.16
    },
    {
      "text": "the first place so if you look at the",
      "start": 174.4,
      "duration": 5.479
    },
    {
      "text": "tokenization algorithms there are",
      "start": 177.36,
      "duration": 4.68
    },
    {
      "text": "essentially three types of tokenization",
      "start": 179.879,
      "duration": 5.92
    },
    {
      "text": "algorithms the first is the word based",
      "start": 182.04,
      "duration": 6.479
    },
    {
      "text": "tokenizer U the second type is the",
      "start": 185.799,
      "duration": 5.761
    },
    {
      "text": "subword based tokenizer and the third",
      "start": 188.519,
      "duration": 5.761
    },
    {
      "text": "type is the character based tokenizer",
      "start": 191.56,
      "duration": 4.48
    },
    {
      "text": "let me walk you through these step by",
      "start": 194.28,
      "duration": 4.44
    },
    {
      "text": "step in the word based tokenizer what we",
      "start": 196.04,
      "duration": 5.4
    },
    {
      "text": "usually do is every word in the sentence",
      "start": 198.72,
      "duration": 5.64
    },
    {
      "text": "is usually one token the tokenizer which",
      "start": 201.44,
      "duration": 5.12
    },
    {
      "text": "we saw in the previous lecture such as",
      "start": 204.36,
      "duration": 6.48
    },
    {
      "text": "for example in this example the um the",
      "start": 206.56,
      "duration": 7.399
    },
    {
      "text": "fox chased the dog the tokens were the",
      "start": 210.84,
      "duration": 6.759
    },
    {
      "text": "fox chased the dog so every word was one",
      "start": 213.959,
      "duration": 5.881
    },
    {
      "text": "token right so that's why it's an",
      "start": 217.599,
      "duration": 5.041
    },
    {
      "text": "example of word based",
      "start": 219.84,
      "duration": 6.36
    },
    {
      "text": "tokenizer uh here I'm taking one more",
      "start": 222.64,
      "duration": 5.48
    },
    {
      "text": "example to illustrate this concept to",
      "start": 226.2,
      "duration": 4.56
    },
    {
      "text": "you if the sentence is my hobby is",
      "start": 228.12,
      "duration": 5.119
    },
    {
      "text": "playing cricket and if the tokens are",
      "start": 230.76,
      "duration": 5.559
    },
    {
      "text": "individual words such as my then hobby",
      "start": 233.239,
      "duration": 6.321
    },
    {
      "text": "then is then playing and then Cricket",
      "start": 236.319,
      "duration": 6.441
    },
    {
      "text": "that that is a word based tokenizer",
      "start": 239.56,
      "duration": 5.399
    },
    {
      "text": "awesome now can you think of the",
      "start": 242.76,
      "duration": 5.36
    },
    {
      "text": "problems associated with the word based",
      "start": 244.959,
      "duration": 5.081
    },
    {
      "text": "tokenizer",
      "start": 248.12,
      "duration": 4.759
    },
    {
      "text": "first uh the main problem is what do you",
      "start": 250.04,
      "duration": 4.8
    },
    {
      "text": "do with Words which are not present in",
      "start": 252.879,
      "duration": 4.04
    },
    {
      "text": "the vocabulary so let's say if you have",
      "start": 254.84,
      "duration": 4.519
    },
    {
      "text": "huge amount of training data and you",
      "start": 256.919,
      "duration": 4.44
    },
    {
      "text": "will break it down into sentences right",
      "start": 259.359,
      "duration": 3.761
    },
    {
      "text": "and then you'll break down the sentences",
      "start": 261.359,
      "duration": 3.881
    },
    {
      "text": "into individual words and then you will",
      "start": 263.12,
      "duration": 5.12
    },
    {
      "text": "assign a token ID to each of these words",
      "start": 265.24,
      "duration": 5.08
    },
    {
      "text": "but when the user is interacting with",
      "start": 268.24,
      "duration": 4.519
    },
    {
      "text": "the llm let's say and the user inputs a",
      "start": 270.32,
      "duration": 4.64
    },
    {
      "text": "word which was not present in the",
      "start": 272.759,
      "duration": 4.44
    },
    {
      "text": "vocabulary these words are also called",
      "start": 274.96,
      "duration": 4.64
    },
    {
      "text": "as out of vocabulary words so you can",
      "start": 277.199,
      "duration": 5.0
    },
    {
      "text": "think of as preparing for an exam but in",
      "start": 279.6,
      "duration": 4.4
    },
    {
      "text": "the exam a question is asked which is",
      "start": 282.199,
      "duration": 3.201
    },
    {
      "text": "completely different from what you have",
      "start": 284.0,
      "duration": 2.6
    },
    {
      "text": "prepared",
      "start": 285.4,
      "duration": 4.239
    },
    {
      "text": "for uh in word based tokenization",
      "start": 286.6,
      "duration": 5.12
    },
    {
      "text": "schemes it's very difficult to know what",
      "start": 289.639,
      "duration": 4.881
    },
    {
      "text": "do we do with out of vocabulary words",
      "start": 291.72,
      "duration": 5.24
    },
    {
      "text": "because consider this case itself my",
      "start": 294.52,
      "duration": 4.0
    },
    {
      "text": "hobby is playing Cricket let's say if",
      "start": 296.96,
      "duration": 3.6
    },
    {
      "text": "the data set was small and only one",
      "start": 298.52,
      "duration": 4.92
    },
    {
      "text": "sentence if a new word is given such as",
      "start": 300.56,
      "duration": 5.52
    },
    {
      "text": "football and that is not present in this",
      "start": 303.44,
      "duration": 3.96
    },
    {
      "text": "data",
      "start": 306.08,
      "duration": 4.64
    },
    {
      "text": "set then usually these tokenization",
      "start": 307.4,
      "duration": 6.0
    },
    {
      "text": "schemes run into an error this is a very",
      "start": 310.72,
      "duration": 5.08
    },
    {
      "text": "simplified example I have constructed",
      "start": 313.4,
      "duration": 4.68
    },
    {
      "text": "but uh this problem shows up when you do",
      "start": 315.8,
      "duration": 5.48
    },
    {
      "text": "word based tokenization a lot usually",
      "start": 318.08,
      "duration": 5.239
    },
    {
      "text": "it's very difficult to deal with out of",
      "start": 321.28,
      "duration": 5.039
    },
    {
      "text": "vocabulary or oo words if they are not",
      "start": 323.319,
      "duration": 5.241
    },
    {
      "text": "present in the vocabulary and what do I",
      "start": 326.319,
      "duration": 4.561
    },
    {
      "text": "mean by vocabulary vocabulary is just a",
      "start": 328.56,
      "duration": 4.639
    },
    {
      "text": "dictionary with a collection of tokens",
      "start": 330.88,
      "duration": 4.439
    },
    {
      "text": "arranged in ascending order and every",
      "start": 333.199,
      "duration": 4.761
    },
    {
      "text": "token corresponds to a token",
      "start": 335.319,
      "duration": 6.041
    },
    {
      "text": "ID great now another problem with this",
      "start": 337.96,
      "duration": 6.959
    },
    {
      "text": "word based tokenization scheme is that",
      "start": 341.36,
      "duration": 7.119
    },
    {
      "text": "uh let's say boy and boys are two words",
      "start": 344.919,
      "duration": 6.161
    },
    {
      "text": "in the vocabulary each of them will have",
      "start": 348.479,
      "duration": 5.361
    },
    {
      "text": "different tokens maybe the token IDs for",
      "start": 351.08,
      "duration": 5.72
    },
    {
      "text": "both of them will be far apart based on",
      "start": 353.84,
      "duration": 4.639
    },
    {
      "text": "uh where they appear or they might be",
      "start": 356.8,
      "duration": 5.08
    },
    {
      "text": "similar also but but the problem is that",
      "start": 358.479,
      "duration": 5.761
    },
    {
      "text": "uh these words are very similar right",
      "start": 361.88,
      "duration": 4.439
    },
    {
      "text": "and when we do this tokenization this",
      "start": 364.24,
      "duration": 6.12
    },
    {
      "text": "similarity is not captured so boys if",
      "start": 366.319,
      "duration": 6.121
    },
    {
      "text": "you think of the word boys the root word",
      "start": 370.36,
      "duration": 5.32
    },
    {
      "text": "is boy so ideally both of these words",
      "start": 372.44,
      "duration": 5.28
    },
    {
      "text": "are very similar to each other but with",
      "start": 375.68,
      "duration": 3.799
    },
    {
      "text": "this tokenization they are treated as",
      "start": 377.72,
      "duration": 4.56
    },
    {
      "text": "separate words and that similarity is",
      "start": 379.479,
      "duration": 3.72
    },
    {
      "text": "not",
      "start": 382.28,
      "duration": 3.039
    },
    {
      "text": "captured these are the main problems",
      "start": 383.199,
      "duration": 4.641
    },
    {
      "text": "with word based tokenization now let's",
      "start": 385.319,
      "duration": 4.28
    },
    {
      "text": "look at the other end of the spectrum",
      "start": 387.84,
      "duration": 4.0
    },
    {
      "text": "which is character based tokenization",
      "start": 389.599,
      "duration": 4.401
    },
    {
      "text": "this is also a very popular tokenization",
      "start": 391.84,
      "duration": 4.68
    },
    {
      "text": "scheme where what we do is that if the",
      "start": 394.0,
      "duration": 4.4
    },
    {
      "text": "sentence is again my hobby is playing",
      "start": 396.52,
      "duration": 4.48
    },
    {
      "text": "Cricket instead of having individual",
      "start": 398.4,
      "duration": 5.4
    },
    {
      "text": "words as tokens in this kind of",
      "start": 401.0,
      "duration": 4.0
    },
    {
      "text": "character based",
      "start": 403.8,
      "duration": 3.92
    },
    {
      "text": "tokenization individual characters are",
      "start": 405.0,
      "duration": 5.84
    },
    {
      "text": "considered as tokens so for example here",
      "start": 407.72,
      "duration": 4.8
    },
    {
      "text": "let's look at which characters are there",
      "start": 410.84,
      "duration": 3.84
    },
    {
      "text": "m is the first character why is the",
      "start": 412.52,
      "duration": 5.16
    },
    {
      "text": "second character uh I'm ignoring white",
      "start": 414.68,
      "duration": 5.799
    },
    {
      "text": "spaces for now so H is the third",
      "start": 417.68,
      "duration": 5.6
    },
    {
      "text": "character o is the fourth character Etc",
      "start": 420.479,
      "duration": 4.921
    },
    {
      "text": "so then the vocabulary or the tokens",
      "start": 423.28,
      "duration": 3.44
    },
    {
      "text": "would contain",
      "start": 425.4,
      "duration": 5.32
    },
    {
      "text": "m y h o b",
      "start": 426.72,
      "duration": 6.68
    },
    {
      "text": "Etc these are the tokens instead of",
      "start": 430.72,
      "duration": 5.8
    },
    {
      "text": "having individual words now can you",
      "start": 433.4,
      "duration": 4.96
    },
    {
      "text": "think of what will be the vocabulary",
      "start": 436.52,
      "duration": 3.16
    },
    {
      "text": "size in this",
      "start": 438.36,
      "duration": 4.399
    },
    {
      "text": "case just think about this for a moment",
      "start": 439.68,
      "duration": 5.079
    },
    {
      "text": "if you are not able to answer this just",
      "start": 442.759,
      "duration": 3.601
    },
    {
      "text": "pause and",
      "start": 444.759,
      "duration": 4.321
    },
    {
      "text": "think okay so this will actually lead to",
      "start": 446.36,
      "duration": 5.8
    },
    {
      "text": "a very small vocabulary because every",
      "start": 449.08,
      "duration": 4.519
    },
    {
      "text": "language has a fixed number of",
      "start": 452.16,
      "duration": 3.28
    },
    {
      "text": "characters if you look at the English",
      "start": 453.599,
      "duration": 5.16
    },
    {
      "text": "language it has 256 characters on the",
      "start": 455.44,
      "duration": 4.759
    },
    {
      "text": "other hand if you look at the total",
      "start": 458.759,
      "duration": 3.081
    },
    {
      "text": "words in the English language it has",
      "start": 460.199,
      "duration": 7.601
    },
    {
      "text": "about 170,000 to 200,000 words uh but",
      "start": 461.84,
      "duration": 8.319
    },
    {
      "text": "what is one of the advantages of the",
      "start": 467.8,
      "duration": 4.92
    },
    {
      "text": "character based tokenization is that it",
      "start": 470.159,
      "duration": 5.32
    },
    {
      "text": "has a very small vocabulary size which",
      "start": 472.72,
      "duration": 4.36
    },
    {
      "text": "means that",
      "start": 475.479,
      "duration": 4.0
    },
    {
      "text": "uh either so if you look at the Engish",
      "start": 477.08,
      "duration": 3.92
    },
    {
      "text": "Eng language there are fixed number of",
      "start": 479.479,
      "duration": 3.201
    },
    {
      "text": "characters right there are 256",
      "start": 481.0,
      "duration": 3.84
    },
    {
      "text": "characters so we will never have the out",
      "start": 482.68,
      "duration": 4.239
    },
    {
      "text": "of vocabulary problem because let's say",
      "start": 484.84,
      "duration": 4.359
    },
    {
      "text": "any new sentence is given by the user",
      "start": 486.919,
      "duration": 3.801
    },
    {
      "text": "you can always break it down into",
      "start": 489.199,
      "duration": 3.0
    },
    {
      "text": "characters even if you don't know the",
      "start": 490.72,
      "duration": 3.08
    },
    {
      "text": "words in that sentence it's fine you",
      "start": 492.199,
      "duration": 3.641
    },
    {
      "text": "break it down into characters and it",
      "start": 493.8,
      "duration": 4.16
    },
    {
      "text": "will be either of the 256 characters",
      "start": 495.84,
      "duration": 3.44
    },
    {
      "text": "which are already present in your",
      "start": 497.96,
      "duration": 4.079
    },
    {
      "text": "vocabulary or dictionary so the out of",
      "start": 499.28,
      "duration": 4.52
    },
    {
      "text": "vocabulary problem will not come into",
      "start": 502.039,
      "duration": 5.081
    },
    {
      "text": "the picture and uh it solves one more",
      "start": 503.8,
      "duration": 5.88
    },
    {
      "text": "problem if you look at the word BAS",
      "start": 507.12,
      "duration": 5.279
    },
    {
      "text": "tokenization right as I told you English",
      "start": 509.68,
      "duration": 5.68
    },
    {
      "text": "language is about 170,000 to 200,000",
      "start": 512.399,
      "duration": 5.56
    },
    {
      "text": "words so if you really want to include",
      "start": 515.36,
      "duration": 4.64
    },
    {
      "text": "everything in the vocabulary you need a",
      "start": 517.959,
      "duration": 4.361
    },
    {
      "text": "vocabulary size which is huge and that",
      "start": 520.0,
      "duration": 5.04
    },
    {
      "text": "is one big problem in word based",
      "start": 522.32,
      "duration": 4.84
    },
    {
      "text": "tokenization this problem is completely",
      "start": 525.04,
      "duration": 3.64
    },
    {
      "text": "solved in the character based",
      "start": 527.16,
      "duration": 4.2
    },
    {
      "text": "tokenization because the vocabulary is",
      "start": 528.68,
      "duration": 4.52
    },
    {
      "text": "based on characters and the vocabulary",
      "start": 531.36,
      "duration": 3.28
    },
    {
      "text": "length is pretty",
      "start": 533.2,
      "duration": 3.72
    },
    {
      "text": "small but then you might think oh this",
      "start": 534.64,
      "duration": 4.24
    },
    {
      "text": "sounds amazing right it literally solves",
      "start": 536.92,
      "duration": 3.479
    },
    {
      "text": "all of the problem problem it solves the",
      "start": 538.88,
      "duration": 3.6
    },
    {
      "text": "out of vocabulary problem it's also",
      "start": 540.399,
      "duration": 4.161
    },
    {
      "text": "computationally and memory efficient",
      "start": 542.48,
      "duration": 3.64
    },
    {
      "text": "because the vocabulary size is very",
      "start": 544.56,
      "duration": 4.519
    },
    {
      "text": "small and uh then that's great then",
      "start": 546.12,
      "duration": 5.68
    },
    {
      "text": "what's the issue there are some problems",
      "start": 549.079,
      "duration": 4.841
    },
    {
      "text": "with character based tokenization and",
      "start": 551.8,
      "duration": 3.76
    },
    {
      "text": "the first major problem is that the",
      "start": 553.92,
      "duration": 3.4
    },
    {
      "text": "meaning which is associated with words",
      "start": 555.56,
      "duration": 4.399
    },
    {
      "text": "is completely lost essentially the",
      "start": 557.32,
      "duration": 4.12
    },
    {
      "text": "advantage of dealing with language",
      "start": 559.959,
      "duration": 3.921
    },
    {
      "text": "models is that words have meanings right",
      "start": 561.44,
      "duration": 3.88
    },
    {
      "text": "so different words and different",
      "start": 563.88,
      "duration": 3.519
    },
    {
      "text": "sentences might be related to each other",
      "start": 565.32,
      "duration": 4.56
    },
    {
      "text": "boy and boys have a common meaning this",
      "start": 567.399,
      "duration": 4.56
    },
    {
      "text": "is completely lost since you're breaking",
      "start": 569.88,
      "duration": 4.04
    },
    {
      "text": "it down into individual",
      "start": 571.959,
      "duration": 3.921
    },
    {
      "text": "characters that's one of the first",
      "start": 573.92,
      "duration": 5.08
    },
    {
      "text": "biggest problem with uh character level",
      "start": 575.88,
      "duration": 5.56
    },
    {
      "text": "tokenization the second problem is that",
      "start": 579.0,
      "duration": 5.0
    },
    {
      "text": "the tokenized sequence is much longer",
      "start": 581.44,
      "duration": 6.079
    },
    {
      "text": "than the initial raw text so for example",
      "start": 584.0,
      "duration": 5.959
    },
    {
      "text": "if there is a word in",
      "start": 587.519,
      "duration": 5.361
    },
    {
      "text": "the let's say there is a word in the",
      "start": 589.959,
      "duration": 5.281
    },
    {
      "text": "text which is",
      "start": 592.88,
      "duration": 5.12
    },
    {
      "text": "dinosaur now in word based tokenization",
      "start": 595.24,
      "duration": 4.8
    },
    {
      "text": "this will be treated as one single token",
      "start": 598.0,
      "duration": 3.68
    },
    {
      "text": "right but in character based",
      "start": 600.04,
      "duration": 3.56
    },
    {
      "text": "tokenization what will happen is that",
      "start": 601.68,
      "duration": 5.56
    },
    {
      "text": "every every single character here d i n",
      "start": 603.6,
      "duration": 7.0
    },
    {
      "text": "o s a UR will be treated as separate",
      "start": 607.24,
      "duration": 5.4
    },
    {
      "text": "token so in character based tokenization",
      "start": 610.6,
      "duration": 3.64
    },
    {
      "text": "the word dinosaur will be actually",
      "start": 612.64,
      "duration": 4.6
    },
    {
      "text": "broken down or split into eight",
      "start": 614.24,
      "duration": 6.039
    },
    {
      "text": "tokens and that is another major problem",
      "start": 617.24,
      "duration": 5.44
    },
    {
      "text": "the tokenized sequence is much longer",
      "start": 620.279,
      "duration": 4.24
    },
    {
      "text": "than the initial raw",
      "start": 622.68,
      "duration": 5.2
    },
    {
      "text": "text now uh so as we can see the word",
      "start": 624.519,
      "duration": 6.361
    },
    {
      "text": "based tokenization has its advantages",
      "start": 627.88,
      "duration": 5.24
    },
    {
      "text": "and disadvantages the disadvantages is",
      "start": 630.88,
      "duration": 3.72
    },
    {
      "text": "that we don't know what to do with out",
      "start": 633.12,
      "duration": 4.36
    },
    {
      "text": "of vocabulary words and the vocabulary",
      "start": 634.6,
      "duration": 5.08
    },
    {
      "text": "size is pretty large the advantage is",
      "start": 637.48,
      "duration": 6.32
    },
    {
      "text": "that uh of course the words every word",
      "start": 639.68,
      "duration": 5.92
    },
    {
      "text": "is a token",
      "start": 643.8,
      "duration": 4.8
    },
    {
      "text": "so the tokenized sequence length will be",
      "start": 645.6,
      "duration": 5.72
    },
    {
      "text": "small like dinosaur it will be just one",
      "start": 648.6,
      "duration": 5.96
    },
    {
      "text": "token so when we tokenize the paragraph",
      "start": 651.32,
      "duration": 5.8
    },
    {
      "text": "it's very small uh because every word",
      "start": 654.56,
      "duration": 5.399
    },
    {
      "text": "will be one token unlike character based",
      "start": 657.12,
      "duration": 4.8
    },
    {
      "text": "tokenization where the tokenized",
      "start": 659.959,
      "duration": 3.841
    },
    {
      "text": "sequence is much longer than the initial",
      "start": 661.92,
      "duration": 3.56
    },
    {
      "text": "text that's the disadvantage of",
      "start": 663.8,
      "duration": 3.76
    },
    {
      "text": "character sequence character",
      "start": 665.48,
      "duration": 4.44
    },
    {
      "text": "tokenization the advantages of character",
      "start": 667.56,
      "duration": 4.12
    },
    {
      "text": "tokenization is that they have very",
      "start": 669.92,
      "duration": 4.159
    },
    {
      "text": "small vocabulary because every language",
      "start": 671.68,
      "duration": 4.399
    },
    {
      "text": "has fixed number of characters and we",
      "start": 674.079,
      "duration": 3.76
    },
    {
      "text": "solve we completely solve the out of",
      "start": 676.079,
      "duration": 4.401
    },
    {
      "text": "vocabulary problem and both of these",
      "start": 677.839,
      "duration": 4.481
    },
    {
      "text": "approaches have a disadvantage that the",
      "start": 680.48,
      "duration": 4.08
    },
    {
      "text": "meaning between words is not captured",
      "start": 682.32,
      "duration": 5.04
    },
    {
      "text": "boy and boys have a common root right",
      "start": 684.56,
      "duration": 4.8
    },
    {
      "text": "that root is cap is not captured",
      "start": 687.36,
      "duration": 4.919
    },
    {
      "text": "tokenization and modernization both have",
      "start": 689.36,
      "duration": 5.56
    },
    {
      "text": "the isation which is common this meaning",
      "start": 692.279,
      "duration": 4.12
    },
    {
      "text": "is completely",
      "start": 694.92,
      "duration": 4.32
    },
    {
      "text": "lost so now so what do we do then we",
      "start": 696.399,
      "duration": 5.68
    },
    {
      "text": "turn to another tokenization algorithm",
      "start": 699.24,
      "duration": 5.36
    },
    {
      "text": "which is called as the subword based",
      "start": 702.079,
      "duration": 5.0
    },
    {
      "text": "tokenization and the bite pair encoding",
      "start": 704.6,
      "duration": 4.28
    },
    {
      "text": "which we are going to see is an example",
      "start": 707.079,
      "duration": 4.681
    },
    {
      "text": "of subword based tokenization algorithm",
      "start": 708.88,
      "duration": 4.92
    },
    {
      "text": "the subword based tokenization is kind",
      "start": 711.76,
      "duration": 4.879
    },
    {
      "text": "of like a Best of Both words words and",
      "start": 713.8,
      "duration": 5.479
    },
    {
      "text": "let's see how it uh why it is the best",
      "start": 716.639,
      "duration": 5.601
    },
    {
      "text": "best of both words so the first thing to",
      "start": 719.279,
      "duration": 4.481
    },
    {
      "text": "remember about subword based",
      "start": 722.24,
      "duration": 3.88
    },
    {
      "text": "tokenization is it does capture some",
      "start": 723.76,
      "duration": 5.0
    },
    {
      "text": "root words which come in many other",
      "start": 726.12,
      "duration": 5.12
    },
    {
      "text": "words so boy and boys it will treat boy",
      "start": 728.76,
      "duration": 5.24
    },
    {
      "text": "as a common root word uh and let's see",
      "start": 731.24,
      "duration": 5.52
    },
    {
      "text": "how it does that okay so in subword",
      "start": 734.0,
      "duration": 4.68
    },
    {
      "text": "based tokenization there are essentially",
      "start": 736.76,
      "duration": 4.4
    },
    {
      "text": "two rules the first rule of this",
      "start": 738.68,
      "duration": 5.279
    },
    {
      "text": "tokenization is that when you get the",
      "start": 741.16,
      "duration": 6.799
    },
    {
      "text": "data set you do not do not split",
      "start": 743.959,
      "duration": 6.081
    },
    {
      "text": "frequently used word into smaller",
      "start": 747.959,
      "duration": 4.44
    },
    {
      "text": "subwords so if there are some words",
      "start": 750.04,
      "duration": 4.56
    },
    {
      "text": "which are coming frequently you should",
      "start": 752.399,
      "duration": 5.321
    },
    {
      "text": "retain those words as it is right so",
      "start": 754.6,
      "duration": 6.56
    },
    {
      "text": "then it retains this from the word",
      "start": 757.72,
      "duration": 5.96
    },
    {
      "text": "tokenization then the rule two is that",
      "start": 761.16,
      "duration": 5.039
    },
    {
      "text": "if there are some words which are very",
      "start": 763.68,
      "duration": 5.0
    },
    {
      "text": "rare which are not occurring too many",
      "start": 766.199,
      "duration": 4.721
    },
    {
      "text": "times then you split these words into",
      "start": 768.68,
      "duration": 5.08
    },
    {
      "text": "smaller meaningful subwords this is",
      "start": 770.92,
      "duration": 4.24
    },
    {
      "text": "extremely",
      "start": 773.76,
      "duration": 3.96
    },
    {
      "text": "important this second part basically",
      "start": 775.16,
      "duration": 4.119
    },
    {
      "text": "says that if there are some words which",
      "start": 777.72,
      "duration": 3.799
    },
    {
      "text": "are rare not appearing too many times",
      "start": 779.279,
      "duration": 3.92
    },
    {
      "text": "you can go on splitting it further you",
      "start": 781.519,
      "duration": 4.041
    },
    {
      "text": "can even go down to the Character level",
      "start": 783.199,
      "duration": 4.241
    },
    {
      "text": "so you can see why it is a mix between",
      "start": 785.56,
      "duration": 4.16
    },
    {
      "text": "word tokenizer and character tokenizer",
      "start": 787.44,
      "duration": 4.759
    },
    {
      "text": "the first rule implies that if the words",
      "start": 789.72,
      "duration": 4.16
    },
    {
      "text": "are occurring many times you return it",
      "start": 792.199,
      "duration": 4.121
    },
    {
      "text": "as a word so this is taken this is a",
      "start": 793.88,
      "duration": 4.639
    },
    {
      "text": "feature taken from the word tokenizer",
      "start": 796.32,
      "duration": 4.16
    },
    {
      "text": "the second rule implies that if the word",
      "start": 798.519,
      "duration": 4.841
    },
    {
      "text": "is rare you can go on splitting it into",
      "start": 800.48,
      "duration": 5.0
    },
    {
      "text": "further subwords and if needed you can",
      "start": 803.36,
      "duration": 4.08
    },
    {
      "text": "drop down to the Character level we",
      "start": 805.48,
      "duration": 3.76
    },
    {
      "text": "don't always drop down to the character",
      "start": 807.44,
      "duration": 4.32
    },
    {
      "text": "level we even stay in the middle but",
      "start": 809.24,
      "duration": 4.08
    },
    {
      "text": "this is a feature from the character",
      "start": 811.76,
      "duration": 3.0
    },
    {
      "text": "level because we are breaking down the",
      "start": 813.32,
      "duration": 4.92
    },
    {
      "text": "word further so to give you an example",
      "start": 814.76,
      "duration": 5.72
    },
    {
      "text": "if the word boy is appearing multiple",
      "start": 818.24,
      "duration": 4.32
    },
    {
      "text": "times in the data set it should not be",
      "start": 820.48,
      "duration": 4.4
    },
    {
      "text": "split further that's from rule number",
      "start": 822.56,
      "duration": 5.16
    },
    {
      "text": "one so then boy is retained as a token",
      "start": 824.88,
      "duration": 6.879
    },
    {
      "text": "but boys if the word boys is encountered",
      "start": 827.72,
      "duration": 6.96
    },
    {
      "text": "uh that should be split into boy and it",
      "start": 831.759,
      "duration": 5.921
    },
    {
      "text": "should be split into",
      "start": 834.68,
      "duration": 5.88
    },
    {
      "text": "s uh because boys might not be appearing",
      "start": 837.68,
      "duration": 4.68
    },
    {
      "text": "too many times and again boys also",
      "start": 840.56,
      "duration": 5.199
    },
    {
      "text": "derives from the word boy so we should",
      "start": 842.36,
      "duration": 5.88
    },
    {
      "text": "divide this word boys into smaller",
      "start": 845.759,
      "duration": 5.041
    },
    {
      "text": "meaningful subwords so boys is divided",
      "start": 848.24,
      "duration": 5.76
    },
    {
      "text": "into boy and S and this is the main",
      "start": 850.8,
      "duration": 6.92
    },
    {
      "text": "essence of subword tokenization words",
      "start": 854.0,
      "duration": 6.56
    },
    {
      "text": "are sometimes broken down into smaller",
      "start": 857.72,
      "duration": 4.76
    },
    {
      "text": "elements and why smaller elements",
      "start": 860.56,
      "duration": 4.32
    },
    {
      "text": "because those smaller elements appear",
      "start": 862.48,
      "duration": 4.32
    },
    {
      "text": "very frequently so for example why is",
      "start": 864.88,
      "duration": 4.399
    },
    {
      "text": "boys broken down into boy and S because",
      "start": 866.8,
      "duration": 5.159
    },
    {
      "text": "boy appears more frequently and S is",
      "start": 869.279,
      "duration": 5.56
    },
    {
      "text": "also another token which appears very",
      "start": 871.959,
      "duration": 5.841
    },
    {
      "text": "frequently this is what is basically",
      "start": 874.839,
      "duration": 5.081
    },
    {
      "text": "done in a subord tokenization scheme at",
      "start": 877.8,
      "duration": 4.8
    },
    {
      "text": "a very high level so let me explain some",
      "start": 879.92,
      "duration": 5.56
    },
    {
      "text": "advantages of subord tokenization the",
      "start": 882.6,
      "duration": 5.76
    },
    {
      "text": "subword splitting helps the model learn",
      "start": 885.48,
      "duration": 4.88
    },
    {
      "text": "that different words with the same root",
      "start": 888.36,
      "duration": 5.96
    },
    {
      "text": "word such as for example token tokens",
      "start": 890.36,
      "duration": 4.68
    },
    {
      "text": "and",
      "start": 894.32,
      "duration": 2.8
    },
    {
      "text": "tokenizing all of these three words",
      "start": 895.04,
      "duration": 3.68
    },
    {
      "text": "essentially have the same root word",
      "start": 897.12,
      "duration": 4.76
    },
    {
      "text": "right token so subord splitting helps",
      "start": 898.72,
      "duration": 4.76
    },
    {
      "text": "the model understand that these",
      "start": 901.88,
      "duration": 3.28
    },
    {
      "text": "different words essentially have the",
      "start": 903.48,
      "duration": 3.76
    },
    {
      "text": "same root word and they are similar in",
      "start": 905.16,
      "duration": 5.0
    },
    {
      "text": "meaning this meaning is lost in word",
      "start": 907.24,
      "duration": 4.88
    },
    {
      "text": "based tokenization and even character",
      "start": 910.16,
      "duration": 4.64
    },
    {
      "text": "based tokenization that is number one",
      "start": 912.12,
      "duration": 6.24
    },
    {
      "text": "the second advantage of subord",
      "start": 914.8,
      "duration": 6.2
    },
    {
      "text": "tokenization is that it also helps the",
      "start": 918.36,
      "duration": 5.159
    },
    {
      "text": "model learn that let's say tokenization",
      "start": 921.0,
      "duration": 4.759
    },
    {
      "text": "and modernization are made up of",
      "start": 923.519,
      "duration": 4.961
    },
    {
      "text": "different root words token tokenize and",
      "start": 925.759,
      "duration": 3.76
    },
    {
      "text": "modernize",
      "start": 928.48,
      "duration": 3.64
    },
    {
      "text": "but have the same suffix",
      "start": 929.519,
      "duration": 5.961
    },
    {
      "text": "isation and are used in same syntactic",
      "start": 932.12,
      "duration": 6.519
    },
    {
      "text": "situations so basically it derives these",
      "start": 935.48,
      "duration": 4.76
    },
    {
      "text": "patterns like tokenization and",
      "start": 938.639,
      "duration": 4.481
    },
    {
      "text": "modernization maybe zation z a t i o n",
      "start": 940.24,
      "duration": 5.88
    },
    {
      "text": "is a is a subword token in this",
      "start": 943.12,
      "duration": 5.199
    },
    {
      "text": "tokenization and then token and then",
      "start": 946.12,
      "duration": 4.36
    },
    {
      "text": "modern and another tokens so then it",
      "start": 948.319,
      "duration": 4.64
    },
    {
      "text": "learns that this isation is a common",
      "start": 950.48,
      "duration": 4.08
    },
    {
      "text": "suffix which appears in both of these",
      "start": 952.959,
      "duration": 3.761
    },
    {
      "text": "words all of these are advantages of",
      "start": 954.56,
      "duration": 4.56
    },
    {
      "text": "breaking down one word into subwords",
      "start": 956.72,
      "duration": 5.359
    },
    {
      "text": "this is what is majorly done in subword",
      "start": 959.12,
      "duration": 5.04
    },
    {
      "text": "tokenization so why are we learning",
      "start": 962.079,
      "duration": 4.281
    },
    {
      "text": "about subword tokenization and what's",
      "start": 964.16,
      "duration": 4.28
    },
    {
      "text": "the relation between bite pair encoding",
      "start": 966.36,
      "duration": 4.919
    },
    {
      "text": "and subo tokenization well the main",
      "start": 968.44,
      "duration": 6.6
    },
    {
      "text": "relation is that uh bite pair encoding",
      "start": 971.279,
      "duration": 7.0
    },
    {
      "text": "or bpe is a subord tokenization",
      "start": 975.04,
      "duration": 5.599
    },
    {
      "text": "algorithm this is very important and",
      "start": 978.279,
      "duration": 5.321
    },
    {
      "text": "that's why we are learning about bpe and",
      "start": 980.639,
      "duration": 5.361
    },
    {
      "text": "that's why modern llms like gpt2 and",
      "start": 983.6,
      "duration": 6.239
    },
    {
      "text": "gpt3 employ bite pair encoding",
      "start": 986.0,
      "duration": 5.8
    },
    {
      "text": "so let us look at the bit of a history",
      "start": 989.839,
      "duration": 3.961
    },
    {
      "text": "of the BP algorithm and let and then",
      "start": 991.8,
      "duration": 4.959
    },
    {
      "text": "we'll see how it is implemented in",
      "start": 993.8,
      "duration": 5.88
    },
    {
      "text": "practice so the bite pair algorithm was",
      "start": 996.759,
      "duration": 4.801
    },
    {
      "text": "actually introduced in",
      "start": 999.68,
      "duration": 5.44
    },
    {
      "text": "1994 and uh it does a very simple thing",
      "start": 1001.56,
      "duration": 6.199
    },
    {
      "text": "it is basically a data compression",
      "start": 1005.12,
      "duration": 5.839
    },
    {
      "text": "algorithm uh and what is done is that we",
      "start": 1007.759,
      "duration": 5.76
    },
    {
      "text": "we scan the data and then mostov common",
      "start": 1010.959,
      "duration": 5.281
    },
    {
      "text": "pair of consecutive bytes so let me",
      "start": 1013.519,
      "duration": 5.841
    },
    {
      "text": "actually use a different color here",
      "start": 1016.24,
      "duration": 5.039
    },
    {
      "text": "this sentence is very important most",
      "start": 1019.36,
      "duration": 5.559
    },
    {
      "text": "common pair of consecutive bytes of data",
      "start": 1021.279,
      "duration": 6.361
    },
    {
      "text": "is replaced with a bite that does not",
      "start": 1024.919,
      "duration": 5.601
    },
    {
      "text": "occur in data so we identify pair of",
      "start": 1027.64,
      "duration": 5.399
    },
    {
      "text": "consecutive bites which occur the most",
      "start": 1030.52,
      "duration": 4.799
    },
    {
      "text": "so we find these pairs which occur the",
      "start": 1033.039,
      "duration": 4.481
    },
    {
      "text": "most frequently and then we replace them",
      "start": 1035.319,
      "duration": 4.0
    },
    {
      "text": "with a bite which does not exist in the",
      "start": 1037.52,
      "duration": 3.679
    },
    {
      "text": "data and we keep on doing this",
      "start": 1039.319,
      "duration": 4.201
    },
    {
      "text": "iteratively I'll show an example for",
      "start": 1041.199,
      "duration": 4.0
    },
    {
      "text": "this so don't worry if you don't",
      "start": 1043.52,
      "duration": 3.6
    },
    {
      "text": "understand this explanation I want to",
      "start": 1045.199,
      "duration": 3.521
    },
    {
      "text": "quickly show you the paper so this is",
      "start": 1047.12,
      "duration": 3.4
    },
    {
      "text": "the the paper which was published in",
      "start": 1048.72,
      "duration": 4.28
    },
    {
      "text": "1994 a new algorithm for data",
      "start": 1050.52,
      "duration": 4.6
    },
    {
      "text": "compression which introduced bite pair",
      "start": 1053.0,
      "duration": 4.96
    },
    {
      "text": "tokenizer or bite pair encoder rather at",
      "start": 1055.12,
      "duration": 4.72
    },
    {
      "text": "that time it was not known that this",
      "start": 1057.96,
      "duration": 3.839
    },
    {
      "text": "will be so useful for modern large",
      "start": 1059.84,
      "duration": 5.04
    },
    {
      "text": "language models but it is quite",
      "start": 1061.799,
      "duration": 5.24
    },
    {
      "text": "useful okay so now let us see a",
      "start": 1064.88,
      "duration": 3.799
    },
    {
      "text": "practical demonstration of this",
      "start": 1067.039,
      "duration": 4.041
    },
    {
      "text": "algorithm so I've taken this example",
      "start": 1068.679,
      "duration": 4.88
    },
    {
      "text": "from Wikipedia because I think it is an",
      "start": 1071.08,
      "duration": 4.719
    },
    {
      "text": "awesome illustration so let's say we",
      "start": 1073.559,
      "duration": 4.561
    },
    {
      "text": "have this original data right and the",
      "start": 1075.799,
      "duration": 5.721
    },
    {
      "text": "original data looks like this a a a b d",
      "start": 1078.12,
      "duration": 7.52
    },
    {
      "text": "a a a b a c okay and now in bite pair",
      "start": 1081.52,
      "duration": 6.159
    },
    {
      "text": "encoding we are going to compress this",
      "start": 1085.64,
      "duration": 4.64
    },
    {
      "text": "data right and let us see what do we do",
      "start": 1087.679,
      "duration": 4.961
    },
    {
      "text": "we first identify the most common pair",
      "start": 1090.28,
      "duration": 5.48
    },
    {
      "text": "of consecutive bytes so let's see let us",
      "start": 1092.64,
      "duration": 5.519
    },
    {
      "text": "see the pair which occurs the most so if",
      "start": 1095.76,
      "duration": 4.08
    },
    {
      "text": "you scan this sequence from left to",
      "start": 1098.159,
      "duration": 4.361
    },
    {
      "text": "right you will see that the bite pair AA",
      "start": 1099.84,
      "duration": 5.8
    },
    {
      "text": "occurs the most right you might say a AA",
      "start": 1102.52,
      "duration": 5.24
    },
    {
      "text": "also occurs the most but that's not a",
      "start": 1105.64,
      "duration": 4.68
    },
    {
      "text": "bite pair because that three characters",
      "start": 1107.76,
      "duration": 4.68
    },
    {
      "text": "the bite pair AA occurs the most it",
      "start": 1110.32,
      "duration": 5.12
    },
    {
      "text": "occurs here it occurs here it occurs",
      "start": 1112.44,
      "duration": 5.08
    },
    {
      "text": "here and it occurs the here so it occurs",
      "start": 1115.44,
      "duration": 4.4
    },
    {
      "text": "four times really so what we will do",
      "start": 1117.52,
      "duration": 4.2
    },
    {
      "text": "next is that we have identified the bite",
      "start": 1119.84,
      "duration": 4.199
    },
    {
      "text": "pair which occurs the most we replace",
      "start": 1121.72,
      "duration": 5.36
    },
    {
      "text": "this bite pair with a bite that does not",
      "start": 1124.039,
      "duration": 5.801
    },
    {
      "text": "exist in the data so what we'll do is",
      "start": 1127.08,
      "duration": 6.0
    },
    {
      "text": "that we will replace it with zed and why",
      "start": 1129.84,
      "duration": 5.319
    },
    {
      "text": "Z because just Zed does not occur in the",
      "start": 1133.08,
      "duration": 4.68
    },
    {
      "text": "data you can use any variable here so",
      "start": 1135.159,
      "duration": 4.961
    },
    {
      "text": "then we'll take these a a and wherever",
      "start": 1137.76,
      "duration": 5.24
    },
    {
      "text": "AA shows up we'll replace it by Zed so",
      "start": 1140.12,
      "duration": 5.08
    },
    {
      "text": "then this first AA will be Zed so then",
      "start": 1143.0,
      "duration": 5.44
    },
    {
      "text": "this new sequence will be Zed a b d then",
      "start": 1145.2,
      "duration": 5.599
    },
    {
      "text": "again Zed to replace this a a so then it",
      "start": 1148.44,
      "duration": 5.68
    },
    {
      "text": "will be Zed and then a b a c so then we",
      "start": 1150.799,
      "duration": 6.201
    },
    {
      "text": "have a b a c so remember what has",
      "start": 1154.12,
      "duration": 4.76
    },
    {
      "text": "happened here is that this AA which I'm",
      "start": 1157.0,
      "duration": 3.36
    },
    {
      "text": "highlighting right now in circle that",
      "start": 1158.88,
      "duration": 4.08
    },
    {
      "text": "has been replaced by Zed and this AA has",
      "start": 1160.36,
      "duration": 4.96
    },
    {
      "text": "been replaced by Zed so now we have a",
      "start": 1162.96,
      "duration": 5.8
    },
    {
      "text": "compressed data correct very good so now",
      "start": 1165.32,
      "duration": 6.359
    },
    {
      "text": "now let us move next next what we do is",
      "start": 1168.76,
      "duration": 5.159
    },
    {
      "text": "we keep on repeating this sequentially",
      "start": 1171.679,
      "duration": 4.24
    },
    {
      "text": "now we again look at this sequence and",
      "start": 1173.919,
      "duration": 4.561
    },
    {
      "text": "find the next common bite pair so we can",
      "start": 1175.919,
      "duration": 6.081
    },
    {
      "text": "see that a is ur occurring once and a is",
      "start": 1178.48,
      "duration": 6.0
    },
    {
      "text": "occurring twice so it is repeating two",
      "start": 1182.0,
      "duration": 4.44
    },
    {
      "text": "times and which is the most frequent",
      "start": 1184.48,
      "duration": 5.04
    },
    {
      "text": "bite pair so we will replace AB by y so",
      "start": 1186.44,
      "duration": 5.52
    },
    {
      "text": "this AB will be replaced by Y and this",
      "start": 1189.52,
      "duration": 5.24
    },
    {
      "text": "AB will be replaced by y why why because",
      "start": 1191.96,
      "duration": 4.64
    },
    {
      "text": "it's a bite which does not occur",
      "start": 1194.76,
      "duration": 5.32
    },
    {
      "text": "anywhere in the U data",
      "start": 1196.6,
      "duration": 5.72
    },
    {
      "text": "so then this AB will be replaced by y as",
      "start": 1200.08,
      "duration": 4.4
    },
    {
      "text": "you can see here and here also the ab",
      "start": 1202.32,
      "duration": 4.4
    },
    {
      "text": "will be replaced by y so then my new",
      "start": 1204.48,
      "duration": 6.319
    },
    {
      "text": "compressed data will be z y d z y a",
      "start": 1206.72,
      "duration": 6.439
    },
    {
      "text": "great this is a compressed data compared",
      "start": 1210.799,
      "duration": 4.321
    },
    {
      "text": "to the original data and this is exactly",
      "start": 1213.159,
      "duration": 4.76
    },
    {
      "text": "how the bite pair algorithm works now we",
      "start": 1215.12,
      "duration": 4.88
    },
    {
      "text": "do this recur do this again and again",
      "start": 1217.919,
      "duration": 3.601
    },
    {
      "text": "right so now let us look at the bite",
      "start": 1220.0,
      "duration": 4.159
    },
    {
      "text": "pair so here AC is the only bite pair",
      "start": 1221.52,
      "duration": 4.639
    },
    {
      "text": "which is left all others are special",
      "start": 1224.159,
      "duration": 5.961
    },
    {
      "text": "tokens but AC only appears once so we we",
      "start": 1226.159,
      "duration": 6.481
    },
    {
      "text": "don't need to encode it this process of",
      "start": 1230.12,
      "duration": 4.6
    },
    {
      "text": "replacing common bite pairs with another",
      "start": 1232.64,
      "duration": 4.399
    },
    {
      "text": "variable is called encoding that's where",
      "start": 1234.72,
      "duration": 4.76
    },
    {
      "text": "the word encoder comes from so we will",
      "start": 1237.039,
      "duration": 4.361
    },
    {
      "text": "not encode this further because it only",
      "start": 1239.48,
      "duration": 4.36
    },
    {
      "text": "appears once if a c were appearing twice",
      "start": 1241.4,
      "duration": 3.84
    },
    {
      "text": "then we would have encoded it with",
      "start": 1243.84,
      "duration": 3.48
    },
    {
      "text": "another variable so this compression",
      "start": 1245.24,
      "duration": 4.6
    },
    {
      "text": "actually stops here you can go one more",
      "start": 1247.32,
      "duration": 5.32
    },
    {
      "text": "layer Deeper by further com replacing",
      "start": 1249.84,
      "duration": 4.8
    },
    {
      "text": "the zy with another variable which is",
      "start": 1252.64,
      "duration": 3.039
    },
    {
      "text": "let's say",
      "start": 1254.64,
      "duration": 6.6
    },
    {
      "text": "w so then it will be WD w a and then you",
      "start": 1255.679,
      "duration": 7.041
    },
    {
      "text": "will stop so you will compress it",
      "start": 1261.24,
      "duration": 3.319
    },
    {
      "text": "further like this so you see the",
      "start": 1262.72,
      "duration": 4.36
    },
    {
      "text": "original data has been compressed to",
      "start": 1264.559,
      "duration": 6.081
    },
    {
      "text": "this right uh to this compressed version",
      "start": 1267.08,
      "duration": 5.64
    },
    {
      "text": "U using the bite pair algorithm so the",
      "start": 1270.64,
      "duration": 3.96
    },
    {
      "text": "algorithm itself is pretty simple you",
      "start": 1272.72,
      "duration": 3.72
    },
    {
      "text": "scan the data from left to right you",
      "start": 1274.6,
      "duration": 3.8
    },
    {
      "text": "identify the bite pairs which occur the",
      "start": 1276.44,
      "duration": 4.32
    },
    {
      "text": "most and then you replace them with a",
      "start": 1278.4,
      "duration": 4.72
    },
    {
      "text": "bite which does not exist in the data",
      "start": 1280.76,
      "duration": 4.6
    },
    {
      "text": "and you do this iteratively until you",
      "start": 1283.12,
      "duration": 4.64
    },
    {
      "text": "reach a stage where no bite pair occurs",
      "start": 1285.36,
      "duration": 4.96
    },
    {
      "text": "more than once that's it that is the",
      "start": 1287.76,
      "duration": 4.799
    },
    {
      "text": "simple bite pair encoding",
      "start": 1290.32,
      "duration": 4.68
    },
    {
      "text": "algorithm now you might think okay what",
      "start": 1292.559,
      "duration": 4.281
    },
    {
      "text": "has that got to do with large language",
      "start": 1295.0,
      "duration": 4.6
    },
    {
      "text": "models right I understand this algorithm",
      "start": 1296.84,
      "duration": 5.24
    },
    {
      "text": "and I understand how it compresses a",
      "start": 1299.6,
      "duration": 5.079
    },
    {
      "text": "given data sequence but what has that",
      "start": 1302.08,
      "duration": 4.88
    },
    {
      "text": "got to do with large language models",
      "start": 1304.679,
      "duration": 5.281
    },
    {
      "text": "well it turns out that the we slightly",
      "start": 1306.96,
      "duration": 5.76
    },
    {
      "text": "tweak the bite pair encoding algorithm",
      "start": 1309.96,
      "duration": 5.599
    },
    {
      "text": "and use this to convert our entire",
      "start": 1312.72,
      "duration": 5.12
    },
    {
      "text": "sentence into subwords which will be",
      "start": 1315.559,
      "duration": 4.6
    },
    {
      "text": "very useful for us and I'll show you",
      "start": 1317.84,
      "duration": 5.76
    },
    {
      "text": "exactly how I'm going to do that so the",
      "start": 1320.159,
      "duration": 6.161
    },
    {
      "text": "bite pair encoding for llm ensures that",
      "start": 1323.6,
      "duration": 5.319
    },
    {
      "text": "the most common words in the vocabulary",
      "start": 1326.32,
      "duration": 4.44
    },
    {
      "text": "are represented as a single token",
      "start": 1328.919,
      "duration": 5.321
    },
    {
      "text": "remember rule number one and rare words",
      "start": 1330.76,
      "duration": 5.88
    },
    {
      "text": "are broken down into two or more subord",
      "start": 1334.24,
      "duration": 4.919
    },
    {
      "text": "tokens this is exactly the same rules",
      "start": 1336.64,
      "duration": 4.12
    },
    {
      "text": "which we had looked at the rule number",
      "start": 1339.159,
      "duration": 4.801
    },
    {
      "text": "one and rule number two so rule number",
      "start": 1340.76,
      "duration": 5.64
    },
    {
      "text": "one is that most commonly used words",
      "start": 1343.96,
      "duration": 4.599
    },
    {
      "text": "should not be split and second is that",
      "start": 1346.4,
      "duration": 4.0
    },
    {
      "text": "that rare words should be split into",
      "start": 1348.559,
      "duration": 5.281
    },
    {
      "text": "meaningful subwords now let's see how uh",
      "start": 1350.4,
      "duration": 4.96
    },
    {
      "text": "how it's related to The Bite pair",
      "start": 1353.84,
      "duration": 3.68
    },
    {
      "text": "encoding algorithm which we saw and we",
      "start": 1355.36,
      "duration": 4.28
    },
    {
      "text": "will be looking at a practical example",
      "start": 1357.52,
      "duration": 4.399
    },
    {
      "text": "for this uh",
      "start": 1359.64,
      "duration": 4.44
    },
    {
      "text": "demonstration okay I will use a color",
      "start": 1361.919,
      "duration": 4.281
    },
    {
      "text": "which is a bit different from the green",
      "start": 1364.08,
      "duration": 4.92
    },
    {
      "text": "one here so that uh there will be a good",
      "start": 1366.2,
      "duration": 6.04
    },
    {
      "text": "contrast so let me use the orange color",
      "start": 1369.0,
      "duration": 5.6
    },
    {
      "text": "okay so for the Practical example we are",
      "start": 1372.24,
      "duration": 4.919
    },
    {
      "text": "going to look at a vocabulary or rather",
      "start": 1374.6,
      "duration": 4.16
    },
    {
      "text": "I should say we are going to look at the",
      "start": 1377.159,
      "duration": 4.081
    },
    {
      "text": "data set of Words which is also called",
      "start": 1378.76,
      "duration": 5.12
    },
    {
      "text": "as Data Corpus which is this so we have",
      "start": 1381.24,
      "duration": 4.6
    },
    {
      "text": "these words in our data set",
      "start": 1383.88,
      "duration": 3.84
    },
    {
      "text": "old",
      "start": 1385.84,
      "duration": 7.0
    },
    {
      "text": "older finest and lowest right let's say",
      "start": 1387.72,
      "duration": 6.959
    },
    {
      "text": "we have these words in the data set",
      "start": 1392.84,
      "duration": 4.4
    },
    {
      "text": "right now and I'm going to show you how",
      "start": 1394.679,
      "duration": 4.401
    },
    {
      "text": "we are going to use the bite pair",
      "start": 1397.24,
      "duration": 3.88
    },
    {
      "text": "encoding algorithm to break these down",
      "start": 1399.08,
      "duration": 3.199
    },
    {
      "text": "into",
      "start": 1401.12,
      "duration": 3.679
    },
    {
      "text": "tokens and you will also see why this is",
      "start": 1402.279,
      "duration": 5.081
    },
    {
      "text": "a subword tokenization scheme so first",
      "start": 1404.799,
      "duration": 5.281
    },
    {
      "text": "of all if we are to use a word based",
      "start": 1407.36,
      "duration": 5.28
    },
    {
      "text": "tokenization then this will have four",
      "start": 1410.08,
      "duration": 5.199
    },
    {
      "text": "tokens old older finest and lowest",
      "start": 1412.64,
      "duration": 5.68
    },
    {
      "text": "that's it similarly if we were to use",
      "start": 1415.279,
      "duration": 4.921
    },
    {
      "text": "the Character level tokenization then",
      "start": 1418.32,
      "duration": 3.839
    },
    {
      "text": "the tokens will be individual characters",
      "start": 1420.2,
      "duration": 5.32
    },
    {
      "text": "like o o will be one token then L will",
      "start": 1422.159,
      "duration": 6.041
    },
    {
      "text": "be another token then D will be another",
      "start": 1425.52,
      "duration": 4.32
    },
    {
      "text": "token",
      "start": 1428.2,
      "duration": 4.0
    },
    {
      "text": "Etc so this is how the word based",
      "start": 1429.84,
      "duration": 4.12
    },
    {
      "text": "tokenization and the character based",
      "start": 1432.2,
      "duration": 3.76
    },
    {
      "text": "tokenization will work but right now we",
      "start": 1433.96,
      "duration": 3.92
    },
    {
      "text": "are going to see the subw based",
      "start": 1435.96,
      "duration": 4.36
    },
    {
      "text": "tokenization using the bite pair",
      "start": 1437.88,
      "duration": 6.039
    },
    {
      "text": "algorithm before we uh proceed further",
      "start": 1440.32,
      "duration": 5.4
    },
    {
      "text": "we'll need to do a pre-processing step",
      "start": 1443.919,
      "duration": 4.64
    },
    {
      "text": "and this is actually done uh even when",
      "start": 1445.72,
      "duration": 4.68
    },
    {
      "text": "we train large language models and when",
      "start": 1448.559,
      "duration": 4.041
    },
    {
      "text": "we are using these tokenizers so",
      "start": 1450.4,
      "duration": 4.0
    },
    {
      "text": "basically when you look at these",
      "start": 1452.6,
      "duration": 4.0
    },
    {
      "text": "different tokens there should be some",
      "start": 1454.4,
      "duration": 4.48
    },
    {
      "text": "ending right so for example when this",
      "start": 1456.6,
      "duration": 5.04
    },
    {
      "text": "old token appears we should have another",
      "start": 1458.88,
      "duration": 6.0
    },
    {
      "text": "end token so that we know that this word",
      "start": 1461.64,
      "duration": 5.44
    },
    {
      "text": "has ended over here so I'm going to",
      "start": 1464.88,
      "duration": 4.399
    },
    {
      "text": "augment every token here with this",
      "start": 1467.08,
      "duration": 5.16
    },
    {
      "text": "additional end token which is called",
      "start": 1469.279,
      "duration": 5.961
    },
    {
      "text": "slw so whenever the algorithm or the",
      "start": 1472.24,
      "duration": 5.439
    },
    {
      "text": "model comes across slw we know that this",
      "start": 1475.24,
      "duration": 4.76
    },
    {
      "text": "word ends over here so I'm going to",
      "start": 1477.679,
      "duration": 4.681
    },
    {
      "text": "replace the tokens in my data set with",
      "start": 1480.0,
      "duration": 5.72
    },
    {
      "text": "adding a w/w at the end like old becomes",
      "start": 1482.36,
      "duration": 8.12
    },
    {
      "text": "old slw older becomes older slw finest",
      "start": 1485.72,
      "duration": 7.92
    },
    {
      "text": "becomes finest slw and lowest becomes",
      "start": 1490.48,
      "duration": 4.679
    },
    {
      "text": "lowest",
      "start": 1493.64,
      "duration": 4.44
    },
    {
      "text": "/w now remember here that if we use the",
      "start": 1495.159,
      "duration": 4.24
    },
    {
      "text": "word based",
      "start": 1498.08,
      "duration": 3.76
    },
    {
      "text": "tokenization uh there is no meaning",
      "start": 1499.399,
      "duration": 5.16
    },
    {
      "text": "which is captured so the fact that old",
      "start": 1501.84,
      "duration": 5.0
    },
    {
      "text": "is the common root between old and older",
      "start": 1504.559,
      "duration": 5.24
    },
    {
      "text": "is not captured number one EST is the",
      "start": 1506.84,
      "duration": 4.959
    },
    {
      "text": "common root between finest and lowest",
      "start": 1509.799,
      "duration": 5.24
    },
    {
      "text": "that's not captured so word based",
      "start": 1511.799,
      "duration": 4.641
    },
    {
      "text": "tokenization character based",
      "start": 1515.039,
      "duration": 3.081
    },
    {
      "text": "tokenization have so many problems",
      "start": 1516.44,
      "duration": 2.88
    },
    {
      "text": "because they don't capture these",
      "start": 1518.12,
      "duration": 3.88
    },
    {
      "text": "meanings or root words and towards the",
      "start": 1519.32,
      "duration": 4.2
    },
    {
      "text": "end of this section we'll see how",
      "start": 1522.0,
      "duration": 3.559
    },
    {
      "text": "subword based tokenization using the",
      "start": 1523.52,
      "duration": 4.08
    },
    {
      "text": "bite pair encoding algorithm actually",
      "start": 1525.559,
      "duration": 4.12
    },
    {
      "text": "captures these root",
      "start": 1527.6,
      "duration": 8.319
    },
    {
      "text": "words like old uh like EST Etc okay so",
      "start": 1529.679,
      "duration": 8.081
    },
    {
      "text": "let's get started with the individual",
      "start": 1535.919,
      "duration": 4.961
    },
    {
      "text": "steps the first step is basically to",
      "start": 1537.76,
      "duration": 6.08
    },
    {
      "text": "split all these words into their",
      "start": 1540.88,
      "duration": 5.64
    },
    {
      "text": "characters um and then make a frequency",
      "start": 1543.84,
      "duration": 4.839
    },
    {
      "text": "table so what we are going to do is that",
      "start": 1546.52,
      "duration": 4.2
    },
    {
      "text": "we are going to take all these words old",
      "start": 1548.679,
      "duration": 4.961
    },
    {
      "text": "older finest and lowest so old appears",
      "start": 1550.72,
      "duration": 5.24
    },
    {
      "text": "seven times in the data set older",
      "start": 1553.64,
      "duration": 4.0
    },
    {
      "text": "appears three times in the data set",
      "start": 1555.96,
      "duration": 4.28
    },
    {
      "text": "these are the frequencies finest appears",
      "start": 1557.64,
      "duration": 5.039
    },
    {
      "text": "nine times in the data set and lowest",
      "start": 1560.24,
      "duration": 4.2
    },
    {
      "text": "essentially appears four times in the",
      "start": 1562.679,
      "duration": 4.12
    },
    {
      "text": "data set so what we are going to do",
      "start": 1564.44,
      "duration": 3.88
    },
    {
      "text": "right now is we are going to split these",
      "start": 1566.799,
      "duration": 4.161
    },
    {
      "text": "words into individual characters so then",
      "start": 1568.32,
      "duration": 4.479
    },
    {
      "text": "here is the table which I have made",
      "start": 1570.96,
      "duration": 6.319
    },
    {
      "text": "remember slw is also there uh so old so",
      "start": 1572.799,
      "duration": 6.76
    },
    {
      "text": "all words have/ W right and totally how",
      "start": 1577.279,
      "duration": 6.4
    },
    {
      "text": "many words do we have 7 + 3 10 + 9 19+ 4",
      "start": 1579.559,
      "duration": 7.12
    },
    {
      "text": "23 and since all words have slw at the",
      "start": 1583.679,
      "duration": 6.281
    },
    {
      "text": "end of it slw comes 23 times similarly",
      "start": 1586.679,
      "duration": 5.801
    },
    {
      "text": "we see that o comes 14",
      "start": 1589.96,
      "duration": 6.76
    },
    {
      "text": "times L comes 14 times D comes 10 times",
      "start": 1592.48,
      "duration": 7.36
    },
    {
      "text": "e comes 16 times Etc and we make this",
      "start": 1596.72,
      "duration": 5.76
    },
    {
      "text": "frequency table list so here you can see",
      "start": 1599.84,
      "duration": 5.959
    },
    {
      "text": "that we have 12 tokens and if we did we",
      "start": 1602.48,
      "duration": 5.48
    },
    {
      "text": "use character level tokenization our",
      "start": 1605.799,
      "duration": 3.961
    },
    {
      "text": "tokenization would end here because all",
      "start": 1607.96,
      "duration": 3.599
    },
    {
      "text": "these characters would be individual",
      "start": 1609.76,
      "duration": 5.039
    },
    {
      "text": "tokens now what we will do similar to if",
      "start": 1611.559,
      "duration": 5.0
    },
    {
      "text": "you remember the bite pair encoding",
      "start": 1614.799,
      "duration": 4.641
    },
    {
      "text": "algorithm we looked at the most frequent",
      "start": 1616.559,
      "duration": 5.561
    },
    {
      "text": "pairing right uh so let me take you back",
      "start": 1619.44,
      "duration": 5.92
    },
    {
      "text": "to this yeah we looked at AA because AA",
      "start": 1622.12,
      "duration": 5.32
    },
    {
      "text": "appeared the most so we looked at that",
      "start": 1625.36,
      "duration": 4.559
    },
    {
      "text": "bite pair which occurred the most this",
      "start": 1627.44,
      "duration": 4.52
    },
    {
      "text": "is exactly what we'll be doing here",
      "start": 1629.919,
      "duration": 3.721
    },
    {
      "text": "we'll be look for the mo we'll be",
      "start": 1631.96,
      "duration": 4.079
    },
    {
      "text": "looking for the most frequent pairing in",
      "start": 1633.64,
      "duration": 3.639
    },
    {
      "text": "the data",
      "start": 1636.039,
      "duration": 3.801
    },
    {
      "text": "set and then what we'll do is that this",
      "start": 1637.279,
      "duration": 4.201
    },
    {
      "text": "is the modification compared to the",
      "start": 1639.84,
      "duration": 3.959
    },
    {
      "text": "original algorithm when we look for the",
      "start": 1641.48,
      "duration": 5.16
    },
    {
      "text": "most frequent pairing we will merge them",
      "start": 1643.799,
      "duration": 5.12
    },
    {
      "text": "we will merge the most fre pairing and",
      "start": 1646.64,
      "duration": 4.44
    },
    {
      "text": "then perform the same iteration again",
      "start": 1648.919,
      "duration": 3.681
    },
    {
      "text": "and again and",
      "start": 1651.08,
      "duration": 3.959
    },
    {
      "text": "again so let me show you how this is",
      "start": 1652.6,
      "duration": 5.0
    },
    {
      "text": "done so in the iteration one as I",
      "start": 1655.039,
      "duration": 5.321
    },
    {
      "text": "mentioned we start with the uh finding",
      "start": 1657.6,
      "duration": 5.559
    },
    {
      "text": "the most frequent pairing right so it so",
      "start": 1660.36,
      "duration": 4.96
    },
    {
      "text": "you the way to do it is look at the",
      "start": 1663.159,
      "duration": 3.921
    },
    {
      "text": "first character which appears the most",
      "start": 1665.32,
      "duration": 4.04
    },
    {
      "text": "so e is that character which appears 16",
      "start": 1667.08,
      "duration": 4.719
    },
    {
      "text": "times right so if you want to look at",
      "start": 1669.36,
      "duration": 4.559
    },
    {
      "text": "the pairing which appears most it most",
      "start": 1671.799,
      "duration": 4.681
    },
    {
      "text": "probably starts with e so it turns out",
      "start": 1673.919,
      "duration": 5.281
    },
    {
      "text": "if you look at these words e and s is",
      "start": 1676.48,
      "duration": 4.36
    },
    {
      "text": "the pairing which appears the most",
      "start": 1679.2,
      "duration": 4.959
    },
    {
      "text": "number of times so e and s here appears",
      "start": 1680.84,
      "duration": 5.959
    },
    {
      "text": "nine times in finest and E and S appears",
      "start": 1684.159,
      "duration": 5.4
    },
    {
      "text": "four times in lowest so e and s is that",
      "start": 1686.799,
      "duration": 5.561
    },
    {
      "text": "pairing which appears 13 number of times",
      "start": 1689.559,
      "duration": 6.24
    },
    {
      "text": "right so uh most common bite pair",
      "start": 1692.36,
      "duration": 6.039
    },
    {
      "text": "starting with e is e and s so what we'll",
      "start": 1695.799,
      "duration": 4.401
    },
    {
      "text": "now be doing is that we'll be going",
      "start": 1698.399,
      "duration": 5.561
    },
    {
      "text": "through the data set again uh and we'll",
      "start": 1700.2,
      "duration": 6.04
    },
    {
      "text": "be merging these two tokens e and s so",
      "start": 1703.96,
      "duration": 4.68
    },
    {
      "text": "now e s will be one token and that's why",
      "start": 1706.24,
      "duration": 5.4
    },
    {
      "text": "it's called subword es will be one token",
      "start": 1708.64,
      "duration": 5.399
    },
    {
      "text": "so now let me show you my token table",
      "start": 1711.64,
      "duration": 4.759
    },
    {
      "text": "again everything else is the same up",
      "start": 1714.039,
      "duration": 4.601
    },
    {
      "text": "token number 12 but look at this token",
      "start": 1716.399,
      "duration": 4.441
    },
    {
      "text": "number 13 which has been added in token",
      "start": 1718.64,
      "duration": 4.44
    },
    {
      "text": "number 13 we have added one more token",
      "start": 1720.84,
      "duration": 3.559
    },
    {
      "text": "which is",
      "start": 1723.08,
      "duration": 3.719
    },
    {
      "text": "es because it's the most frequent",
      "start": 1724.399,
      "duration": 6.081
    },
    {
      "text": "pairing and Es appears 13 times but",
      "start": 1726.799,
      "duration": 5.681
    },
    {
      "text": "remember when we add es we have to",
      "start": 1730.48,
      "duration": 4.439
    },
    {
      "text": "subtract something from E and we have to",
      "start": 1732.48,
      "duration": 5.84
    },
    {
      "text": "subtract from s because now uh ES has",
      "start": 1734.919,
      "duration": 5.841
    },
    {
      "text": "been included so we subtract 13 from the",
      "start": 1738.32,
      "duration": 4.68
    },
    {
      "text": "e count so now the the number of time",
      "start": 1740.76,
      "duration": 4.799
    },
    {
      "text": "only e appears is three the number of",
      "start": 1743.0,
      "duration": 5.12
    },
    {
      "text": "time only s appears is zero this is very",
      "start": 1745.559,
      "duration": 4.561
    },
    {
      "text": "interesting to know so the number of",
      "start": 1748.12,
      "duration": 4.439
    },
    {
      "text": "time only s appears is zero so s it",
      "start": 1750.12,
      "duration": 5.08
    },
    {
      "text": "seems always appears with e so es is a",
      "start": 1752.559,
      "duration": 4.801
    },
    {
      "text": "subword see this we would not have",
      "start": 1755.2,
      "duration": 3.68
    },
    {
      "text": "discovered if we just did character",
      "start": 1757.36,
      "duration": 4.48
    },
    {
      "text": "level tokenization or uh Word level it",
      "start": 1758.88,
      "duration": 5.799
    },
    {
      "text": "seems that e and s always so s only",
      "start": 1761.84,
      "duration": 4.959
    },
    {
      "text": "comes with e in this data set we have",
      "start": 1764.679,
      "duration": 4.161
    },
    {
      "text": "already obtained our first",
      "start": 1766.799,
      "duration": 6.041
    },
    {
      "text": "site so now this is my new uh this is my",
      "start": 1768.84,
      "duration": 5.839
    },
    {
      "text": "new token library and this is my",
      "start": 1772.84,
      "duration": 4.0
    },
    {
      "text": "additional token and now we are going to",
      "start": 1774.679,
      "duration": 4.521
    },
    {
      "text": "actually continuously keep on doing this",
      "start": 1776.84,
      "duration": 6.92
    },
    {
      "text": "process to find uh frequency or to find",
      "start": 1779.2,
      "duration": 6.4
    },
    {
      "text": "tokens which appear the most number of",
      "start": 1783.76,
      "duration": 4.039
    },
    {
      "text": "times so in the previous iteration we",
      "start": 1785.6,
      "duration": 5.439
    },
    {
      "text": "saw that e and s was the bite pair right",
      "start": 1787.799,
      "duration": 5.321
    },
    {
      "text": "which occurred most number of times ands",
      "start": 1791.039,
      "duration": 5.0
    },
    {
      "text": "it appeared 13 times but now es is a",
      "start": 1793.12,
      "duration": 5.52
    },
    {
      "text": "separate token for us so now using that",
      "start": 1796.039,
      "duration": 5.76
    },
    {
      "text": "token we see that EST is again a bite",
      "start": 1798.64,
      "duration": 5.8
    },
    {
      "text": "pair because es is one token and T is",
      "start": 1801.799,
      "duration": 5.401
    },
    {
      "text": "another token so es s and t becomes a",
      "start": 1804.44,
      "duration": 4.959
    },
    {
      "text": "bite pair in the second iteration and",
      "start": 1807.2,
      "duration": 6.28
    },
    {
      "text": "EST appears 9 + 3 which is again 13",
      "start": 1809.399,
      "duration": 6.76
    },
    {
      "text": "times so now in the next iteration what",
      "start": 1813.48,
      "duration": 6.079
    },
    {
      "text": "we'll be doing essentially is that U let",
      "start": 1816.159,
      "duration": 5.88
    },
    {
      "text": "me show you iteration number two in the",
      "start": 1819.559,
      "duration": 4.24
    },
    {
      "text": "iteration number two we'll merge the",
      "start": 1822.039,
      "duration": 4.161
    },
    {
      "text": "tokens es s and t because they have",
      "start": 1823.799,
      "duration": 4.921
    },
    {
      "text": "appeared 13 times in the data set see we",
      "start": 1826.2,
      "duration": 4.24
    },
    {
      "text": "are doing the same thing what we did in",
      "start": 1828.72,
      "duration": 4.64
    },
    {
      "text": "the earlier bite pair encoding for that",
      "start": 1830.44,
      "duration": 5.68
    },
    {
      "text": "character so let me just showed you show",
      "start": 1833.36,
      "duration": 5.24
    },
    {
      "text": "you here remember here what we did after",
      "start": 1836.12,
      "duration": 5.12
    },
    {
      "text": "AA so AA was done right and then we",
      "start": 1838.6,
      "duration": 4.919
    },
    {
      "text": "merged this and then we looked at the",
      "start": 1841.24,
      "duration": 3.96
    },
    {
      "text": "second sequence which appeared the most",
      "start": 1843.519,
      "duration": 4.121
    },
    {
      "text": "which is AB that is actually very",
      "start": 1845.2,
      "duration": 4.4
    },
    {
      "text": "similar to what we are doing here es",
      "start": 1847.64,
      "duration": 3.8
    },
    {
      "text": "appears the most so we created one more",
      "start": 1849.6,
      "duration": 4.36
    },
    {
      "text": "token for ES then we looked at another",
      "start": 1851.44,
      "duration": 4.239
    },
    {
      "text": "bite pair which is appearing the most",
      "start": 1853.96,
      "duration": 4.719
    },
    {
      "text": "and that is es s and t so now what we'll",
      "start": 1855.679,
      "duration": 5.441
    },
    {
      "text": "be doing is that we'll merge es andt",
      "start": 1858.679,
      "duration": 4.041
    },
    {
      "text": "into one",
      "start": 1861.12,
      "duration": 5.159
    },
    {
      "text": "token and uh so EST comes to be 13 times",
      "start": 1862.72,
      "duration": 5.799
    },
    {
      "text": "and we'll then subtract 13 from the",
      "start": 1866.279,
      "duration": 5.64
    },
    {
      "text": "previous token es so now only es appears",
      "start": 1868.519,
      "duration": 7.28
    },
    {
      "text": "zero times and EST appears 13 times see",
      "start": 1871.919,
      "duration": 6.72
    },
    {
      "text": "we have constructed a new token EST so",
      "start": 1875.799,
      "duration": 5.521
    },
    {
      "text": "now remember what I said earlier the",
      "start": 1878.639,
      "duration": 5.081
    },
    {
      "text": "previous World level tokenizers and the",
      "start": 1881.32,
      "duration": 4.16
    },
    {
      "text": "Character level tokenizers could not",
      "start": 1883.72,
      "duration": 4.24
    },
    {
      "text": "identify that EST is a common roote",
      "start": 1885.48,
      "duration": 5.48
    },
    {
      "text": "between finest and lowest but with our",
      "start": 1887.96,
      "duration": 4.8
    },
    {
      "text": "bite pair encoding algorithm we have",
      "start": 1890.96,
      "duration": 4.92
    },
    {
      "text": "already created a new token for EST so",
      "start": 1892.76,
      "duration": 5.039
    },
    {
      "text": "this algorithm has already identified",
      "start": 1895.88,
      "duration": 4.96
    },
    {
      "text": "that EST is a common root",
      "start": 1897.799,
      "duration": 6.12
    },
    {
      "text": "World great so up till now we have done",
      "start": 1900.84,
      "duration": 6.12
    },
    {
      "text": "iteration number three and uh we see",
      "start": 1903.919,
      "duration": 5.281
    },
    {
      "text": "that okay I think yeah up till now we",
      "start": 1906.96,
      "duration": 4.319
    },
    {
      "text": "have done I think two iterations and now",
      "start": 1909.2,
      "duration": 5.0
    },
    {
      "text": "up till now we have merged es and T into",
      "start": 1911.279,
      "duration": 4.64
    },
    {
      "text": "one common token",
      "start": 1914.2,
      "duration": 5.24
    },
    {
      "text": "awesome now let us take look at this slw",
      "start": 1915.919,
      "duration": 7.681
    },
    {
      "text": "token now we can see that EST and /w",
      "start": 1919.44,
      "duration": 6.28
    },
    {
      "text": "basically appears 13 times again it's",
      "start": 1923.6,
      "duration": 5.64
    },
    {
      "text": "the same thing over here so EST always",
      "start": 1925.72,
      "duration": 6.16
    },
    {
      "text": "comes with slw now EST is one token so",
      "start": 1929.24,
      "duration": 6.319
    },
    {
      "text": "EST and slw forms a bite pair and this",
      "start": 1931.88,
      "duration": 6.639
    },
    {
      "text": "bite pair again occurs 9 + 4 which is 13",
      "start": 1935.559,
      "duration": 4.801
    },
    {
      "text": "times which is much more than any other",
      "start": 1938.519,
      "duration": 4.241
    },
    {
      "text": "bite pair in these words so we'll",
      "start": 1940.36,
      "duration": 6.08
    },
    {
      "text": "combine EST and /w into one more token",
      "start": 1942.76,
      "duration": 5.36
    },
    {
      "text": "so in the third iteration we are going",
      "start": 1946.44,
      "duration": 4.199
    },
    {
      "text": "to combine EST and /w into one more",
      "start": 1948.12,
      "duration": 5.84
    },
    {
      "text": "token so now this becomes our",
      "start": 1950.639,
      "duration": 6.16
    },
    {
      "text": "word do you understand why we combined",
      "start": 1953.96,
      "duration": 5.839
    },
    {
      "text": "slw with one more token we could have",
      "start": 1956.799,
      "duration": 5.641
    },
    {
      "text": "just left it at EST right but if we left",
      "start": 1959.799,
      "duration": 4.76
    },
    {
      "text": "it at EST then essentially there would",
      "start": 1962.44,
      "duration": 3.719
    },
    {
      "text": "have been no difference between words",
      "start": 1964.559,
      "duration": 5.36
    },
    {
      "text": "like estimate and highest so estimate",
      "start": 1966.159,
      "duration": 6.281
    },
    {
      "text": "and highest both have EST but the words",
      "start": 1969.919,
      "duration": 5.201
    },
    {
      "text": "in our data set are highest and lowest",
      "start": 1972.44,
      "duration": 5.56
    },
    {
      "text": "so EST is the ending sequence in all of",
      "start": 1975.12,
      "duration": 5.12
    },
    {
      "text": "our words in our data set and we need to",
      "start": 1978.0,
      "duration": 3.96
    },
    {
      "text": "encode that information that it is an",
      "start": 1980.24,
      "duration": 5.319
    },
    {
      "text": "ending sequence so estw allows us to",
      "start": 1981.96,
      "duration": 6.24
    },
    {
      "text": "encode this information so now the",
      "start": 1985.559,
      "duration": 5.08
    },
    {
      "text": "tokenizer knows that whenever EST comes",
      "start": 1988.2,
      "duration": 4.599
    },
    {
      "text": "it's always followed by slw which means",
      "start": 1990.639,
      "duration": 5.801
    },
    {
      "text": "the word ends after estd so now our",
      "start": 1992.799,
      "duration": 5.88
    },
    {
      "text": "algorithm or the tokenizer can",
      "start": 1996.44,
      "duration": 4.44
    },
    {
      "text": "differentiate between estimate and",
      "start": 1998.679,
      "duration": 4.96
    },
    {
      "text": "highest because in estimate EST does not",
      "start": 2000.88,
      "duration": 4.519
    },
    {
      "text": "end with a",
      "start": 2003.639,
      "duration": 4.52
    },
    {
      "text": "/w so now if you look at the Tok which",
      "start": 2005.399,
      "duration": 5.081
    },
    {
      "text": "we have earlier we had all these 12",
      "start": 2008.159,
      "duration": 4.64
    },
    {
      "text": "tokens but now we created es then we",
      "start": 2010.48,
      "duration": 4.72
    },
    {
      "text": "merged it to EST and finally we created",
      "start": 2012.799,
      "duration": 3.281
    },
    {
      "text": "this",
      "start": 2015.2,
      "duration": 3.16
    },
    {
      "text": "estw so now these two tokens are",
      "start": 2016.08,
      "duration": 5.439
    },
    {
      "text": "actually not needed es and EST so now",
      "start": 2018.36,
      "duration": 5.039
    },
    {
      "text": "let's look at another other bite pairs",
      "start": 2021.519,
      "duration": 4.321
    },
    {
      "text": "which occur a lot so it turns out that o",
      "start": 2023.399,
      "duration": 4.561
    },
    {
      "text": "and L is another bite pair which occurs",
      "start": 2025.84,
      "duration": 4.16
    },
    {
      "text": "10 times because it's present in old and",
      "start": 2027.96,
      "duration": 4.4
    },
    {
      "text": "older so what we'll do is that we'll",
      "start": 2030.0,
      "duration": 4.159
    },
    {
      "text": "create one more we'll merge these two",
      "start": 2032.36,
      "duration": 4.12
    },
    {
      "text": "and create one more token for o and L it",
      "start": 2034.159,
      "duration": 4.64
    },
    {
      "text": "appears 10 times and we'll subtract that",
      "start": 2036.48,
      "duration": 5.88
    },
    {
      "text": "count 10 from the O and L so o",
      "start": 2038.799,
      "duration": 4.961
    },
    {
      "text": "individually or with some other",
      "start": 2042.36,
      "duration": 3.08
    },
    {
      "text": "character appears 14 times so we",
      "start": 2043.76,
      "duration": 4.0
    },
    {
      "text": "subtract 10 because now we have created",
      "start": 2045.44,
      "duration": 4.439
    },
    {
      "text": "one more token for o which appears 10",
      "start": 2047.76,
      "duration": 5.399
    },
    {
      "text": "times similarly L appears 14 times",
      "start": 2049.879,
      "duration": 5.96
    },
    {
      "text": "overall and we subtract 10 from it",
      "start": 2053.159,
      "duration": 6.641
    },
    {
      "text": "because L comes with o 10 times right",
      "start": 2055.839,
      "duration": 6.24
    },
    {
      "text": "and now what we do is that o l is one",
      "start": 2059.8,
      "duration": 5.64
    },
    {
      "text": "token so now we see that o l and D has",
      "start": 2062.079,
      "duration": 5.681
    },
    {
      "text": "appeared 10 times so this bite pair has",
      "start": 2065.44,
      "duration": 4.239
    },
    {
      "text": "appear 10 times so we merge this bite",
      "start": 2067.76,
      "duration": 4.68
    },
    {
      "text": "pair so then o d now becomes another",
      "start": 2069.679,
      "duration": 5.48
    },
    {
      "text": "token which appears 10 times so you see",
      "start": 2072.44,
      "duration": 4.88
    },
    {
      "text": "the meaning which our bite pair encoder",
      "start": 2075.159,
      "duration": 4.76
    },
    {
      "text": "has captured we have constructed one",
      "start": 2077.32,
      "duration": 4.96
    },
    {
      "text": "token which is old we have another token",
      "start": 2079.919,
      "duration": 4.401
    },
    {
      "text": "which is",
      "start": 2082.28,
      "duration": 4.799
    },
    {
      "text": "estw these tokens are subwords so they",
      "start": 2084.32,
      "duration": 4.599
    },
    {
      "text": "are neither full words nor characters",
      "start": 2087.079,
      "duration": 3.6
    },
    {
      "text": "they are subwords but they encode the",
      "start": 2088.919,
      "duration": 4.92
    },
    {
      "text": "root representation so old is one token",
      "start": 2090.679,
      "duration": 6.041
    },
    {
      "text": "and this actually tells us that uh old",
      "start": 2093.839,
      "duration": 5.321
    },
    {
      "text": "is the root word which comes in",
      "start": 2096.72,
      "duration": 5.48
    },
    {
      "text": "Old it which comes in old as well as",
      "start": 2099.16,
      "duration": 5.76
    },
    {
      "text": "older and our BP algorithm has actually",
      "start": 2102.2,
      "duration": 5.159
    },
    {
      "text": "captured that perfectly that's awesome",
      "start": 2104.92,
      "duration": 4.439
    },
    {
      "text": "right all these root words were not",
      "start": 2107.359,
      "duration": 6.321
    },
    {
      "text": "captured by just the word uh word em or",
      "start": 2109.359,
      "duration": 6.961
    },
    {
      "text": "the word encoder word tokenizer and the",
      "start": 2113.68,
      "duration": 4.12
    },
    {
      "text": "Character level",
      "start": 2116.32,
      "duration": 3.759
    },
    {
      "text": "tokenizer now you might be seeing here",
      "start": 2117.8,
      "duration": 6.2
    },
    {
      "text": "that these f i and N appear nine",
      "start": 2120.079,
      "duration": 6.721
    },
    {
      "text": "times uh it's fine that they appear nine",
      "start": 2124.0,
      "duration": 5.119
    },
    {
      "text": "times but we we just have one word with",
      "start": 2126.8,
      "duration": 3.24
    },
    {
      "text": "these",
      "start": 2129.119,
      "duration": 3.601
    },
    {
      "text": "characters so if you look at our data",
      "start": 2130.04,
      "duration": 4.76
    },
    {
      "text": "set again let's see where f i and N",
      "start": 2132.72,
      "duration": 7.72
    },
    {
      "text": "appear so the words are finest so all of",
      "start": 2134.8,
      "duration": 7.12
    },
    {
      "text": "these words actually only appear in",
      "start": 2140.44,
      "duration": 3.6
    },
    {
      "text": "finest so it's no it does not make sense",
      "start": 2141.92,
      "duration": 6.08
    },
    {
      "text": "to merge them into one um one token",
      "start": 2144.04,
      "duration": 5.799
    },
    {
      "text": "because that token does not the",
      "start": 2148.0,
      "duration": 3.48
    },
    {
      "text": "frequency of that token appearing in",
      "start": 2149.839,
      "duration": 3.801
    },
    {
      "text": "other words is not too much why did we",
      "start": 2151.48,
      "duration": 4.24
    },
    {
      "text": "merge EST into one token because it",
      "start": 2153.64,
      "duration": 4.4
    },
    {
      "text": "appears in multiple words why why did we",
      "start": 2155.72,
      "duration": 4.16
    },
    {
      "text": "merge old in one token because it",
      "start": 2158.04,
      "duration": 4.92
    },
    {
      "text": "appears in multiple words so see the",
      "start": 2159.88,
      "duration": 5.719
    },
    {
      "text": "rule of uh subord tokenization the first",
      "start": 2162.96,
      "duration": 5.04
    },
    {
      "text": "rule is that that word which occurs",
      "start": 2165.599,
      "duration": 5.24
    },
    {
      "text": "multiple times you keep it as it is old",
      "start": 2168.0,
      "duration": 5.119
    },
    {
      "text": "so we kept old as it is right it is a",
      "start": 2170.839,
      "duration": 5.441
    },
    {
      "text": "separate token uh but that word which is",
      "start": 2173.119,
      "duration": 5.881
    },
    {
      "text": "not used too many times like older it",
      "start": 2176.28,
      "duration": 4.92
    },
    {
      "text": "needs to be split into old and then e",
      "start": 2179.0,
      "duration": 4.72
    },
    {
      "text": "and then R are separate tokens similarly",
      "start": 2181.2,
      "duration": 5.8
    },
    {
      "text": "in finest EST is one token fi and N are",
      "start": 2183.72,
      "duration": 4.56
    },
    {
      "text": "separate tokens",
      "start": 2187.0,
      "duration": 4.44
    },
    {
      "text": "in lowest EST is one token and L O and W",
      "start": 2188.28,
      "duration": 4.44
    },
    {
      "text": "are separate",
      "start": 2191.44,
      "duration": 3.84
    },
    {
      "text": "tokens and this helps us to retain root",
      "start": 2192.72,
      "duration": 4.96
    },
    {
      "text": "wordss in sub",
      "start": 2195.28,
      "duration": 4.72
    },
    {
      "text": "tokenization I hope you are I hope you",
      "start": 2197.68,
      "duration": 4.52
    },
    {
      "text": "have understood this concept now what we",
      "start": 2200.0,
      "duration": 5.72
    },
    {
      "text": "can do is that uh this EST and old are",
      "start": 2202.2,
      "duration": 5.159
    },
    {
      "text": "final tokens which we constructed by",
      "start": 2205.72,
      "duration": 3.84
    },
    {
      "text": "merging now let us actually remove all",
      "start": 2207.359,
      "duration": 4.521
    },
    {
      "text": "of those tokens whose frequency is zero",
      "start": 2209.56,
      "duration": 4.92
    },
    {
      "text": "so we can remove s we can remove so let",
      "start": 2211.88,
      "duration": 5.8
    },
    {
      "text": "me Mark them with a different color so s",
      "start": 2214.48,
      "duration": 5.32
    },
    {
      "text": "has a frequency of z t has a frequency",
      "start": 2217.68,
      "duration": 5.439
    },
    {
      "text": "of Z es EST has frequency of z o has a",
      "start": 2219.8,
      "duration": 5.6
    },
    {
      "text": "frequency of Z so let's remove this so",
      "start": 2223.119,
      "duration": 4.161
    },
    {
      "text": "then our final table looks like these",
      "start": 2225.4,
      "duration": 4.84
    },
    {
      "text": "these are the final tokens in our subord",
      "start": 2227.28,
      "duration": 5.12
    },
    {
      "text": "tokenizer which is obtained using the",
      "start": 2230.24,
      "duration": 4.92
    },
    {
      "text": "bite pair encoding algorithm how did we",
      "start": 2232.4,
      "duration": 4.52
    },
    {
      "text": "obtain these tokens we just looked at",
      "start": 2235.16,
      "duration": 4.159
    },
    {
      "text": "bite pairs which occur the most then we",
      "start": 2236.92,
      "duration": 4.24
    },
    {
      "text": "merged them into one and then we",
      "start": 2239.319,
      "duration": 5.601
    },
    {
      "text": "repeated this process uh until we",
      "start": 2241.16,
      "duration": 6.36
    },
    {
      "text": "obtained until we reached a stage where",
      "start": 2244.92,
      "duration": 4.0
    },
    {
      "text": "enough number of tokens have been",
      "start": 2247.52,
      "duration": 3.52
    },
    {
      "text": "created or until we reached a stage",
      "start": 2248.92,
      "duration": 4.399
    },
    {
      "text": "where our iterations have stopped and",
      "start": 2251.04,
      "duration": 4.079
    },
    {
      "text": "this is the final",
      "start": 2253.319,
      "duration": 4.641
    },
    {
      "text": "tokenized uh final uh tokens which we'll",
      "start": 2255.119,
      "duration": 4.761
    },
    {
      "text": "be using for next steps of the large",
      "start": 2257.96,
      "duration": 3.52
    },
    {
      "text": "language model training which are vector",
      "start": 2259.88,
      "duration": 4.12
    },
    {
      "text": "embedding Etc this is how the subword",
      "start": 2261.48,
      "duration": 4.68
    },
    {
      "text": "tokenizer works and this is exactly how",
      "start": 2264.0,
      "duration": 5.2
    },
    {
      "text": "bite pair encoder which is a subword",
      "start": 2266.16,
      "duration": 5.959
    },
    {
      "text": "tokenizer it works for uh training",
      "start": 2269.2,
      "duration": 6.159
    },
    {
      "text": "models like gpt2 or gpt3 very few people",
      "start": 2272.119,
      "duration": 4.881
    },
    {
      "text": "have this understanding but I hope this",
      "start": 2275.359,
      "duration": 4.201
    },
    {
      "text": "lecture has intuitively made it clear",
      "start": 2277.0,
      "duration": 4.599
    },
    {
      "text": "for you how the bite pair tokenizer",
      "start": 2279.56,
      "duration": 5.279
    },
    {
      "text": "actually works so now this list of 11",
      "start": 2281.599,
      "duration": 5.881
    },
    {
      "text": "tokens will serve as our vocabulary why",
      "start": 2284.839,
      "duration": 4.441
    },
    {
      "text": "is it called subword tokenizer because",
      "start": 2287.48,
      "duration": 3.72
    },
    {
      "text": "these are subwords right EST is not a",
      "start": 2289.28,
      "duration": 3.72
    },
    {
      "text": "full word neither it's a character it's",
      "start": 2291.2,
      "duration": 4.24
    },
    {
      "text": "a subword but it's the root of many",
      "start": 2293.0,
      "duration": 5.76
    },
    {
      "text": "words o is also the root of many",
      "start": 2295.44,
      "duration": 5.8
    },
    {
      "text": "words now you must be thinking when do",
      "start": 2298.76,
      "duration": 5.48
    },
    {
      "text": "we stop this uh merging when do we stop",
      "start": 2301.24,
      "duration": 5.119
    },
    {
      "text": "these iterations so you have to specify",
      "start": 2304.24,
      "duration": 4.4
    },
    {
      "text": "a stopping criteria usually if you look",
      "start": 2306.359,
      "duration": 5.24
    },
    {
      "text": "at gpt2 or gpt3 you run this bite par",
      "start": 2308.64,
      "duration": 5.32
    },
    {
      "text": "coding algorithm on a huge number of",
      "start": 2311.599,
      "duration": 5.081
    },
    {
      "text": "tokens right so the stopping criteria",
      "start": 2313.96,
      "duration": 4.6
    },
    {
      "text": "can be if the token count becomes a",
      "start": 2316.68,
      "duration": 3.919
    },
    {
      "text": "certain number then you stop or just the",
      "start": 2318.56,
      "duration": 4.2
    },
    {
      "text": "number of iterations can be the stop uh",
      "start": 2320.599,
      "duration": 4.76
    },
    {
      "text": "stopping count so I'm in this lecture",
      "start": 2322.76,
      "duration": 4.4
    },
    {
      "text": "I'm not covering the stopping criteria",
      "start": 2325.359,
      "duration": 3.801
    },
    {
      "text": "in too much detail but I just want to",
      "start": 2327.16,
      "duration": 3.56
    },
    {
      "text": "give you an intuition of how the",
      "start": 2329.16,
      "duration": 3.76
    },
    {
      "text": "stopping criteria can actually be",
      "start": 2330.72,
      "duration": 4.639
    },
    {
      "text": "formulated or computed it's based on the",
      "start": 2332.92,
      "duration": 5.399
    },
    {
      "text": "token count or the number of iterations",
      "start": 2335.359,
      "duration": 5.881
    },
    {
      "text": "to give you an example uh or to mention",
      "start": 2338.319,
      "duration": 5.321
    },
    {
      "text": "one more advantage of bite pair encoding",
      "start": 2341.24,
      "duration": 4.8
    },
    {
      "text": "here you can see that we it's actually",
      "start": 2343.64,
      "duration": 4.959
    },
    {
      "text": "better than Word level encoding right",
      "start": 2346.04,
      "duration": 4.64
    },
    {
      "text": "because uh let's say if you have two",
      "start": 2348.599,
      "duration": 4.601
    },
    {
      "text": "words boy and boys both won't be",
      "start": 2350.68,
      "duration": 4.96
    },
    {
      "text": "separate tokens in this just boy will be",
      "start": 2353.2,
      "duration": 4.84
    },
    {
      "text": "a token so bite pair encoder also",
      "start": 2355.64,
      "duration": 4.84
    },
    {
      "text": "reduces the number of tokens which we",
      "start": 2358.04,
      "duration": 5.319
    },
    {
      "text": "have compared to word encoding let's see",
      "start": 2360.48,
      "duration": 6.32
    },
    {
      "text": "uh and that usually helps so in gpt2 or",
      "start": 2363.359,
      "duration": 5.881
    },
    {
      "text": "gpt3 when it was trained I think it used",
      "start": 2366.8,
      "duration": 5.72
    },
    {
      "text": "about 50,000 more than around 57,000",
      "start": 2369.24,
      "duration": 5.2
    },
    {
      "text": "tokens I think when when bite pair",
      "start": 2372.52,
      "duration": 3.12
    },
    {
      "text": "encoding was",
      "start": 2374.44,
      "duration": 5.56
    },
    {
      "text": "done uh for gpt2 or gpt3 so bite pair",
      "start": 2375.64,
      "duration": 6.719
    },
    {
      "text": "encoding solves the out of vocabulary",
      "start": 2380.0,
      "duration": 4.119
    },
    {
      "text": "problem why does it solve the out of",
      "start": 2382.359,
      "duration": 3.841
    },
    {
      "text": "vocabulary problem because we also have",
      "start": 2384.119,
      "duration": 4.401
    },
    {
      "text": "subwords now and some of it might even",
      "start": 2386.2,
      "duration": 5.48
    },
    {
      "text": "be characters so character level",
      "start": 2388.52,
      "duration": 4.68
    },
    {
      "text": "tokenization solves the out of",
      "start": 2391.68,
      "duration": 3.639
    },
    {
      "text": "vocabulary problem right and bite pair",
      "start": 2393.2,
      "duration": 5.28
    },
    {
      "text": "encoding or subord encoding retains some",
      "start": 2395.319,
      "duration": 4.76
    },
    {
      "text": "properties from it so you can see some",
      "start": 2398.48,
      "duration": 4.0
    },
    {
      "text": "tokens here are characters which is good",
      "start": 2400.079,
      "duration": 4.28
    },
    {
      "text": "but it also solves the problem of the",
      "start": 2402.48,
      "duration": 4.28
    },
    {
      "text": "word level encoding and one of the major",
      "start": 2404.359,
      "duration": 4.401
    },
    {
      "text": "problem is that root meanings are not",
      "start": 2406.76,
      "duration": 3.88
    },
    {
      "text": "captured but now they are captured over",
      "start": 2408.76,
      "duration": 4.48
    },
    {
      "text": "here and it also solves the problem of",
      "start": 2410.64,
      "duration": 5.24
    },
    {
      "text": "the word level encoding being just a",
      "start": 2413.24,
      "duration": 5.16
    },
    {
      "text": "huge bunch of numbers here that is not",
      "start": 2415.88,
      "duration": 4.08
    },
    {
      "text": "the problem because we break it down",
      "start": 2418.4,
      "duration": 3.679
    },
    {
      "text": "into characters and subwords so the",
      "start": 2419.96,
      "duration": 4.08
    },
    {
      "text": "total length of the vocabulary is also",
      "start": 2422.079,
      "duration": 3.401
    },
    {
      "text": "shorter than if you would have just",
      "start": 2424.04,
      "duration": 3.84
    },
    {
      "text": "considered Word level embedding so",
      "start": 2425.48,
      "duration": 5.119
    },
    {
      "text": "subord embedding solves it it solves the",
      "start": 2427.88,
      "duration": 4.76
    },
    {
      "text": "out of vocabulary problem it also gives",
      "start": 2430.599,
      "duration": 4.441
    },
    {
      "text": "us a vocabulary size which is",
      "start": 2432.64,
      "duration": 4.84
    },
    {
      "text": "manageable and it also retains the root",
      "start": 2435.04,
      "duration": 5.6
    },
    {
      "text": "words meanings such as EST and",
      "start": 2437.48,
      "duration": 5.839
    },
    {
      "text": "old awesome right and that's why because",
      "start": 2440.64,
      "duration": 5.6
    },
    {
      "text": "of all these advantages this is the bite",
      "start": 2443.319,
      "duration": 5.081
    },
    {
      "text": "pair encoding algorithm is used for",
      "start": 2446.24,
      "duration": 5.0
    },
    {
      "text": "tokenization in gpt2 and",
      "start": 2448.4,
      "duration": 5.56
    },
    {
      "text": "gpt3 now let me return back to the code",
      "start": 2451.24,
      "duration": 4.2
    },
    {
      "text": "I could have just taken you through the",
      "start": 2453.96,
      "duration": 2.92
    },
    {
      "text": "code today but then you would not have",
      "start": 2455.44,
      "duration": 3.399
    },
    {
      "text": "UND OD how the bite pair algorithm",
      "start": 2456.88,
      "duration": 3.08
    },
    {
      "text": "actually",
      "start": 2458.839,
      "duration": 4.201
    },
    {
      "text": "works now implementing BP from scratch",
      "start": 2459.96,
      "duration": 5.84
    },
    {
      "text": "is relatively complicated so we will be",
      "start": 2463.04,
      "duration": 4.72
    },
    {
      "text": "using a python open source Library",
      "start": 2465.8,
      "duration": 4.4
    },
    {
      "text": "called tick token so if you I'll share",
      "start": 2467.76,
      "duration": 4.64
    },
    {
      "text": "the link of this library in the chat so",
      "start": 2470.2,
      "duration": 4.48
    },
    {
      "text": "this is a library which is essentially a",
      "start": 2472.4,
      "duration": 4.24
    },
    {
      "text": "bite pair encoder it's a bite pair",
      "start": 2474.68,
      "duration": 4.439
    },
    {
      "text": "encoder which is used for open a models",
      "start": 2476.64,
      "duration": 5.52
    },
    {
      "text": "so open a themselves use tick token for",
      "start": 2479.119,
      "duration": 6.2
    },
    {
      "text": "tokenizing the sentences from the data",
      "start": 2482.16,
      "duration": 5.199
    },
    {
      "text": "you can see that it has about 11,000",
      "start": 2485.319,
      "duration": 4.641
    },
    {
      "text": "stars and about 780 Forks so it's a",
      "start": 2487.359,
      "duration": 5.161
    },
    {
      "text": "pretty popular",
      "start": 2489.96,
      "duration": 6.0
    },
    {
      "text": "repository okay so uh now we are going",
      "start": 2492.52,
      "duration": 5.72
    },
    {
      "text": "to implement the bite pair encoder in",
      "start": 2495.96,
      "duration": 4.76
    },
    {
      "text": "Python and I'm going to show you this",
      "start": 2498.24,
      "duration": 4.4
    },
    {
      "text": "implementation in a Hands-On",
      "start": 2500.72,
      "duration": 4.24
    },
    {
      "text": "demonstration the first thing to do is",
      "start": 2502.64,
      "duration": 5.24
    },
    {
      "text": "install tick token because this is we'll",
      "start": 2504.96,
      "duration": 5.44
    },
    {
      "text": "be using the bite pair encoder from uh",
      "start": 2507.88,
      "duration": 4.36
    },
    {
      "text": "tick token and here you can see that it",
      "start": 2510.4,
      "duration": 4.199
    },
    {
      "text": "takes a it takes some time to install",
      "start": 2512.24,
      "duration": 5.0
    },
    {
      "text": "this uh for me when I first install",
      "start": 2514.599,
      "duration": 4.52
    },
    {
      "text": "insted it it took about 1 minute but now",
      "start": 2517.24,
      "duration": 4.119
    },
    {
      "text": "it's a bit faster so requirements are",
      "start": 2519.119,
      "duration": 5.521
    },
    {
      "text": "already satisfied um and you'll see that",
      "start": 2521.359,
      "duration": 4.76
    },
    {
      "text": "it will get installed now in some time",
      "start": 2524.64,
      "duration": 3.16
    },
    {
      "text": "see it's already installed so now I have",
      "start": 2526.119,
      "duration": 3.96
    },
    {
      "text": "installed tick token and let me just",
      "start": 2527.8,
      "duration": 3.96
    },
    {
      "text": "print out the version of tick token",
      "start": 2530.079,
      "duration": 4.721
    },
    {
      "text": "which I'm using here so you can see that",
      "start": 2531.76,
      "duration": 5.64
    },
    {
      "text": "the origion of tick token is 6 which is",
      "start": 2534.8,
      "duration": 3.519
    },
    {
      "text": "good",
      "start": 2537.4,
      "duration": 4.12
    },
    {
      "text": "enough now what we have to do is that we",
      "start": 2538.319,
      "duration": 5.161
    },
    {
      "text": "once the tick token is installed we can",
      "start": 2541.52,
      "duration": 3.799
    },
    {
      "text": "instantiate the bite pair encoding",
      "start": 2543.48,
      "duration": 4.48
    },
    {
      "text": "tokenizer from tick token so the way to",
      "start": 2545.319,
      "duration": 6.841
    },
    {
      "text": "do this is just uh use tick token do get",
      "start": 2547.96,
      "duration": 7.0
    },
    {
      "text": "encoding gpt2 and we store it in this",
      "start": 2552.16,
      "duration": 6.04
    },
    {
      "text": "object called tokenizer so now tokenizer",
      "start": 2554.96,
      "duration": 6.2
    },
    {
      "text": "essentially has is that is similar to",
      "start": 2558.2,
      "duration": 5.2
    },
    {
      "text": "the simple tokenizer version two class",
      "start": 2561.16,
      "duration": 4.159
    },
    {
      "text": "or version one class which we defined in",
      "start": 2563.4,
      "duration": 4.28
    },
    {
      "text": "python in the last lecture remember we",
      "start": 2565.319,
      "duration": 6.081
    },
    {
      "text": "defined this uh simple tokenizer version",
      "start": 2567.68,
      "duration": 7.159
    },
    {
      "text": "one class and uh we defined the simple",
      "start": 2571.4,
      "duration": 5.36
    },
    {
      "text": "tokenizer version two class here which",
      "start": 2574.839,
      "duration": 4.841
    },
    {
      "text": "also include special tokens so now in",
      "start": 2576.76,
      "duration": 4.319
    },
    {
      "text": "just one line of code we have",
      "start": 2579.68,
      "duration": 3.36
    },
    {
      "text": "essentially defined this tokenizer which",
      "start": 2581.079,
      "duration": 4.321
    },
    {
      "text": "is something similar but now it will",
      "start": 2583.04,
      "duration": 5.039
    },
    {
      "text": "it's initiated an instance is created",
      "start": 2585.4,
      "duration": 4.8
    },
    {
      "text": "through this tick token Library so this",
      "start": 2588.079,
      "duration": 3.441
    },
    {
      "text": "is a bite pair",
      "start": 2590.2,
      "duration": 4.48
    },
    {
      "text": "tokenizer and uh similar to the",
      "start": 2591.52,
      "duration": 5.079
    },
    {
      "text": "tokenizer class which we had created if",
      "start": 2594.68,
      "duration": 4.36
    },
    {
      "text": "you see every tokenizer class has an",
      "start": 2596.599,
      "duration": 5.041
    },
    {
      "text": "encode method and a decode method what",
      "start": 2599.04,
      "duration": 4.64
    },
    {
      "text": "the encode method does is basically it",
      "start": 2601.64,
      "duration": 4.76
    },
    {
      "text": "uses that tokenizer and it converts",
      "start": 2603.68,
      "duration": 5.6
    },
    {
      "text": "words into token IDs what the decode",
      "start": 2606.4,
      "duration": 4.64
    },
    {
      "text": "method does is that it decodes those",
      "start": 2609.28,
      "duration": 4.88
    },
    {
      "text": "token IDs back into individual words so",
      "start": 2611.04,
      "duration": 6.519
    },
    {
      "text": "let's see how this uh a tokenizer",
      "start": 2614.16,
      "duration": 5.48
    },
    {
      "text": "actually works so this is the bite pair",
      "start": 2617.559,
      "duration": 5.121
    },
    {
      "text": "encoding tokenizer from gpt2 and let's",
      "start": 2619.64,
      "duration": 5.16
    },
    {
      "text": "give it some text and try to encode and",
      "start": 2622.68,
      "duration": 5.52
    },
    {
      "text": "decode right and I'm testing uh this",
      "start": 2624.8,
      "duration": 5.2
    },
    {
      "text": "tokenizer because I've given it a",
      "start": 2628.2,
      "duration": 4.359
    },
    {
      "text": "complex sentence see so the first part",
      "start": 2630.0,
      "duration": 5.04
    },
    {
      "text": "of the sentence is hello do you like T",
      "start": 2632.559,
      "duration": 4.601
    },
    {
      "text": "then we have given one more thing which",
      "start": 2635.04,
      "duration": 4.039
    },
    {
      "text": "is called end of text which means that a",
      "start": 2637.16,
      "duration": 3.76
    },
    {
      "text": "new text is starting here this is the",
      "start": 2639.079,
      "duration": 3.881
    },
    {
      "text": "end of text is usually done to separate",
      "start": 2640.92,
      "duration": 4.52
    },
    {
      "text": "entire documents but here I'm just using",
      "start": 2642.96,
      "duration": 5.24
    },
    {
      "text": "it to illustrate and in fact gpt2",
      "start": 2645.44,
      "duration": 5.639
    },
    {
      "text": "actively uses end of text so just to",
      "start": 2648.2,
      "duration": 5.56
    },
    {
      "text": "give a demonstration to you when gp2",
      "start": 2651.079,
      "duration": 5.561
    },
    {
      "text": "when gpt2 or gpt3 is trained on large",
      "start": 2653.76,
      "duration": 5.799
    },
    {
      "text": "amounts of data sets here is how the",
      "start": 2656.64,
      "duration": 8.0
    },
    {
      "text": "data sets are loaded so see uh what gpt2",
      "start": 2659.559,
      "duration": 7.161
    },
    {
      "text": "does is that when it has data from",
      "start": 2664.64,
      "duration": 4.52
    },
    {
      "text": "different text sources it usually shows",
      "start": 2666.72,
      "duration": 4.24
    },
    {
      "text": "when a particular text has ended and",
      "start": 2669.16,
      "duration": 3.84
    },
    {
      "text": "when the other text has started so end",
      "start": 2670.96,
      "duration": 4.44
    },
    {
      "text": "of text is actually a part of the gpt2",
      "start": 2673.0,
      "duration": 5.4
    },
    {
      "text": "vocabulary itself so I have given this",
      "start": 2675.4,
      "duration": 5.12
    },
    {
      "text": "end of text here and then the second",
      "start": 2678.4,
      "duration": 4.32
    },
    {
      "text": "sentence is in The sunl Terraces of some",
      "start": 2680.52,
      "duration": 5.36
    },
    {
      "text": "unknown place now look at this if we are",
      "start": 2682.72,
      "duration": 5.96
    },
    {
      "text": "using a word level tokenizer this this",
      "start": 2685.88,
      "duration": 4.4
    },
    {
      "text": "will lead to the out of vocabulary",
      "start": 2688.68,
      "duration": 3.84
    },
    {
      "text": "problem because this word cannot be in",
      "start": 2690.28,
      "duration": 5.0
    },
    {
      "text": "the vocabulary of a word level tokenizer",
      "start": 2692.52,
      "duration": 4.52
    },
    {
      "text": "and that's the advantage of bite pair",
      "start": 2695.28,
      "duration": 4.279
    },
    {
      "text": "encoding in bite pair encoding we have",
      "start": 2697.04,
      "duration": 4.279
    },
    {
      "text": "characters as tokens and we even have",
      "start": 2699.559,
      "duration": 4.321
    },
    {
      "text": "subwords as tokens so definitely this",
      "start": 2701.319,
      "duration": 5.721
    },
    {
      "text": "will be encoded because might be some",
      "start": 2703.88,
      "duration": 5.36
    },
    {
      "text": "character can encode it so some unknown",
      "start": 2707.04,
      "duration": 4.519
    },
    {
      "text": "and place might be subwords which are",
      "start": 2709.24,
      "duration": 4.04
    },
    {
      "text": "individual tokens they can also be used",
      "start": 2711.559,
      "duration": 4.8
    },
    {
      "text": "to encode this so bite pair encoder will",
      "start": 2713.28,
      "duration": 5.52
    },
    {
      "text": "take care of this this entire word which",
      "start": 2716.359,
      "duration": 5.281
    },
    {
      "text": "usually does not exist as a single word",
      "start": 2718.8,
      "duration": 5.6
    },
    {
      "text": "so let us test this actually and uh I've",
      "start": 2721.64,
      "duration": 4.52
    },
    {
      "text": "run this right now and let us see the",
      "start": 2724.4,
      "duration": 4.12
    },
    {
      "text": "different tokens so see hello do you",
      "start": 2726.16,
      "duration": 4.08
    },
    {
      "text": "like T it all of these have been",
      "start": 2728.52,
      "duration": 4.16
    },
    {
      "text": "converted into token IDs I want you to",
      "start": 2730.24,
      "duration": 5.599
    },
    {
      "text": "take a look at this 50256 token so this",
      "start": 2732.68,
      "duration": 5.879
    },
    {
      "text": "is the token ID of end of text and this",
      "start": 2735.839,
      "duration": 5.681
    },
    {
      "text": "is also the vocabulary size of the",
      "start": 2738.559,
      "duration": 6.28
    },
    {
      "text": "tokenization scheme used in gpt2 or gpt3",
      "start": 2741.52,
      "duration": 5.079
    },
    {
      "text": "so if we were to use a World level",
      "start": 2744.839,
      "duration": 4.041
    },
    {
      "text": "tokenizer English language has about",
      "start": 2746.599,
      "duration": 5.361
    },
    {
      "text": "170,000 to 200,000 words so the",
      "start": 2748.88,
      "duration": 4.6
    },
    {
      "text": "vocabulary size would have been this",
      "start": 2751.96,
      "duration": 5.639
    },
    {
      "text": "much but now that we have used the",
      "start": 2753.48,
      "duration": 6.04
    },
    {
      "text": "bite pair tokenizer or bite pair",
      "start": 2757.599,
      "duration": 4.401
    },
    {
      "text": "encoding the vocabulary size has reduced",
      "start": 2759.52,
      "duration": 4.839
    },
    {
      "text": "by around 13 from",
      "start": 2762.0,
      "duration": 4.88
    },
    {
      "text": "150,000 and with with a vocabulary of",
      "start": 2764.359,
      "duration": 4.041
    },
    {
      "text": "around 50,000",
      "start": 2766.88,
      "duration": 4.0
    },
    {
      "text": "subwords uh we are able to get the",
      "start": 2768.4,
      "duration": 5.6
    },
    {
      "text": "amazing performance from gpt2 gpt3 or",
      "start": 2770.88,
      "duration": 6.04
    },
    {
      "text": "GPT 4 I think has much higher level of",
      "start": 2774.0,
      "duration": 6.28
    },
    {
      "text": "uh tokens but even gpt2 has good enough",
      "start": 2776.92,
      "duration": 7.04
    },
    {
      "text": "performance and it has 50,2 56 tokens",
      "start": 2780.28,
      "duration": 6.44
    },
    {
      "text": "awesome and here you can see that I did",
      "start": 2783.96,
      "duration": 4.359
    },
    {
      "text": "not not get an error because some",
      "start": 2786.72,
      "duration": 4.04
    },
    {
      "text": "unknown place was not encoded the",
      "start": 2788.319,
      "duration": 4.881
    },
    {
      "text": "tokenizer is able to encode random words",
      "start": 2790.76,
      "duration": 4.599
    },
    {
      "text": "like these which also look wrong and",
      "start": 2793.2,
      "duration": 3.84
    },
    {
      "text": "here you can see these are the encodings",
      "start": 2795.359,
      "duration": 3.48
    },
    {
      "text": "for this sub some unknown place so the",
      "start": 2797.04,
      "duration": 4.84
    },
    {
      "text": "end of text is this in the sunlight",
      "start": 2798.839,
      "duration": 5.041
    },
    {
      "text": "Terraces of some unknown place so I",
      "start": 2801.88,
      "duration": 3.64
    },
    {
      "text": "think some unknown place is broken down",
      "start": 2803.88,
      "duration": 4.0
    },
    {
      "text": "into subwords three or four subwords and",
      "start": 2805.52,
      "duration": 4.0
    },
    {
      "text": "then it's tokenized by",
      "start": 2807.88,
      "duration": 4.92
    },
    {
      "text": "gpt2 so this is how subord tokenization",
      "start": 2809.52,
      "duration": 5.36
    },
    {
      "text": "can handle out of",
      "start": 2812.8,
      "duration": 4.759
    },
    {
      "text": "vocabulary awesome so the code prints",
      "start": 2814.88,
      "duration": 5.12
    },
    {
      "text": "these IDs which we just saw and now what",
      "start": 2817.559,
      "duration": 4.481
    },
    {
      "text": "we can do is we can convert the token",
      "start": 2820.0,
      "duration": 4.319
    },
    {
      "text": "IDs back into text using the decode",
      "start": 2822.04,
      "duration": 4.4
    },
    {
      "text": "method remember every tokenizer is",
      "start": 2824.319,
      "duration": 4.481
    },
    {
      "text": "encode method as well as decode method",
      "start": 2826.44,
      "duration": 4.679
    },
    {
      "text": "so we can do tokenizer do decode and put",
      "start": 2828.8,
      "duration": 5.24
    },
    {
      "text": "all of these integers and here you see",
      "start": 2831.119,
      "duration": 5.161
    },
    {
      "text": "it decodes exactly the same sentence",
      "start": 2834.04,
      "duration": 4.48
    },
    {
      "text": "what we gave to it so the decoded",
      "start": 2836.28,
      "duration": 4.36
    },
    {
      "text": "sentence is hello do you like T then end",
      "start": 2838.52,
      "duration": 4.839
    },
    {
      "text": "of text in The sunl Terraces of some",
      "start": 2840.64,
      "duration": 6.28
    },
    {
      "text": "unknown place exactly similar to the uh",
      "start": 2843.359,
      "duration": 5.96
    },
    {
      "text": "text which it had G which we had given",
      "start": 2846.92,
      "duration": 4.399
    },
    {
      "text": "which means that the encoder and decoder",
      "start": 2849.319,
      "duration": 4.52
    },
    {
      "text": "is working very well uh and the bite",
      "start": 2851.319,
      "duration": 4.321
    },
    {
      "text": "pair encoding has its advantages of",
      "start": 2853.839,
      "duration": 4.161
    },
    {
      "text": "dealing with out of vocabulary so here",
      "start": 2855.64,
      "duration": 4.08
    },
    {
      "text": "we see two advantages of bite pair",
      "start": 2858.0,
      "duration": 3.48
    },
    {
      "text": "encoding first is that it reduces the",
      "start": 2859.72,
      "duration": 3.96
    },
    {
      "text": "vocabulary length second is that it",
      "start": 2861.48,
      "duration": 4.879
    },
    {
      "text": "knows how to deal with unknown text",
      "start": 2863.68,
      "duration": 5.439
    },
    {
      "text": "awesome so we can make two noteworthy",
      "start": 2866.359,
      "duration": 4.801
    },
    {
      "text": "observations based on the token IDs and",
      "start": 2869.119,
      "duration": 4.44
    },
    {
      "text": "the decoded text first as I already",
      "start": 2871.16,
      "duration": 4.08
    },
    {
      "text": "mentioned the end of text token is",
      "start": 2873.559,
      "duration": 3.28
    },
    {
      "text": "assigned to a relatively large token",
      "start": 2875.24,
      "duration": 3.839
    },
    {
      "text": "token ID which is",
      "start": 2876.839,
      "duration": 6.161
    },
    {
      "text": "50256 uh that is the last token in the",
      "start": 2879.079,
      "duration": 7.04
    },
    {
      "text": "BP tokenizer used for gpt2 so in fact",
      "start": 2883.0,
      "duration": 5.24
    },
    {
      "text": "the BP tokenizer which was used to train",
      "start": 2886.119,
      "duration": 5.281
    },
    {
      "text": "models like gpt2 gpt3 and the original",
      "start": 2888.24,
      "duration": 5.24
    },
    {
      "text": "model used in chat GPT has a total",
      "start": 2891.4,
      "duration": 4.88
    },
    {
      "text": "vocabulary size of 50257 keep this in",
      "start": 2893.48,
      "duration": 5.56
    },
    {
      "text": "mind now you know what this means and",
      "start": 2896.28,
      "duration": 5.64
    },
    {
      "text": "end of text is the last largest token ID",
      "start": 2899.04,
      "duration": 5.96
    },
    {
      "text": "and it's also the last token in the",
      "start": 2901.92,
      "duration": 5.639
    },
    {
      "text": "vocabulary the second uh thing to note",
      "start": 2905.0,
      "duration": 4.88
    },
    {
      "text": "which I already mentioned is that the BP",
      "start": 2907.559,
      "duration": 4.56
    },
    {
      "text": "tokenizer encodes and decodes unknown",
      "start": 2909.88,
      "duration": 4.6
    },
    {
      "text": "words such as some unknown Place",
      "start": 2912.119,
      "duration": 5.121
    },
    {
      "text": "correctly the BPA tokenizer can handle",
      "start": 2914.48,
      "duration": 4.96
    },
    {
      "text": "any unknown word and how does it achieve",
      "start": 2917.24,
      "duration": 4.44
    },
    {
      "text": "this without mention so we did not give",
      "start": 2919.44,
      "duration": 4.879
    },
    {
      "text": "special tokens for unknown words right",
      "start": 2921.68,
      "duration": 4.48
    },
    {
      "text": "uh how did it achieve this and this this",
      "start": 2924.319,
      "duration": 4.24
    },
    {
      "text": "is because of how the tokenization was",
      "start": 2926.16,
      "duration": 6.04
    },
    {
      "text": "done and because of how the uh BP",
      "start": 2928.559,
      "duration": 5.601
    },
    {
      "text": "tokenizer actually goes down to the",
      "start": 2932.2,
      "duration": 5.0
    },
    {
      "text": "level of uh let me show this the BP",
      "start": 2934.16,
      "duration": 5.8
    },
    {
      "text": "tokenizer goes down to the level of",
      "start": 2937.2,
      "duration": 5.119
    },
    {
      "text": "characters and subwords that's why it is",
      "start": 2939.96,
      "duration": 4.8
    },
    {
      "text": "able to deal with unknown text so the",
      "start": 2942.319,
      "duration": 4.52
    },
    {
      "text": "algorithm underlying BP breaks down",
      "start": 2944.76,
      "duration": 4.319
    },
    {
      "text": "words that aren't in its predefined",
      "start": 2946.839,
      "duration": 4.441
    },
    {
      "text": "vocabulary to smaller subword units or",
      "start": 2949.079,
      "duration": 4.441
    },
    {
      "text": "even individual characters this is",
      "start": 2951.28,
      "duration": 5.079
    },
    {
      "text": "exactly what we saw over",
      "start": 2953.52,
      "duration": 6.36
    },
    {
      "text": "here uh and this enables it to handle",
      "start": 2956.359,
      "duration": 6.121
    },
    {
      "text": "out of vocabulary words so thanks to the",
      "start": 2959.88,
      "duration": 5.28
    },
    {
      "text": "BP algorithm if the tokenizer encounters",
      "start": 2962.48,
      "duration": 5.319
    },
    {
      "text": "an unfamiliar word during tokenization",
      "start": 2965.16,
      "duration": 4.52
    },
    {
      "text": "it can represent it as a sequence of",
      "start": 2967.799,
      "duration": 3.881
    },
    {
      "text": "subword tokens or",
      "start": 2969.68,
      "duration": 4.36
    },
    {
      "text": "characters that is the advantage of BP",
      "start": 2971.68,
      "duration": 4.08
    },
    {
      "text": "tokenizer and that's why we are having",
      "start": 2974.04,
      "duration": 5.319
    },
    {
      "text": "this lecture in the first place great",
      "start": 2975.76,
      "duration": 5.599
    },
    {
      "text": "now let us actually take one last",
      "start": 2979.359,
      "duration": 4.081
    },
    {
      "text": "example to illustrate how the BP",
      "start": 2981.359,
      "duration": 4.081
    },
    {
      "text": "tokenizer deals with unknown tokens",
      "start": 2983.44,
      "duration": 4.159
    },
    {
      "text": "right so let's say we have this this",
      "start": 2985.44,
      "duration": 5.879
    },
    {
      "text": "sentence a k w i r w i e r completely",
      "start": 2987.599,
      "duration": 6.081
    },
    {
      "text": "random words which do not make any make",
      "start": 2991.319,
      "duration": 4.641
    },
    {
      "text": "any sense but if you pass these random",
      "start": 2993.68,
      "duration": 3.56
    },
    {
      "text": "words to",
      "start": 2995.96,
      "duration": 4.56
    },
    {
      "text": "uh the tick token gpt2 encoder bite pair",
      "start": 2997.24,
      "duration": 5.359
    },
    {
      "text": "encoder you'll see that it does not show",
      "start": 3000.52,
      "duration": 4.76
    },
    {
      "text": "an error in fact it even encodes these",
      "start": 3002.599,
      "duration": 6.52
    },
    {
      "text": "so a k w i r w i e r is encoded as these",
      "start": 3005.28,
      "duration": 5.64
    },
    {
      "text": "tokens because it broken down into",
      "start": 3009.119,
      "duration": 4.401
    },
    {
      "text": "subwords maybe e r is a commonly",
      "start": 3010.92,
      "duration": 4.679
    },
    {
      "text": "occurring subword in all the vocabulary",
      "start": 3013.52,
      "duration": 4.0
    },
    {
      "text": "which we have so then the token for this",
      "start": 3015.599,
      "duration": 2.76
    },
    {
      "text": "is",
      "start": 3017.52,
      "duration": 3.92
    },
    {
      "text": "25959 maybe AK is a commonly occurring",
      "start": 3018.359,
      "duration": 6.041
    },
    {
      "text": "subo maybe W is just a simple character",
      "start": 3021.44,
      "duration": 5.399
    },
    {
      "text": "maybe IR is a commonly occurring subo",
      "start": 3024.4,
      "duration": 5.24
    },
    {
      "text": "maybe I is just a character so you can",
      "start": 3026.839,
      "duration": 6.72
    },
    {
      "text": "see this what underneath the hood what",
      "start": 3029.64,
      "duration": 6.159
    },
    {
      "text": "this tokenizer must have done is that it",
      "start": 3033.559,
      "duration": 3.961
    },
    {
      "text": "must have broken this down into words",
      "start": 3035.799,
      "duration": 3.881
    },
    {
      "text": "and subwords based on what is present in",
      "start": 3037.52,
      "duration": 4.559
    },
    {
      "text": "the vocabulary and then to each of these",
      "start": 3039.68,
      "duration": 5.96
    },
    {
      "text": "word to each of the subword or character",
      "start": 3042.079,
      "duration": 5.681
    },
    {
      "text": "it would have assigned a token ID so the",
      "start": 3045.64,
      "duration": 3.88
    },
    {
      "text": "tokenizer would have scanned this from",
      "start": 3047.76,
      "duration": 3.359
    },
    {
      "text": "left to right and broken it down into",
      "start": 3049.52,
      "duration": 3.92
    },
    {
      "text": "characters or subwords and then assigned",
      "start": 3051.119,
      "duration": 4.281
    },
    {
      "text": "each character or subword a unique token",
      "start": 3053.44,
      "duration": 4.639
    },
    {
      "text": "ID that's how the encoding works and if",
      "start": 3055.4,
      "duration": 5.0
    },
    {
      "text": "you decode the integers you get back",
      "start": 3058.079,
      "duration": 3.961
    },
    {
      "text": "exactly the same sentence which you",
      "start": 3060.4,
      "duration": 4.76
    },
    {
      "text": "started with and the reason I covered",
      "start": 3062.04,
      "duration": 5.0
    },
    {
      "text": "this lecture in so much detail is",
      "start": 3065.16,
      "duration": 3.72
    },
    {
      "text": "because in the previous lecture we saw",
      "start": 3067.04,
      "duration": 4.039
    },
    {
      "text": "how to do tokenization from scratch and",
      "start": 3068.88,
      "duration": 4.28
    },
    {
      "text": "today we saw how what is bite pair",
      "start": 3071.079,
      "duration": 4.561
    },
    {
      "text": "encoding and how you can install tick",
      "start": 3073.16,
      "duration": 5.28
    },
    {
      "text": "token and Implement your own bite pair",
      "start": 3075.64,
      "duration": 6.56
    },
    {
      "text": "encoder or tokenizer with gpt2 is using",
      "start": 3078.44,
      "duration": 6.24
    },
    {
      "text": "and we saw how it handles unknown words",
      "start": 3082.2,
      "duration": 4.72
    },
    {
      "text": "uh how it captures the root meaning and",
      "start": 3084.68,
      "duration": 5.2
    },
    {
      "text": "how the vocabulary size is about",
      "start": 3086.92,
      "duration": 5.36
    },
    {
      "text": "50,000 uh this brings us to the end of",
      "start": 3089.88,
      "duration": 4.439
    },
    {
      "text": "today's lecture in the next lecture",
      "start": 3092.28,
      "duration": 4.4
    },
    {
      "text": "we'll be looking at data sampling batch",
      "start": 3094.319,
      "duration": 5.961
    },
    {
      "text": "sizes context length Etc before we feed",
      "start": 3096.68,
      "duration": 6.04
    },
    {
      "text": "the embed or before we feed the",
      "start": 3100.28,
      "duration": 5.12
    },
    {
      "text": "tokenizers into the word embedding so in",
      "start": 3102.72,
      "duration": 5.28
    },
    {
      "text": "our progression of learning right now",
      "start": 3105.4,
      "duration": 6.36
    },
    {
      "text": "let me show you um one plot so that I",
      "start": 3108.0,
      "duration": 5.599
    },
    {
      "text": "can illustrate up till where we have",
      "start": 3111.76,
      "duration": 6.76
    },
    {
      "text": "covered until now so if you look at uh",
      "start": 3113.599,
      "duration": 4.921
    },
    {
      "text": "if you look at yeah if you look at this",
      "start": 3118.96,
      "duration": 4.44
    },
    {
      "text": "schematic right",
      "start": 3120.799,
      "duration": 5.081
    },
    {
      "text": "here yeah if you look at this schematic",
      "start": 3123.4,
      "duration": 5.199
    },
    {
      "text": "right here until now we have looked at",
      "start": 3125.88,
      "duration": 5.08
    },
    {
      "text": "how to tokenize an input text and how to",
      "start": 3128.599,
      "duration": 5.041
    },
    {
      "text": "convert it into token IDs and we saw",
      "start": 3130.96,
      "duration": 4.44
    },
    {
      "text": "implementing our own tokenizer in the",
      "start": 3133.64,
      "duration": 3.6
    },
    {
      "text": "previous lecture and using a bite pair",
      "start": 3135.4,
      "duration": 4.199
    },
    {
      "text": "encoder in this lecture we still have to",
      "start": 3137.24,
      "duration": 4.24
    },
    {
      "text": "see token embeddings so we'll get to",
      "start": 3139.599,
      "duration": 4.161
    },
    {
      "text": "this point and before that we'll also",
      "start": 3141.48,
      "duration": 4.72
    },
    {
      "text": "see a bit about data sampling context",
      "start": 3143.76,
      "duration": 3.96
    },
    {
      "text": "length and batch",
      "start": 3146.2,
      "duration": 3.879
    },
    {
      "text": "sizes this brings us to the end of this",
      "start": 3147.72,
      "duration": 4.2
    },
    {
      "text": "lecture thank you so much everyone I",
      "start": 3150.079,
      "duration": 4.681
    },
    {
      "text": "really wanted to have a good mix of uh",
      "start": 3151.92,
      "duration": 4.84
    },
    {
      "text": "whiteboard writing and whiteboard",
      "start": 3154.76,
      "duration": 3.88
    },
    {
      "text": "understanding which we did a lot in",
      "start": 3156.76,
      "duration": 4.52
    },
    {
      "text": "today's lecture so uh if you see our",
      "start": 3158.64,
      "duration": 4.88
    },
    {
      "text": "whiteboard notes we did a number of",
      "start": 3161.28,
      "duration": 5.039
    },
    {
      "text": "itations of the BP encoder I explained",
      "start": 3163.52,
      "duration": 4.4
    },
    {
      "text": "it to you intuitively I hope you",
      "start": 3166.319,
      "duration": 3.681
    },
    {
      "text": "understood this please mention in the",
      "start": 3167.92,
      "duration": 4.04
    },
    {
      "text": "chat if something is unclear and I'll be",
      "start": 3170.0,
      "duration": 3.599
    },
    {
      "text": "happy to explain it in the comment",
      "start": 3171.96,
      "duration": 5.159
    },
    {
      "text": "section and then we also saw how to code",
      "start": 3173.599,
      "duration": 7.881
    },
    {
      "text": "the bite pair encoder uh using uh the",
      "start": 3177.119,
      "duration": 7.48
    },
    {
      "text": "tick token library in Python so now what",
      "start": 3181.48,
      "duration": 4.76
    },
    {
      "text": "I encourage you all to do is take some",
      "start": 3184.599,
      "duration": 5.72
    },
    {
      "text": "unknown sentences and try using the uh",
      "start": 3186.24,
      "duration": 5.96
    },
    {
      "text": "this tick token library and the bite",
      "start": 3190.319,
      "duration": 3.921
    },
    {
      "text": "pair encoder and try to just see what",
      "start": 3192.2,
      "duration": 4.599
    },
    {
      "text": "are the results uh if you if you",
      "start": 3194.24,
      "duration": 4.839
    },
    {
      "text": "encounter any error for any sentence if",
      "start": 3196.799,
      "duration": 4.121
    },
    {
      "text": "the tokenizer is not able to encode it",
      "start": 3199.079,
      "duration": 3.921
    },
    {
      "text": "will be awesome and I'll highlight that",
      "start": 3200.92,
      "duration": 4.56
    },
    {
      "text": "comment in the next video thank you so",
      "start": 3203.0,
      "duration": 4.04
    },
    {
      "text": "much everyone and I look forward to",
      "start": 3205.48,
      "duration": 5.52
    },
    {
      "text": "seeing you in the next lecture",
      "start": 3207.04,
      "duration": 3.96
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the large language models from scratch Series today we are going to learn about a very important topic which is called as bite pair encoding in many lectures or even video content which you see on large language models when tokenization is covered the concept of bite pair encoding is rarely explained however behind modern algorithms such as gpt2 gpt3 Etc the tokenizer which these modern llms use is usually bite pair encoding so you really need to understand what this means and how encoding how this encoding is done for modern L large language models so let's get started with today's lecture first let me take you to the Google collab notebook um until now if you have followed the previous lecture what we did in the previous lecture is that we implemented a simple tokenization scheme if you remember what we covered in the previous lecture let me just uh take you quickly through that in the previous lecture what we did was we basically took sentences we converted them into tokens and then we converted this into vocabul so then every word of the token was arranged in an ascending order and then a ID or a numerical ID or a token ID was assigned to each token so here every word was a unique token along with special characters like comma full stop exclamation mark Etc um now in today's lecture we are going to cover a much more sophisticated tokenizing tokenization scheme why sophistic ated I'll come to that in a moment but just remember that today we are going to learn about the bite pair encoding or BP tokenizer this BP tokenizer which we are going to cover today was used to train large language models such as gpt2 gpt3 and the original model used in Chad GPT if you have not seen the previous lecture on tokenization I highly encourage you to watch that so that this lecture will become very clear to you because in the previous lecture we implemented our own tokenizer completely from scratch and today we are going to learn about a bit more advanced concept and this scheme this bpe scheme is used in all modern llms and so it's very important for us to learn so let me take you to the Whiteboard right now and explain the concept of bite pair tokenizer and why do we even need it in the first place so if you look at the tokenization algorithms there are essentially three types of tokenization algorithms the first is the word based tokenizer U the second type is the subword based tokenizer and the third type is the character based tokenizer let me walk you through these step by step in the word based tokenizer what we usually do is every word in the sentence is usually one token the tokenizer which we saw in the previous lecture such as for example in this example the um the fox chased the dog the tokens were the fox chased the dog so every word was one token right so that's why it's an example of word based tokenizer uh here I'm taking one more example to illustrate this concept to you if the sentence is my hobby is playing cricket and if the tokens are individual words such as my then hobby then is then playing and then Cricket that that is a word based tokenizer awesome now can you think of the problems associated with the word based tokenizer first uh the main problem is what do you do with Words which are not present in the vocabulary so let's say if you have huge amount of training data and you will break it down into sentences right and then you'll break down the sentences into individual words and then you will assign a token ID to each of these words but when the user is interacting with the llm let's say and the user inputs a word which was not present in the vocabulary these words are also called as out of vocabulary words so you can think of as preparing for an exam but in the exam a question is asked which is completely different from what you have prepared for uh in word based tokenization schemes it's very difficult to know what do we do with out of vocabulary words because consider this case itself my hobby is playing Cricket let's say if the data set was small and only one sentence if a new word is given such as football and that is not present in this data set then usually these tokenization schemes run into an error this is a very simplified example I have constructed but uh this problem shows up when you do word based tokenization a lot usually it's very difficult to deal with out of vocabulary or oo words if they are not present in the vocabulary and what do I mean by vocabulary vocabulary is just a dictionary with a collection of tokens arranged in ascending order and every token corresponds to a token ID great now another problem with this word based tokenization scheme is that uh let's say boy and boys are two words in the vocabulary each of them will have different tokens maybe the token IDs for both of them will be far apart based on uh where they appear or they might be similar also but but the problem is that uh these words are very similar right and when we do this tokenization this similarity is not captured so boys if you think of the word boys the root word is boy so ideally both of these words are very similar to each other but with this tokenization they are treated as separate words and that similarity is not captured these are the main problems with word based tokenization now let's look at the other end of the spectrum which is character based tokenization this is also a very popular tokenization scheme where what we do is that if the sentence is again my hobby is playing Cricket instead of having individual words as tokens in this kind of character based tokenization individual characters are considered as tokens so for example here let's look at which characters are there m is the first character why is the second character uh I'm ignoring white spaces for now so H is the third character o is the fourth character Etc so then the vocabulary or the tokens would contain m y h o b Etc these are the tokens instead of having individual words now can you think of what will be the vocabulary size in this case just think about this for a moment if you are not able to answer this just pause and think okay so this will actually lead to a very small vocabulary because every language has a fixed number of characters if you look at the English language it has 256 characters on the other hand if you look at the total words in the English language it has about 170,000 to 200,000 words uh but what is one of the advantages of the character based tokenization is that it has a very small vocabulary size which means that uh either so if you look at the Engish Eng language there are fixed number of characters right there are 256 characters so we will never have the out of vocabulary problem because let's say any new sentence is given by the user you can always break it down into characters even if you don't know the words in that sentence it's fine you break it down into characters and it will be either of the 256 characters which are already present in your vocabulary or dictionary so the out of vocabulary problem will not come into the picture and uh it solves one more problem if you look at the word BAS tokenization right as I told you English language is about 170,000 to 200,000 words so if you really want to include everything in the vocabulary you need a vocabulary size which is huge and that is one big problem in word based tokenization this problem is completely solved in the character based tokenization because the vocabulary is based on characters and the vocabulary length is pretty small but then you might think oh this sounds amazing right it literally solves all of the problem problem it solves the out of vocabulary problem it's also computationally and memory efficient because the vocabulary size is very small and uh then that's great then what's the issue there are some problems with character based tokenization and the first major problem is that the meaning which is associated with words is completely lost essentially the advantage of dealing with language models is that words have meanings right so different words and different sentences might be related to each other boy and boys have a common meaning this is completely lost since you're breaking it down into individual characters that's one of the first biggest problem with uh character level tokenization the second problem is that the tokenized sequence is much longer than the initial raw text so for example if there is a word in the let's say there is a word in the text which is dinosaur now in word based tokenization this will be treated as one single token right but in character based tokenization what will happen is that every every single character here d i n o s a UR will be treated as separate token so in character based tokenization the word dinosaur will be actually broken down or split into eight tokens and that is another major problem the tokenized sequence is much longer than the initial raw text now uh so as we can see the word based tokenization has its advantages and disadvantages the disadvantages is that we don't know what to do with out of vocabulary words and the vocabulary size is pretty large the advantage is that uh of course the words every word is a token so the tokenized sequence length will be small like dinosaur it will be just one token so when we tokenize the paragraph it's very small uh because every word will be one token unlike character based tokenization where the tokenized sequence is much longer than the initial text that's the disadvantage of character sequence character tokenization the advantages of character tokenization is that they have very small vocabulary because every language has fixed number of characters and we solve we completely solve the out of vocabulary problem and both of these approaches have a disadvantage that the meaning between words is not captured boy and boys have a common root right that root is cap is not captured tokenization and modernization both have the isation which is common this meaning is completely lost so now so what do we do then we turn to another tokenization algorithm which is called as the subword based tokenization and the bite pair encoding which we are going to see is an example of subword based tokenization algorithm the subword based tokenization is kind of like a Best of Both words words and let's see how it uh why it is the best best of both words so the first thing to remember about subword based tokenization is it does capture some root words which come in many other words so boy and boys it will treat boy as a common root word uh and let's see how it does that okay so in subword based tokenization there are essentially two rules the first rule of this tokenization is that when you get the data set you do not do not split frequently used word into smaller subwords so if there are some words which are coming frequently you should retain those words as it is right so then it retains this from the word tokenization then the rule two is that if there are some words which are very rare which are not occurring too many times then you split these words into smaller meaningful subwords this is extremely important this second part basically says that if there are some words which are rare not appearing too many times you can go on splitting it further you can even go down to the Character level so you can see why it is a mix between word tokenizer and character tokenizer the first rule implies that if the words are occurring many times you return it as a word so this is taken this is a feature taken from the word tokenizer the second rule implies that if the word is rare you can go on splitting it into further subwords and if needed you can drop down to the Character level we don't always drop down to the character level we even stay in the middle but this is a feature from the character level because we are breaking down the word further so to give you an example if the word boy is appearing multiple times in the data set it should not be split further that's from rule number one so then boy is retained as a token but boys if the word boys is encountered uh that should be split into boy and it should be split into s uh because boys might not be appearing too many times and again boys also derives from the word boy so we should divide this word boys into smaller meaningful subwords so boys is divided into boy and S and this is the main essence of subword tokenization words are sometimes broken down into smaller elements and why smaller elements because those smaller elements appear very frequently so for example why is boys broken down into boy and S because boy appears more frequently and S is also another token which appears very frequently this is what is basically done in a subord tokenization scheme at a very high level so let me explain some advantages of subord tokenization the subword splitting helps the model learn that different words with the same root word such as for example token tokens and tokenizing all of these three words essentially have the same root word right token so subord splitting helps the model understand that these different words essentially have the same root word and they are similar in meaning this meaning is lost in word based tokenization and even character based tokenization that is number one the second advantage of subord tokenization is that it also helps the model learn that let's say tokenization and modernization are made up of different root words token tokenize and modernize but have the same suffix isation and are used in same syntactic situations so basically it derives these patterns like tokenization and modernization maybe zation z a t i o n is a is a subword token in this tokenization and then token and then modern and another tokens so then it learns that this isation is a common suffix which appears in both of these words all of these are advantages of breaking down one word into subwords this is what is majorly done in subword tokenization so why are we learning about subword tokenization and what's the relation between bite pair encoding and subo tokenization well the main relation is that uh bite pair encoding or bpe is a subord tokenization algorithm this is very important and that's why we are learning about bpe and that's why modern llms like gpt2 and gpt3 employ bite pair encoding so let us look at the bit of a history of the BP algorithm and let and then we'll see how it is implemented in practice so the bite pair algorithm was actually introduced in 1994 and uh it does a very simple thing it is basically a data compression algorithm uh and what is done is that we we scan the data and then mostov common pair of consecutive bytes so let me actually use a different color here this sentence is very important most common pair of consecutive bytes of data is replaced with a bite that does not occur in data so we identify pair of consecutive bites which occur the most so we find these pairs which occur the most frequently and then we replace them with a bite which does not exist in the data and we keep on doing this iteratively I'll show an example for this so don't worry if you don't understand this explanation I want to quickly show you the paper so this is the the paper which was published in 1994 a new algorithm for data compression which introduced bite pair tokenizer or bite pair encoder rather at that time it was not known that this will be so useful for modern large language models but it is quite useful okay so now let us see a practical demonstration of this algorithm so I've taken this example from Wikipedia because I think it is an awesome illustration so let's say we have this original data right and the original data looks like this a a a b d a a a b a c okay and now in bite pair encoding we are going to compress this data right and let us see what do we do we first identify the most common pair of consecutive bytes so let's see let us see the pair which occurs the most so if you scan this sequence from left to right you will see that the bite pair AA occurs the most right you might say a AA also occurs the most but that's not a bite pair because that three characters the bite pair AA occurs the most it occurs here it occurs here it occurs here and it occurs the here so it occurs four times really so what we will do next is that we have identified the bite pair which occurs the most we replace this bite pair with a bite that does not exist in the data so what we'll do is that we will replace it with zed and why Z because just Zed does not occur in the data you can use any variable here so then we'll take these a a and wherever AA shows up we'll replace it by Zed so then this first AA will be Zed so then this new sequence will be Zed a b d then again Zed to replace this a a so then it will be Zed and then a b a c so then we have a b a c so remember what has happened here is that this AA which I'm highlighting right now in circle that has been replaced by Zed and this AA has been replaced by Zed so now we have a compressed data correct very good so now now let us move next next what we do is we keep on repeating this sequentially now we again look at this sequence and find the next common bite pair so we can see that a is ur occurring once and a is occurring twice so it is repeating two times and which is the most frequent bite pair so we will replace AB by y so this AB will be replaced by Y and this AB will be replaced by y why why because it's a bite which does not occur anywhere in the U data so then this AB will be replaced by y as you can see here and here also the ab will be replaced by y so then my new compressed data will be z y d z y a great this is a compressed data compared to the original data and this is exactly how the bite pair algorithm works now we do this recur do this again and again right so now let us look at the bite pair so here AC is the only bite pair which is left all others are special tokens but AC only appears once so we we don't need to encode it this process of replacing common bite pairs with another variable is called encoding that's where the word encoder comes from so we will not encode this further because it only appears once if a c were appearing twice then we would have encoded it with another variable so this compression actually stops here you can go one more layer Deeper by further com replacing the zy with another variable which is let's say w so then it will be WD w a and then you will stop so you will compress it further like this so you see the original data has been compressed to this right uh to this compressed version U using the bite pair algorithm so the algorithm itself is pretty simple you scan the data from left to right you identify the bite pairs which occur the most and then you replace them with a bite which does not exist in the data and you do this iteratively until you reach a stage where no bite pair occurs more than once that's it that is the simple bite pair encoding algorithm now you might think okay what has that got to do with large language models right I understand this algorithm and I understand how it compresses a given data sequence but what has that got to do with large language models well it turns out that the we slightly tweak the bite pair encoding algorithm and use this to convert our entire sentence into subwords which will be very useful for us and I'll show you exactly how I'm going to do that so the bite pair encoding for llm ensures that the most common words in the vocabulary are represented as a single token remember rule number one and rare words are broken down into two or more subord tokens this is exactly the same rules which we had looked at the rule number one and rule number two so rule number one is that most commonly used words should not be split and second is that that rare words should be split into meaningful subwords now let's see how uh how it's related to The Bite pair encoding algorithm which we saw and we will be looking at a practical example for this uh demonstration okay I will use a color which is a bit different from the green one here so that uh there will be a good contrast so let me use the orange color okay so for the Practical example we are going to look at a vocabulary or rather I should say we are going to look at the data set of Words which is also called as Data Corpus which is this so we have these words in our data set old older finest and lowest right let's say we have these words in the data set right now and I'm going to show you how we are going to use the bite pair encoding algorithm to break these down into tokens and you will also see why this is a subword tokenization scheme so first of all if we are to use a word based tokenization then this will have four tokens old older finest and lowest that's it similarly if we were to use the Character level tokenization then the tokens will be individual characters like o o will be one token then L will be another token then D will be another token Etc so this is how the word based tokenization and the character based tokenization will work but right now we are going to see the subw based tokenization using the bite pair algorithm before we uh proceed further we'll need to do a pre-processing step and this is actually done uh even when we train large language models and when we are using these tokenizers so basically when you look at these different tokens there should be some ending right so for example when this old token appears we should have another end token so that we know that this word has ended over here so I'm going to augment every token here with this additional end token which is called slw so whenever the algorithm or the model comes across slw we know that this word ends over here so I'm going to replace the tokens in my data set with adding a w/w at the end like old becomes old slw older becomes older slw finest becomes finest slw and lowest becomes lowest /w now remember here that if we use the word based tokenization uh there is no meaning which is captured so the fact that old is the common root between old and older is not captured number one EST is the common root between finest and lowest that's not captured so word based tokenization character based tokenization have so many problems because they don't capture these meanings or root words and towards the end of this section we'll see how subword based tokenization using the bite pair encoding algorithm actually captures these root words like old uh like EST Etc okay so let's get started with the individual steps the first step is basically to split all these words into their characters um and then make a frequency table so what we are going to do is that we are going to take all these words old older finest and lowest so old appears seven times in the data set older appears three times in the data set these are the frequencies finest appears nine times in the data set and lowest essentially appears four times in the data set so what we are going to do right now is we are going to split these words into individual characters so then here is the table which I have made remember slw is also there uh so old so all words have/ W right and totally how many words do we have 7 + 3 10 + 9 19+ 4 23 and since all words have slw at the end of it slw comes 23 times similarly we see that o comes 14 times L comes 14 times D comes 10 times e comes 16 times Etc and we make this frequency table list so here you can see that we have 12 tokens and if we did we use character level tokenization our tokenization would end here because all these characters would be individual tokens now what we will do similar to if you remember the bite pair encoding algorithm we looked at the most frequent pairing right uh so let me take you back to this yeah we looked at AA because AA appeared the most so we looked at that bite pair which occurred the most this is exactly what we'll be doing here we'll be look for the mo we'll be looking for the most frequent pairing in the data set and then what we'll do is that this is the modification compared to the original algorithm when we look for the most frequent pairing we will merge them we will merge the most fre pairing and then perform the same iteration again and again and again so let me show you how this is done so in the iteration one as I mentioned we start with the uh finding the most frequent pairing right so it so you the way to do it is look at the first character which appears the most so e is that character which appears 16 times right so if you want to look at the pairing which appears most it most probably starts with e so it turns out if you look at these words e and s is the pairing which appears the most number of times so e and s here appears nine times in finest and E and S appears four times in lowest so e and s is that pairing which appears 13 number of times right so uh most common bite pair starting with e is e and s so what we'll now be doing is that we'll be going through the data set again uh and we'll be merging these two tokens e and s so now e s will be one token and that's why it's called subword es will be one token so now let me show you my token table again everything else is the same up token number 12 but look at this token number 13 which has been added in token number 13 we have added one more token which is es because it's the most frequent pairing and Es appears 13 times but remember when we add es we have to subtract something from E and we have to subtract from s because now uh ES has been included so we subtract 13 from the e count so now the the number of time only e appears is three the number of time only s appears is zero this is very interesting to know so the number of time only s appears is zero so s it seems always appears with e so es is a subword see this we would not have discovered if we just did character level tokenization or uh Word level it seems that e and s always so s only comes with e in this data set we have already obtained our first site so now this is my new uh this is my new token library and this is my additional token and now we are going to actually continuously keep on doing this process to find uh frequency or to find tokens which appear the most number of times so in the previous iteration we saw that e and s was the bite pair right which occurred most number of times ands it appeared 13 times but now es is a separate token for us so now using that token we see that EST is again a bite pair because es is one token and T is another token so es s and t becomes a bite pair in the second iteration and EST appears 9 + 3 which is again 13 times so now in the next iteration what we'll be doing essentially is that U let me show you iteration number two in the iteration number two we'll merge the tokens es s and t because they have appeared 13 times in the data set see we are doing the same thing what we did in the earlier bite pair encoding for that character so let me just showed you show you here remember here what we did after AA so AA was done right and then we merged this and then we looked at the second sequence which appeared the most which is AB that is actually very similar to what we are doing here es appears the most so we created one more token for ES then we looked at another bite pair which is appearing the most and that is es s and t so now what we'll be doing is that we'll merge es andt into one token and uh so EST comes to be 13 times and we'll then subtract 13 from the previous token es so now only es appears zero times and EST appears 13 times see we have constructed a new token EST so now remember what I said earlier the previous World level tokenizers and the Character level tokenizers could not identify that EST is a common roote between finest and lowest but with our bite pair encoding algorithm we have already created a new token for EST so this algorithm has already identified that EST is a common root World great so up till now we have done iteration number three and uh we see that okay I think yeah up till now we have done I think two iterations and now up till now we have merged es and T into one common token awesome now let us take look at this slw token now we can see that EST and /w basically appears 13 times again it's the same thing over here so EST always comes with slw now EST is one token so EST and slw forms a bite pair and this bite pair again occurs 9 + 4 which is 13 times which is much more than any other bite pair in these words so we'll combine EST and /w into one more token so in the third iteration we are going to combine EST and /w into one more token so now this becomes our word do you understand why we combined slw with one more token we could have just left it at EST right but if we left it at EST then essentially there would have been no difference between words like estimate and highest so estimate and highest both have EST but the words in our data set are highest and lowest so EST is the ending sequence in all of our words in our data set and we need to encode that information that it is an ending sequence so estw allows us to encode this information so now the tokenizer knows that whenever EST comes it's always followed by slw which means the word ends after estd so now our algorithm or the tokenizer can differentiate between estimate and highest because in estimate EST does not end with a /w so now if you look at the Tok which we have earlier we had all these 12 tokens but now we created es then we merged it to EST and finally we created this estw so now these two tokens are actually not needed es and EST so now let's look at another other bite pairs which occur a lot so it turns out that o and L is another bite pair which occurs 10 times because it's present in old and older so what we'll do is that we'll create one more we'll merge these two and create one more token for o and L it appears 10 times and we'll subtract that count 10 from the O and L so o individually or with some other character appears 14 times so we subtract 10 because now we have created one more token for o which appears 10 times similarly L appears 14 times overall and we subtract 10 from it because L comes with o 10 times right and now what we do is that o l is one token so now we see that o l and D has appeared 10 times so this bite pair has appear 10 times so we merge this bite pair so then o d now becomes another token which appears 10 times so you see the meaning which our bite pair encoder has captured we have constructed one token which is old we have another token which is estw these tokens are subwords so they are neither full words nor characters they are subwords but they encode the root representation so old is one token and this actually tells us that uh old is the root word which comes in Old it which comes in old as well as older and our BP algorithm has actually captured that perfectly that's awesome right all these root words were not captured by just the word uh word em or the word encoder word tokenizer and the Character level tokenizer now you might be seeing here that these f i and N appear nine times uh it's fine that they appear nine times but we we just have one word with these characters so if you look at our data set again let's see where f i and N appear so the words are finest so all of these words actually only appear in finest so it's no it does not make sense to merge them into one um one token because that token does not the frequency of that token appearing in other words is not too much why did we merge EST into one token because it appears in multiple words why why did we merge old in one token because it appears in multiple words so see the rule of uh subord tokenization the first rule is that that word which occurs multiple times you keep it as it is old so we kept old as it is right it is a separate token uh but that word which is not used too many times like older it needs to be split into old and then e and then R are separate tokens similarly in finest EST is one token fi and N are separate tokens in lowest EST is one token and L O and W are separate tokens and this helps us to retain root wordss in sub tokenization I hope you are I hope you have understood this concept now what we can do is that uh this EST and old are final tokens which we constructed by merging now let us actually remove all of those tokens whose frequency is zero so we can remove s we can remove so let me Mark them with a different color so s has a frequency of z t has a frequency of Z es EST has frequency of z o has a frequency of Z so let's remove this so then our final table looks like these these are the final tokens in our subord tokenizer which is obtained using the bite pair encoding algorithm how did we obtain these tokens we just looked at bite pairs which occur the most then we merged them into one and then we repeated this process uh until we obtained until we reached a stage where enough number of tokens have been created or until we reached a stage where our iterations have stopped and this is the final tokenized uh final uh tokens which we'll be using for next steps of the large language model training which are vector embedding Etc this is how the subword tokenizer works and this is exactly how bite pair encoder which is a subword tokenizer it works for uh training models like gpt2 or gpt3 very few people have this understanding but I hope this lecture has intuitively made it clear for you how the bite pair tokenizer actually works so now this list of 11 tokens will serve as our vocabulary why is it called subword tokenizer because these are subwords right EST is not a full word neither it's a character it's a subword but it's the root of many words o is also the root of many words now you must be thinking when do we stop this uh merging when do we stop these iterations so you have to specify a stopping criteria usually if you look at gpt2 or gpt3 you run this bite par coding algorithm on a huge number of tokens right so the stopping criteria can be if the token count becomes a certain number then you stop or just the number of iterations can be the stop uh stopping count so I'm in this lecture I'm not covering the stopping criteria in too much detail but I just want to give you an intuition of how the stopping criteria can actually be formulated or computed it's based on the token count or the number of iterations to give you an example uh or to mention one more advantage of bite pair encoding here you can see that we it's actually better than Word level encoding right because uh let's say if you have two words boy and boys both won't be separate tokens in this just boy will be a token so bite pair encoder also reduces the number of tokens which we have compared to word encoding let's see uh and that usually helps so in gpt2 or gpt3 when it was trained I think it used about 50,000 more than around 57,000 tokens I think when when bite pair encoding was done uh for gpt2 or gpt3 so bite pair encoding solves the out of vocabulary problem why does it solve the out of vocabulary problem because we also have subwords now and some of it might even be characters so character level tokenization solves the out of vocabulary problem right and bite pair encoding or subord encoding retains some properties from it so you can see some tokens here are characters which is good but it also solves the problem of the word level encoding and one of the major problem is that root meanings are not captured but now they are captured over here and it also solves the problem of the word level encoding being just a huge bunch of numbers here that is not the problem because we break it down into characters and subwords so the total length of the vocabulary is also shorter than if you would have just considered Word level embedding so subord embedding solves it it solves the out of vocabulary problem it also gives us a vocabulary size which is manageable and it also retains the root words meanings such as EST and old awesome right and that's why because of all these advantages this is the bite pair encoding algorithm is used for tokenization in gpt2 and gpt3 now let me return back to the code I could have just taken you through the code today but then you would not have UND OD how the bite pair algorithm actually works now implementing BP from scratch is relatively complicated so we will be using a python open source Library called tick token so if you I'll share the link of this library in the chat so this is a library which is essentially a bite pair encoder it's a bite pair encoder which is used for open a models so open a themselves use tick token for tokenizing the sentences from the data you can see that it has about 11,000 stars and about 780 Forks so it's a pretty popular repository okay so uh now we are going to implement the bite pair encoder in Python and I'm going to show you this implementation in a Hands-On demonstration the first thing to do is install tick token because this is we'll be using the bite pair encoder from uh tick token and here you can see that it takes a it takes some time to install this uh for me when I first install insted it it took about 1 minute but now it's a bit faster so requirements are already satisfied um and you'll see that it will get installed now in some time see it's already installed so now I have installed tick token and let me just print out the version of tick token which I'm using here so you can see that the origion of tick token is 6 which is good enough now what we have to do is that we once the tick token is installed we can instantiate the bite pair encoding tokenizer from tick token so the way to do this is just uh use tick token do get encoding gpt2 and we store it in this object called tokenizer so now tokenizer essentially has is that is similar to the simple tokenizer version two class or version one class which we defined in python in the last lecture remember we defined this uh simple tokenizer version one class and uh we defined the simple tokenizer version two class here which also include special tokens so now in just one line of code we have essentially defined this tokenizer which is something similar but now it will it's initiated an instance is created through this tick token Library so this is a bite pair tokenizer and uh similar to the tokenizer class which we had created if you see every tokenizer class has an encode method and a decode method what the encode method does is basically it uses that tokenizer and it converts words into token IDs what the decode method does is that it decodes those token IDs back into individual words so let's see how this uh a tokenizer actually works so this is the bite pair encoding tokenizer from gpt2 and let's give it some text and try to encode and decode right and I'm testing uh this tokenizer because I've given it a complex sentence see so the first part of the sentence is hello do you like T then we have given one more thing which is called end of text which means that a new text is starting here this is the end of text is usually done to separate entire documents but here I'm just using it to illustrate and in fact gpt2 actively uses end of text so just to give a demonstration to you when gp2 when gpt2 or gpt3 is trained on large amounts of data sets here is how the data sets are loaded so see uh what gpt2 does is that when it has data from different text sources it usually shows when a particular text has ended and when the other text has started so end of text is actually a part of the gpt2 vocabulary itself so I have given this end of text here and then the second sentence is in The sunl Terraces of some unknown place now look at this if we are using a word level tokenizer this this will lead to the out of vocabulary problem because this word cannot be in the vocabulary of a word level tokenizer and that's the advantage of bite pair encoding in bite pair encoding we have characters as tokens and we even have subwords as tokens so definitely this will be encoded because might be some character can encode it so some unknown and place might be subwords which are individual tokens they can also be used to encode this so bite pair encoder will take care of this this entire word which usually does not exist as a single word so let us test this actually and uh I've run this right now and let us see the different tokens so see hello do you like T it all of these have been converted into token IDs I want you to take a look at this 50256 token so this is the token ID of end of text and this is also the vocabulary size of the tokenization scheme used in gpt2 or gpt3 so if we were to use a World level tokenizer English language has about 170,000 to 200,000 words so the vocabulary size would have been this much but now that we have used the bite pair tokenizer or bite pair encoding the vocabulary size has reduced by around 13 from 150,000 and with with a vocabulary of around 50,000 subwords uh we are able to get the amazing performance from gpt2 gpt3 or GPT 4 I think has much higher level of uh tokens but even gpt2 has good enough performance and it has 50,2 56 tokens awesome and here you can see that I did not not get an error because some unknown place was not encoded the tokenizer is able to encode random words like these which also look wrong and here you can see these are the encodings for this sub some unknown place so the end of text is this in the sunlight Terraces of some unknown place so I think some unknown place is broken down into subwords three or four subwords and then it's tokenized by gpt2 so this is how subord tokenization can handle out of vocabulary awesome so the code prints these IDs which we just saw and now what we can do is we can convert the token IDs back into text using the decode method remember every tokenizer is encode method as well as decode method so we can do tokenizer do decode and put all of these integers and here you see it decodes exactly the same sentence what we gave to it so the decoded sentence is hello do you like T then end of text in The sunl Terraces of some unknown place exactly similar to the uh text which it had G which we had given which means that the encoder and decoder is working very well uh and the bite pair encoding has its advantages of dealing with out of vocabulary so here we see two advantages of bite pair encoding first is that it reduces the vocabulary length second is that it knows how to deal with unknown text awesome so we can make two noteworthy observations based on the token IDs and the decoded text first as I already mentioned the end of text token is assigned to a relatively large token token ID which is 50256 uh that is the last token in the BP tokenizer used for gpt2 so in fact the BP tokenizer which was used to train models like gpt2 gpt3 and the original model used in chat GPT has a total vocabulary size of 50257 keep this in mind now you know what this means and end of text is the last largest token ID and it's also the last token in the vocabulary the second uh thing to note which I already mentioned is that the BP tokenizer encodes and decodes unknown words such as some unknown Place correctly the BPA tokenizer can handle any unknown word and how does it achieve this without mention so we did not give special tokens for unknown words right uh how did it achieve this and this this is because of how the tokenization was done and because of how the uh BP tokenizer actually goes down to the level of uh let me show this the BP tokenizer goes down to the level of characters and subwords that's why it is able to deal with unknown text so the algorithm underlying BP breaks down words that aren't in its predefined vocabulary to smaller subword units or even individual characters this is exactly what we saw over here uh and this enables it to handle out of vocabulary words so thanks to the BP algorithm if the tokenizer encounters an unfamiliar word during tokenization it can represent it as a sequence of subword tokens or characters that is the advantage of BP tokenizer and that's why we are having this lecture in the first place great now let us actually take one last example to illustrate how the BP tokenizer deals with unknown tokens right so let's say we have this this sentence a k w i r w i e r completely random words which do not make any make any sense but if you pass these random words to uh the tick token gpt2 encoder bite pair encoder you'll see that it does not show an error in fact it even encodes these so a k w i r w i e r is encoded as these tokens because it broken down into subwords maybe e r is a commonly occurring subword in all the vocabulary which we have so then the token for this is 25959 maybe AK is a commonly occurring subo maybe W is just a simple character maybe IR is a commonly occurring subo maybe I is just a character so you can see this what underneath the hood what this tokenizer must have done is that it must have broken this down into words and subwords based on what is present in the vocabulary and then to each of these word to each of the subword or character it would have assigned a token ID so the tokenizer would have scanned this from left to right and broken it down into characters or subwords and then assigned each character or subword a unique token ID that's how the encoding works and if you decode the integers you get back exactly the same sentence which you started with and the reason I covered this lecture in so much detail is because in the previous lecture we saw how to do tokenization from scratch and today we saw how what is bite pair encoding and how you can install tick token and Implement your own bite pair encoder or tokenizer with gpt2 is using and we saw how it handles unknown words uh how it captures the root meaning and how the vocabulary size is about 50,000 uh this brings us to the end of today's lecture in the next lecture we'll be looking at data sampling batch sizes context length Etc before we feed the embed or before we feed the tokenizers into the word embedding so in our progression of learning right now let me show you um one plot so that I can illustrate up till where we have covered until now so if you look at uh if you look at yeah if you look at this schematic right here yeah if you look at this schematic right here until now we have looked at how to tokenize an input text and how to convert it into token IDs and we saw implementing our own tokenizer in the previous lecture and using a bite pair encoder in this lecture we still have to see token embeddings so we'll get to this point and before that we'll also see a bit about data sampling context length and batch sizes this brings us to the end of this lecture thank you so much everyone I really wanted to have a good mix of uh whiteboard writing and whiteboard understanding which we did a lot in today's lecture so uh if you see our whiteboard notes we did a number of itations of the BP encoder I explained it to you intuitively I hope you understood this please mention in the chat if something is unclear and I'll be happy to explain it in the comment section and then we also saw how to code the bite pair encoder uh using uh the tick token library in Python so now what I encourage you all to do is take some unknown sentences and try using the uh this tick token library and the bite pair encoder and try to just see what are the results uh if you if you encounter any error for any sentence if the tokenizer is not able to encode it will be awesome and I'll highlight that comment in the next video thank you so much everyone and I look forward to seeing you in the next lecture"
}