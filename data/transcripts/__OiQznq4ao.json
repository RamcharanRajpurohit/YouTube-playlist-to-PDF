{
  "video": {
    "video_id": "__OiQznq4ao",
    "title": "Instruction fine-tuning: Loading pre-trained LLM weights",
    "duration": 1169.0,
    "index": 39
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.0
    },
    {
      "text": "hello everyone and welcome to this",
      "start": 5.52,
      "duration": 4.44
    },
    {
      "text": "lecture in the build large language",
      "start": 8.0,
      "duration": 5.36
    },
    {
      "text": "models from scratch Series today we are",
      "start": 9.96,
      "duration": 6.04
    },
    {
      "text": "going to look at step number four in the",
      "start": 13.36,
      "duration": 5.079
    },
    {
      "text": "instruction fine-tuning Hands-On project",
      "start": 16.0,
      "duration": 4.76
    },
    {
      "text": "which we have started and that step is",
      "start": 18.439,
      "duration": 4.801
    },
    {
      "text": "loading a pre-trained large language",
      "start": 20.76,
      "duration": 5.16
    },
    {
      "text": "model until now we have looked at three",
      "start": 23.24,
      "duration": 5.119
    },
    {
      "text": "steps the first one involved data set",
      "start": 25.92,
      "duration": 4.839
    },
    {
      "text": "download and formatting the second",
      "start": 28.359,
      "duration": 4.561
    },
    {
      "text": "involved batching the data set and the",
      "start": 30.759,
      "duration": 5.201
    },
    {
      "text": "third involved creating data loaders so",
      "start": 32.92,
      "duration": 5.04
    },
    {
      "text": "here's the data set which we have been",
      "start": 35.96,
      "duration": 5.04
    },
    {
      "text": "looking at which consists of",
      "start": 37.96,
      "duration": 6.68
    },
    {
      "text": "1100 entries of instructions inputs and",
      "start": 41.0,
      "duration": 6.12
    },
    {
      "text": "outputs they are in various fields from",
      "start": 44.64,
      "duration": 4.52
    },
    {
      "text": "English to geography to general",
      "start": 47.12,
      "duration": 4.48
    },
    {
      "text": "knowledge Etc and essentially through",
      "start": 49.16,
      "duration": 4.52
    },
    {
      "text": "this data set our aim is to teach the",
      "start": 51.6,
      "duration": 6.639
    },
    {
      "text": "llm to be able to correctly follow",
      "start": 53.68,
      "duration": 6.8
    },
    {
      "text": "instructions um and that's what",
      "start": 58.239,
      "duration": 5.721
    },
    {
      "text": "finetuning refers to in pre-training if",
      "start": 60.48,
      "duration": 5.679
    },
    {
      "text": "we just used a pre-trained llm it does",
      "start": 63.96,
      "duration": 4.519
    },
    {
      "text": "not follow instructions very well so",
      "start": 66.159,
      "duration": 3.921
    },
    {
      "text": "that's why we need to train it with a",
      "start": 68.479,
      "duration": 5.0
    },
    {
      "text": "specific data set now before we come to",
      "start": 70.08,
      "duration": 6.399
    },
    {
      "text": "loading a pre-trend llm we first needed",
      "start": 73.479,
      "duration": 5.601
    },
    {
      "text": "to spend a lot of time on downloading",
      "start": 76.479,
      "duration": 4.921
    },
    {
      "text": "this data set then converting it into a",
      "start": 79.08,
      "duration": 5.039
    },
    {
      "text": "format called The alpaka Prompt style",
      "start": 81.4,
      "duration": 4.6
    },
    {
      "text": "then batching the data set which is",
      "start": 84.119,
      "duration": 3.161
    },
    {
      "text": "creating different batches and",
      "start": 86.0,
      "duration": 3.6
    },
    {
      "text": "ultimately creating data loaders now",
      "start": 87.28,
      "duration": 3.56
    },
    {
      "text": "that we we are ready with the data",
      "start": 89.6,
      "duration": 3.479
    },
    {
      "text": "loaders we can go ahead and load a",
      "start": 90.84,
      "duration": 3.239
    },
    {
      "text": "pre-trained",
      "start": 93.079,
      "duration": 3.121
    },
    {
      "text": "llm what does it mean loading a",
      "start": 94.079,
      "duration": 4.161
    },
    {
      "text": "pre-trained llm well what we are going",
      "start": 96.2,
      "duration": 4.879
    },
    {
      "text": "to do here is that this is the llm",
      "start": 98.24,
      "duration": 4.44
    },
    {
      "text": "architecture which we have constructed",
      "start": 101.079,
      "duration": 3.761
    },
    {
      "text": "in this lecture series and there are",
      "start": 102.68,
      "duration": 4.079
    },
    {
      "text": "several places where there are trainable",
      "start": 104.84,
      "duration": 4.0
    },
    {
      "text": "weights so for example in multi-head",
      "start": 106.759,
      "duration": 4.521
    },
    {
      "text": "attention there are the query key value",
      "start": 108.84,
      "duration": 5.04
    },
    {
      "text": "weight metries then in feed forward",
      "start": 111.28,
      "duration": 4.839
    },
    {
      "text": "neural network there are the neurons and",
      "start": 113.88,
      "duration": 4.96
    },
    {
      "text": "their weights in layer normalization one",
      "start": 116.119,
      "duration": 4.36
    },
    {
      "text": "and two there are the scale and the",
      "start": 118.84,
      "duration": 3.559
    },
    {
      "text": "shift parameters which have parameters",
      "start": 120.479,
      "duration": 4.28
    },
    {
      "text": "or weights associated with it then there",
      "start": 122.399,
      "duration": 4.08
    },
    {
      "text": "is the token embedding layer and the",
      "start": 124.759,
      "duration": 4.241
    },
    {
      "text": "positional embedding layer there are",
      "start": 126.479,
      "duration": 4.28
    },
    {
      "text": "trainable weights associated with these",
      "start": 129.0,
      "duration": 4.16
    },
    {
      "text": "two layers as well if you add up all of",
      "start": 130.759,
      "duration": 4.041
    },
    {
      "text": "these trainable weights then we have",
      "start": 133.16,
      "duration": 3.6
    },
    {
      "text": "more than 1 million or more than 100",
      "start": 134.8,
      "duration": 3.76
    },
    {
      "text": "million trainable such",
      "start": 136.76,
      "duration": 3.96
    },
    {
      "text": "parameters uh in the pre-training",
      "start": 138.56,
      "duration": 4.28
    },
    {
      "text": "process these parameters are trained on",
      "start": 140.72,
      "duration": 5.159
    },
    {
      "text": "a huge amount of data so for example",
      "start": 142.84,
      "duration": 6.56
    },
    {
      "text": "gpt2 gpt2 if you search about gpt2",
      "start": 145.879,
      "duration": 7.0
    },
    {
      "text": "weights released um gpt2 weights",
      "start": 149.4,
      "duration": 5.8
    },
    {
      "text": "released open AI actually pre-trained",
      "start": 152.879,
      "duration": 5.281
    },
    {
      "text": "gpt2 on a huge amount of data and not",
      "start": 155.2,
      "duration": 4.8
    },
    {
      "text": "just that when they pre-trained the llm",
      "start": 158.16,
      "duration": 3.28
    },
    {
      "text": "and optimized",
      "start": 160.0,
      "duration": 4.2
    },
    {
      "text": "the trainable weights optimized all of",
      "start": 161.44,
      "duration": 4.2
    },
    {
      "text": "these weights which I just described",
      "start": 164.2,
      "duration": 3.52
    },
    {
      "text": "over here they released those weights",
      "start": 165.64,
      "duration": 4.72
    },
    {
      "text": "publicly which means that as users we",
      "start": 167.72,
      "duration": 4.4
    },
    {
      "text": "can have access to those weights and we",
      "start": 170.36,
      "duration": 4.44
    },
    {
      "text": "can utilize those weights now when open",
      "start": 172.12,
      "duration": 4.32
    },
    {
      "text": "a released their weights they released",
      "start": 174.8,
      "duration": 4.0
    },
    {
      "text": "it for four different models so if you",
      "start": 176.44,
      "duration": 4.32
    },
    {
      "text": "see the open a g pt2 weights they",
      "start": 178.8,
      "duration": 4.439
    },
    {
      "text": "released it for actually multiple models",
      "start": 180.76,
      "duration": 7.92
    },
    {
      "text": "124 million 150 1558 million 345 million",
      "start": 183.239,
      "duration": 8.801
    },
    {
      "text": "774 million Etc so you have different",
      "start": 188.68,
      "duration": 5.32
    },
    {
      "text": "folders corresponding to the parameter",
      "start": 192.04,
      "duration": 4.16
    },
    {
      "text": "size and then based on each folder you",
      "start": 194.0,
      "duration": 4.84
    },
    {
      "text": "can download these weights on your local",
      "start": 196.2,
      "duration": 5.16
    },
    {
      "text": "PC why are we downloading these weights",
      "start": 198.84,
      "duration": 4.679
    },
    {
      "text": "in the first place before fine tuning",
      "start": 201.36,
      "duration": 4.159
    },
    {
      "text": "the reason we are downloading these",
      "start": 203.519,
      "duration": 5.241
    },
    {
      "text": "weights and reusing them is that when",
      "start": 205.519,
      "duration": 5.841
    },
    {
      "text": "the when we reuse these weights it means",
      "start": 208.76,
      "duration": 6.08
    },
    {
      "text": "that a lot of information is already",
      "start": 211.36,
      "duration": 5.799
    },
    {
      "text": "captured correctly so for example when",
      "start": 214.84,
      "duration": 4.52
    },
    {
      "text": "we do token embedding we want to project",
      "start": 217.159,
      "duration": 4.841
    },
    {
      "text": "the words or tokens into a higher",
      "start": 219.36,
      "duration": 4.76
    },
    {
      "text": "dimensional Vector space right and we",
      "start": 222.0,
      "duration": 3.519
    },
    {
      "text": "want to project them so that the",
      "start": 224.12,
      "duration": 4.039
    },
    {
      "text": "semantic meaning is captured to train a",
      "start": 225.519,
      "duration": 4.601
    },
    {
      "text": "huge neural network again from scratch",
      "start": 228.159,
      "duration": 4.401
    },
    {
      "text": "here would take very a very long time",
      "start": 230.12,
      "duration": 4.679
    },
    {
      "text": "for us and computational resources",
      "start": 232.56,
      "duration": 3.8
    },
    {
      "text": "instead we will directly use the",
      "start": 234.799,
      "duration": 4.08
    },
    {
      "text": "pre-trained gpt2 weights so that will",
      "start": 236.36,
      "duration": 4.92
    },
    {
      "text": "help us in fact in all of these",
      "start": 238.879,
      "duration": 5.601
    },
    {
      "text": "parameters reusing the gpt2 weights",
      "start": 241.28,
      "duration": 6.08
    },
    {
      "text": "actually helps us uh because the model",
      "start": 244.48,
      "duration": 4.399
    },
    {
      "text": "now starts from a much more",
      "start": 247.36,
      "duration": 4.159
    },
    {
      "text": "knowledgeable State instead of a random",
      "start": 248.879,
      "duration": 4.841
    },
    {
      "text": "initialization so the main goal of",
      "start": 251.519,
      "duration": 4.481
    },
    {
      "text": "reusing the pre-trained weights is that",
      "start": 253.72,
      "duration": 3.56
    },
    {
      "text": "the",
      "start": 256.0,
      "duration": 3.68
    },
    {
      "text": "model",
      "start": 257.28,
      "duration": 4.52
    },
    {
      "text": "starts",
      "start": 259.68,
      "duration": 7.079
    },
    {
      "text": "from a knowledgeable state",
      "start": 261.8,
      "duration": 4.959
    },
    {
      "text": "instead",
      "start": 269.919,
      "duration": 2.521
    },
    {
      "text": "of instead of a random",
      "start": 273.24,
      "duration": 4.399
    },
    {
      "text": "initialization so we are going to load",
      "start": 283.24,
      "duration": 4.2
    },
    {
      "text": "these pre-train weights and then we are",
      "start": 285.8,
      "duration": 3.8
    },
    {
      "text": "going to train on the specific data set",
      "start": 287.44,
      "duration": 4.4
    },
    {
      "text": "which we have curated so we are going to",
      "start": 289.6,
      "duration": 4.28
    },
    {
      "text": "load the pre-train weights and then use",
      "start": 291.84,
      "duration": 4.6
    },
    {
      "text": "these training data loaders testing data",
      "start": 293.88,
      "duration": 4.8
    },
    {
      "text": "loaders and validation data loaders so",
      "start": 296.44,
      "duration": 3.96
    },
    {
      "text": "the weights and parameters which we load",
      "start": 298.68,
      "duration": 3.64
    },
    {
      "text": "from gpt2 of course they will change",
      "start": 300.4,
      "duration": 3.56
    },
    {
      "text": "during the process because we will do",
      "start": 302.32,
      "duration": 3.719
    },
    {
      "text": "one more training procedure on the",
      "start": 303.96,
      "duration": 4.64
    },
    {
      "text": "specific data set but at least we won't",
      "start": 306.039,
      "duration": 4.401
    },
    {
      "text": "start from a random state of all these",
      "start": 308.6,
      "duration": 3.159
    },
    {
      "text": "parameters we'll start from a",
      "start": 310.44,
      "duration": 3.199
    },
    {
      "text": "knowledgeable State and so the",
      "start": 311.759,
      "duration": 4.561
    },
    {
      "text": "computational time it will take to fine",
      "start": 313.639,
      "duration": 5.28
    },
    {
      "text": "tune or to train on our specific uh data",
      "start": 316.32,
      "duration": 4.48
    },
    {
      "text": "set will be much lower so that will be",
      "start": 318.919,
      "duration": 3.921
    },
    {
      "text": "much more efficient that's why the fine",
      "start": 320.8,
      "duration": 4.119
    },
    {
      "text": "tuning process always happens after the",
      "start": 322.84,
      "duration": 4.28
    },
    {
      "text": "pre-training process in the pre-training",
      "start": 324.919,
      "duration": 3.521
    },
    {
      "text": "we always take the model to a",
      "start": 327.12,
      "duration": 3.44
    },
    {
      "text": "knowledgeable state and then we train it",
      "start": 328.44,
      "duration": 3.96
    },
    {
      "text": "again on a specific data set so that it",
      "start": 330.56,
      "duration": 4.479
    },
    {
      "text": "can further improve its weights and its",
      "start": 332.4,
      "duration": 5.519
    },
    {
      "text": "parameters so now let us go into code",
      "start": 335.039,
      "duration": 5.201
    },
    {
      "text": "and let us load the pre-train llm which",
      "start": 337.919,
      "duration": 5.921
    },
    {
      "text": "means that let us use the weights um",
      "start": 340.24,
      "duration": 6.56
    },
    {
      "text": "which we get from gpt2 one thing which I",
      "start": 343.84,
      "duration": 4.479
    },
    {
      "text": "would like to mention here is that when",
      "start": 346.8,
      "duration": 3.839
    },
    {
      "text": "we developed the spam versus no spam",
      "start": 348.319,
      "duration": 5.121
    },
    {
      "text": "email classifier and when we use the GPT",
      "start": 350.639,
      "duration": 4.961
    },
    {
      "text": "to weights we used the weights from a",
      "start": 353.44,
      "duration": 5.479
    },
    {
      "text": "model which is 117 million parameter but",
      "start": 355.6,
      "duration": 5.439
    },
    {
      "text": "now we we have to use the model which is",
      "start": 358.919,
      "duration": 4.72
    },
    {
      "text": "much higher because we because I saw",
      "start": 361.039,
      "duration": 5.841
    },
    {
      "text": "that a smaller parameter which is 124",
      "start": 363.639,
      "duration": 5.68
    },
    {
      "text": "million parameter model does not perform",
      "start": 366.88,
      "duration": 4.599
    },
    {
      "text": "very well in the instruction fine tuning",
      "start": 369.319,
      "duration": 4.28
    },
    {
      "text": "task that's why we have to use a larger",
      "start": 371.479,
      "duration": 4.481
    },
    {
      "text": "model so as I've indicated on the",
      "start": 373.599,
      "duration": 5.32
    },
    {
      "text": "Whiteboard we will load gpt2 with 355",
      "start": 375.96,
      "duration": 6.079
    },
    {
      "text": "million model 355 million uh which are",
      "start": 378.919,
      "duration": 5.84
    },
    {
      "text": "the number of model",
      "start": 382.039,
      "duration": 5.201
    },
    {
      "text": "parameters okay awesome so instead of",
      "start": 384.759,
      "duration": 5.241
    },
    {
      "text": "using the smallest 124 million parameter",
      "start": 387.24,
      "duration": 5.2
    },
    {
      "text": "model as before we load the medium siiz",
      "start": 390.0,
      "duration": 5.88
    },
    {
      "text": "model with 355 million parameters right",
      "start": 392.44,
      "duration": 6.479
    },
    {
      "text": "and uh that takes approximately",
      "start": 395.88,
      "duration": 6.56
    },
    {
      "text": "1.42 gabt of space so please make sure",
      "start": 398.919,
      "duration": 5.361
    },
    {
      "text": "that as I'm showing you the code below",
      "start": 402.44,
      "duration": 3.599
    },
    {
      "text": "you have this much amount of space on",
      "start": 404.28,
      "duration": 4.84
    },
    {
      "text": "your local machine I'm not using any",
      "start": 406.039,
      "duration": 6.761
    },
    {
      "text": "fancy GPU or any uh High processing",
      "start": 409.12,
      "duration": 5.6
    },
    {
      "text": "speed computer I'm simply using a",
      "start": 412.8,
      "duration": 3.679
    },
    {
      "text": "MacBook Air",
      "start": 414.72,
      "duration": 4.4
    },
    {
      "text": "2020 so these These are the model",
      "start": 416.479,
      "duration": 5.12
    },
    {
      "text": "configur ation for which GPT 2's weights",
      "start": 419.12,
      "duration": 4.079
    },
    {
      "text": "are publicly released and we are going",
      "start": 421.599,
      "duration": 4.361
    },
    {
      "text": "to use the GPT medium which is the 355",
      "start": 423.199,
      "duration": 4.961
    },
    {
      "text": "million parameter model it has the",
      "start": 425.96,
      "duration": 5.48
    },
    {
      "text": "context size of 1024 it has",
      "start": 428.16,
      "duration": 7.12
    },
    {
      "text": "24 uh Transformer blocks and within each",
      "start": 431.44,
      "duration": 5.96
    },
    {
      "text": "Transformer block There are 16 attention",
      "start": 435.28,
      "duration": 4.8
    },
    {
      "text": "heads so in choose model we have chosen",
      "start": 437.4,
      "duration": 6.28
    },
    {
      "text": "gpt2 medium 355 million parameters and",
      "start": 440.08,
      "duration": 5.239
    },
    {
      "text": "in the base configuration we have",
      "start": 443.68,
      "duration": 4.88
    },
    {
      "text": "vocabulary size 50257 context length of",
      "start": 445.319,
      "duration": 7.28
    },
    {
      "text": "1024 right and uh dropout rate we are",
      "start": 448.56,
      "duration": 6.84
    },
    {
      "text": "using zero and the query key value bias",
      "start": 452.599,
      "duration": 5.32
    },
    {
      "text": "we are setting this to be equal to true",
      "start": 455.4,
      "duration": 4.44
    },
    {
      "text": "and that is because when we initialize",
      "start": 457.919,
      "duration": 3.641
    },
    {
      "text": "the query key and the value weight",
      "start": 459.84,
      "duration": 4.0
    },
    {
      "text": "matrices we are also going to initialize",
      "start": 461.56,
      "duration": 5.88
    },
    {
      "text": "the bias terms 50257 is essentially the",
      "start": 463.84,
      "duration": 5.44
    },
    {
      "text": "vocabulary size on which gpt2 was",
      "start": 467.44,
      "duration": 2.719
    },
    {
      "text": "trained",
      "start": 469.28,
      "duration": 3.359
    },
    {
      "text": "on then what we are going to do is that",
      "start": 470.159,
      "duration": 4.081
    },
    {
      "text": "we are going to run this download and",
      "start": 472.639,
      "duration": 4.321
    },
    {
      "text": "load gpt2 function and I'm just going to",
      "start": 474.24,
      "duration": 4.959
    },
    {
      "text": "show you this function briefly what this",
      "start": 476.96,
      "duration": 4.44
    },
    {
      "text": "down download and load gpt2 function",
      "start": 479.199,
      "duration": 5.641
    },
    {
      "text": "actually does is that it uh it accesses",
      "start": 481.4,
      "duration": 6.079
    },
    {
      "text": "this URL and it downloads the seven",
      "start": 484.84,
      "duration": 5.28
    },
    {
      "text": "files onto the local machine so you can",
      "start": 487.479,
      "duration": 4.68
    },
    {
      "text": "even access these files on kaggle if you",
      "start": 490.12,
      "duration": 4.88
    },
    {
      "text": "click on this 355 if you click on the",
      "start": 492.159,
      "duration": 5.44
    },
    {
      "text": "355 million and click on the seven files",
      "start": 495.0,
      "duration": 4.039
    },
    {
      "text": "you can get these seven files and you",
      "start": 497.599,
      "duration": 4.121
    },
    {
      "text": "can download it to your local PC however",
      "start": 499.039,
      "duration": 4.28
    },
    {
      "text": "just downloading these files is not",
      "start": 501.72,
      "duration": 3.599
    },
    {
      "text": "enough you need to do some",
      "start": 503.319,
      "duration": 3.961
    },
    {
      "text": "pre-processing steps on this file so",
      "start": 505.319,
      "duration": 5.401
    },
    {
      "text": "that um so check this function which is",
      "start": 507.28,
      "duration": 6.08
    },
    {
      "text": "load gpt2 parameters from the tensorflow",
      "start": 510.72,
      "duration": 4.52
    },
    {
      "text": "checkpoint what this does is that after",
      "start": 513.36,
      "duration": 4.72
    },
    {
      "text": "downloading the parameters uh from the",
      "start": 515.24,
      "duration": 4.799
    },
    {
      "text": "seven files it actually converts it into",
      "start": 518.08,
      "duration": 4.759
    },
    {
      "text": "a specific dictionary and through this",
      "start": 520.039,
      "duration": 4.721
    },
    {
      "text": "dictionary you can handle or access",
      "start": 522.839,
      "duration": 3.641
    },
    {
      "text": "these parameters in a much more easier",
      "start": 524.76,
      "duration": 3.92
    },
    {
      "text": "manner we have covered the details of",
      "start": 526.48,
      "duration": 3.72
    },
    {
      "text": "this in a previous lecture which is",
      "start": 528.68,
      "duration": 4.32
    },
    {
      "text": "titled pre-training using gpt2 weights",
      "start": 530.2,
      "duration": 4.44
    },
    {
      "text": "and that's a 1 hour lecture in which",
      "start": 533.0,
      "duration": 3.32
    },
    {
      "text": "this code has been explained in a lot of",
      "start": 534.64,
      "duration": 4.12
    },
    {
      "text": "detail for now all you need to know is",
      "start": 536.32,
      "duration": 4.4
    },
    {
      "text": "that when you you run this code it first",
      "start": 538.76,
      "duration": 3.8
    },
    {
      "text": "of all downloads the seven files it",
      "start": 540.72,
      "duration": 4.559
    },
    {
      "text": "creates a folder called 355 million and",
      "start": 542.56,
      "duration": 5.44
    },
    {
      "text": "it downloads the seven files and then it",
      "start": 545.279,
      "duration": 5.321
    },
    {
      "text": "also has this the thing which it returns",
      "start": 548.0,
      "duration": 4.959
    },
    {
      "text": "mostly is that this params dictionary",
      "start": 550.6,
      "duration": 3.88
    },
    {
      "text": "the params dictionary essentially",
      "start": 552.959,
      "duration": 3.44
    },
    {
      "text": "consists of the Transformer blocks the",
      "start": 554.48,
      "duration": 3.32
    },
    {
      "text": "token embeddings the positional",
      "start": 556.399,
      "duration": 3.721
    },
    {
      "text": "embeddings the final normalization layer",
      "start": 557.8,
      "duration": 4.64
    },
    {
      "text": "parameters which are very accessible if",
      "start": 560.12,
      "duration": 4.36
    },
    {
      "text": "you just download the seven files it",
      "start": 562.44,
      "duration": 4.72
    },
    {
      "text": "becomes difficult for us to integrate",
      "start": 564.48,
      "duration": 4.24
    },
    {
      "text": "these parameters with the code which we",
      "start": 567.16,
      "duration": 3.359
    },
    {
      "text": "have written earlier here that's why",
      "start": 568.72,
      "duration": 4.64
    },
    {
      "text": "it's very essential for you to run this",
      "start": 570.519,
      "duration": 6.241
    },
    {
      "text": "load gpt2 params from TF checkpoint uh",
      "start": 573.36,
      "duration": 7.2
    },
    {
      "text": "so that the params dictionary is created",
      "start": 576.76,
      "duration": 6.0
    },
    {
      "text": "successfully uh if you don't understand",
      "start": 580.56,
      "duration": 3.959
    },
    {
      "text": "this code right now it's fine all you'll",
      "start": 582.76,
      "duration": 4.079
    },
    {
      "text": "have to do is just execute this line of",
      "start": 584.519,
      "duration": 4.281
    },
    {
      "text": "code which is settings comma params",
      "start": 586.839,
      "duration": 4.68
    },
    {
      "text": "equal to download and load gpt2 what",
      "start": 588.8,
      "duration": 4.279
    },
    {
      "text": "this will essentially do is that first",
      "start": 591.519,
      "duration": 4.241
    },
    {
      "text": "of all from GPT download 3 we have to",
      "start": 593.079,
      "duration": 4.961
    },
    {
      "text": "import this function so this file which",
      "start": 595.76,
      "duration": 4.0
    },
    {
      "text": "I showed you over here is called GPT",
      "start": 598.04,
      "duration": 4.16
    },
    {
      "text": "download 3py from this file we are",
      "start": 599.76,
      "duration": 5.079
    },
    {
      "text": "importing this function and uh the",
      "start": 602.2,
      "duration": 4.319
    },
    {
      "text": "results of that function we are storing",
      "start": 604.839,
      "duration": 3.401
    },
    {
      "text": "in something called settings comma",
      "start": 606.519,
      "duration": 5.121
    },
    {
      "text": "params so when you run up till here the",
      "start": 608.24,
      "duration": 6.68
    },
    {
      "text": "1.42 gbt model which has been shown over",
      "start": 611.64,
      "duration": 5.879
    },
    {
      "text": "here so see this is the 1.42 GB that",
      "start": 614.92,
      "duration": 5.12
    },
    {
      "text": "will be downloaded so for me this took a",
      "start": 617.519,
      "duration": 3.961
    },
    {
      "text": "long time to download Because the",
      "start": 620.04,
      "duration": 3.359
    },
    {
      "text": "Internet connection was not very strong",
      "start": 621.48,
      "duration": 4.599
    },
    {
      "text": "and uh the file kept pausing a lot in",
      "start": 623.399,
      "duration": 5.201
    },
    {
      "text": "the middle so please make sure that you",
      "start": 626.079,
      "duration": 4.361
    },
    {
      "text": "are sitting in an area which has strong",
      "start": 628.6,
      "duration": 3.72
    },
    {
      "text": "internet connection and you have memory",
      "start": 630.44,
      "duration": 5.36
    },
    {
      "text": "on your device if you have 1.4 or 1.5 GB",
      "start": 632.32,
      "duration": 6.959
    },
    {
      "text": "in a strong uh internet area I think",
      "start": 635.8,
      "duration": 5.159
    },
    {
      "text": "this download should happen in a much",
      "start": 639.279,
      "duration": 3.721
    },
    {
      "text": "quicker Manner and then what we are",
      "start": 640.959,
      "duration": 4.32
    },
    {
      "text": "going to do is that after we download",
      "start": 643.0,
      "duration": 3.959
    },
    {
      "text": "these weights we are going to load",
      "start": 645.279,
      "duration": 4.36
    },
    {
      "text": "weights into GPT what this function does",
      "start": 646.959,
      "duration": 5.081
    },
    {
      "text": "is that we have constructed this GPT",
      "start": 649.639,
      "duration": 4.88
    },
    {
      "text": "architecture right at all of the places",
      "start": 652.04,
      "duration": 4.12
    },
    {
      "text": "where there are trainable parameters",
      "start": 654.519,
      "duration": 4.32
    },
    {
      "text": "this function appropriately Maps the",
      "start": 656.16,
      "duration": 5.04
    },
    {
      "text": "download parameters in the params",
      "start": 658.839,
      "duration": 5.481
    },
    {
      "text": "dictionary into our model so remember I",
      "start": 661.2,
      "duration": 5.4
    },
    {
      "text": "mentioned that this function essentially",
      "start": 664.32,
      "duration": 4.639
    },
    {
      "text": "returns this dictionary called as params",
      "start": 666.6,
      "duration": 4.679
    },
    {
      "text": "in which the weights and parameters have",
      "start": 668.959,
      "duration": 4.641
    },
    {
      "text": "been arranged in a specific format when",
      "start": 671.279,
      "duration": 4.081
    },
    {
      "text": "you run this function load weights into",
      "start": 673.6,
      "duration": 4.88
    },
    {
      "text": "GPT the parameters from that params",
      "start": 675.36,
      "duration": 5.32
    },
    {
      "text": "dictionary are downloaded into our",
      "start": 678.48,
      "duration": 5.76
    },
    {
      "text": "model and you can actually control F",
      "start": 680.68,
      "duration": 5.36
    },
    {
      "text": "contrl F",
      "start": 684.24,
      "duration": 3.76
    },
    {
      "text": "load",
      "start": 686.04,
      "duration": 4.16
    },
    {
      "text": "load with",
      "start": 688.0,
      "duration": 5.639
    },
    {
      "text": "into GPT and you'll see that we have",
      "start": 690.2,
      "duration": 5.319
    },
    {
      "text": "defined this function before load",
      "start": 693.639,
      "duration": 5.401
    },
    {
      "text": "weights into GPT um what this function",
      "start": 695.519,
      "duration": 5.32
    },
    {
      "text": "asence essentially does is that the",
      "start": 699.04,
      "duration": 4.08
    },
    {
      "text": "params dictionary it Maps the values",
      "start": 700.839,
      "duration": 4.281
    },
    {
      "text": "extracted from the params dictionary",
      "start": 703.12,
      "duration": 4.04
    },
    {
      "text": "into the GPT architecture which we have",
      "start": 705.12,
      "duration": 4.12
    },
    {
      "text": "constructed before so you can think of",
      "start": 707.16,
      "duration": 4.16
    },
    {
      "text": "this whole code code block as one",
      "start": 709.24,
      "duration": 4.24
    },
    {
      "text": "assignment step we are assigning the",
      "start": 711.32,
      "duration": 4.56
    },
    {
      "text": "downloaded parameters to our model we",
      "start": 713.48,
      "duration": 4.2
    },
    {
      "text": "have had a separate lecture to explain",
      "start": 715.88,
      "duration": 3.639
    },
    {
      "text": "this fully so I'm not covering this in",
      "start": 717.68,
      "duration": 4.719
    },
    {
      "text": "detail right now let me take you to the",
      "start": 719.519,
      "duration": 6.041
    },
    {
      "text": "current code which we are on so uh then",
      "start": 722.399,
      "duration": 4.88
    },
    {
      "text": "you have to run after you run the",
      "start": 725.56,
      "duration": 3.24
    },
    {
      "text": "settings comma params then you have to",
      "start": 727.279,
      "duration": 3.921
    },
    {
      "text": "run load weights into GPT which loads",
      "start": 728.8,
      "duration": 4.24
    },
    {
      "text": "the downloaded weights from the 355",
      "start": 731.2,
      "duration": 5.28
    },
    {
      "text": "million model into uh your GPT",
      "start": 733.04,
      "duration": 5.2
    },
    {
      "text": "architecture and then you set the model",
      "start": 736.48,
      "duration": 5.08
    },
    {
      "text": "into evaluation mode so if your laptop",
      "start": 738.24,
      "duration": 5.2
    },
    {
      "text": "does not have a strong configuration",
      "start": 741.56,
      "duration": 3.6
    },
    {
      "text": "with respect to memory or processing",
      "start": 743.44,
      "duration": 3.959
    },
    {
      "text": "speed you can even choose the model here",
      "start": 745.16,
      "duration": 4.84
    },
    {
      "text": "to be gpt2 small which which has 124",
      "start": 747.399,
      "duration": 4.401
    },
    {
      "text": "million parameters and that should take",
      "start": 750.0,
      "duration": 4.24
    },
    {
      "text": "you onethird of the time or half of the",
      "start": 751.8,
      "duration": 4.8
    },
    {
      "text": "time it takes you to load the weights of",
      "start": 754.24,
      "duration": 5.48
    },
    {
      "text": "this GPT to medium model if on the other",
      "start": 756.6,
      "duration": 4.96
    },
    {
      "text": "hand your laptop has a very high",
      "start": 759.72,
      "duration": 4.16
    },
    {
      "text": "processing speed and if you have GPU",
      "start": 761.56,
      "duration": 4.519
    },
    {
      "text": "access I recommend you can use gpt2",
      "start": 763.88,
      "duration": 5.0
    },
    {
      "text": "large or even gpt2 X Excel if you have",
      "start": 766.079,
      "duration": 5.281
    },
    {
      "text": "GPU access because then the results",
      "start": 768.88,
      "duration": 4.079
    },
    {
      "text": "which you will get will be much better",
      "start": 771.36,
      "duration": 4.919
    },
    {
      "text": "than uh what we will obtain with smaller",
      "start": 772.959,
      "duration": 5.921
    },
    {
      "text": "models okay when you run when you run",
      "start": 776.279,
      "duration": 5.36
    },
    {
      "text": "this file you'll see that you'll get",
      "start": 778.88,
      "duration": 4.92
    },
    {
      "text": "outputs such as these I I have already",
      "start": 781.639,
      "duration": 4.921
    },
    {
      "text": "downloaded the gpt2 parameters so I'm",
      "start": 783.8,
      "duration": 4.399
    },
    {
      "text": "getting file already",
      "start": 786.56,
      "duration": 4.24
    },
    {
      "text": "exists now what we can do is that before",
      "start": 788.199,
      "duration": 4.401
    },
    {
      "text": "diving into fine tuning the model which",
      "start": 790.8,
      "duration": 4.479
    },
    {
      "text": "we'll come to in the subsequent section",
      "start": 792.6,
      "duration": 4.4
    },
    {
      "text": "what we can do is that we can actually",
      "start": 795.279,
      "duration": 4.36
    },
    {
      "text": "check the pre- trend llms performance on",
      "start": 797.0,
      "duration": 5.04
    },
    {
      "text": "one of the validation tasks by comparing",
      "start": 799.639,
      "duration": 4.56
    },
    {
      "text": "its output to the expected response we",
      "start": 802.04,
      "duration": 4.0
    },
    {
      "text": "have not even trained or fine-tuned our",
      "start": 804.199,
      "duration": 5.041
    },
    {
      "text": "llm on the data set yet on this data set",
      "start": 806.04,
      "duration": 5.0
    },
    {
      "text": "but I just want to check the performance",
      "start": 809.24,
      "duration": 4.56
    },
    {
      "text": "of this llm by taking some very specific",
      "start": 811.04,
      "duration": 5.12
    },
    {
      "text": "example so what I'm going to do now is",
      "start": 813.8,
      "duration": 4.32
    },
    {
      "text": "that I'm going to take this example from",
      "start": 816.16,
      "duration": 4.08
    },
    {
      "text": "our data set and the example is that",
      "start": 818.12,
      "duration": 4.04
    },
    {
      "text": "below is an instruction that describes a",
      "start": 820.24,
      "duration": 4.52
    },
    {
      "text": "task Write a response that appropriately",
      "start": 822.16,
      "duration": 5.08
    },
    {
      "text": "completes the request and the",
      "start": 824.76,
      "duration": 4.8
    },
    {
      "text": "instruction is that convert the active",
      "start": 827.24,
      "duration": 4.36
    },
    {
      "text": "sentence to passive and the sentence is",
      "start": 829.56,
      "duration": 5.36
    },
    {
      "text": "the chef Cooks the meal every day um and",
      "start": 831.6,
      "duration": 5.039
    },
    {
      "text": "you can see that I've taken this",
      "start": 834.92,
      "duration": 3.839
    },
    {
      "text": "sentence from the validation data it is",
      "start": 836.639,
      "duration": 3.841
    },
    {
      "text": "the first sentence from the validation",
      "start": 838.759,
      "duration": 3.64
    },
    {
      "text": "data so actually let me check it out",
      "start": 840.48,
      "duration": 5.039
    },
    {
      "text": "over here so if I put Chef yeah this is",
      "start": 842.399,
      "duration": 5.8
    },
    {
      "text": "the instruction input and output so the",
      "start": 845.519,
      "duration": 4.201
    },
    {
      "text": "instruction is convert the active",
      "start": 848.199,
      "duration": 3.241
    },
    {
      "text": "sentence to passive the chef Cooks the",
      "start": 849.72,
      "duration": 3.28
    },
    {
      "text": "meal every",
      "start": 851.44,
      "duration": 4.56
    },
    {
      "text": "day and the correct",
      "start": 853.0,
      "duration": 7.399
    },
    {
      "text": "output uh I think",
      "start": 856.0,
      "duration": 4.399
    },
    {
      "text": "convert yeah I think this is the one let",
      "start": 862.8,
      "duration": 5.2
    },
    {
      "text": "me check it again yeah the instruction",
      "start": 865.519,
      "duration": 4.081
    },
    {
      "text": "is convert the active sentence to",
      "start": 868.0,
      "duration": 3.12
    },
    {
      "text": "passive the chef Cooks the meal every",
      "start": 869.6,
      "duration": 3.919
    },
    {
      "text": "day it's this sentence exactly and the",
      "start": 871.12,
      "duration": 4.2
    },
    {
      "text": "correct output is the meal is cooked by",
      "start": 873.519,
      "duration": 3.921
    },
    {
      "text": "the chef every day so if the G if the",
      "start": 875.32,
      "duration": 4.36
    },
    {
      "text": "llm is fine tuned correctly the output",
      "start": 877.44,
      "duration": 3.959
    },
    {
      "text": "which it should give should somewhat be",
      "start": 879.68,
      "duration": 3.839
    },
    {
      "text": "of a passive tense so if the active",
      "start": 881.399,
      "duration": 3.8
    },
    {
      "text": "tense is the chef Cooks the meal every",
      "start": 883.519,
      "duration": 3.401
    },
    {
      "text": "day the passive tense should be the meal",
      "start": 885.199,
      "duration": 4.241
    },
    {
      "text": "is cooked by the chef every day we have",
      "start": 886.92,
      "duration": 4.64
    },
    {
      "text": "not trained our llm on this data at all",
      "start": 889.44,
      "duration": 4.12
    },
    {
      "text": "so we are not expecting to get a correct",
      "start": 891.56,
      "duration": 4.079
    },
    {
      "text": "answer but I just want to see how wrong",
      "start": 893.56,
      "duration": 4.079
    },
    {
      "text": "we are or how far off we are from the",
      "start": 895.639,
      "duration": 4.281
    },
    {
      "text": "correct answer so you can use the",
      "start": 897.639,
      "duration": 3.841
    },
    {
      "text": "generate function which you have defined",
      "start": 899.92,
      "duration": 3.4
    },
    {
      "text": "previously what this function does is",
      "start": 901.48,
      "duration": 3.32
    },
    {
      "text": "that you have to pass in the maximum",
      "start": 903.32,
      "duration": 3.92
    },
    {
      "text": "number of new tokens to generate and",
      "start": 904.8,
      "duration": 4.599
    },
    {
      "text": "then based on the model and based on the",
      "start": 907.24,
      "duration": 4.44
    },
    {
      "text": "trained weights remember we are just",
      "start": 909.399,
      "duration": 4.401
    },
    {
      "text": "using the weights downloaded from GPT to",
      "start": 911.68,
      "duration": 4.76
    },
    {
      "text": "medium based on this based on these",
      "start": 913.8,
      "duration": 4.44
    },
    {
      "text": "weights the next tokens or the new",
      "start": 916.44,
      "duration": 3.8
    },
    {
      "text": "tokens will be generate new tokens will",
      "start": 918.24,
      "duration": 4.399
    },
    {
      "text": "be generated and here we have to specify",
      "start": 920.24,
      "duration": 4.2
    },
    {
      "text": "the maximum number of new tokens which",
      "start": 922.639,
      "duration": 3.601
    },
    {
      "text": "I'm mentioning to be",
      "start": 924.44,
      "duration": 4.639
    },
    {
      "text": "35 and the context length is of course",
      "start": 926.24,
      "duration": 5.76
    },
    {
      "text": "1024 which is the context length of the",
      "start": 929.079,
      "duration": 5.041
    },
    {
      "text": "medium",
      "start": 932.0,
      "duration": 4.399
    },
    {
      "text": "configuration uh that's it actually and",
      "start": 934.12,
      "duration": 4.32
    },
    {
      "text": "then you have to pass in the input text",
      "start": 936.399,
      "duration": 4.36
    },
    {
      "text": "so I'm passing in the input text over",
      "start": 938.44,
      "duration": 4.0
    },
    {
      "text": "here and then the model which is the",
      "start": 940.759,
      "duration": 4.32
    },
    {
      "text": "model with the pre-trained weights and",
      "start": 942.44,
      "duration": 4.12
    },
    {
      "text": "then the generated text we have to",
      "start": 945.079,
      "duration": 4.68
    },
    {
      "text": "convert it back from token IDs into text",
      "start": 946.56,
      "duration": 5.199
    },
    {
      "text": "so let us print this out right now and",
      "start": 949.759,
      "duration": 4.32
    },
    {
      "text": "let us see the",
      "start": 951.759,
      "duration": 5.281
    },
    {
      "text": "response so the response here so one",
      "start": 954.079,
      "duration": 4.521
    },
    {
      "text": "thing to mention is that when you use",
      "start": 957.04,
      "duration": 4.359
    },
    {
      "text": "the erated text uh it Returns the",
      "start": 958.6,
      "duration": 5.44
    },
    {
      "text": "combined input as well as output this",
      "start": 961.399,
      "duration": 4.521
    },
    {
      "text": "Behavior was convenient in the previous",
      "start": 964.04,
      "duration": 4.2
    },
    {
      "text": "chapters because pre-trained llms are",
      "start": 965.92,
      "duration": 5.0
    },
    {
      "text": "primarily designed to complete the text",
      "start": 968.24,
      "duration": 4.36
    },
    {
      "text": "right so we can predict the input and we",
      "start": 970.92,
      "duration": 3.479
    },
    {
      "text": "can predict the output so it will just",
      "start": 972.6,
      "duration": 3.679
    },
    {
      "text": "look like an input output pair where the",
      "start": 974.399,
      "duration": 4.24
    },
    {
      "text": "input is completed but now we actually",
      "start": 976.279,
      "duration": 4.401
    },
    {
      "text": "just need to focus on the model",
      "start": 978.639,
      "duration": 4.161
    },
    {
      "text": "generated response right every time we",
      "start": 980.68,
      "duration": 4.2
    },
    {
      "text": "print the generated text we don't need",
      "start": 982.8,
      "duration": 4.0
    },
    {
      "text": "the input text we don't need the",
      "start": 984.88,
      "duration": 3.68
    },
    {
      "text": "instruction we just need the response",
      "start": 986.8,
      "duration": 2.76
    },
    {
      "text": "right",
      "start": 988.56,
      "duration": 2.88
    },
    {
      "text": "uh so now what I'm doing is that when",
      "start": 989.56,
      "duration": 3.6
    },
    {
      "text": "you print out the response we need to",
      "start": 991.44,
      "duration": 3.28
    },
    {
      "text": "subtract the length of the input",
      "start": 993.16,
      "duration": 3.28
    },
    {
      "text": "instruction from the start of the",
      "start": 994.72,
      "duration": 4.4
    },
    {
      "text": "generated text so we have to just print",
      "start": 996.44,
      "duration": 4.92
    },
    {
      "text": "out the response here so we mention that",
      "start": 999.12,
      "duration": 4.12
    },
    {
      "text": "you subtract the input you subtract the",
      "start": 1001.36,
      "duration": 4.08
    },
    {
      "text": "instruction and just print the response",
      "start": 1003.24,
      "duration": 4.279
    },
    {
      "text": "text so here is the response which our",
      "start": 1005.44,
      "duration": 4.399
    },
    {
      "text": "model gives the response which is given",
      "start": 1007.519,
      "duration": 4.041
    },
    {
      "text": "by the model is the chef Cooks the meal",
      "start": 1009.839,
      "duration": 3.56
    },
    {
      "text": "every day so it has just recycled the",
      "start": 1011.56,
      "duration": 4.48
    },
    {
      "text": "first sentence in the response itself it",
      "start": 1013.399,
      "duration": 5.081
    },
    {
      "text": "has included an instruction and in the",
      "start": 1016.04,
      "duration": 4.159
    },
    {
      "text": "instruction it is retaining the same",
      "start": 1018.48,
      "duration": 3.359
    },
    {
      "text": "instruction convert the active sentence",
      "start": 1020.199,
      "duration": 4.081
    },
    {
      "text": "to passive the shf cooks the which means",
      "start": 1021.839,
      "duration": 4.12
    },
    {
      "text": "that the model has not at all followed",
      "start": 1024.28,
      "duration": 3.279
    },
    {
      "text": "my",
      "start": 1025.959,
      "duration": 4.48
    },
    {
      "text": "instruction uh in fact the pre-train",
      "start": 1027.559,
      "duration": 4.841
    },
    {
      "text": "model is not yet capable of currently",
      "start": 1030.439,
      "duration": 3.24
    },
    {
      "text": "correctly following the given",
      "start": 1032.4,
      "duration": 3.48
    },
    {
      "text": "instruction it creates a response",
      "start": 1033.679,
      "duration": 4.601
    },
    {
      "text": "section that is good but it simply",
      "start": 1035.88,
      "duration": 4.6
    },
    {
      "text": "repeats the original input sentence it",
      "start": 1038.28,
      "duration": 3.96
    },
    {
      "text": "simply repeats the original input",
      "start": 1040.48,
      "duration": 3.92
    },
    {
      "text": "sentence and it also repeats a part of",
      "start": 1042.24,
      "duration": 4.599
    },
    {
      "text": "the instruction just as it is but it",
      "start": 1044.4,
      "duration": 4.6
    },
    {
      "text": "fails to convert the active sentence to",
      "start": 1046.839,
      "duration": 5.08
    },
    {
      "text": "passive voice that's the main uh thing",
      "start": 1049.0,
      "duration": 4.96
    },
    {
      "text": "which I want to convey to you that",
      "start": 1051.919,
      "duration": 4.441
    },
    {
      "text": "without fine-tuning the model itself",
      "start": 1053.96,
      "duration": 5.24
    },
    {
      "text": "just using the pre-trained weights it's",
      "start": 1056.36,
      "duration": 5.36
    },
    {
      "text": "not doing a good job at all in fact it",
      "start": 1059.2,
      "duration": 4.32
    },
    {
      "text": "fails to convert the active sentence to",
      "start": 1061.72,
      "duration": 3.839
    },
    {
      "text": "the passive it's recycling the same text",
      "start": 1063.52,
      "duration": 4.08
    },
    {
      "text": "which we provided as an instruction and",
      "start": 1065.559,
      "duration": 3.841
    },
    {
      "text": "it's creating this hashtag hashtag",
      "start": 1067.6,
      "duration": 3.48
    },
    {
      "text": "hashtag instruction in the response",
      "start": 1069.4,
      "duration": 4.279
    },
    {
      "text": "itself that is also not good so now what",
      "start": 1071.08,
      "duration": 4.4
    },
    {
      "text": "we'll do in the next section is that we",
      "start": 1073.679,
      "duration": 3.641
    },
    {
      "text": "are going to implement the finetuning",
      "start": 1075.48,
      "duration": 4.559
    },
    {
      "text": "process to imp to improve the model's",
      "start": 1077.32,
      "duration": 5.16
    },
    {
      "text": "ability to comprehend and appropriately",
      "start": 1080.039,
      "duration": 4.76
    },
    {
      "text": "respond to such requests so the reason",
      "start": 1082.48,
      "duration": 4.559
    },
    {
      "text": "fine tuning exists in the first place is",
      "start": 1084.799,
      "duration": 4.481
    },
    {
      "text": "because without fine tuning even if we",
      "start": 1087.039,
      "duration": 4.961
    },
    {
      "text": "load the weights from a parameter which",
      "start": 1089.28,
      "duration": 5.759
    },
    {
      "text": "is 355 million or even if you load from",
      "start": 1092.0,
      "duration": 5.88
    },
    {
      "text": "774 million you'll see that even for a",
      "start": 1095.039,
      "duration": 7.481
    },
    {
      "text": "774 million gpt2 param gpt2 model the",
      "start": 1097.88,
      "duration": 7.08
    },
    {
      "text": "response is not coherent without fine",
      "start": 1102.52,
      "duration": 4.519
    },
    {
      "text": "tuning and that's why in the next",
      "start": 1104.96,
      "duration": 3.48
    },
    {
      "text": "section we are going to look at fine",
      "start": 1107.039,
      "duration": 4.0
    },
    {
      "text": "tuning the llm on instruction data which",
      "start": 1108.44,
      "duration": 4.719
    },
    {
      "text": "means that we'll actually model or we'll",
      "start": 1111.039,
      "duration": 3.841
    },
    {
      "text": "actually modify the weights and",
      "start": 1113.159,
      "duration": 4.76
    },
    {
      "text": "parameters of this GPT model so that it",
      "start": 1114.88,
      "duration": 5.48
    },
    {
      "text": "can try to understand these instruction",
      "start": 1117.919,
      "duration": 4.681
    },
    {
      "text": "input output pairs from this specific",
      "start": 1120.36,
      "duration": 3.92
    },
    {
      "text": "data",
      "start": 1122.6,
      "duration": 4.16
    },
    {
      "text": "set uh okay everyone this brings us to",
      "start": 1124.28,
      "duration": 4.24
    },
    {
      "text": "the end of the lecture where we looked",
      "start": 1126.76,
      "duration": 4.64
    },
    {
      "text": "at loading a pre-trained llm I initially",
      "start": 1128.52,
      "duration": 4.8
    },
    {
      "text": "plan to cover the fine tuning in this",
      "start": 1131.4,
      "duration": 3.639
    },
    {
      "text": "lecture itself but I thought it will be",
      "start": 1133.32,
      "duration": 3.719
    },
    {
      "text": "good to cover it in the next lecture",
      "start": 1135.039,
      "duration": 3.441
    },
    {
      "text": "otherwise the duration of the lecture",
      "start": 1137.039,
      "duration": 3.64
    },
    {
      "text": "would have been pretty long I hope you",
      "start": 1138.48,
      "duration": 3.88
    },
    {
      "text": "are liking these lectures we are now",
      "start": 1140.679,
      "duration": 3.521
    },
    {
      "text": "very close to actually finishing the",
      "start": 1142.36,
      "duration": 4.64
    },
    {
      "text": "entire finetuning and then extracting",
      "start": 1144.2,
      "duration": 5.32
    },
    {
      "text": "the responses evaluating the responses",
      "start": 1147.0,
      "duration": 3.919
    },
    {
      "text": "and the code file which I'm going to",
      "start": 1149.52,
      "duration": 3.48
    },
    {
      "text": "share with you can be used to perform a",
      "start": 1150.919,
      "duration": 3.88
    },
    {
      "text": "wide range of instruction fine tuning",
      "start": 1153.0,
      "duration": 4.24
    },
    {
      "text": "tasks so thanks a lot everyone in the",
      "start": 1154.799,
      "duration": 4.681
    },
    {
      "text": "next lecture I'll explain the",
      "start": 1157.24,
      "duration": 4.559
    },
    {
      "text": "fine-tuning the model process and I look",
      "start": 1159.48,
      "duration": 3.76
    },
    {
      "text": "forward to seeing you in the next",
      "start": 1161.799,
      "duration": 4.441
    },
    {
      "text": "lecture",
      "start": 1163.24,
      "duration": 3.0
    }
  ],
  "full_text": "[Music] hello everyone and welcome to this lecture in the build large language models from scratch Series today we are going to look at step number four in the instruction fine-tuning Hands-On project which we have started and that step is loading a pre-trained large language model until now we have looked at three steps the first one involved data set download and formatting the second involved batching the data set and the third involved creating data loaders so here's the data set which we have been looking at which consists of 1100 entries of instructions inputs and outputs they are in various fields from English to geography to general knowledge Etc and essentially through this data set our aim is to teach the llm to be able to correctly follow instructions um and that's what finetuning refers to in pre-training if we just used a pre-trained llm it does not follow instructions very well so that's why we need to train it with a specific data set now before we come to loading a pre-trend llm we first needed to spend a lot of time on downloading this data set then converting it into a format called The alpaka Prompt style then batching the data set which is creating different batches and ultimately creating data loaders now that we we are ready with the data loaders we can go ahead and load a pre-trained llm what does it mean loading a pre-trained llm well what we are going to do here is that this is the llm architecture which we have constructed in this lecture series and there are several places where there are trainable weights so for example in multi-head attention there are the query key value weight metries then in feed forward neural network there are the neurons and their weights in layer normalization one and two there are the scale and the shift parameters which have parameters or weights associated with it then there is the token embedding layer and the positional embedding layer there are trainable weights associated with these two layers as well if you add up all of these trainable weights then we have more than 1 million or more than 100 million trainable such parameters uh in the pre-training process these parameters are trained on a huge amount of data so for example gpt2 gpt2 if you search about gpt2 weights released um gpt2 weights released open AI actually pre-trained gpt2 on a huge amount of data and not just that when they pre-trained the llm and optimized the trainable weights optimized all of these weights which I just described over here they released those weights publicly which means that as users we can have access to those weights and we can utilize those weights now when open a released their weights they released it for four different models so if you see the open a g pt2 weights they released it for actually multiple models 124 million 150 1558 million 345 million 774 million Etc so you have different folders corresponding to the parameter size and then based on each folder you can download these weights on your local PC why are we downloading these weights in the first place before fine tuning the reason we are downloading these weights and reusing them is that when the when we reuse these weights it means that a lot of information is already captured correctly so for example when we do token embedding we want to project the words or tokens into a higher dimensional Vector space right and we want to project them so that the semantic meaning is captured to train a huge neural network again from scratch here would take very a very long time for us and computational resources instead we will directly use the pre-trained gpt2 weights so that will help us in fact in all of these parameters reusing the gpt2 weights actually helps us uh because the model now starts from a much more knowledgeable State instead of a random initialization so the main goal of reusing the pre-trained weights is that the model starts from a knowledgeable state instead of instead of a random initialization so we are going to load these pre-train weights and then we are going to train on the specific data set which we have curated so we are going to load the pre-train weights and then use these training data loaders testing data loaders and validation data loaders so the weights and parameters which we load from gpt2 of course they will change during the process because we will do one more training procedure on the specific data set but at least we won't start from a random state of all these parameters we'll start from a knowledgeable State and so the computational time it will take to fine tune or to train on our specific uh data set will be much lower so that will be much more efficient that's why the fine tuning process always happens after the pre-training process in the pre-training we always take the model to a knowledgeable state and then we train it again on a specific data set so that it can further improve its weights and its parameters so now let us go into code and let us load the pre-train llm which means that let us use the weights um which we get from gpt2 one thing which I would like to mention here is that when we developed the spam versus no spam email classifier and when we use the GPT to weights we used the weights from a model which is 117 million parameter but now we we have to use the model which is much higher because we because I saw that a smaller parameter which is 124 million parameter model does not perform very well in the instruction fine tuning task that's why we have to use a larger model so as I've indicated on the Whiteboard we will load gpt2 with 355 million model 355 million uh which are the number of model parameters okay awesome so instead of using the smallest 124 million parameter model as before we load the medium siiz model with 355 million parameters right and uh that takes approximately 1.42 gabt of space so please make sure that as I'm showing you the code below you have this much amount of space on your local machine I'm not using any fancy GPU or any uh High processing speed computer I'm simply using a MacBook Air 2020 so these These are the model configur ation for which GPT 2's weights are publicly released and we are going to use the GPT medium which is the 355 million parameter model it has the context size of 1024 it has 24 uh Transformer blocks and within each Transformer block There are 16 attention heads so in choose model we have chosen gpt2 medium 355 million parameters and in the base configuration we have vocabulary size 50257 context length of 1024 right and uh dropout rate we are using zero and the query key value bias we are setting this to be equal to true and that is because when we initialize the query key and the value weight matrices we are also going to initialize the bias terms 50257 is essentially the vocabulary size on which gpt2 was trained on then what we are going to do is that we are going to run this download and load gpt2 function and I'm just going to show you this function briefly what this down download and load gpt2 function actually does is that it uh it accesses this URL and it downloads the seven files onto the local machine so you can even access these files on kaggle if you click on this 355 if you click on the 355 million and click on the seven files you can get these seven files and you can download it to your local PC however just downloading these files is not enough you need to do some pre-processing steps on this file so that um so check this function which is load gpt2 parameters from the tensorflow checkpoint what this does is that after downloading the parameters uh from the seven files it actually converts it into a specific dictionary and through this dictionary you can handle or access these parameters in a much more easier manner we have covered the details of this in a previous lecture which is titled pre-training using gpt2 weights and that's a 1 hour lecture in which this code has been explained in a lot of detail for now all you need to know is that when you you run this code it first of all downloads the seven files it creates a folder called 355 million and it downloads the seven files and then it also has this the thing which it returns mostly is that this params dictionary the params dictionary essentially consists of the Transformer blocks the token embeddings the positional embeddings the final normalization layer parameters which are very accessible if you just download the seven files it becomes difficult for us to integrate these parameters with the code which we have written earlier here that's why it's very essential for you to run this load gpt2 params from TF checkpoint uh so that the params dictionary is created successfully uh if you don't understand this code right now it's fine all you'll have to do is just execute this line of code which is settings comma params equal to download and load gpt2 what this will essentially do is that first of all from GPT download 3 we have to import this function so this file which I showed you over here is called GPT download 3py from this file we are importing this function and uh the results of that function we are storing in something called settings comma params so when you run up till here the 1.42 gbt model which has been shown over here so see this is the 1.42 GB that will be downloaded so for me this took a long time to download Because the Internet connection was not very strong and uh the file kept pausing a lot in the middle so please make sure that you are sitting in an area which has strong internet connection and you have memory on your device if you have 1.4 or 1.5 GB in a strong uh internet area I think this download should happen in a much quicker Manner and then what we are going to do is that after we download these weights we are going to load weights into GPT what this function does is that we have constructed this GPT architecture right at all of the places where there are trainable parameters this function appropriately Maps the download parameters in the params dictionary into our model so remember I mentioned that this function essentially returns this dictionary called as params in which the weights and parameters have been arranged in a specific format when you run this function load weights into GPT the parameters from that params dictionary are downloaded into our model and you can actually control F contrl F load load with into GPT and you'll see that we have defined this function before load weights into GPT um what this function asence essentially does is that the params dictionary it Maps the values extracted from the params dictionary into the GPT architecture which we have constructed before so you can think of this whole code code block as one assignment step we are assigning the downloaded parameters to our model we have had a separate lecture to explain this fully so I'm not covering this in detail right now let me take you to the current code which we are on so uh then you have to run after you run the settings comma params then you have to run load weights into GPT which loads the downloaded weights from the 355 million model into uh your GPT architecture and then you set the model into evaluation mode so if your laptop does not have a strong configuration with respect to memory or processing speed you can even choose the model here to be gpt2 small which which has 124 million parameters and that should take you onethird of the time or half of the time it takes you to load the weights of this GPT to medium model if on the other hand your laptop has a very high processing speed and if you have GPU access I recommend you can use gpt2 large or even gpt2 X Excel if you have GPU access because then the results which you will get will be much better than uh what we will obtain with smaller models okay when you run when you run this file you'll see that you'll get outputs such as these I I have already downloaded the gpt2 parameters so I'm getting file already exists now what we can do is that before diving into fine tuning the model which we'll come to in the subsequent section what we can do is that we can actually check the pre- trend llms performance on one of the validation tasks by comparing its output to the expected response we have not even trained or fine-tuned our llm on the data set yet on this data set but I just want to check the performance of this llm by taking some very specific example so what I'm going to do now is that I'm going to take this example from our data set and the example is that below is an instruction that describes a task Write a response that appropriately completes the request and the instruction is that convert the active sentence to passive and the sentence is the chef Cooks the meal every day um and you can see that I've taken this sentence from the validation data it is the first sentence from the validation data so actually let me check it out over here so if I put Chef yeah this is the instruction input and output so the instruction is convert the active sentence to passive the chef Cooks the meal every day and the correct output uh I think convert yeah I think this is the one let me check it again yeah the instruction is convert the active sentence to passive the chef Cooks the meal every day it's this sentence exactly and the correct output is the meal is cooked by the chef every day so if the G if the llm is fine tuned correctly the output which it should give should somewhat be of a passive tense so if the active tense is the chef Cooks the meal every day the passive tense should be the meal is cooked by the chef every day we have not trained our llm on this data at all so we are not expecting to get a correct answer but I just want to see how wrong we are or how far off we are from the correct answer so you can use the generate function which you have defined previously what this function does is that you have to pass in the maximum number of new tokens to generate and then based on the model and based on the trained weights remember we are just using the weights downloaded from GPT to medium based on this based on these weights the next tokens or the new tokens will be generate new tokens will be generated and here we have to specify the maximum number of new tokens which I'm mentioning to be 35 and the context length is of course 1024 which is the context length of the medium configuration uh that's it actually and then you have to pass in the input text so I'm passing in the input text over here and then the model which is the model with the pre-trained weights and then the generated text we have to convert it back from token IDs into text so let us print this out right now and let us see the response so the response here so one thing to mention is that when you use the erated text uh it Returns the combined input as well as output this Behavior was convenient in the previous chapters because pre-trained llms are primarily designed to complete the text right so we can predict the input and we can predict the output so it will just look like an input output pair where the input is completed but now we actually just need to focus on the model generated response right every time we print the generated text we don't need the input text we don't need the instruction we just need the response right uh so now what I'm doing is that when you print out the response we need to subtract the length of the input instruction from the start of the generated text so we have to just print out the response here so we mention that you subtract the input you subtract the instruction and just print the response text so here is the response which our model gives the response which is given by the model is the chef Cooks the meal every day so it has just recycled the first sentence in the response itself it has included an instruction and in the instruction it is retaining the same instruction convert the active sentence to passive the shf cooks the which means that the model has not at all followed my instruction uh in fact the pre-train model is not yet capable of currently correctly following the given instruction it creates a response section that is good but it simply repeats the original input sentence it simply repeats the original input sentence and it also repeats a part of the instruction just as it is but it fails to convert the active sentence to passive voice that's the main uh thing which I want to convey to you that without fine-tuning the model itself just using the pre-trained weights it's not doing a good job at all in fact it fails to convert the active sentence to the passive it's recycling the same text which we provided as an instruction and it's creating this hashtag hashtag hashtag instruction in the response itself that is also not good so now what we'll do in the next section is that we are going to implement the finetuning process to imp to improve the model's ability to comprehend and appropriately respond to such requests so the reason fine tuning exists in the first place is because without fine tuning even if we load the weights from a parameter which is 355 million or even if you load from 774 million you'll see that even for a 774 million gpt2 param gpt2 model the response is not coherent without fine tuning and that's why in the next section we are going to look at fine tuning the llm on instruction data which means that we'll actually model or we'll actually modify the weights and parameters of this GPT model so that it can try to understand these instruction input output pairs from this specific data set uh okay everyone this brings us to the end of the lecture where we looked at loading a pre-trained llm I initially plan to cover the fine tuning in this lecture itself but I thought it will be good to cover it in the next lecture otherwise the duration of the lecture would have been pretty long I hope you are liking these lectures we are now very close to actually finishing the entire finetuning and then extracting the responses evaluating the responses and the code file which I'm going to share with you can be used to perform a wide range of instruction fine tuning tasks so thanks a lot everyone in the next lecture I'll explain the fine-tuning the model process and I look forward to seeing you in the next lecture"
}