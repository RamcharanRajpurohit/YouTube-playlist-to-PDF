{
  "video": {
    "video_id": "bkUkcyL_Xxc",
    "title": "Data Batching in LLM instruction fine-tuning | Hands on project | Live Python coding",
    "duration": 3122.0,
    "index": 37
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 7.56
    },
    {
      "text": "hello everyone and welcome to this",
      "start": 5.44,
      "duration": 4.119
    },
    {
      "text": "lecture in the build large language",
      "start": 7.56,
      "duration": 5.56
    },
    {
      "text": "models from scratch Series today we are",
      "start": 9.559,
      "duration": 5.481
    },
    {
      "text": "going to continue with the instruction",
      "start": 13.12,
      "duration": 4.079
    },
    {
      "text": "fine tuning Hands-On example which we",
      "start": 15.04,
      "duration": 4.44
    },
    {
      "text": "started in the previous lecture and",
      "start": 17.199,
      "duration": 4.24
    },
    {
      "text": "today we are going to look at organizing",
      "start": 19.48,
      "duration": 3.639
    },
    {
      "text": "data into training",
      "start": 21.439,
      "duration": 4.241
    },
    {
      "text": "batches let's first quickly recap what",
      "start": 23.119,
      "duration": 4.841
    },
    {
      "text": "we saw in the previous lecture in the",
      "start": 25.68,
      "duration": 3.999
    },
    {
      "text": "previous lecture we started looking at",
      "start": 27.96,
      "duration": 4.119
    },
    {
      "text": "in instruction fine tuning so we saw",
      "start": 29.679,
      "duration": 5.081
    },
    {
      "text": "that pre-trained llms are good at text",
      "start": 32.079,
      "duration": 4.721
    },
    {
      "text": "completion but they really struggle with",
      "start": 34.76,
      "duration": 4.56
    },
    {
      "text": "following Specific Instructions those",
      "start": 36.8,
      "duration": 4.8
    },
    {
      "text": "instructions can be fix the grammar in",
      "start": 39.32,
      "duration": 4.64
    },
    {
      "text": "the text or convert this text into",
      "start": 41.6,
      "duration": 5.119
    },
    {
      "text": "passive voice so pre-trained llms",
      "start": 43.96,
      "duration": 5.079
    },
    {
      "text": "struggle with these and that's why we",
      "start": 46.719,
      "duration": 4.041
    },
    {
      "text": "need to do the process of instruction",
      "start": 49.039,
      "duration": 3.881
    },
    {
      "text": "fine tuning in the process of",
      "start": 50.76,
      "duration": 4.279
    },
    {
      "text": "instruction fine tuning we need to",
      "start": 52.92,
      "duration": 4.799
    },
    {
      "text": "provide the llm with a data set and the",
      "start": 55.039,
      "duration": 4.441
    },
    {
      "text": "data set consists of a list of",
      "start": 57.719,
      "duration": 4.041
    },
    {
      "text": "instruction and the desired responses",
      "start": 59.48,
      "duration": 3.28
    },
    {
      "text": "which we",
      "start": 61.76,
      "duration": 3.64
    },
    {
      "text": "want so here's the data set which we",
      "start": 62.76,
      "duration": 4.719
    },
    {
      "text": "will actually use this is a data set",
      "start": 65.4,
      "duration": 5.44
    },
    {
      "text": "which consists of 1100 instruction and",
      "start": 67.479,
      "duration": 5.921
    },
    {
      "text": "output pairs so you can see a sample",
      "start": 70.84,
      "duration": 4.12
    },
    {
      "text": "instruction output pair such as",
      "start": 73.4,
      "duration": 3.28
    },
    {
      "text": "translate the following sentence into",
      "start": 74.96,
      "duration": 4.24
    },
    {
      "text": "French that's the instruction the input",
      "start": 76.68,
      "duration": 4.439
    },
    {
      "text": "is where is the nearest restaurant and",
      "start": 79.2,
      "duration": 4.48
    },
    {
      "text": "the output is the French translation",
      "start": 81.119,
      "duration": 4.601
    },
    {
      "text": "similarly here the instruction is",
      "start": 83.68,
      "duration": 5.04
    },
    {
      "text": "rewrite the following sentence so that",
      "start": 85.72,
      "duration": 5.28
    },
    {
      "text": "it is in active voice",
      "start": 88.72,
      "duration": 5.48
    },
    {
      "text": "the input is the cake was baked by Sarah",
      "start": 91.0,
      "duration": 6.24
    },
    {
      "text": "and the output is Sarah baked the cake",
      "start": 94.2,
      "duration": 5.08
    },
    {
      "text": "in some instruction output pairs there",
      "start": 97.24,
      "duration": 4.44
    },
    {
      "text": "is no input so for example convert the",
      "start": 99.28,
      "duration": 4.479
    },
    {
      "text": "active sentence to passive and then the",
      "start": 101.68,
      "duration": 3.6
    },
    {
      "text": "sentence is given in the instruction",
      "start": 103.759,
      "duration": 3.841
    },
    {
      "text": "itself so that does not have an input",
      "start": 105.28,
      "duration": 4.24
    },
    {
      "text": "and we have the output",
      "start": 107.6,
      "duration": 4.159
    },
    {
      "text": "directly then what is the capital of",
      "start": 109.52,
      "duration": 4.52
    },
    {
      "text": "Indonesia that does not have an input",
      "start": 111.759,
      "duration": 3.841
    },
    {
      "text": "but directly we have the output the",
      "start": 114.04,
      "duration": 3.359
    },
    {
      "text": "capital of Indonesia is",
      "start": 115.6,
      "duration": 4.72
    },
    {
      "text": "Jakarta so these are the instru ction",
      "start": 117.399,
      "duration": 5.08
    },
    {
      "text": "and the response pairs which we give as",
      "start": 120.32,
      "duration": 5.839
    },
    {
      "text": "the data to pre-train a large language",
      "start": 122.479,
      "duration": 6.241
    },
    {
      "text": "model so first thing which we did in",
      "start": 126.159,
      "duration": 4.32
    },
    {
      "text": "yesterday's lecture is that we",
      "start": 128.72,
      "duration": 4.72
    },
    {
      "text": "downloaded the data set and then we",
      "start": 130.479,
      "duration": 5.921
    },
    {
      "text": "formatted the data set what does it mean",
      "start": 133.44,
      "duration": 5.48
    },
    {
      "text": "formatting we converted the data set",
      "start": 136.4,
      "duration": 4.88
    },
    {
      "text": "into a format which is called as alpaka",
      "start": 138.92,
      "duration": 5.2
    },
    {
      "text": "prompt style so remember when I showed",
      "start": 141.28,
      "duration": 5.319
    },
    {
      "text": "you that when we looked at the data set",
      "start": 144.12,
      "duration": 5.36
    },
    {
      "text": "it had these three keys or these three",
      "start": 146.599,
      "duration": 5.841
    },
    {
      "text": "values instruction input and output it",
      "start": 149.48,
      "duration": 5.24
    },
    {
      "text": "turns out that there is a specific way",
      "start": 152.44,
      "duration": 4.76
    },
    {
      "text": "this needs to be converted into a prompt",
      "start": 154.72,
      "duration": 4.599
    },
    {
      "text": "before we feed it to the large language",
      "start": 157.2,
      "duration": 5.039
    },
    {
      "text": "model for fine tuning and that prompt is",
      "start": 159.319,
      "duration": 5.321
    },
    {
      "text": "called as the alpaka prompt style so",
      "start": 162.239,
      "duration": 5.64
    },
    {
      "text": "Stanford maintains this Stanford alpaka",
      "start": 164.64,
      "duration": 5.2
    },
    {
      "text": "repository where they give you",
      "start": 167.879,
      "duration": 3.681
    },
    {
      "text": "instructions on when you have the",
      "start": 169.84,
      "duration": 3.72
    },
    {
      "text": "instruction response pairs such as what",
      "start": 171.56,
      "duration": 4.36
    },
    {
      "text": "we have here how can you convert them",
      "start": 173.56,
      "duration": 5.679
    },
    {
      "text": "into a prompt so here is the specific",
      "start": 175.92,
      "duration": 6.08
    },
    {
      "text": "formatting for the alpaka style prompt",
      "start": 179.239,
      "duration": 4.841
    },
    {
      "text": "where you when you have an instruction",
      "start": 182.0,
      "duration": 4.28
    },
    {
      "text": "input and response The Prompt needs to",
      "start": 184.08,
      "duration": 4.32
    },
    {
      "text": "be constructed like this below is an",
      "start": 186.28,
      "duration": 4.72
    },
    {
      "text": "instruction that describes a task paired",
      "start": 188.4,
      "duration": 4.8
    },
    {
      "text": "with an input that provides further",
      "start": 191.0,
      "duration": 4.879
    },
    {
      "text": "context Write a response that",
      "start": 193.2,
      "duration": 5.36
    },
    {
      "text": "appropriately completes the request and",
      "start": 195.879,
      "duration": 4.44
    },
    {
      "text": "then you mention the instruction and",
      "start": 198.56,
      "duration": 3.84
    },
    {
      "text": "give the instruction which you had in",
      "start": 200.319,
      "duration": 4.521
    },
    {
      "text": "your data set then you mention the input",
      "start": 202.4,
      "duration": 3.919
    },
    {
      "text": "give the input which you have in the",
      "start": 204.84,
      "duration": 2.88
    },
    {
      "text": "data set and then you collect the",
      "start": 206.319,
      "duration": 3.56
    },
    {
      "text": "response and show the output which you",
      "start": 207.72,
      "duration": 6.12
    },
    {
      "text": "had uh in the data set so what we did in",
      "start": 209.879,
      "duration": 5.92
    },
    {
      "text": "the previous lecture actually was that",
      "start": 213.84,
      "duration": 5.52
    },
    {
      "text": "we we used this alpaka prompt style we",
      "start": 215.799,
      "duration": 6.201
    },
    {
      "text": "took our data set and we converted this",
      "start": 219.36,
      "duration": 5.84
    },
    {
      "text": "data set into an alpaka prompt style and",
      "start": 222.0,
      "duration": 4.959
    },
    {
      "text": "then we partition the data set into",
      "start": 225.2,
      "duration": 4.119
    },
    {
      "text": "training testing and validation we use",
      "start": 226.959,
      "duration": 5.721
    },
    {
      "text": "85% for training 10% for testing and the",
      "start": 229.319,
      "duration": 6.28
    },
    {
      "text": "remaining 5% for validation so let me",
      "start": 232.68,
      "duration": 5.36
    },
    {
      "text": "show you the code which we used in the",
      "start": 235.599,
      "duration": 4.961
    },
    {
      "text": "previous lecture in this part of the",
      "start": 238.04,
      "duration": 4.759
    },
    {
      "text": "code what we did is we prepared the data",
      "start": 240.56,
      "duration": 6.16
    },
    {
      "text": "set so we made an API call to this URL",
      "start": 242.799,
      "duration": 5.841
    },
    {
      "text": "we downloaded the data set and here you",
      "start": 246.72,
      "duration": 4.56
    },
    {
      "text": "can see there are 1100 entries we",
      "start": 248.64,
      "duration": 5.04
    },
    {
      "text": "printed the 50th example in this data",
      "start": 251.28,
      "duration": 5.479
    },
    {
      "text": "set we printed the 999th example of this",
      "start": 253.68,
      "duration": 5.0
    },
    {
      "text": "data set and we saw that they matched",
      "start": 256.759,
      "duration": 4.361
    },
    {
      "text": "what we actually have in the data set",
      "start": 258.68,
      "duration": 4.079
    },
    {
      "text": "this confirmed that the data set was",
      "start": 261.12,
      "duration": 3.88
    },
    {
      "text": "loaded correctly then we wrote a",
      "start": 262.759,
      "duration": 4.801
    },
    {
      "text": "function called format input to convert",
      "start": 265.0,
      "duration": 4.44
    },
    {
      "text": "the instructions into the alpaka prompt",
      "start": 267.56,
      "duration": 2.84
    },
    {
      "text": "format",
      "start": 269.44,
      "duration": 2.759
    },
    {
      "text": "this format input function takes the",
      "start": 270.4,
      "duration": 4.76
    },
    {
      "text": "entry and the entry is essentially the",
      "start": 272.199,
      "duration": 4.84
    },
    {
      "text": "instruction input output pairs like",
      "start": 275.16,
      "duration": 4.319
    },
    {
      "text": "these and then it converts it into the",
      "start": 277.039,
      "duration": 3.72
    },
    {
      "text": "alpaka",
      "start": 279.479,
      "duration": 3.801
    },
    {
      "text": "prompt so here we have the instruction",
      "start": 280.759,
      "duration": 4.561
    },
    {
      "text": "plus the input and then we append the",
      "start": 283.28,
      "duration": 4.96
    },
    {
      "text": "response as an output so then here you",
      "start": 285.32,
      "duration": 5.0
    },
    {
      "text": "can see that whenever you have an entry",
      "start": 288.24,
      "duration": 5.72
    },
    {
      "text": "such as this when you pass it into the",
      "start": 290.32,
      "duration": 5.719
    },
    {
      "text": "model when you pass it into the format",
      "start": 293.96,
      "duration": 4.519
    },
    {
      "text": "input you have the first line then you",
      "start": 296.039,
      "duration": 4.521
    },
    {
      "text": "have the instruction and then the input",
      "start": 298.479,
      "duration": 3.801
    },
    {
      "text": "and then you have to append this model",
      "start": 300.56,
      "duration": 3.72
    },
    {
      "text": "input to the desired response and then",
      "start": 302.28,
      "duration": 4.28
    },
    {
      "text": "you have the entire prompt like this so",
      "start": 304.28,
      "duration": 4.16
    },
    {
      "text": "then this is the alpaka style prompt",
      "start": 306.56,
      "duration": 4.72
    },
    {
      "text": "which has this first first sentence then",
      "start": 308.44,
      "duration": 4.84
    },
    {
      "text": "it has the instruction it has the input",
      "start": 311.28,
      "duration": 3.919
    },
    {
      "text": "and it has the response the large",
      "start": 313.28,
      "duration": 3.639
    },
    {
      "text": "language model will be trained with",
      "start": 315.199,
      "duration": 4.081
    },
    {
      "text": "these prompts which are then constructed",
      "start": 316.919,
      "duration": 4.761
    },
    {
      "text": "for all of the instruction input output",
      "start": 319.28,
      "duration": 4.16
    },
    {
      "text": "pairs which we have in the data set all",
      "start": 321.68,
      "duration": 3.4
    },
    {
      "text": "1100",
      "start": 323.44,
      "duration": 4.96
    },
    {
      "text": "pairs right so now okay one last thing",
      "start": 325.08,
      "duration": 5.239
    },
    {
      "text": "is this splitting the data set into",
      "start": 328.4,
      "duration": 4.16
    },
    {
      "text": "training testing and validation so here",
      "start": 330.319,
      "duration": 4.801
    },
    {
      "text": "you can see that we have used 85% for",
      "start": 332.56,
      "duration": 5.68
    },
    {
      "text": "training 10% for testing and the",
      "start": 335.12,
      "duration": 5.76
    },
    {
      "text": "remaining 5% for validation you can",
      "start": 338.24,
      "duration": 4.399
    },
    {
      "text": "print out the training set length which",
      "start": 340.88,
      "duration": 5.0
    },
    {
      "text": "is 935 out of the 1100 pairs the",
      "start": 342.639,
      "duration": 6.161
    },
    {
      "text": "validation set is 55 and the test set is",
      "start": 345.88,
      "duration": 7.2
    },
    {
      "text": "110 out of the 1100 pairs awesome now",
      "start": 348.8,
      "duration": 5.679
    },
    {
      "text": "what we are going to do in today's",
      "start": 353.08,
      "duration": 3.2
    },
    {
      "text": "lecture is we are going to come to step",
      "start": 354.479,
      "duration": 4.041
    },
    {
      "text": "number two and step number two is",
      "start": 356.28,
      "duration": 4.96
    },
    {
      "text": "batching the data set",
      "start": 358.52,
      "duration": 5.72
    },
    {
      "text": "this is a bit more complicated than what",
      "start": 361.24,
      "duration": 4.72
    },
    {
      "text": "we have seen before and that's why I'm",
      "start": 364.24,
      "duration": 3.92
    },
    {
      "text": "dedicating this entire lecture to this",
      "start": 365.96,
      "duration": 4.359
    },
    {
      "text": "there are number of finer and subtle",
      "start": 368.16,
      "duration": 3.72
    },
    {
      "text": "things which you need to understand in",
      "start": 370.319,
      "duration": 3.88
    },
    {
      "text": "this lecture and that will really help",
      "start": 371.88,
      "duration": 4.439
    },
    {
      "text": "you to understand the details of the",
      "start": 374.199,
      "duration": 4.681
    },
    {
      "text": "fine tuning process so please pay close",
      "start": 376.319,
      "duration": 5.32
    },
    {
      "text": "attention to whatever I'm showing on the",
      "start": 378.88,
      "duration": 4.719
    },
    {
      "text": "Whiteboard and then I'll show you the",
      "start": 381.639,
      "duration": 3.56
    },
    {
      "text": "entire process through",
      "start": 383.599,
      "duration": 4.16
    },
    {
      "text": "code what does it mean batching the data",
      "start": 385.199,
      "duration": 5.44
    },
    {
      "text": "set well it means that let's say we have",
      "start": 387.759,
      "duration": 6.201
    },
    {
      "text": "uh one data which is like this so I have",
      "start": 390.639,
      "duration": 5.4
    },
    {
      "text": "taken a screenshot here and then I'll",
      "start": 393.96,
      "duration": 4.679
    },
    {
      "text": "bring it to my whiteboard okay so we",
      "start": 396.039,
      "duration": 4.56
    },
    {
      "text": "have one data like this which is the",
      "start": 398.639,
      "duration": 6.0
    },
    {
      "text": "instruction input output uh and we have",
      "start": 400.599,
      "duration": 6.921
    },
    {
      "text": "actually let me use the prompt itself",
      "start": 404.639,
      "duration": 4.441
    },
    {
      "text": "because the data will be converted into",
      "start": 407.52,
      "duration": 3.56
    },
    {
      "text": "a prompt right so let's say for the",
      "start": 409.08,
      "duration": 5.239
    },
    {
      "text": "First Data I have a prompt like",
      "start": 411.08,
      "duration": 8.239
    },
    {
      "text": "this um right and uh for the second data",
      "start": 414.319,
      "duration": 8.0
    },
    {
      "text": "I have a prompt like this for the third",
      "start": 419.319,
      "duration": 5.361
    },
    {
      "text": "data let's say I have a prompt like this",
      "start": 422.319,
      "duration": 5.481
    },
    {
      "text": "now when we have data batches we need to",
      "start": 424.68,
      "duration": 5.76
    },
    {
      "text": "convert this we need to convert all of",
      "start": 427.8,
      "duration": 5.36
    },
    {
      "text": "the data set into a batch so let's say",
      "start": 430.44,
      "duration": 4.64
    },
    {
      "text": "I'm having a batch which has three data",
      "start": 433.16,
      "duration": 4.4
    },
    {
      "text": "samples so let's say this is my",
      "start": 435.08,
      "duration": 6.48
    },
    {
      "text": "batch and this is the data sample number",
      "start": 437.56,
      "duration": 8.359
    },
    {
      "text": "one this is the data sample number",
      "start": 441.56,
      "duration": 7.199
    },
    {
      "text": "two and here is the data sample number",
      "start": 445.919,
      "duration": 4.361
    },
    {
      "text": "three",
      "start": 448.759,
      "duration": 3.921
    },
    {
      "text": "right what I want to do now is that I",
      "start": 450.28,
      "duration": 4.8
    },
    {
      "text": "want to have I want to convert the first",
      "start": 452.68,
      "duration": 5.639
    },
    {
      "text": "data set into a numerical representation",
      "start": 455.08,
      "duration": 5.799
    },
    {
      "text": "so that it forms the first row the",
      "start": 458.319,
      "duration": 4.361
    },
    {
      "text": "second data set needs to be converted",
      "start": 460.879,
      "duration": 3.641
    },
    {
      "text": "into a numerical representation so that",
      "start": 462.68,
      "duration": 4.239
    },
    {
      "text": "it forms the second row and the third",
      "start": 464.52,
      "duration": 4.119
    },
    {
      "text": "data set needs to be converted into a",
      "start": 466.919,
      "duration": 3.361
    },
    {
      "text": "numerical representation so that it",
      "start": 468.639,
      "duration": 3.84
    },
    {
      "text": "forms the third row this whole lecture",
      "start": 470.28,
      "duration": 4.08
    },
    {
      "text": "is about how we are going to construct",
      "start": 472.479,
      "duration": 4.321
    },
    {
      "text": "this numerical representation so that",
      "start": 474.36,
      "duration": 4.839
    },
    {
      "text": "the size of the first row is the same as",
      "start": 476.8,
      "duration": 4.48
    },
    {
      "text": "the size of the second row and it's the",
      "start": 479.199,
      "duration": 4.0
    },
    {
      "text": "same as the size of the third row and",
      "start": 481.28,
      "duration": 4.44
    },
    {
      "text": "when I say size I mean the number of",
      "start": 483.199,
      "duration": 5.201
    },
    {
      "text": "columns which I'm highlighting right now",
      "start": 485.72,
      "duration": 5.64
    },
    {
      "text": "how will we make sure that these prompts",
      "start": 488.4,
      "duration": 5.079
    },
    {
      "text": "which are different for different",
      "start": 491.36,
      "duration": 3.72
    },
    {
      "text": "instruction output pairs right their",
      "start": 493.479,
      "duration": 3.761
    },
    {
      "text": "length may also be different so for",
      "start": 495.08,
      "duration": 4.32
    },
    {
      "text": "example The Prompt which is constructed",
      "start": 497.24,
      "duration": 4.639
    },
    {
      "text": "for this pair will be very different",
      "start": 499.4,
      "duration": 4.079
    },
    {
      "text": "than the prompt which is for this the",
      "start": 501.879,
      "duration": 3.32
    },
    {
      "text": "length also will be different so how do",
      "start": 503.479,
      "duration": 4.801
    },
    {
      "text": "I batch it into these kind of numerical",
      "start": 505.199,
      "duration": 4.801
    },
    {
      "text": "representations",
      "start": 508.28,
      "duration": 3.72
    },
    {
      "text": "and we are going to follow a sequential",
      "start": 510.0,
      "duration": 3.919
    },
    {
      "text": "workflow for this and I'm first I'm",
      "start": 512.0,
      "duration": 3.599
    },
    {
      "text": "going to explain to you the entire",
      "start": 513.919,
      "duration": 3.881
    },
    {
      "text": "workflow on the Whiteboard and then I",
      "start": 515.599,
      "duration": 4.56
    },
    {
      "text": "will take you through code earlier I was",
      "start": 517.8,
      "duration": 4.039
    },
    {
      "text": "thinking I'll show you on the Whiteboard",
      "start": 520.159,
      "duration": 3.081
    },
    {
      "text": "take you through code show you on the",
      "start": 521.839,
      "duration": 2.721
    },
    {
      "text": "Whiteboard take you through code and",
      "start": 523.24,
      "duration": 3.92
    },
    {
      "text": "cycle this multiple times but I found",
      "start": 524.56,
      "duration": 4.52
    },
    {
      "text": "that for this lecture in particular it's",
      "start": 527.16,
      "duration": 4.04
    },
    {
      "text": "much better if you first have a visual",
      "start": 529.08,
      "duration": 4.28
    },
    {
      "text": "understanding of the entire flow have it",
      "start": 531.2,
      "duration": 5.0
    },
    {
      "text": "as a mind map and then you can follow",
      "start": 533.36,
      "duration": 5.479
    },
    {
      "text": "the code as I'm going through it okay so",
      "start": 536.2,
      "duration": 4.6
    },
    {
      "text": "first we have the data and we'll format",
      "start": 538.839,
      "duration": 3.801
    },
    {
      "text": "it using the prompt template using the",
      "start": 540.8,
      "duration": 4.12
    },
    {
      "text": "alpaka prompt template and then the",
      "start": 542.64,
      "duration": 4.96
    },
    {
      "text": "formatted data looks something like this",
      "start": 544.92,
      "duration": 5.28
    },
    {
      "text": "right the first step to convert the data",
      "start": 547.6,
      "duration": 5.52
    },
    {
      "text": "into a numerical representation as I",
      "start": 550.2,
      "duration": 4.759
    },
    {
      "text": "mentioned over here is to tokenize the",
      "start": 553.12,
      "duration": 4.92
    },
    {
      "text": "data so we are going to use a tokenizer",
      "start": 554.959,
      "duration": 5.681
    },
    {
      "text": "and that tokenizer is going to be the",
      "start": 558.04,
      "duration": 4.96
    },
    {
      "text": "bite pair encoder which is used by open",
      "start": 560.64,
      "duration": 4.72
    },
    {
      "text": "a what this tokenizer does is that it",
      "start": 563.0,
      "duration": 4.2
    },
    {
      "text": "takes a sentence and converts it into a",
      "start": 565.36,
      "duration": 4.56
    },
    {
      "text": "bunch of token IDs right",
      "start": 567.2,
      "duration": 5.04
    },
    {
      "text": "so that's the first step for every so",
      "start": 569.92,
      "duration": 4.24
    },
    {
      "text": "let's say this is the prompt below is an",
      "start": 572.24,
      "duration": 3.599
    },
    {
      "text": "instruction that describes a task Write",
      "start": 574.16,
      "duration": 3.44
    },
    {
      "text": "a response that appropriately con",
      "start": 575.839,
      "duration": 3.56
    },
    {
      "text": "completes the request let's say there is",
      "start": 577.6,
      "duration": 3.6
    },
    {
      "text": "an instruction there is an input and",
      "start": 579.399,
      "duration": 3.961
    },
    {
      "text": "there is a response I'm going to take",
      "start": 581.2,
      "duration": 3.8
    },
    {
      "text": "this prompt and I'm going to convert",
      "start": 583.36,
      "duration": 4.719
    },
    {
      "text": "this entire prompt into token IDs that's",
      "start": 585.0,
      "duration": 6.12
    },
    {
      "text": "going to be my first step right so I",
      "start": 588.079,
      "duration": 5.44
    },
    {
      "text": "have written it in a lot of detail over",
      "start": 591.12,
      "duration": 5.0
    },
    {
      "text": "here so let's say if this is the let's",
      "start": 593.519,
      "duration": 5.56
    },
    {
      "text": "say if this is the data set which you",
      "start": 596.12,
      "duration": 5.32
    },
    {
      "text": "have then you convert it into the alpaka",
      "start": 599.079,
      "duration": 4.2
    },
    {
      "text": "prompt style and then it starts looking",
      "start": 601.44,
      "duration": 4.0
    },
    {
      "text": "like this and then it then you convert",
      "start": 603.279,
      "duration": 4.841
    },
    {
      "text": "it into a bunch of token idies using the",
      "start": 605.44,
      "duration": 4.6
    },
    {
      "text": "tick token",
      "start": 608.12,
      "duration": 5.36
    },
    {
      "text": "Library using the tick token library",
      "start": 610.04,
      "duration": 7.32
    },
    {
      "text": "right uh and this you do for the first",
      "start": 613.48,
      "duration": 6.28
    },
    {
      "text": "instruction input output data then you",
      "start": 617.36,
      "duration": 4.44
    },
    {
      "text": "do the same for the second you convert",
      "start": 619.76,
      "duration": 4.079
    },
    {
      "text": "it into the alpaka format and then you",
      "start": 621.8,
      "duration": 4.64
    },
    {
      "text": "convert it into a bunch of token IDs and",
      "start": 623.839,
      "duration": 5.921
    },
    {
      "text": "you do the same process for all of these",
      "start": 626.44,
      "duration": 5.639
    },
    {
      "text": "um instruction input output data which",
      "start": 629.76,
      "duration": 4.36
    },
    {
      "text": "you have in your data set that's the",
      "start": 632.079,
      "duration": 4.961
    },
    {
      "text": "first step but you see the problem here",
      "start": 634.12,
      "duration": 4.64
    },
    {
      "text": "the number of token IDs which are",
      "start": 637.04,
      "duration": 3.64
    },
    {
      "text": "present let's say the number of token",
      "start": 638.76,
      "duration": 4.199
    },
    {
      "text": "IDs which are present over here will be",
      "start": 640.68,
      "duration": 3.88
    },
    {
      "text": "different than the number of token IDs",
      "start": 642.959,
      "duration": 3.041
    },
    {
      "text": "which are present over here because of",
      "start": 644.56,
      "duration": 3.16
    },
    {
      "text": "course the length of the prompt might",
      "start": 646.0,
      "duration": 5.2
    },
    {
      "text": "differ right so if you look at this this",
      "start": 647.72,
      "duration": 5.2
    },
    {
      "text": "first prompt it looks longer than the",
      "start": 651.2,
      "duration": 4.12
    },
    {
      "text": "second prompt so the number of token IDs",
      "start": 652.92,
      "duration": 4.159
    },
    {
      "text": "in the first prompt are greater than the",
      "start": 655.32,
      "duration": 4.16
    },
    {
      "text": "number of token IDs in the second prompt",
      "start": 657.079,
      "duration": 4.681
    },
    {
      "text": "but as you but as I told you over here",
      "start": 659.48,
      "duration": 4.96
    },
    {
      "text": "we need the numerical representation of",
      "start": 661.76,
      "duration": 4.48
    },
    {
      "text": "all the samples in one batch so let's",
      "start": 664.44,
      "duration": 3.68
    },
    {
      "text": "say sample one and Sample two are in one",
      "start": 666.24,
      "duration": 4.279
    },
    {
      "text": "batch we need the number of columns",
      "start": 668.12,
      "duration": 4.519
    },
    {
      "text": "which is the numerical representation or",
      "start": 670.519,
      "duration": 4.361
    },
    {
      "text": "the number of token IDs we need the",
      "start": 672.639,
      "duration": 4.681
    },
    {
      "text": "number of token IDs for each sample to",
      "start": 674.88,
      "duration": 4.84
    },
    {
      "text": "be same so I'm just mentioning it over",
      "start": 677.32,
      "duration": 5.72
    },
    {
      "text": "here we need the number of token IDs for",
      "start": 679.72,
      "duration": 6.6
    },
    {
      "text": "each sample to be same so how do we make",
      "start": 683.04,
      "duration": 5.479
    },
    {
      "text": "sure that the number of token IDs of all",
      "start": 686.32,
      "duration": 4.8
    },
    {
      "text": "samples are the same and that brings us",
      "start": 688.519,
      "duration": 5.241
    },
    {
      "text": "to The Next Step so until now we saw the",
      "start": 691.12,
      "duration": 4.719
    },
    {
      "text": "tokenization the next step is that we",
      "start": 693.76,
      "duration": 4.84
    },
    {
      "text": "are going to adjust the length of",
      "start": 695.839,
      "duration": 5.721
    },
    {
      "text": "every uh a set of token IDs and we are",
      "start": 698.6,
      "duration": 5.76
    },
    {
      "text": "going to pad them with tokens so that",
      "start": 701.56,
      "duration": 5.92
    },
    {
      "text": "the length of all uh numerical",
      "start": 704.36,
      "duration": 5.68
    },
    {
      "text": "representations is exactly the same the",
      "start": 707.48,
      "duration": 4.52
    },
    {
      "text": "way we are going to do this is as has",
      "start": 710.04,
      "duration": 3.96
    },
    {
      "text": "been shown over here so let's say if we",
      "start": 712.0,
      "duration": 4.32
    },
    {
      "text": "have the first batch and the token IDs",
      "start": 714.0,
      "duration": 5.399
    },
    {
      "text": "for the first input are 0 1 2 3 4 the",
      "start": 716.32,
      "duration": 5.0
    },
    {
      "text": "token IDs for the second input in this",
      "start": 719.399,
      "duration": 4.321
    },
    {
      "text": "batch are five and six the token IDs for",
      "start": 721.32,
      "duration": 5.4
    },
    {
      "text": "the third input is 7 8 and N this is one",
      "start": 723.72,
      "duration": 5.119
    },
    {
      "text": "batch now take a look at the length of",
      "start": 726.72,
      "duration": 3.72
    },
    {
      "text": "the token ID here the length of the",
      "start": 728.839,
      "duration": 4.68
    },
    {
      "text": "token ID is five here it's two and here",
      "start": 730.44,
      "duration": 5.8
    },
    {
      "text": "it's three so it's not the same right so",
      "start": 733.519,
      "duration": 4.68
    },
    {
      "text": "what we'll do is that in each batch",
      "start": 736.24,
      "duration": 4.64
    },
    {
      "text": "we'll find that sequence which has the",
      "start": 738.199,
      "duration": 7.121
    },
    {
      "text": "longest number of longest length so this",
      "start": 740.88,
      "duration": 6.48
    },
    {
      "text": "clearly has the longest length so we",
      "start": 745.32,
      "duration": 4.319
    },
    {
      "text": "keep it as it is but the remaining ones",
      "start": 747.36,
      "duration": 6.279
    },
    {
      "text": "we pad them with these tokens so that",
      "start": 749.639,
      "duration": 6.841
    },
    {
      "text": "their length becomes equal to the",
      "start": 753.639,
      "duration": 5.161
    },
    {
      "text": "longest length so for example the input",
      "start": 756.48,
      "duration": 4.28
    },
    {
      "text": "two has only two tokens five and six so",
      "start": 758.8,
      "duration": 4.96
    },
    {
      "text": "we P three additional tokens",
      "start": 760.76,
      "duration": 5.879
    },
    {
      "text": "50256 uh similarly the third input has",
      "start": 763.76,
      "duration": 6.079
    },
    {
      "text": "three token ID 7 8 and N we pad it with",
      "start": 766.639,
      "duration": 6.2
    },
    {
      "text": "two additional token IDs now if you see",
      "start": 769.839,
      "duration": 4.601
    },
    {
      "text": "because of this padding procedure which",
      "start": 772.839,
      "duration": 4.0
    },
    {
      "text": "is done the length of all of these three",
      "start": 774.44,
      "duration": 4.759
    },
    {
      "text": "is equal is the same and there are five",
      "start": 776.839,
      "duration": 4.201
    },
    {
      "text": "token IDs in all the numerical",
      "start": 779.199,
      "duration": 4.32
    },
    {
      "text": "representation that is exactly what I",
      "start": 781.04,
      "duration": 5.68
    },
    {
      "text": "want now you may be thinking what's the",
      "start": 783.519,
      "duration": 7.401
    },
    {
      "text": "50256 that's the end of text token so",
      "start": 786.72,
      "duration": 7.04
    },
    {
      "text": "gpt2 uh if you look at gpt2 it has a",
      "start": 790.92,
      "duration": 5.44
    },
    {
      "text": "vocabulary size of",
      "start": 793.76,
      "duration": 6.28
    },
    {
      "text": "5257 um so 50257 is the vocabulary size",
      "start": 796.36,
      "duration": 5.719
    },
    {
      "text": "so that means the first token has a",
      "start": 800.04,
      "duration": 4.28
    },
    {
      "text": "token ID of zero and the last token has",
      "start": 802.079,
      "duration": 4.0
    },
    {
      "text": "a token ID of",
      "start": 804.32,
      "duration": 5.0
    },
    {
      "text": "50256 and this last token which has the",
      "start": 806.079,
      "duration": 5.921
    },
    {
      "text": "token ID of 50256 corresponds to the end",
      "start": 809.32,
      "duration": 3.959
    },
    {
      "text": "of text",
      "start": 812.0,
      "duration": 4.32
    },
    {
      "text": "token so it conveys that one particular",
      "start": 813.279,
      "duration": 5.041
    },
    {
      "text": "text sample has ended and it's the start",
      "start": 816.32,
      "duration": 4.199
    },
    {
      "text": "of a new text sample so we are just",
      "start": 818.32,
      "duration": 3.84
    },
    {
      "text": "appending it with the end of text",
      "start": 820.519,
      "duration": 3.68
    },
    {
      "text": "because it does not mean anything if we",
      "start": 822.16,
      "duration": 4.0
    },
    {
      "text": "use any other token ID it might be",
      "start": 824.199,
      "duration": 4.041
    },
    {
      "text": "associated with a token so that might",
      "start": 826.16,
      "duration": 4.239
    },
    {
      "text": "confuse the training so that's why we",
      "start": 828.24,
      "duration": 3.76
    },
    {
      "text": "append it with this with this end of",
      "start": 830.399,
      "duration": 4.161
    },
    {
      "text": "text token ID which is",
      "start": 832.0,
      "duration": 4.839
    },
    {
      "text": "50256 now one thing which we are going",
      "start": 834.56,
      "duration": 3.959
    },
    {
      "text": "to do here is that let's say this is the",
      "start": 836.839,
      "duration": 2.8
    },
    {
      "text": "first batch",
      "start": 838.519,
      "duration": 2.641
    },
    {
      "text": "and when you look at the second batch we",
      "start": 839.639,
      "duration": 3.2
    },
    {
      "text": "are going to implement the exact same",
      "start": 841.16,
      "duration": 3.28
    },
    {
      "text": "procedure we are going to find that",
      "start": 842.839,
      "duration": 4.041
    },
    {
      "text": "token with the largest token length and",
      "start": 844.44,
      "duration": 5.24
    },
    {
      "text": "we are going to append this",
      "start": 846.88,
      "duration": 5.079
    },
    {
      "text": "50256 to all the",
      "start": 849.68,
      "duration": 5.0
    },
    {
      "text": "remaining uh token sequences so that the",
      "start": 851.959,
      "duration": 5.521
    },
    {
      "text": "length of all becomes the same so each",
      "start": 854.68,
      "duration": 4.04
    },
    {
      "text": "batch we are going to process",
      "start": 857.48,
      "duration": 3.32
    },
    {
      "text": "sequentially and differently in each",
      "start": 858.72,
      "duration": 3.76
    },
    {
      "text": "batch we are first going to find that",
      "start": 860.8,
      "duration": 4.039
    },
    {
      "text": "representation which has the largest",
      "start": 862.48,
      "duration": 4.44
    },
    {
      "text": "number of tokens and to all the other",
      "start": 864.839,
      "duration": 3.761
    },
    {
      "text": "representations we are going to append",
      "start": 866.92,
      "duration": 6.44
    },
    {
      "text": "50 256 so that the length of uh so that",
      "start": 868.6,
      "duration": 8.0
    },
    {
      "text": "the length of the converted or the token",
      "start": 873.36,
      "duration": 6.2
    },
    {
      "text": "ID number for all of the inputs is equal",
      "start": 876.6,
      "duration": 5.28
    },
    {
      "text": "is the same and then it starts looking",
      "start": 879.56,
      "duration": 4.0
    },
    {
      "text": "like a batch so here you see this",
      "start": 881.88,
      "duration": 3.319
    },
    {
      "text": "definitely looks like a batch because",
      "start": 883.56,
      "duration": 3.88
    },
    {
      "text": "their number of columns are the same and",
      "start": 885.199,
      "duration": 4.161
    },
    {
      "text": "then I can process all of these together",
      "start": 887.44,
      "duration": 3.24
    },
    {
      "text": "in a",
      "start": 889.36,
      "duration": 3.88
    },
    {
      "text": "batch right so that's the step number",
      "start": 890.68,
      "duration": 4.8
    },
    {
      "text": "three we adjust to the same length with",
      "start": 893.24,
      "duration": 5.12
    },
    {
      "text": "padding tokens so we add the end of text",
      "start": 895.48,
      "duration": 5.2
    },
    {
      "text": "tokens to p add the data samples so that",
      "start": 898.36,
      "duration": 4.52
    },
    {
      "text": "in a batch all of the samples have the",
      "start": 900.68,
      "duration": 4.24
    },
    {
      "text": "same length that is the same number of",
      "start": 902.88,
      "duration": 5.079
    },
    {
      "text": "token IDs right The Next Step what we",
      "start": 904.92,
      "duration": 4.56
    },
    {
      "text": "are going to do is that we are going to",
      "start": 907.959,
      "duration": 4.401
    },
    {
      "text": "create Target token IDs which means that",
      "start": 909.48,
      "duration": 4.479
    },
    {
      "text": "uh let me show you what it actually",
      "start": 912.36,
      "duration": 4.64
    },
    {
      "text": "means so let's say if you have an input",
      "start": 913.959,
      "duration": 5.88
    },
    {
      "text": "right um and that's this prompt over",
      "start": 917.0,
      "duration": 4.68
    },
    {
      "text": "here let's say this is the prompt and",
      "start": 919.839,
      "duration": 3.401
    },
    {
      "text": "that has been converted into a bunch of",
      "start": 921.68,
      "duration": 4.0
    },
    {
      "text": "token IDs which look something like this",
      "start": 923.24,
      "duration": 4.279
    },
    {
      "text": "we need some output right which is the",
      "start": 925.68,
      "duration": 4.36
    },
    {
      "text": "true Target which we want to approximate",
      "start": 927.519,
      "duration": 4.201
    },
    {
      "text": "similar to what we did for large",
      "start": 930.04,
      "duration": 4.0
    },
    {
      "text": "language model training the target is",
      "start": 931.72,
      "duration": 4.559
    },
    {
      "text": "constructed by Shifting the input to the",
      "start": 934.04,
      "duration": 4.88
    },
    {
      "text": "right by one so let's say the input is 0",
      "start": 936.279,
      "duration": 6.441
    },
    {
      "text": "1 2 3 4 right the target will be you",
      "start": 938.92,
      "duration": 7.039
    },
    {
      "text": "forget the first uh entry here and you",
      "start": 942.72,
      "duration": 5.359
    },
    {
      "text": "take the remaining entries and then you",
      "start": 945.959,
      "duration": 4.161
    },
    {
      "text": "add a padding token",
      "start": 948.079,
      "duration": 4.481
    },
    {
      "text": "50256 so that the length of the Target",
      "start": 950.12,
      "duration": 5.0
    },
    {
      "text": "and the length of the input is equal to",
      "start": 952.56,
      "duration": 4.48
    },
    {
      "text": "are similar to each other or are exactly",
      "start": 955.12,
      "duration": 2.8
    },
    {
      "text": "same",
      "start": 957.04,
      "duration": 3.279
    },
    {
      "text": "rather here is the second input so if",
      "start": 957.92,
      "duration": 6.399
    },
    {
      "text": "the input is 56 50256 50256",
      "start": 960.319,
      "duration": 6.481
    },
    {
      "text": "50256 uh the way we construct the target",
      "start": 964.319,
      "duration": 5.241
    },
    {
      "text": "is that we get rid of this first index",
      "start": 966.8,
      "duration": 4.399
    },
    {
      "text": "we take all of the remaining in the",
      "start": 969.56,
      "duration": 3.48
    },
    {
      "text": "input and then we pad it with an extra",
      "start": 971.199,
      "duration": 3.2
    },
    {
      "text": "token which is",
      "start": 973.04,
      "duration": 3.919
    },
    {
      "text": "50256 do keep this in mind this is the",
      "start": 974.399,
      "duration": 4.601
    },
    {
      "text": "most important step in the fine tuning",
      "start": 976.959,
      "duration": 4.24
    },
    {
      "text": "process and sometimes this is also a",
      "start": 979.0,
      "duration": 4.72
    },
    {
      "text": "step which is very hard to grasp so",
      "start": 981.199,
      "duration": 4.481
    },
    {
      "text": "think about what is exactly happening in",
      "start": 983.72,
      "duration": 4.679
    },
    {
      "text": "this so let's say uh if we have a prompt",
      "start": 985.68,
      "duration": 4.44
    },
    {
      "text": "and I'm I'm going to focus on this",
      "start": 988.399,
      "duration": 4.521
    },
    {
      "text": "prompt further now so that I can explain",
      "start": 990.12,
      "duration": 5.04
    },
    {
      "text": "to you why we construct the target like",
      "start": 992.92,
      "duration": 4.44
    },
    {
      "text": "like this when I learned about this for",
      "start": 995.16,
      "duration": 5.159
    },
    {
      "text": "the first time I really did not",
      "start": 997.36,
      "duration": 5.52
    },
    {
      "text": "understand what is actually happening so",
      "start": 1000.319,
      "duration": 5.88
    },
    {
      "text": "if we have a if we have a prompt like",
      "start": 1002.88,
      "duration": 4.519
    },
    {
      "text": "this",
      "start": 1006.199,
      "duration": 4.361
    },
    {
      "text": "right what we are essentially doing in",
      "start": 1007.399,
      "duration": 5.281
    },
    {
      "text": "the when we construct the target pair is",
      "start": 1010.56,
      "duration": 5.639
    },
    {
      "text": "that let's say why is the target pair",
      "start": 1012.68,
      "duration": 5.399
    },
    {
      "text": "shifted to the right by one because we",
      "start": 1016.199,
      "duration": 4.56
    },
    {
      "text": "are still doing the next prediction task",
      "start": 1018.079,
      "duration": 6.24
    },
    {
      "text": "so if my let's actually let me take the",
      "start": 1020.759,
      "duration": 6.361
    },
    {
      "text": "screenshot of this also so that I can",
      "start": 1024.319,
      "duration": 5.081
    },
    {
      "text": "explain to you what is going",
      "start": 1027.12,
      "duration": 6.88
    },
    {
      "text": "on so if I take a screenshot of",
      "start": 1029.4,
      "duration": 8.799
    },
    {
      "text": "this and I'll paste this over",
      "start": 1034.0,
      "duration": 4.199
    },
    {
      "text": "here what this input Target",
      "start": 1038.839,
      "duration": 6.36
    },
    {
      "text": "sequence means is that if you have the",
      "start": 1042.16,
      "duration": 6.32
    },
    {
      "text": "input as zero the output should be equal",
      "start": 1045.199,
      "duration": 4.161
    },
    {
      "text": "to 1",
      "start": 1048.48,
      "duration": 3.48
    },
    {
      "text": "if the input is 0 and 1 the target is",
      "start": 1049.36,
      "duration": 5.72
    },
    {
      "text": "equal to 2 if the input is 0 1 and 2 the",
      "start": 1051.96,
      "duration": 5.88
    },
    {
      "text": "target is equal to 3 if the input is 0 1",
      "start": 1055.08,
      "duration": 7.0
    },
    {
      "text": "2 3 0 1 2 3 the target is equal to 4 and",
      "start": 1057.84,
      "duration": 6.68
    },
    {
      "text": "if the input is 0 1 2 3 4 then we are at",
      "start": 1062.08,
      "duration": 4.52
    },
    {
      "text": "the end of the sentence you can think",
      "start": 1064.52,
      "duration": 3.76
    },
    {
      "text": "about what we are really training the",
      "start": 1066.6,
      "duration": 3.079
    },
    {
      "text": "model over here we are training the",
      "start": 1068.28,
      "duration": 4.6
    },
    {
      "text": "model that if below is the input then",
      "start": 1069.679,
      "duration": 6.201
    },
    {
      "text": "below is is the output if below is is",
      "start": 1072.88,
      "duration": 6.0
    },
    {
      "text": "the input below is and is the output",
      "start": 1075.88,
      "duration": 4.919
    },
    {
      "text": "similarly as this happens sequentially",
      "start": 1078.88,
      "duration": 3.6
    },
    {
      "text": "we'll train the model that if below is",
      "start": 1080.799,
      "duration": 4.281
    },
    {
      "text": "an instruction task which describes",
      "start": 1082.48,
      "duration": 4.36
    },
    {
      "text": "write a sequ Write a response that",
      "start": 1085.08,
      "duration": 3.44
    },
    {
      "text": "appropriately completes the request if",
      "start": 1086.84,
      "duration": 4.64
    },
    {
      "text": "this much is the input then this this",
      "start": 1088.52,
      "duration": 6.36
    },
    {
      "text": "should be the output then we train the",
      "start": 1091.48,
      "duration": 5.559
    },
    {
      "text": "model that if this much is the input",
      "start": 1094.88,
      "duration": 4.72
    },
    {
      "text": "this should be the output I agree that",
      "start": 1097.039,
      "duration": 3.961
    },
    {
      "text": "there is some redundant training which",
      "start": 1099.6,
      "duration": 2.76
    },
    {
      "text": "is happening here because all the",
      "start": 1101.0,
      "duration": 2.96
    },
    {
      "text": "training which we really need to do is",
      "start": 1102.36,
      "duration": 3.679
    },
    {
      "text": "tell that if this much is the input",
      "start": 1103.96,
      "duration": 3.959
    },
    {
      "text": "instruction and the input this is the",
      "start": 1106.039,
      "duration": 3.961
    },
    {
      "text": "response which needs to be constructed",
      "start": 1107.919,
      "duration": 3.721
    },
    {
      "text": "and the way we are going about this is",
      "start": 1110.0,
      "duration": 3.4
    },
    {
      "text": "through this next word prediction task",
      "start": 1111.64,
      "duration": 4.159
    },
    {
      "text": "or next token prediction task that's why",
      "start": 1113.4,
      "duration": 4.32
    },
    {
      "text": "we shift that's why the target is the",
      "start": 1115.799,
      "duration": 3.601
    },
    {
      "text": "input which is shifted to the right by",
      "start": 1117.72,
      "duration": 3.8
    },
    {
      "text": "one please keep this in mind this is a",
      "start": 1119.4,
      "duration": 5.12
    },
    {
      "text": "bit not easy to understand and it's not",
      "start": 1121.52,
      "duration": 6.44
    },
    {
      "text": "intuitive as well when I first learned",
      "start": 1124.52,
      "duration": 5.24
    },
    {
      "text": "about instruction find tuning I thought",
      "start": 1127.96,
      "duration": 4.44
    },
    {
      "text": "that the input should be input should be",
      "start": 1129.76,
      "duration": 4.68
    },
    {
      "text": "this much and the target should be the",
      "start": 1132.4,
      "duration": 4.36
    },
    {
      "text": "response right but the input is this",
      "start": 1134.44,
      "duration": 5.119
    },
    {
      "text": "full thing the input is this full thing",
      "start": 1136.76,
      "duration": 4.52
    },
    {
      "text": "and the target is this full",
      "start": 1139.559,
      "duration": 4.041
    },
    {
      "text": "representation just shifted to the right",
      "start": 1141.28,
      "duration": 5.56
    },
    {
      "text": "by one so within the input itself we",
      "start": 1143.6,
      "duration": 5.92
    },
    {
      "text": "have the input and the output actually",
      "start": 1146.84,
      "duration": 4.6
    },
    {
      "text": "that is exactly how llms work we are",
      "start": 1149.52,
      "duration": 3.8
    },
    {
      "text": "using the next word predi or the next",
      "start": 1151.44,
      "duration": 4.28
    },
    {
      "text": "token prediction task so what this does",
      "start": 1153.32,
      "duration": 4.719
    },
    {
      "text": "is that as we are predicting the next",
      "start": 1155.72,
      "duration": 4.4
    },
    {
      "text": "token in the sequence of training we",
      "start": 1158.039,
      "duration": 4.441
    },
    {
      "text": "reach a stage with this much is the",
      "start": 1160.12,
      "duration": 5.559
    },
    {
      "text": "input the output will be the response so",
      "start": 1162.48,
      "duration": 5.16
    },
    {
      "text": "through learning how to predict the next",
      "start": 1165.679,
      "duration": 4.641
    },
    {
      "text": "token the llm learns to follow the",
      "start": 1167.64,
      "duration": 4.6
    },
    {
      "text": "instructions so there's a lot of",
      "start": 1170.32,
      "duration": 3.64
    },
    {
      "text": "similarity between instruction fine",
      "start": 1172.24,
      "duration": 3.36
    },
    {
      "text": "tuning and the pre-training process",
      "start": 1173.96,
      "duration": 3.68
    },
    {
      "text": "itself in the pre-training we did",
      "start": 1175.6,
      "duration": 3.8
    },
    {
      "text": "exactly the same thing we had the target",
      "start": 1177.64,
      "duration": 3.96
    },
    {
      "text": "shifted by one and it's nonintuitive",
      "start": 1179.4,
      "duration": 4.36
    },
    {
      "text": "that in instruction fine tuning the same",
      "start": 1181.6,
      "duration": 5.8
    },
    {
      "text": "thing can work but it does work because",
      "start": 1183.76,
      "duration": 5.6
    },
    {
      "text": "as the llm learns to predict the next",
      "start": 1187.4,
      "duration": 4.159
    },
    {
      "text": "token it learns to take the instruction",
      "start": 1189.36,
      "duration": 4.679
    },
    {
      "text": "the input and predict the",
      "start": 1191.559,
      "duration": 4.441
    },
    {
      "text": "response okay so I hope you have",
      "start": 1194.039,
      "duration": 4.041
    },
    {
      "text": "understood this part about how to create",
      "start": 1196.0,
      "duration": 4.44
    },
    {
      "text": "the target token IDs so the way to",
      "start": 1198.08,
      "duration": 4.56
    },
    {
      "text": "create the target token IDs is that we",
      "start": 1200.44,
      "duration": 4.08
    },
    {
      "text": "shift the inputs by one and we add an",
      "start": 1202.64,
      "duration": 3.96
    },
    {
      "text": "additional padding token to indicate",
      "start": 1204.52,
      "duration": 4.159
    },
    {
      "text": "that it's the end of the sentence so",
      "start": 1206.6,
      "duration": 3.92
    },
    {
      "text": "whenever we shift the input to the right",
      "start": 1208.679,
      "duration": 4.48
    },
    {
      "text": "by one we add this extra padding token",
      "start": 1210.52,
      "duration": 5.08
    },
    {
      "text": "right so that is how the target token",
      "start": 1213.159,
      "duration": 4.76
    },
    {
      "text": "IDs are created for training and the",
      "start": 1215.6,
      "duration": 5.199
    },
    {
      "text": "last step is that we'll replace in the",
      "start": 1217.919,
      "duration": 6.201
    },
    {
      "text": "Target token IDs this 5256 which is",
      "start": 1220.799,
      "duration": 4.76
    },
    {
      "text": "there wherever it is coming we'll",
      "start": 1224.12,
      "duration": 4.559
    },
    {
      "text": "replace it with minus 100 and there is a",
      "start": 1225.559,
      "duration": 5.321
    },
    {
      "text": "specific way to do this so if you look",
      "start": 1228.679,
      "duration": 5.561
    },
    {
      "text": "at this last part which is replace the",
      "start": 1230.88,
      "duration": 5.6
    },
    {
      "text": "padding tokens with placeholders so",
      "start": 1234.24,
      "duration": 4.6
    },
    {
      "text": "let's say this is my first Target tensor",
      "start": 1236.48,
      "duration": 4.88
    },
    {
      "text": "right um or let's look at the second",
      "start": 1238.84,
      "duration": 5.56
    },
    {
      "text": "target tensor let's look at 50256 there",
      "start": 1241.36,
      "duration": 6.199
    },
    {
      "text": "are four 50256 values right I'll leave",
      "start": 1244.4,
      "duration": 4.92
    },
    {
      "text": "this first one because that indicates",
      "start": 1247.559,
      "duration": 4.081
    },
    {
      "text": "the end of text and I'll replace all of",
      "start": 1249.32,
      "duration": 5.04
    },
    {
      "text": "the remaining with the value of minus",
      "start": 1251.64,
      "duration": 5.44
    },
    {
      "text": "100 similarly if you look at Target",
      "start": 1254.36,
      "duration": 6.16
    },
    {
      "text": "three uh it has three 50256 values I'll",
      "start": 1257.08,
      "duration": 5.2
    },
    {
      "text": "leave the first one because it",
      "start": 1260.52,
      "duration": 3.8
    },
    {
      "text": "symbolizes end of text and I'll replace",
      "start": 1262.28,
      "duration": 5.84
    },
    {
      "text": "all of the remaining 50256 with - 100",
      "start": 1264.32,
      "duration": 6.2
    },
    {
      "text": "similarly if you look at Target one now",
      "start": 1268.12,
      "duration": 5.08
    },
    {
      "text": "uh this 50256 represents end of text so",
      "start": 1270.52,
      "duration": 4.639
    },
    {
      "text": "I won't I won't replace this it remains",
      "start": 1273.2,
      "duration": 4.12
    },
    {
      "text": "like this so that is something to keep",
      "start": 1275.159,
      "duration": 4.76
    },
    {
      "text": "in mind we don't replace all the 50256",
      "start": 1277.32,
      "duration": 5.56
    },
    {
      "text": "tokon IDS with minus 100 we only replace",
      "start": 1279.919,
      "duration": 4.961
    },
    {
      "text": "the we leave out the first one and",
      "start": 1282.88,
      "duration": 4.36
    },
    {
      "text": "replace all of the rest with minus 100",
      "start": 1284.88,
      "duration": 4.36
    },
    {
      "text": "so you you might be thinking what is the",
      "start": 1287.24,
      "duration": 4.36
    },
    {
      "text": "significance of minus 100 and I'll",
      "start": 1289.24,
      "duration": 4.52
    },
    {
      "text": "explain to this in a lot of detail when",
      "start": 1291.6,
      "duration": 4.52
    },
    {
      "text": "we come to code but for now just know",
      "start": 1293.76,
      "duration": 4.279
    },
    {
      "text": "that it comes from this cross entropy",
      "start": 1296.12,
      "duration": 5.679
    },
    {
      "text": "loss ignore index so ignore index so by",
      "start": 1298.039,
      "duration": 6.161
    },
    {
      "text": "default the ignore index for pytorch is",
      "start": 1301.799,
      "duration": 5.041
    },
    {
      "text": "equal to minus 100 so when we mention",
      "start": 1304.2,
      "duration": 4.959
    },
    {
      "text": "minus 100 here it kind of makes sure",
      "start": 1306.84,
      "duration": 4.56
    },
    {
      "text": "that when we calculate the loss function",
      "start": 1309.159,
      "duration": 4.561
    },
    {
      "text": "all of these token IDs which do not",
      "start": 1311.4,
      "duration": 3.92
    },
    {
      "text": "matter at all we have randomly added",
      "start": 1313.72,
      "duration": 3.4
    },
    {
      "text": "them to the Target right they are not",
      "start": 1315.32,
      "duration": 4.0
    },
    {
      "text": "included in the loss function this makes",
      "start": 1317.12,
      "duration": 4.2
    },
    {
      "text": "the training much more efficient and",
      "start": 1319.32,
      "duration": 3.959
    },
    {
      "text": "does not unnecessarily include tokens",
      "start": 1321.32,
      "duration": 3.08
    },
    {
      "text": "which are not",
      "start": 1323.279,
      "duration": 4.201
    },
    {
      "text": "important this first 50256 needed to be",
      "start": 1324.4,
      "duration": 5.6
    },
    {
      "text": "retained because it indicates the end of",
      "start": 1327.48,
      "duration": 5.88
    },
    {
      "text": "uh end of a particular uh end of a",
      "start": 1330.0,
      "duration": 5.159
    },
    {
      "text": "particular text so that is very",
      "start": 1333.36,
      "duration": 4.96
    },
    {
      "text": "important but the remaining 50256 token",
      "start": 1335.159,
      "duration": 4.76
    },
    {
      "text": "IDs are not needed and we can just",
      "start": 1338.32,
      "duration": 4.8
    },
    {
      "text": "replace them with a value of minus 100",
      "start": 1339.919,
      "duration": 6.041
    },
    {
      "text": "awesome so now uh we have understood",
      "start": 1343.12,
      "duration": 4.799
    },
    {
      "text": "this whole procedure so let me repeat",
      "start": 1345.96,
      "duration": 3.959
    },
    {
      "text": "the whole proced procedure ones so data",
      "start": 1347.919,
      "duration": 4.561
    },
    {
      "text": "batching is actually done in five steps",
      "start": 1349.919,
      "duration": 4.081
    },
    {
      "text": "and we are going to see all of these",
      "start": 1352.48,
      "duration": 4.04
    },
    {
      "text": "five five steps in code shortly the",
      "start": 1354.0,
      "duration": 4.72
    },
    {
      "text": "first step is to of course format the",
      "start": 1356.52,
      "duration": 4.32
    },
    {
      "text": "data using the promt template then",
      "start": 1358.72,
      "duration": 4.24
    },
    {
      "text": "convert the promt template into token",
      "start": 1360.84,
      "duration": 4.959
    },
    {
      "text": "IDs now all of these token IDs won't",
      "start": 1362.96,
      "duration": 6.079
    },
    {
      "text": "have equal length so the next step after",
      "start": 1365.799,
      "duration": 6.48
    },
    {
      "text": "this is to append U the data samples",
      "start": 1369.039,
      "duration": 6.12
    },
    {
      "text": "with the padding tokens of 50256 so that",
      "start": 1372.279,
      "duration": 5.361
    },
    {
      "text": "in each batch the length of every data",
      "start": 1375.159,
      "duration": 4.041
    },
    {
      "text": "sample is the same",
      "start": 1377.64,
      "duration": 3.12
    },
    {
      "text": "and that is equal to the sample which",
      "start": 1379.2,
      "duration": 4.479
    },
    {
      "text": "has the maximum number of token IDs then",
      "start": 1380.76,
      "duration": 4.519
    },
    {
      "text": "what we do is that we create Target",
      "start": 1383.679,
      "duration": 3.761
    },
    {
      "text": "token IDs which will be needed because",
      "start": 1385.279,
      "duration": 4.161
    },
    {
      "text": "we need to know what the right answer is",
      "start": 1387.44,
      "duration": 4.32
    },
    {
      "text": "and the right answer is just the input",
      "start": 1389.44,
      "duration": 4.64
    },
    {
      "text": "shifted to the right by one this is a",
      "start": 1391.76,
      "duration": 4.36
    },
    {
      "text": "bit counterintuitive but it does work",
      "start": 1394.08,
      "duration": 3.719
    },
    {
      "text": "because the llm learns to predict the",
      "start": 1396.12,
      "duration": 4.159
    },
    {
      "text": "next token and in that process it learns",
      "start": 1397.799,
      "duration": 4.24
    },
    {
      "text": "that here is the instruction here is the",
      "start": 1400.279,
      "duration": 3.721
    },
    {
      "text": "input and I have to predict the",
      "start": 1402.039,
      "duration": 4.361
    },
    {
      "text": "output and then finally the last thing",
      "start": 1404.0,
      "duration": 5.279
    },
    {
      "text": "is we we replace all all the tokens",
      "start": 1406.4,
      "duration": 5.6
    },
    {
      "text": "except for the First 50256 with a value",
      "start": 1409.279,
      "duration": 5.121
    },
    {
      "text": "of minus 100 and the reason we do this",
      "start": 1412.0,
      "duration": 4.279
    },
    {
      "text": "is that we need to exclude these tokens",
      "start": 1414.4,
      "duration": 4.159
    },
    {
      "text": "from the training loss and py torch",
      "start": 1416.279,
      "duration": 4.081
    },
    {
      "text": "cross entropy loss implementation has",
      "start": 1418.559,
      "duration": 4.801
    },
    {
      "text": "the ignore index of minus 100 and that's",
      "start": 1420.36,
      "duration": 5.36
    },
    {
      "text": "why we are going to use this minus 100",
      "start": 1423.36,
      "duration": 4.08
    },
    {
      "text": "we'll see this in a lot of detail when",
      "start": 1425.72,
      "duration": 4.559
    },
    {
      "text": "we come to code right so I hope you have",
      "start": 1427.44,
      "duration": 5.0
    },
    {
      "text": "all understood this",
      "start": 1430.279,
      "duration": 5.081
    },
    {
      "text": "process uh this is very important for",
      "start": 1432.44,
      "duration": 4.8
    },
    {
      "text": "you to have as a road map because now we",
      "start": 1435.36,
      "duration": 3.84
    },
    {
      "text": "are going to go to code code and",
      "start": 1437.24,
      "duration": 3.679
    },
    {
      "text": "everything in the code will be much more",
      "start": 1439.2,
      "duration": 3.56
    },
    {
      "text": "clearer and easy to understand once you",
      "start": 1440.919,
      "duration": 3.481
    },
    {
      "text": "remember this road map which I've",
      "start": 1442.76,
      "duration": 4.2
    },
    {
      "text": "introduced over here so let us jump",
      "start": 1444.4,
      "duration": 5.32
    },
    {
      "text": "directly right into code right now so as",
      "start": 1446.96,
      "duration": 4.959
    },
    {
      "text": "I mentioned before until now we have",
      "start": 1449.72,
      "duration": 4.24
    },
    {
      "text": "converted the data set into training",
      "start": 1451.919,
      "duration": 4.401
    },
    {
      "text": "testing and validation batches right the",
      "start": 1453.96,
      "duration": 5.319
    },
    {
      "text": "training is 85% of the data testing is",
      "start": 1456.32,
      "duration": 5.76
    },
    {
      "text": "10% of the data and the remaining 5% is",
      "start": 1459.279,
      "duration": 5.041
    },
    {
      "text": "for validation the next step is to",
      "start": 1462.08,
      "duration": 4.64
    },
    {
      "text": "organize the data into training batches",
      "start": 1464.32,
      "duration": 4.239
    },
    {
      "text": "as we saw on the Whiteboard first first",
      "start": 1466.72,
      "duration": 3.6
    },
    {
      "text": "what we are going to do is",
      "start": 1468.559,
      "duration": 4.081
    },
    {
      "text": "that uh we are going to code an",
      "start": 1470.32,
      "duration": 4.8
    },
    {
      "text": "instruction data set class what this",
      "start": 1472.64,
      "duration": 5.039
    },
    {
      "text": "class does is that it takes in the data",
      "start": 1475.12,
      "duration": 5.559
    },
    {
      "text": "set in this format and it converts it",
      "start": 1477.679,
      "duration": 4.761
    },
    {
      "text": "first of all into this alpaka style",
      "start": 1480.679,
      "duration": 4.0
    },
    {
      "text": "format that's the first step over here",
      "start": 1482.44,
      "duration": 4.0
    },
    {
      "text": "if you recall the first step is format",
      "start": 1484.679,
      "duration": 4.961
    },
    {
      "text": "data using prom template so we are going",
      "start": 1486.44,
      "duration": 4.32
    },
    {
      "text": "to use",
      "start": 1489.64,
      "duration": 3.84
    },
    {
      "text": "the format input function which I",
      "start": 1490.76,
      "duration": 4.279
    },
    {
      "text": "explained to you at the start of this",
      "start": 1493.48,
      "duration": 3.48
    },
    {
      "text": "lecture this format input function over",
      "start": 1495.039,
      "duration": 5.721
    },
    {
      "text": "here which takes in the um instruction",
      "start": 1496.96,
      "duration": 6.24
    },
    {
      "text": "input output pair like this and",
      "start": 1500.76,
      "duration": 5.12
    },
    {
      "text": "converts U that pair into this first",
      "start": 1503.2,
      "duration": 4.56
    },
    {
      "text": "sentence instruction and the input and",
      "start": 1505.88,
      "duration": 4.24
    },
    {
      "text": "then we need to append the response or",
      "start": 1507.76,
      "duration": 5.08
    },
    {
      "text": "the output so here if you look at the",
      "start": 1510.12,
      "duration": 5.84
    },
    {
      "text": "code this instruction class data set",
      "start": 1512.84,
      "duration": 5.559
    },
    {
      "text": "when we create an instance of this class",
      "start": 1515.96,
      "duration": 4.68
    },
    {
      "text": "it first creates an object self. data",
      "start": 1518.399,
      "duration": 5.361
    },
    {
      "text": "and assigns the data set uh which is",
      "start": 1520.64,
      "duration": 4.759
    },
    {
      "text": "something like this you can think of the",
      "start": 1523.76,
      "duration": 3.84
    },
    {
      "text": "data set like this and then what we do",
      "start": 1525.399,
      "duration": 4.681
    },
    {
      "text": "is that for each entry in the data set",
      "start": 1527.6,
      "duration": 5.04
    },
    {
      "text": "it first applies this format input",
      "start": 1530.08,
      "duration": 4.24
    },
    {
      "text": "function to the entry and then appends",
      "start": 1532.64,
      "duration": 4.0
    },
    {
      "text": "the response so the full text in the",
      "start": 1534.32,
      "duration": 4.76
    },
    {
      "text": "alpaka format is created and then what",
      "start": 1536.64,
      "duration": 4.639
    },
    {
      "text": "we do is that as I mentioned in the next",
      "start": 1539.08,
      "duration": 4.479
    },
    {
      "text": "step over here we are going to tokenize",
      "start": 1541.279,
      "duration": 6.64
    },
    {
      "text": "the formatted data so we uh we first",
      "start": 1543.559,
      "duration": 7.081
    },
    {
      "text": "Define a empty list and then we start",
      "start": 1547.919,
      "duration": 5.281
    },
    {
      "text": "appending the token IDs to this Mt list",
      "start": 1550.64,
      "duration": 4.759
    },
    {
      "text": "so let's say if you have a prompt which",
      "start": 1553.2,
      "duration": 4.64
    },
    {
      "text": "looks like this each of the tokens here",
      "start": 1555.399,
      "duration": 4.681
    },
    {
      "text": "are conver ConEd into token IDs and then",
      "start": 1557.84,
      "duration": 4.839
    },
    {
      "text": "appended to a list so for every prompt",
      "start": 1560.08,
      "duration": 4.88
    },
    {
      "text": "corresponding to each input output pair",
      "start": 1562.679,
      "duration": 4.6
    },
    {
      "text": "now we have a list of token IDs which is",
      "start": 1564.96,
      "duration": 4.599
    },
    {
      "text": "mentioned in the step one over here see",
      "start": 1567.279,
      "duration": 4.52
    },
    {
      "text": "here what we are doing is that every",
      "start": 1569.559,
      "duration": 4.12
    },
    {
      "text": "prompt we are converting it into token",
      "start": 1571.799,
      "duration": 4.521
    },
    {
      "text": "IDs and for that the tokenizer which we",
      "start": 1573.679,
      "duration": 5.521
    },
    {
      "text": "are going to use is we also need to pass",
      "start": 1576.32,
      "duration": 4.92
    },
    {
      "text": "this to the instruction data set class",
      "start": 1579.2,
      "duration": 4.359
    },
    {
      "text": "but it's going to be the tick token",
      "start": 1581.24,
      "duration": 4.799
    },
    {
      "text": "Library I'll share the link to this uh",
      "start": 1583.559,
      "duration": 5.081
    },
    {
      "text": "in the chat we have had a separate",
      "start": 1586.039,
      "duration": 4.441
    },
    {
      "text": "lecture on bite pair encoder in this",
      "start": 1588.64,
      "duration": 3.88
    },
    {
      "text": "lecture Series so if you want to",
      "start": 1590.48,
      "duration": 3.96
    },
    {
      "text": "understand about this library in detail",
      "start": 1592.52,
      "duration": 5.24
    },
    {
      "text": "I highly encourage you to uh watch that",
      "start": 1594.44,
      "duration": 6.2
    },
    {
      "text": "lecture right so now what does the",
      "start": 1597.76,
      "duration": 5.039
    },
    {
      "text": "instruction class ENT instruction data",
      "start": 1600.64,
      "duration": 4.12
    },
    {
      "text": "set class essentially return return well",
      "start": 1602.799,
      "duration": 5.24
    },
    {
      "text": "it returns for every uh for every data",
      "start": 1604.76,
      "duration": 5.56
    },
    {
      "text": "which is in this format it converts it",
      "start": 1608.039,
      "duration": 4.481
    },
    {
      "text": "into the alpaka style prompt and then it",
      "start": 1610.32,
      "duration": 6.04
    },
    {
      "text": "returns uh a bunch of token IDs for",
      "start": 1612.52,
      "duration": 6.12
    },
    {
      "text": "every entry",
      "start": 1616.36,
      "duration": 7.559
    },
    {
      "text": "awesome so uh let's go to the next part",
      "start": 1618.64,
      "duration": 7.84
    },
    {
      "text": "now before coming to the next part I",
      "start": 1623.919,
      "duration": 4.681
    },
    {
      "text": "just want to show you the end of text",
      "start": 1626.48,
      "duration": 4.76
    },
    {
      "text": "token and its corresponding token ID as",
      "start": 1628.6,
      "duration": 5.04
    },
    {
      "text": "we had seen on the Whiteboard and it's",
      "start": 1631.24,
      "duration": 3.52
    },
    {
      "text": "indeed",
      "start": 1633.64,
      "duration": 3.639
    },
    {
      "text": "5256 so that's why we are using this",
      "start": 1634.76,
      "duration": 5.24
    },
    {
      "text": "50256 token ID because it conveys the",
      "start": 1637.279,
      "duration": 5.88
    },
    {
      "text": "end of text okay now as I mentioned what",
      "start": 1640.0,
      "duration": 4.72
    },
    {
      "text": "we are going to do here is that we are",
      "start": 1643.159,
      "duration": 4.24
    },
    {
      "text": "going to define a custom colate function",
      "start": 1644.72,
      "duration": 4.64
    },
    {
      "text": "what this custom colet function does is",
      "start": 1647.399,
      "duration": 4.28
    },
    {
      "text": "that the name may sound complex but it",
      "start": 1649.36,
      "duration": 3.919
    },
    {
      "text": "actually does a very simple thing it",
      "start": 1651.679,
      "duration": 4.321
    },
    {
      "text": "takes the inputs in each data set that's",
      "start": 1653.279,
      "duration": 4.76
    },
    {
      "text": "the first thing it takes the input in",
      "start": 1656.0,
      "duration": 4.559
    },
    {
      "text": "each batch it finds that input with the",
      "start": 1658.039,
      "duration": 5.441
    },
    {
      "text": "maximum length and then it appends the",
      "start": 1660.559,
      "duration": 6.761
    },
    {
      "text": "50256 or pads the 50256 token ID to all",
      "start": 1663.48,
      "duration": 5.6
    },
    {
      "text": "other inputs that's the only thing which",
      "start": 1667.32,
      "duration": 2.64
    },
    {
      "text": "it is",
      "start": 1669.08,
      "duration": 4.079
    },
    {
      "text": "doing so this custom colate draft one it",
      "start": 1669.96,
      "duration": 5.559
    },
    {
      "text": "takes the batch so you can think of the",
      "start": 1673.159,
      "duration": 5.601
    },
    {
      "text": "batch as coming in this format like this",
      "start": 1675.519,
      "duration": 5.481
    },
    {
      "text": "uh and then it has you have to give the",
      "start": 1678.76,
      "duration": 4.799
    },
    {
      "text": "padding token ID which is 50256 and the",
      "start": 1681.0,
      "duration": 5.159
    },
    {
      "text": "device which is CPU so this function",
      "start": 1683.559,
      "duration": 4.6
    },
    {
      "text": "implements four steps first it finds the",
      "start": 1686.159,
      "duration": 4.12
    },
    {
      "text": "longest sequence in the batch and then",
      "start": 1688.159,
      "duration": 4.12
    },
    {
      "text": "it pads the other sequences so that the",
      "start": 1690.279,
      "duration": 4.0
    },
    {
      "text": "length is equal to the longest sequence",
      "start": 1692.279,
      "duration": 4.321
    },
    {
      "text": "that's it and then it converts the list",
      "start": 1694.279,
      "duration": 4.4
    },
    {
      "text": "of inputs into a tensor and transfers to",
      "start": 1696.6,
      "duration": 4.52
    },
    {
      "text": "our Target device which is the CPU so",
      "start": 1698.679,
      "duration": 4.24
    },
    {
      "text": "this entire thing is converted into a",
      "start": 1701.12,
      "duration": 4.24
    },
    {
      "text": "tensor what Ive marked with this orange",
      "start": 1702.919,
      "duration": 5.24
    },
    {
      "text": "color over here that's the function of",
      "start": 1705.36,
      "duration": 4.799
    },
    {
      "text": "of the custom colla draft so let's see",
      "start": 1708.159,
      "duration": 4.481
    },
    {
      "text": "how it does it the first thing this",
      "start": 1710.159,
      "duration": 5.041
    },
    {
      "text": "function does is that it will find the",
      "start": 1712.64,
      "duration": 4.24
    },
    {
      "text": "longest sequence in the batch and it",
      "start": 1715.2,
      "duration": 4.0
    },
    {
      "text": "will add it by one so let's say if you",
      "start": 1716.88,
      "duration": 5.08
    },
    {
      "text": "have these three it if you have these",
      "start": 1719.2,
      "duration": 4.4
    },
    {
      "text": "three the longest sequence length is",
      "start": 1721.96,
      "duration": 3.4
    },
    {
      "text": "five and then it will add it by one so",
      "start": 1723.6,
      "duration": 3.76
    },
    {
      "text": "then it will be six there is a reason",
      "start": 1725.36,
      "duration": 3.64
    },
    {
      "text": "why you add it by one and I'll come to",
      "start": 1727.36,
      "duration": 4.36
    },
    {
      "text": "that later but after you add it by one",
      "start": 1729.0,
      "duration": 4.559
    },
    {
      "text": "what you do is that for every item in",
      "start": 1731.72,
      "duration": 4.079
    },
    {
      "text": "the batch you first add a token ID so",
      "start": 1733.559,
      "duration": 4.84
    },
    {
      "text": "even for the first one even for for the",
      "start": 1735.799,
      "duration": 5.6
    },
    {
      "text": "first item you add this 50256 token",
      "start": 1738.399,
      "duration": 4.801
    },
    {
      "text": "that's the first thing which you do and",
      "start": 1741.399,
      "duration": 5.201
    },
    {
      "text": "then you pad the 50256 again so that the",
      "start": 1743.2,
      "duration": 6.28
    },
    {
      "text": "length is equal to the maximum length",
      "start": 1746.6,
      "duration": 5.28
    },
    {
      "text": "and then what you do is you",
      "start": 1749.48,
      "duration": 4.88
    },
    {
      "text": "remove uh you remove the extra added",
      "start": 1751.88,
      "duration": 5.039
    },
    {
      "text": "token so here essentially what we are",
      "start": 1754.36,
      "duration": 5.36
    },
    {
      "text": "doing is that let's say if you have",
      "start": 1756.919,
      "duration": 5.801
    },
    {
      "text": "uh I'll actually remove this in the code",
      "start": 1759.72,
      "duration": 5.72
    },
    {
      "text": "what we are trying to do is that we add",
      "start": 1762.72,
      "duration": 5.64
    },
    {
      "text": "first a 50256 token ID to all of these",
      "start": 1765.44,
      "duration": 4.56
    },
    {
      "text": "so even to the first one we add this",
      "start": 1768.36,
      "duration": 4.84
    },
    {
      "text": "50256 token and to the other ones we add",
      "start": 1770.0,
      "duration": 6.08
    },
    {
      "text": "the 50256 token then this will be added",
      "start": 1773.2,
      "duration": 5.12
    },
    {
      "text": "one 2 three three more times so total it",
      "start": 1776.08,
      "duration": 4.599
    },
    {
      "text": "will be added four times and here we'll",
      "start": 1778.32,
      "duration": 4.959
    },
    {
      "text": "add it a total of three times right but",
      "start": 1780.679,
      "duration": 3.961
    },
    {
      "text": "then you might think why are we adding",
      "start": 1783.279,
      "duration": 4.52
    },
    {
      "text": "an extra 50256 token because here we",
      "start": 1784.64,
      "duration": 5.56
    },
    {
      "text": "don't need to add 50256 here also we",
      "start": 1787.799,
      "duration": 4.401
    },
    {
      "text": "need to add three times here also we",
      "start": 1790.2,
      "duration": 3.959
    },
    {
      "text": "need to add it two times so then we get",
      "start": 1792.2,
      "duration": 5.079
    },
    {
      "text": "rid of that extra token later the reason",
      "start": 1794.159,
      "duration": 5.12
    },
    {
      "text": "we do this EXT extra addition is that it",
      "start": 1797.279,
      "duration": 3.921
    },
    {
      "text": "later helps us to create the target",
      "start": 1799.279,
      "duration": 3.88
    },
    {
      "text": "token because if you already add an",
      "start": 1801.2,
      "duration": 4.88
    },
    {
      "text": "extra token creating the target creating",
      "start": 1803.159,
      "duration": 4.601
    },
    {
      "text": "the target is just simple because then",
      "start": 1806.08,
      "duration": 3.479
    },
    {
      "text": "you just use this much to create the",
      "start": 1807.76,
      "duration": 4.36
    },
    {
      "text": "target as we saw before the target is",
      "start": 1809.559,
      "duration": 4.12
    },
    {
      "text": "just the input you remove the first",
      "start": 1812.12,
      "duration": 3.559
    },
    {
      "text": "element and then you add",
      "start": 1813.679,
      "duration": 6.0
    },
    {
      "text": "50256 so earlier adding the 50256 token",
      "start": 1815.679,
      "duration": 6.48
    },
    {
      "text": "to all of the inputs in this part of the",
      "start": 1819.679,
      "duration": 4.401
    },
    {
      "text": "code it's important because it easily",
      "start": 1822.159,
      "duration": 5.041
    },
    {
      "text": "helps us to create the target ID uh to",
      "start": 1824.08,
      "duration": 5.88
    },
    {
      "text": "create the target for every inputs so",
      "start": 1827.2,
      "duration": 4.199
    },
    {
      "text": "essentially what we do is we add an",
      "start": 1829.96,
      "duration": 4.319
    },
    {
      "text": "extra 50256 and then get rid of it later",
      "start": 1831.399,
      "duration": 4.841
    },
    {
      "text": "and then we pad everything all the other",
      "start": 1834.279,
      "duration": 4.841
    },
    {
      "text": "inputs with 50256 so that the length is",
      "start": 1836.24,
      "duration": 6.279
    },
    {
      "text": "equal to the maximum token ID length or",
      "start": 1839.12,
      "duration": 5.559
    },
    {
      "text": "the that input which has the maximum",
      "start": 1842.519,
      "duration": 4.04
    },
    {
      "text": "length so",
      "start": 1844.679,
      "duration": 4.36
    },
    {
      "text": "essentially uh when you reach this part",
      "start": 1846.559,
      "duration": 5.521
    },
    {
      "text": "of the code every item in the batch so",
      "start": 1849.039,
      "duration": 5.441
    },
    {
      "text": "all of these three items essentially",
      "start": 1852.08,
      "duration": 4.839
    },
    {
      "text": "will have the same length that's what is",
      "start": 1854.48,
      "duration": 4.079
    },
    {
      "text": "happening in the code and ultimately we",
      "start": 1856.919,
      "duration": 6.0
    },
    {
      "text": "convert this um into a tensor and the",
      "start": 1858.559,
      "duration": 6.36
    },
    {
      "text": "tensor is the input tensor which is",
      "start": 1862.919,
      "duration": 4.24
    },
    {
      "text": "returned by this function custom colate",
      "start": 1864.919,
      "duration": 4.521
    },
    {
      "text": "draft one now let's see a practical",
      "start": 1867.159,
      "duration": 4.64
    },
    {
      "text": "application of this if you have",
      "start": 1869.44,
      "duration": 4.64
    },
    {
      "text": "uh these three inputs like this if",
      "start": 1871.799,
      "duration": 4.72
    },
    {
      "text": "inputs one has the size of five inputs",
      "start": 1874.08,
      "duration": 4.28
    },
    {
      "text": "two has the size of two and inputs three",
      "start": 1876.519,
      "duration": 3.921
    },
    {
      "text": "has the size of three the batch you",
      "start": 1878.36,
      "duration": 3.64
    },
    {
      "text": "create a batch with these three inputs",
      "start": 1880.44,
      "duration": 4.04
    },
    {
      "text": "and then you pass these you pass this",
      "start": 1882.0,
      "duration": 4.76
    },
    {
      "text": "batch into the custom Cola draft one now",
      "start": 1884.48,
      "duration": 4.84
    },
    {
      "text": "let's see the output as you can see the",
      "start": 1886.76,
      "duration": 4.399
    },
    {
      "text": "first input remains unchanged it has",
      "start": 1889.32,
      "duration": 3.52
    },
    {
      "text": "five token IDs because those are the",
      "start": 1891.159,
      "duration": 4.601
    },
    {
      "text": "maximum length in the second inputs we",
      "start": 1892.84,
      "duration": 5.839
    },
    {
      "text": "pad 50256 three times so that the length",
      "start": 1895.76,
      "duration": 5.2
    },
    {
      "text": "becomes same as the first input sequence",
      "start": 1898.679,
      "duration": 4.96
    },
    {
      "text": "and in the third input we pad 50256 two",
      "start": 1900.96,
      "duration": 4.16
    },
    {
      "text": "times so that the length becomes the",
      "start": 1903.639,
      "duration": 3.76
    },
    {
      "text": "same as the first two so now you can see",
      "start": 1905.12,
      "duration": 4.32
    },
    {
      "text": "we have an input stenor in which every",
      "start": 1907.399,
      "duration": 5.12
    },
    {
      "text": "row has the L has five columns",
      "start": 1909.44,
      "duration": 6.199
    },
    {
      "text": "awesome so as we can see",
      "start": 1912.519,
      "duration": 5.841
    },
    {
      "text": "here all inputs have been ped to the",
      "start": 1915.639,
      "duration": 5.0
    },
    {
      "text": "length of the longest input list inputs",
      "start": 1918.36,
      "duration": 5.24
    },
    {
      "text": "one which contains five token IDs",
      "start": 1920.639,
      "duration": 5.841
    },
    {
      "text": "awesome uh so until now we have reached",
      "start": 1923.6,
      "duration": 4.72
    },
    {
      "text": "this stage where we have padded the",
      "start": 1926.48,
      "duration": 3.799
    },
    {
      "text": "inputs with the token IDs and now we",
      "start": 1928.32,
      "duration": 4.16
    },
    {
      "text": "have to implement the next part of this",
      "start": 1930.279,
      "duration": 4.321
    },
    {
      "text": "process the next part is essentially",
      "start": 1932.48,
      "duration": 4.72
    },
    {
      "text": "creating Target token IDs for",
      "start": 1934.6,
      "duration": 5.0
    },
    {
      "text": "training so until now we have just",
      "start": 1937.2,
      "duration": 4.16
    },
    {
      "text": "implemented our first custom colate",
      "start": 1939.6,
      "duration": 4.52
    },
    {
      "text": "function to create batches from list of",
      "start": 1941.36,
      "duration": 5.0
    },
    {
      "text": "inputs however as you have learned in",
      "start": 1944.12,
      "duration": 4.399
    },
    {
      "text": "previous lessons we also need to create",
      "start": 1946.36,
      "duration": 4.48
    },
    {
      "text": "batches with the target token IDs right",
      "start": 1948.519,
      "duration": 3.801
    },
    {
      "text": "because we need to know what the real",
      "start": 1950.84,
      "duration": 4.319
    },
    {
      "text": "answer is the target token IDs are",
      "start": 1952.32,
      "duration": 4.599
    },
    {
      "text": "crucial because they represent what we",
      "start": 1955.159,
      "duration": 4.281
    },
    {
      "text": "want the model to generate and based on",
      "start": 1956.919,
      "duration": 4.441
    },
    {
      "text": "the target token IDs itself we'll get",
      "start": 1959.44,
      "duration": 3.839
    },
    {
      "text": "the loss function",
      "start": 1961.36,
      "duration": 4.96
    },
    {
      "text": "ultimately so as I explained to you",
      "start": 1963.279,
      "duration": 4.76
    },
    {
      "text": "thoroughly on the Whiteboard the way to",
      "start": 1966.32,
      "duration": 4.44
    },
    {
      "text": "get the target token ID is just to shift",
      "start": 1968.039,
      "duration": 3.441
    },
    {
      "text": "the",
      "start": 1970.76,
      "duration": 3.24
    },
    {
      "text": "input uh to the right by one and then",
      "start": 1971.48,
      "duration": 4.52
    },
    {
      "text": "add an additional padding token towards",
      "start": 1974.0,
      "duration": 4.279
    },
    {
      "text": "the end and that's exactly what we are",
      "start": 1976.0,
      "duration": 4.24
    },
    {
      "text": "going to do in the code so if you see in",
      "start": 1978.279,
      "duration": 4.961
    },
    {
      "text": "the code until this part it Remains the",
      "start": 1980.24,
      "duration": 5.76
    },
    {
      "text": "Same we have the inputs um and they're",
      "start": 1983.24,
      "duration": 5.399
    },
    {
      "text": "padded by the 50256 token and now if you",
      "start": 1986.0,
      "duration": 4.76
    },
    {
      "text": "see the targets token it just shifted to",
      "start": 1988.639,
      "duration": 4.681
    },
    {
      "text": "the right by one so here you see one",
      "start": 1990.76,
      "duration": 4.279
    },
    {
      "text": "colon which means that you forget the",
      "start": 1993.32,
      "duration": 4.199
    },
    {
      "text": "first entry and you take the remaining",
      "start": 1995.039,
      "duration": 4.721
    },
    {
      "text": "entries uh and here you don't even need",
      "start": 1997.519,
      "duration": 4.921
    },
    {
      "text": "to add the 50256 token we because we",
      "start": 1999.76,
      "duration": 5.32
    },
    {
      "text": "have already added added an extra 5256",
      "start": 2002.44,
      "duration": 5.079
    },
    {
      "text": "token and this is why we add that extra",
      "start": 2005.08,
      "duration": 4.959
    },
    {
      "text": "5 0256 token as I showed you earlier",
      "start": 2007.519,
      "duration": 6.12
    },
    {
      "text": "because here you see actually in the",
      "start": 2010.039,
      "duration": 5.281
    },
    {
      "text": "first input you don't need to add the",
      "start": 2013.639,
      "duration": 5.0
    },
    {
      "text": "50256 token but if you add it it makes",
      "start": 2015.32,
      "duration": 6.719
    },
    {
      "text": "it very easy to create the target um it",
      "start": 2018.639,
      "duration": 5.88
    },
    {
      "text": "makes it very easy to create the the",
      "start": 2022.039,
      "duration": 4.88
    },
    {
      "text": "target sensor why because you just",
      "start": 2024.519,
      "duration": 4.441
    },
    {
      "text": "ignore the first element and take",
      "start": 2026.919,
      "duration": 4.36
    },
    {
      "text": "everything from the second element so",
      "start": 2028.96,
      "duration": 4.319
    },
    {
      "text": "here you ignore the first element which",
      "start": 2031.279,
      "duration": 4.081
    },
    {
      "text": "is zero and take everything from the",
      "start": 2033.279,
      "duration": 4.081
    },
    {
      "text": "second element so the target will be 1 2",
      "start": 2035.36,
      "duration": 3.199
    },
    {
      "text": "3 4 4",
      "start": 2037.36,
      "duration": 3.96
    },
    {
      "text": "5256 in the second the target will be 6",
      "start": 2038.559,
      "duration": 6.921
    },
    {
      "text": "50256 50256 50256 and one more",
      "start": 2041.32,
      "duration": 6.56
    },
    {
      "text": "5256 so it's the inputs which are",
      "start": 2045.48,
      "duration": 5.28
    },
    {
      "text": "shifted to the right by one so that is",
      "start": 2047.88,
      "duration": 5.08
    },
    {
      "text": "how you create the target tensor and",
      "start": 2050.76,
      "duration": 4.56
    },
    {
      "text": "then you just return the input tensor",
      "start": 2052.96,
      "duration": 3.639
    },
    {
      "text": "and the target",
      "start": 2055.32,
      "duration": 3.759
    },
    {
      "text": "tensor so the simplest way to think",
      "start": 2056.599,
      "duration": 4.32
    },
    {
      "text": "about this code is that until now we",
      "start": 2059.079,
      "duration": 3.481
    },
    {
      "text": "have made sure that the inputs are of",
      "start": 2060.919,
      "duration": 3.401
    },
    {
      "text": "the same length due to the padding which",
      "start": 2062.56,
      "duration": 3.88
    },
    {
      "text": "we have done and the targets is the",
      "start": 2064.32,
      "duration": 3.88
    },
    {
      "text": "inputs which are shifted to the right by",
      "start": 2066.44,
      "duration": 4.0
    },
    {
      "text": "one this is the very important process",
      "start": 2068.2,
      "duration": 3.959
    },
    {
      "text": "and as I mentioned to you this is the",
      "start": 2070.44,
      "duration": 6.84
    },
    {
      "text": "non-intuitive step because uh the true",
      "start": 2072.159,
      "duration": 7.841
    },
    {
      "text": "value is just the input which is shifted",
      "start": 2077.28,
      "duration": 4.839
    },
    {
      "text": "to the right by one and that is what is",
      "start": 2080.0,
      "duration": 3.919
    },
    {
      "text": "nonintuitive you might think that the",
      "start": 2082.119,
      "duration": 3.361
    },
    {
      "text": "response needs to be given in the True",
      "start": 2083.919,
      "duration": 3.76
    },
    {
      "text": "Value right why is the instruction and",
      "start": 2085.48,
      "duration": 4.84
    },
    {
      "text": "input also given in the True Value but",
      "start": 2087.679,
      "duration": 4.2
    },
    {
      "text": "it's given because in the next word",
      "start": 2090.32,
      "duration": 3.48
    },
    {
      "text": "prediction task the llm automatically",
      "start": 2091.879,
      "duration": 3.121
    },
    {
      "text": "learns that when you have the",
      "start": 2093.8,
      "duration": 2.64
    },
    {
      "text": "instruction and the input you have to",
      "start": 2095.0,
      "duration": 4.28
    },
    {
      "text": "predict the response",
      "start": 2096.44,
      "duration": 4.88
    },
    {
      "text": "this was a bit harder for me to explain",
      "start": 2099.28,
      "duration": 4.76
    },
    {
      "text": "but I hope you have got this idea um in",
      "start": 2101.32,
      "duration": 4.6
    },
    {
      "text": "the code if it's difficult to understand",
      "start": 2104.04,
      "duration": 3.76
    },
    {
      "text": "just try imagining it through the visual",
      "start": 2105.92,
      "duration": 3.52
    },
    {
      "text": "representation which I showed to you on",
      "start": 2107.8,
      "duration": 3.88
    },
    {
      "text": "the Whiteboard you can even try going",
      "start": 2109.44,
      "duration": 3.96
    },
    {
      "text": "back as you are learning this lecture to",
      "start": 2111.68,
      "duration": 3.919
    },
    {
      "text": "see the Whiteboard explanation before",
      "start": 2113.4,
      "duration": 4.48
    },
    {
      "text": "you try to understand the code so there",
      "start": 2115.599,
      "duration": 3.801
    },
    {
      "text": "are actually only two things which are",
      "start": 2117.88,
      "duration": 4.16
    },
    {
      "text": "happening in this custom colate draft 2",
      "start": 2119.4,
      "duration": 5.24
    },
    {
      "text": "it takes it truncates the last token for",
      "start": 2122.04,
      "duration": 4.48
    },
    {
      "text": "the inputs so that everything is of the",
      "start": 2124.64,
      "duration": 3.76
    },
    {
      "text": "same length and it shifts",
      "start": 2126.52,
      "duration": 3.72
    },
    {
      "text": "the input to the right by one to get the",
      "start": 2128.4,
      "duration": 4.32
    },
    {
      "text": "target tens and we can check this now",
      "start": 2130.24,
      "duration": 4.16
    },
    {
      "text": "let's say we have these three inputs as",
      "start": 2132.72,
      "duration": 3.68
    },
    {
      "text": "before we create a batch of these three",
      "start": 2134.4,
      "duration": 4.4
    },
    {
      "text": "inputs and then we call the custom col",
      "start": 2136.4,
      "duration": 4.56
    },
    {
      "text": "draft two function on this batch and",
      "start": 2138.8,
      "duration": 4.279
    },
    {
      "text": "we'll print the inputs and the targets",
      "start": 2140.96,
      "duration": 4.48
    },
    {
      "text": "so let's see as we learned before the",
      "start": 2143.079,
      "duration": 4.801
    },
    {
      "text": "inputs is just the first row Remains the",
      "start": 2145.44,
      "duration": 4.76
    },
    {
      "text": "Same the second row has three 50256",
      "start": 2147.88,
      "duration": 5.32
    },
    {
      "text": "tokens padded the third row has 250 256",
      "start": 2150.2,
      "duration": 5.0
    },
    {
      "text": "tokens padded and let's look at the",
      "start": 2153.2,
      "duration": 3.72
    },
    {
      "text": "Target if you look at the first row of",
      "start": 2155.2,
      "duration": 3.2
    },
    {
      "text": "the targets",
      "start": 2156.92,
      "duration": 3.96
    },
    {
      "text": "it's the first dra of the inputs you",
      "start": 2158.4,
      "duration": 4.199
    },
    {
      "text": "shift to the right by one so you take",
      "start": 2160.88,
      "duration": 3.959
    },
    {
      "text": "the remaining four and then you have 50",
      "start": 2162.599,
      "duration": 4.921
    },
    {
      "text": "256 token similarly if you look at the",
      "start": 2164.839,
      "duration": 4.921
    },
    {
      "text": "second row of the target it's basically",
      "start": 2167.52,
      "duration": 4.24
    },
    {
      "text": "the second row of the input you shift to",
      "start": 2169.76,
      "duration": 3.48
    },
    {
      "text": "the right by one which means you take",
      "start": 2171.76,
      "duration": 3.839
    },
    {
      "text": "only the remaining four values and then",
      "start": 2173.24,
      "duration": 3.879
    },
    {
      "text": "you add an extra",
      "start": 2175.599,
      "duration": 4.48
    },
    {
      "text": "5256 similarly if you take the third row",
      "start": 2177.119,
      "duration": 5.641
    },
    {
      "text": "of the targets it's essentially the",
      "start": 2180.079,
      "duration": 4.481
    },
    {
      "text": "third input but you shift to the right",
      "start": 2182.76,
      "duration": 3.559
    },
    {
      "text": "by one so you ignore the first and you",
      "start": 2184.56,
      "duration": 3.88
    },
    {
      "text": "take all the four and then you append an",
      "start": 2186.319,
      "duration": 5.28
    },
    {
      "text": "extra 50256 token ID towards the end",
      "start": 2188.44,
      "duration": 4.679
    },
    {
      "text": "that's how you get the inputs and the",
      "start": 2191.599,
      "duration": 3.681
    },
    {
      "text": "target tensor so the first tensor",
      "start": 2193.119,
      "duration": 4.681
    },
    {
      "text": "represents the inputs and the second",
      "start": 2195.28,
      "duration": 4.48
    },
    {
      "text": "tensor represents the",
      "start": 2197.8,
      "duration": 3.519
    },
    {
      "text": "target",
      "start": 2199.76,
      "duration": 3.96
    },
    {
      "text": "awesome now after this step is",
      "start": 2201.319,
      "duration": 4.161
    },
    {
      "text": "implemented we come to the next step",
      "start": 2203.72,
      "duration": 4.0
    },
    {
      "text": "which is essentially creating uh or",
      "start": 2205.48,
      "duration": 4.0
    },
    {
      "text": "replacing the padding tokens with",
      "start": 2207.72,
      "duration": 4.44
    },
    {
      "text": "placeholders which means that except for",
      "start": 2209.48,
      "duration": 5.4
    },
    {
      "text": "the first 50256 we'll replace all the",
      "start": 2212.16,
      "duration": 6.04
    },
    {
      "text": "remaining with minus 100 uh so in the",
      "start": 2214.88,
      "duration": 5.199
    },
    {
      "text": "next step we assign a minus 100",
      "start": 2218.2,
      "duration": 4.119
    },
    {
      "text": "placeholder to all the padding tokens",
      "start": 2220.079,
      "duration": 4.401
    },
    {
      "text": "this special value allows us to exclude",
      "start": 2222.319,
      "duration": 4.0
    },
    {
      "text": "these padding tokens from contributing",
      "start": 2224.48,
      "duration": 3.92
    },
    {
      "text": "to the training loss calculation as we",
      "start": 2226.319,
      "duration": 3.681
    },
    {
      "text": "saw on the white",
      "start": 2228.4,
      "duration": 3.679
    },
    {
      "text": "board",
      "start": 2230.0,
      "duration": 5.4
    },
    {
      "text": "um okay so in the following code okay",
      "start": 2232.079,
      "duration": 5.121
    },
    {
      "text": "one more thing to mention is that as I",
      "start": 2235.4,
      "duration": 3.28
    },
    {
      "text": "told you on the Whiteboard when we",
      "start": 2237.2,
      "duration": 5.0
    },
    {
      "text": "replace this 5025 tokens with minus 100",
      "start": 2238.68,
      "duration": 6.8
    },
    {
      "text": "we retain one 50256 token and the reason",
      "start": 2242.2,
      "duration": 5.6
    },
    {
      "text": "we retain one end of text token is",
      "start": 2245.48,
      "duration": 4.48
    },
    {
      "text": "because it allows the llm to learn when",
      "start": 2247.8,
      "duration": 4.16
    },
    {
      "text": "to generate an end of text token in the",
      "start": 2249.96,
      "duration": 3.119
    },
    {
      "text": "response to",
      "start": 2251.96,
      "duration": 3.52
    },
    {
      "text": "instructions which we use an which we",
      "start": 2253.079,
      "duration": 4.361
    },
    {
      "text": "use as an indicator that the generated",
      "start": 2255.48,
      "duration": 4.44
    },
    {
      "text": "response is now complete so you need one",
      "start": 2257.44,
      "duration": 5.399
    },
    {
      "text": "5256 token ID to say that or to",
      "start": 2259.92,
      "duration": 5.199
    },
    {
      "text": "represent that this is indeed the end of",
      "start": 2262.839,
      "duration": 5.041
    },
    {
      "text": "text so now what we have to do is that",
      "start": 2265.119,
      "duration": 5.0
    },
    {
      "text": "we have to take this custom colate draft",
      "start": 2267.88,
      "duration": 4.0
    },
    {
      "text": "two and then we have to modify it",
      "start": 2270.119,
      "duration": 3.801
    },
    {
      "text": "further so most of the function is the",
      "start": 2271.88,
      "duration": 4.439
    },
    {
      "text": "same which now I'm calling Custom colate",
      "start": 2273.92,
      "duration": 4.48
    },
    {
      "text": "function which takes in my batch my",
      "start": 2276.319,
      "duration": 4.921
    },
    {
      "text": "padding token ID and my ignore index so",
      "start": 2278.4,
      "duration": 4.04
    },
    {
      "text": "that's minus",
      "start": 2281.24,
      "duration": 4.72
    },
    {
      "text": "100 so what this does now is that until",
      "start": 2282.44,
      "duration": 5.56
    },
    {
      "text": "now here the steps are the same you get",
      "start": 2285.96,
      "duration": 5.0
    },
    {
      "text": "the inputs and the target sensor but now",
      "start": 2288.0,
      "duration": 4.839
    },
    {
      "text": "what you do is that you take the target",
      "start": 2290.96,
      "duration": 5.32
    },
    {
      "text": "sensor only and all the indexes except",
      "start": 2292.839,
      "duration": 6.201
    },
    {
      "text": "for the first 50256 you replace it with",
      "start": 2296.28,
      "duration": 5.72
    },
    {
      "text": "the ignore index so you first create a",
      "start": 2299.04,
      "duration": 5.68
    },
    {
      "text": "mask and that mask has all the indexes",
      "start": 2302.0,
      "duration": 4.92
    },
    {
      "text": "which has the padding token ID then you",
      "start": 2304.72,
      "duration": 4.04
    },
    {
      "text": "ignore the first index which has the",
      "start": 2306.92,
      "duration": 4.48
    },
    {
      "text": "padding token ID that's the first 50256",
      "start": 2308.76,
      "duration": 4.52
    },
    {
      "text": "value and then you replace all the",
      "start": 2311.4,
      "duration": 3.48
    },
    {
      "text": "remaining ones with ignore index which",
      "start": 2313.28,
      "duration": 3.72
    },
    {
      "text": "is minus",
      "start": 2314.88,
      "duration": 6.719
    },
    {
      "text": "100 uh okay so this now creates my input",
      "start": 2317.0,
      "duration": 7.119
    },
    {
      "text": "this now creates my input sensor and",
      "start": 2321.599,
      "duration": 5.361
    },
    {
      "text": "this creates my Target stenor and these",
      "start": 2324.119,
      "duration": 4.96
    },
    {
      "text": "are both returned by this function which",
      "start": 2326.96,
      "duration": 5.119
    },
    {
      "text": "is custom colate",
      "start": 2329.079,
      "duration": 3.0
    },
    {
      "text": "function okay until now if you note",
      "start": 2332.2,
      "duration": 5.36
    },
    {
      "text": "through the Whiteboard what we have",
      "start": 2336.079,
      "duration": 3.921
    },
    {
      "text": "implemented is that we have implemented",
      "start": 2337.56,
      "duration": 4.4
    },
    {
      "text": "this part of the code where you can see",
      "start": 2340.0,
      "duration": 4.64
    },
    {
      "text": "in the figure that",
      "start": 2341.96,
      "duration": 6.96
    },
    {
      "text": "uh here so except for the first 50256 we",
      "start": 2344.64,
      "duration": 7.16
    },
    {
      "text": "replace all of the remaining 50256 with",
      "start": 2348.92,
      "duration": 4.8
    },
    {
      "text": "the value of minus",
      "start": 2351.8,
      "duration": 4.799
    },
    {
      "text": "100 and now we are going to see why we",
      "start": 2353.72,
      "duration": 5.8
    },
    {
      "text": "replace the remaining 50256 token IDs",
      "start": 2356.599,
      "duration": 8.081
    },
    {
      "text": "with minus 100 so to to see why we",
      "start": 2359.52,
      "duration": 7.599
    },
    {
      "text": "replace the remaining token IDs with",
      "start": 2364.68,
      "duration": 4.48
    },
    {
      "text": "minus 100 we are going to see some",
      "start": 2367.119,
      "duration": 4.96
    },
    {
      "text": "implementations using pytorch but before",
      "start": 2369.16,
      "duration": 5.32
    },
    {
      "text": "that let's actually see whether our",
      "start": 2372.079,
      "duration": 5.161
    },
    {
      "text": "custom colate function is really working",
      "start": 2374.48,
      "duration": 4.68
    },
    {
      "text": "so to test that you take three inputs",
      "start": 2377.24,
      "duration": 4.92
    },
    {
      "text": "you have the inputs one as 0 1 2 3 4 You",
      "start": 2379.16,
      "duration": 5.64
    },
    {
      "text": "have the inputs two as 5 comma 6 and you",
      "start": 2382.16,
      "duration": 5.36
    },
    {
      "text": "have the inputs three as 7 8 and 9 you",
      "start": 2384.8,
      "duration": 5.039
    },
    {
      "text": "create a batch with these three inputs",
      "start": 2387.52,
      "duration": 3.96
    },
    {
      "text": "similar to the batch we have created",
      "start": 2389.839,
      "duration": 3.841
    },
    {
      "text": "before and then you create the inputs",
      "start": 2391.48,
      "duration": 4.08
    },
    {
      "text": "and targets based on the custom collate",
      "start": 2393.68,
      "duration": 4.36
    },
    {
      "text": "function now if you see the inputs and",
      "start": 2395.56,
      "duration": 4.2
    },
    {
      "text": "targets tensor which we had obtained",
      "start": 2398.04,
      "duration": 4.48
    },
    {
      "text": "before the inputs and targets tensor",
      "start": 2399.76,
      "duration": 4.76
    },
    {
      "text": "which we obtain now is actually exactly",
      "start": 2402.52,
      "duration": 6.04
    },
    {
      "text": "the same except that in the Target",
      "start": 2404.52,
      "duration": 7.079
    },
    {
      "text": "sensor uh except for the first",
      "start": 2408.56,
      "duration": 6.4
    },
    {
      "text": "50256 all the remaining 50256 values",
      "start": 2411.599,
      "duration": 6.121
    },
    {
      "text": "have been replaced with minus 100 this",
      "start": 2414.96,
      "duration": 4.639
    },
    {
      "text": "is the only change which has been done",
      "start": 2417.72,
      "duration": 4.119
    },
    {
      "text": "there is no changes to the input sensor",
      "start": 2419.599,
      "duration": 4.401
    },
    {
      "text": "the only change happens in the Target",
      "start": 2421.839,
      "duration": 5.561
    },
    {
      "text": "sensor where except for the first 50256",
      "start": 2424.0,
      "duration": 4.88
    },
    {
      "text": "all the remaining have been replaced",
      "start": 2427.4,
      "duration": 3.12
    },
    {
      "text": "with minus",
      "start": 2428.88,
      "duration": 5.68
    },
    {
      "text": "100 and now you can appreciate this code",
      "start": 2430.52,
      "duration": 5.599
    },
    {
      "text": "which is the custom colate function",
      "start": 2434.56,
      "duration": 4.12
    },
    {
      "text": "where in this 15 to 20 lines of code",
      "start": 2436.119,
      "duration": 4.601
    },
    {
      "text": "what we are essentially doing is that we",
      "start": 2438.68,
      "duration": 3.8
    },
    {
      "text": "we are implementing all of the steps",
      "start": 2440.72,
      "duration": 3.879
    },
    {
      "text": "which we have learned on the Whiteboard",
      "start": 2442.48,
      "duration": 4.8
    },
    {
      "text": "we implementing all of the steps over",
      "start": 2444.599,
      "duration": 6.601
    },
    {
      "text": "here so essentially we are implementing",
      "start": 2447.28,
      "duration": 6.88
    },
    {
      "text": "the we are implementing the padding we",
      "start": 2451.2,
      "duration": 4.96
    },
    {
      "text": "are implementing creating Target token",
      "start": 2454.16,
      "duration": 4.24
    },
    {
      "text": "IDs we are also implementing replacing",
      "start": 2456.16,
      "duration": 4.0
    },
    {
      "text": "the padding tokens with the placeholder",
      "start": 2458.4,
      "duration": 4.04
    },
    {
      "text": "value of minus 100 and that's also",
      "start": 2460.16,
      "duration": 4.04
    },
    {
      "text": "called ignore",
      "start": 2462.44,
      "duration": 4.879
    },
    {
      "text": "index so up till now it's working right",
      "start": 2464.2,
      "duration": 5.56
    },
    {
      "text": "but now you might be thinking that the",
      "start": 2467.319,
      "duration": 4.121
    },
    {
      "text": "modified colate function works as",
      "start": 2469.76,
      "duration": 3.839
    },
    {
      "text": "expected altering the target list by",
      "start": 2471.44,
      "duration": 4.919
    },
    {
      "text": "inserting the token ID of minus 100 but",
      "start": 2473.599,
      "duration": 5.041
    },
    {
      "text": "what is the logic behind this adjustment",
      "start": 2476.359,
      "duration": 5.201
    },
    {
      "text": "why do we replace with minus 100 so let",
      "start": 2478.64,
      "duration": 5.4
    },
    {
      "text": "us take a small demonstration uh and I'm",
      "start": 2481.56,
      "duration": 4.88
    },
    {
      "text": "going to show you the categorical or the",
      "start": 2484.04,
      "duration": 4.96
    },
    {
      "text": "cross entropy loss calculation right and",
      "start": 2486.44,
      "duration": 5.28
    },
    {
      "text": "the cross entropy loss calculation is",
      "start": 2489.0,
      "duration": 5.0
    },
    {
      "text": "based on a logic sensor and a Target",
      "start": 2491.72,
      "duration": 4.76
    },
    {
      "text": "sensor I'm not going into details of",
      "start": 2494.0,
      "duration": 4.16
    },
    {
      "text": "this Logics and targets because I have",
      "start": 2496.48,
      "duration": 4.24
    },
    {
      "text": "just taken two sample examples but for",
      "start": 2498.16,
      "duration": 4.679
    },
    {
      "text": "now you can think of it that the true",
      "start": 2500.72,
      "duration": 5.32
    },
    {
      "text": "answer is 0a 1 and the logits predicted",
      "start": 2502.839,
      "duration": 5.721
    },
    {
      "text": "are minus minus one and one for the",
      "start": 2506.04,
      "duration": 4.6
    },
    {
      "text": "first training example and for the",
      "start": 2508.56,
      "duration": 3.88
    },
    {
      "text": "second training example the Logics",
      "start": 2510.64,
      "duration": 5.28
    },
    {
      "text": "predictor are Min -.5 and 1.5 so this",
      "start": 2512.44,
      "duration": 5.08
    },
    {
      "text": "this is my predicted value and these are",
      "start": 2515.92,
      "duration": 4.08
    },
    {
      "text": "the targets to calculate the loss",
      "start": 2517.52,
      "duration": 4.64
    },
    {
      "text": "between the prediction and the target we",
      "start": 2520.0,
      "duration": 5.04
    },
    {
      "text": "use the cross entropy loss so I'll share",
      "start": 2522.16,
      "duration": 4.8
    },
    {
      "text": "the link to the pytorch Cross entropy",
      "start": 2525.04,
      "duration": 4.36
    },
    {
      "text": "loss which which calculates this loss",
      "start": 2526.96,
      "duration": 4.04
    },
    {
      "text": "between the prediction and the Target",
      "start": 2529.4,
      "duration": 3.16
    },
    {
      "text": "and if you print out the loss you'll get",
      "start": 2531.0,
      "duration": 5.079
    },
    {
      "text": "it to be 1.1 1269 that's good then what",
      "start": 2532.56,
      "duration": 6.36
    },
    {
      "text": "we do is that we add one more additional",
      "start": 2536.079,
      "duration": 5.161
    },
    {
      "text": "token ID in the prediction so in the",
      "start": 2538.92,
      "duration": 4.919
    },
    {
      "text": "logits two now we have three training",
      "start": 2541.24,
      "duration": 4.76
    },
    {
      "text": "examples with three predictions and we",
      "start": 2543.839,
      "duration": 3.961
    },
    {
      "text": "have three true answers",
      "start": 2546.0,
      "duration": 4.8
    },
    {
      "text": "so then we are using the categorical or",
      "start": 2547.8,
      "duration": 4.84
    },
    {
      "text": "the cross entropy function to calculate",
      "start": 2550.8,
      "duration": 4.36
    },
    {
      "text": "the loss between the prediction and",
      "start": 2552.64,
      "duration": 4.64
    },
    {
      "text": "between the actual value and here if you",
      "start": 2555.16,
      "duration": 3.679
    },
    {
      "text": "print out the loss you'll see that the",
      "start": 2557.28,
      "duration": 4.279
    },
    {
      "text": "loss is 7936 it differs from the",
      "start": 2558.839,
      "duration": 4.641
    },
    {
      "text": "previous loss because we added one more",
      "start": 2561.559,
      "duration": 4.481
    },
    {
      "text": "example now what I want to show you is",
      "start": 2563.48,
      "duration": 5.0
    },
    {
      "text": "that let's say instead of the targets",
      "start": 2566.04,
      "duration": 4.88
    },
    {
      "text": "two being 0 1 and one what if the",
      "start": 2568.48,
      "duration": 5.48
    },
    {
      "text": "targets three is 0 1 and minus",
      "start": 2570.92,
      "duration": 5.6
    },
    {
      "text": "100 so the Logics two will remain the",
      "start": 2573.96,
      "duration": 4.2
    },
    {
      "text": "same so the logits which are my",
      "start": 2576.52,
      "duration": 4.319
    },
    {
      "text": "predictions will remain the same but now",
      "start": 2578.16,
      "duration": 6.28
    },
    {
      "text": "the targets are 0 1 and minus 100 now I",
      "start": 2580.839,
      "duration": 5.24
    },
    {
      "text": "want to show you a interesting thing",
      "start": 2584.44,
      "duration": 4.36
    },
    {
      "text": "even if you add a minus 100 here and if",
      "start": 2586.079,
      "duration": 4.561
    },
    {
      "text": "you have a third example when you",
      "start": 2588.8,
      "duration": 4.44
    },
    {
      "text": "calculate this new loss you'll see that",
      "start": 2590.64,
      "duration": 5.56
    },
    {
      "text": "the loss for this is the same as the",
      "start": 2593.24,
      "duration": 5.64
    },
    {
      "text": "first loss which you had obtained so",
      "start": 2596.2,
      "duration": 4.24
    },
    {
      "text": "it's almost like adding the third",
      "start": 2598.88,
      "duration": 3.84
    },
    {
      "text": "training example made no difference at",
      "start": 2600.44,
      "duration": 6.2
    },
    {
      "text": "all so the loss here is 1.1 1269 and if",
      "start": 2602.72,
      "duration": 5.96
    },
    {
      "text": "you saw the the loss in the first logits",
      "start": 2606.64,
      "duration": 4.36
    },
    {
      "text": "one and targets one that was also 1.12",
      "start": 2608.68,
      "duration": 5.0
    },
    {
      "text": "69 so essentially there was no effect of",
      "start": 2611.0,
      "duration": 4.96
    },
    {
      "text": "this third prediction and the reason",
      "start": 2613.68,
      "duration": 3.6
    },
    {
      "text": "there was no effect of this third",
      "start": 2615.96,
      "duration": 3.32
    },
    {
      "text": "prediction is that the targets had minus",
      "start": 2617.28,
      "duration": 4.6
    },
    {
      "text": "100 that is the effect of ignore index",
      "start": 2619.28,
      "duration": 4.279
    },
    {
      "text": "equal to minus",
      "start": 2621.88,
      "duration": 4.52
    },
    {
      "text": "100 so in other words the cross entropy",
      "start": 2623.559,
      "duration": 5.121
    },
    {
      "text": "loss function ignored the third entry in",
      "start": 2626.4,
      "duration": 4.8
    },
    {
      "text": "the targets three Vector the token ID",
      "start": 2628.68,
      "duration": 5.639
    },
    {
      "text": "corresponding to minus 100 so you can",
      "start": 2631.2,
      "duration": 5.44
    },
    {
      "text": "try reading replacing the minus 100 with",
      "start": 2634.319,
      "duration": 4.601
    },
    {
      "text": "another token and then the loss will not",
      "start": 2636.64,
      "duration": 4.919
    },
    {
      "text": "be the same it's only for minus 100 so",
      "start": 2638.92,
      "duration": 4.56
    },
    {
      "text": "what's special aboutus 100 that it's",
      "start": 2641.559,
      "duration": 4.721
    },
    {
      "text": "ignored by the cross entropy loss well",
      "start": 2643.48,
      "duration": 4.8
    },
    {
      "text": "the default setting of the Cross entropy",
      "start": 2646.28,
      "duration": 5.279
    },
    {
      "text": "function in pytorch is cross entropy",
      "start": 2648.28,
      "duration": 6.2
    },
    {
      "text": "ignore index equal to minus 100 and you",
      "start": 2651.559,
      "duration": 5.04
    },
    {
      "text": "can see this over here also if you look",
      "start": 2654.48,
      "duration": 4.119
    },
    {
      "text": "at Cross entropy loss formulation you'll",
      "start": 2656.599,
      "duration": 3.641
    },
    {
      "text": "see that the ignore index is equal to",
      "start": 2658.599,
      "duration": 3.96
    },
    {
      "text": "minus 100 over here that's the default",
      "start": 2660.24,
      "duration": 4.119
    },
    {
      "text": "setting of the pytorch Cross entropy",
      "start": 2662.559,
      "duration": 4.721
    },
    {
      "text": "loss this means that pytorch ignores all",
      "start": 2664.359,
      "duration": 4.681
    },
    {
      "text": "the targets which are labeled with minus",
      "start": 2667.28,
      "duration": 5.279
    },
    {
      "text": "100 so now if my target tensor has minus",
      "start": 2669.04,
      "duration": 5.68
    },
    {
      "text": "100 over here it will ignore all of",
      "start": 2672.559,
      "duration": 4.0
    },
    {
      "text": "those predictions corresponding to these",
      "start": 2674.72,
      "duration": 3.96
    },
    {
      "text": "indices which have minus 100 in the",
      "start": 2676.559,
      "duration": 4.481
    },
    {
      "text": "targets and that's good for us because",
      "start": 2678.68,
      "duration": 4.28
    },
    {
      "text": "these anyways don't represent anything",
      "start": 2681.04,
      "duration": 3.72
    },
    {
      "text": "meaningful that's why they won't",
      "start": 2682.96,
      "duration": 4.639
    },
    {
      "text": "contribute to our loss",
      "start": 2684.76,
      "duration": 5.559
    },
    {
      "text": "function um so in this chapter we",
      "start": 2687.599,
      "duration": 4.361
    },
    {
      "text": "actually take advantage of this ignore",
      "start": 2690.319,
      "duration": 3.481
    },
    {
      "text": "index to ignore the additional end of",
      "start": 2691.96,
      "duration": 5.76
    },
    {
      "text": "text padding tokens that we that we use",
      "start": 2693.8,
      "duration": 7.84
    },
    {
      "text": "to pad so we we take advantage of this",
      "start": 2697.72,
      "duration": 5.639
    },
    {
      "text": "ignore index to ignore the additional",
      "start": 2701.64,
      "duration": 3.679
    },
    {
      "text": "end of text tokens that we used to pad",
      "start": 2703.359,
      "duration": 4.161
    },
    {
      "text": "the training examples to have the same",
      "start": 2705.319,
      "duration": 4.401
    },
    {
      "text": "length in each batch however as I",
      "start": 2707.52,
      "duration": 5.599
    },
    {
      "text": "mentioned we kept one 15256 token ID so",
      "start": 2709.72,
      "duration": 5.359
    },
    {
      "text": "this I will reiterate again because",
      "start": 2713.119,
      "duration": 4.681
    },
    {
      "text": "sometimes students forget this we we we",
      "start": 2715.079,
      "duration": 5.161
    },
    {
      "text": "retain this this first",
      "start": 2717.8,
      "duration": 5.559
    },
    {
      "text": "50256 we retain the first",
      "start": 2720.24,
      "duration": 6.079
    },
    {
      "text": "50256 in all of the targets so even if",
      "start": 2723.359,
      "duration": 4.161
    },
    {
      "text": "you see in the",
      "start": 2726.319,
      "duration": 3.601
    },
    {
      "text": "even if you replace the other 50256 with",
      "start": 2727.52,
      "duration": 4.48
    },
    {
      "text": "minus 100 you retain the first",
      "start": 2729.92,
      "duration": 4.48
    },
    {
      "text": "one and you retain the first one because",
      "start": 2732.0,
      "duration": 4.28
    },
    {
      "text": "it helps the llm to learn to generate",
      "start": 2734.4,
      "duration": 4.199
    },
    {
      "text": "end of text tokens and that's an",
      "start": 2736.28,
      "duration": 4.64
    },
    {
      "text": "indicator that the response is",
      "start": 2738.599,
      "duration": 4.801
    },
    {
      "text": "complete so this is the entire process",
      "start": 2740.92,
      "duration": 4.24
    },
    {
      "text": "and this is the entire workflow for",
      "start": 2743.4,
      "duration": 3.36
    },
    {
      "text": "implementing the batching in the",
      "start": 2745.16,
      "duration": 3.72
    },
    {
      "text": "training data set there are very few",
      "start": 2746.76,
      "duration": 3.68
    },
    {
      "text": "videos which explain this batching",
      "start": 2748.88,
      "duration": 4.439
    },
    {
      "text": "process in detail but as I say so many",
      "start": 2750.44,
      "duration": 5.0
    },
    {
      "text": "times the devil lies in the details so",
      "start": 2753.319,
      "duration": 4.081
    },
    {
      "text": "if you directly go to the model training",
      "start": 2755.44,
      "duration": 3.36
    },
    {
      "text": "right now I could have shown you that",
      "start": 2757.4,
      "duration": 4.56
    },
    {
      "text": "directly without explaining this lecture",
      "start": 2758.8,
      "duration": 4.759
    },
    {
      "text": "but there is so much information to",
      "start": 2761.96,
      "duration": 3.359
    },
    {
      "text": "learn here this padding tokens this",
      "start": 2763.559,
      "duration": 5.161
    },
    {
      "text": "minus 100 then creating Target token IDs",
      "start": 2765.319,
      "duration": 5.601
    },
    {
      "text": "which are just shifted uh to the right",
      "start": 2768.72,
      "duration": 4.0
    },
    {
      "text": "hand side by one all of this information",
      "start": 2770.92,
      "duration": 3.36
    },
    {
      "text": "would have been lost if I directly",
      "start": 2772.72,
      "duration": 5.359
    },
    {
      "text": "jumped to the fine tuning process so in",
      "start": 2774.28,
      "duration": 5.68
    },
    {
      "text": "the instruction fine tuning creating",
      "start": 2778.079,
      "duration": 3.961
    },
    {
      "text": "these batches is very important to learn",
      "start": 2779.96,
      "duration": 4.159
    },
    {
      "text": "because as I explained to you there is a",
      "start": 2782.04,
      "duration": 4.68
    },
    {
      "text": "specific fstep process to do this first",
      "start": 2784.119,
      "duration": 4.44
    },
    {
      "text": "you you have to convert the data to the",
      "start": 2786.72,
      "duration": 4.0
    },
    {
      "text": "alpaka prom template then you have to",
      "start": 2788.559,
      "duration": 4.161
    },
    {
      "text": "tokenize the formatted data into token",
      "start": 2790.72,
      "duration": 4.16
    },
    {
      "text": "ID then you have to append padding",
      "start": 2792.72,
      "duration": 4.8
    },
    {
      "text": "tokens to all the tokenized input",
      "start": 2794.88,
      "duration": 4.84
    },
    {
      "text": "sequences in in each batch remember we",
      "start": 2797.52,
      "duration": 4.88
    },
    {
      "text": "are doing this separately for each batch",
      "start": 2799.72,
      "duration": 4.839
    },
    {
      "text": "so that in each batch the length of all",
      "start": 2802.4,
      "duration": 5.04
    },
    {
      "text": "input sequences should be the same then",
      "start": 2804.559,
      "duration": 4.881
    },
    {
      "text": "we create the target token IDs which is",
      "start": 2807.44,
      "duration": 4.0
    },
    {
      "text": "just the input token ID tensor shifted",
      "start": 2809.44,
      "duration": 4.48
    },
    {
      "text": "to the right by one and then we replace",
      "start": 2811.44,
      "duration": 6.08
    },
    {
      "text": "the padding tokens with uh 50256 withus",
      "start": 2813.92,
      "duration": 6.12
    },
    {
      "text": "100 except for the first 50256 which",
      "start": 2817.52,
      "duration": 4.48
    },
    {
      "text": "indicates the end of",
      "start": 2820.04,
      "duration": 4.36
    },
    {
      "text": "text okay one last thing which I want to",
      "start": 2822.0,
      "duration": 5.079
    },
    {
      "text": "cover is that sometimes some researchers",
      "start": 2824.4,
      "duration": 6.439
    },
    {
      "text": "also mask the target token IDs so if",
      "start": 2827.079,
      "duration": 5.361
    },
    {
      "text": "this is",
      "start": 2830.839,
      "duration": 3.881
    },
    {
      "text": "my this is my prompt and that's",
      "start": 2832.44,
      "duration": 6.72
    },
    {
      "text": "tokenized into this right as input IDs",
      "start": 2834.72,
      "duration": 6.639
    },
    {
      "text": "in Target we we shift to the right by",
      "start": 2839.16,
      "duration": 4.159
    },
    {
      "text": "one right so as I told you here we shift",
      "start": 2841.359,
      "duration": 4.561
    },
    {
      "text": "to the right by one so the target tensor",
      "start": 2843.319,
      "duration": 4.641
    },
    {
      "text": "is now the in put shifted to the right",
      "start": 2845.92,
      "duration": 6.76
    },
    {
      "text": "which means only this part is the target",
      "start": 2847.96,
      "duration": 7.119
    },
    {
      "text": "text but now as I mentioned to you",
      "start": 2852.68,
      "duration": 4.919
    },
    {
      "text": "before why should I learn all these",
      "start": 2855.079,
      "duration": 4.641
    },
    {
      "text": "other things in the Target all I should",
      "start": 2857.599,
      "duration": 5.041
    },
    {
      "text": "learn is the response right so what if I",
      "start": 2859.72,
      "duration": 5.28
    },
    {
      "text": "mask the entire other thing in the",
      "start": 2862.64,
      "duration": 4.52
    },
    {
      "text": "target with the tokens minus",
      "start": 2865.0,
      "duration": 5.599
    },
    {
      "text": "100 so only the response matters to me I",
      "start": 2867.16,
      "duration": 5.32
    },
    {
      "text": "want my llm to learn the instruction",
      "start": 2870.599,
      "duration": 4.24
    },
    {
      "text": "input and then produce this response why",
      "start": 2872.48,
      "duration": 3.96
    },
    {
      "text": "should I keep the instruction and the",
      "start": 2874.839,
      "duration": 4.48
    },
    {
      "text": "input as I mentioned here in the Target",
      "start": 2876.44,
      "duration": 5.76
    },
    {
      "text": "text isn't that doing unnecessary",
      "start": 2879.319,
      "duration": 5.681
    },
    {
      "text": "computations so why don't we replace the",
      "start": 2882.2,
      "duration": 4.76
    },
    {
      "text": "token ID is in the Target text with",
      "start": 2885.0,
      "duration": 3.64
    },
    {
      "text": "minus 100 so that they are ignored in",
      "start": 2886.96,
      "duration": 4.0
    },
    {
      "text": "the loss function calculation and in",
      "start": 2888.64,
      "duration": 5.6
    },
    {
      "text": "fact this part is actually still not yet",
      "start": 2890.96,
      "duration": 5.879
    },
    {
      "text": "finalized so let me explain this part a",
      "start": 2894.24,
      "duration": 5.079
    },
    {
      "text": "bit better in addition to masking out",
      "start": 2896.839,
      "duration": 4.641
    },
    {
      "text": "padding tokens it is common to mask out",
      "start": 2899.319,
      "duration": 4.28
    },
    {
      "text": "the target token IDs that correspond to",
      "start": 2901.48,
      "duration": 4.0
    },
    {
      "text": "the instructions as I mentioned over",
      "start": 2903.599,
      "duration": 3.601
    },
    {
      "text": "here this is called mask asking the",
      "start": 2905.48,
      "duration": 3.079
    },
    {
      "text": "target token",
      "start": 2907.2,
      "duration": 4.32
    },
    {
      "text": "IDs um by masking out the target token",
      "start": 2908.559,
      "duration": 4.76
    },
    {
      "text": "IDs that correspond to the instruction",
      "start": 2911.52,
      "duration": 4.24
    },
    {
      "text": "the llm cross entropy loss is only",
      "start": 2913.319,
      "duration": 4.321
    },
    {
      "text": "calculated for the generated response",
      "start": 2915.76,
      "duration": 4.96
    },
    {
      "text": "Target IDs right and by masking out the",
      "start": 2917.64,
      "duration": 5.0
    },
    {
      "text": "instruction tokens the model is trained",
      "start": 2920.72,
      "duration": 3.76
    },
    {
      "text": "to focus on generating accurate",
      "start": 2922.64,
      "duration": 3.959
    },
    {
      "text": "responses rather than memorizing",
      "start": 2924.48,
      "duration": 3.879
    },
    {
      "text": "instructions and that helps with",
      "start": 2926.599,
      "duration": 4.641
    },
    {
      "text": "overfitting or reducing overfitting so",
      "start": 2928.359,
      "duration": 4.801
    },
    {
      "text": "if this instruction is not even present",
      "start": 2931.24,
      "duration": 4.2
    },
    {
      "text": "in the Target the llm won't memorize",
      "start": 2933.16,
      "duration": 5.56
    },
    {
      "text": "this as the output so it will reduce",
      "start": 2935.44,
      "duration": 6.44
    },
    {
      "text": "overfitting now there is still there are",
      "start": 2938.72,
      "duration": 5.28
    },
    {
      "text": "some researchers who are trying this but",
      "start": 2941.88,
      "duration": 3.8
    },
    {
      "text": "it's not yet confirmed which approach",
      "start": 2944.0,
      "duration": 4.839
    },
    {
      "text": "works the best so as is mentioned here",
      "start": 2945.68,
      "duration": 5.0
    },
    {
      "text": "currently researchers are divided on",
      "start": 2948.839,
      "duration": 3.441
    },
    {
      "text": "whether masking the instructions is",
      "start": 2950.68,
      "duration": 3.119
    },
    {
      "text": "universally beneficial during",
      "start": 2952.28,
      "duration": 4.039
    },
    {
      "text": "instruction fine tuning for instance a",
      "start": 2953.799,
      "duration": 4.961
    },
    {
      "text": "recent paper titled instruction tuning",
      "start": 2956.319,
      "duration": 5.0
    },
    {
      "text": "with loss over instructions demonstrated",
      "start": 2958.76,
      "duration": 4.16
    },
    {
      "text": "that not masking the instruction",
      "start": 2961.319,
      "duration": 4.28
    },
    {
      "text": "benefits the llm performance so let me",
      "start": 2962.92,
      "duration": 4.199
    },
    {
      "text": "actually display this paper paper so",
      "start": 2965.599,
      "duration": 4.76
    },
    {
      "text": "that you can see see this paper um so",
      "start": 2967.119,
      "duration": 5.281
    },
    {
      "text": "this paper is instruction tuning with",
      "start": 2970.359,
      "duration": 3.76
    },
    {
      "text": "loss over instructions this paper",
      "start": 2972.4,
      "duration": 4.88
    },
    {
      "text": "demonstrated that uh fine",
      "start": 2974.119,
      "duration": 6.321
    },
    {
      "text": "tuning with masking is actually not good",
      "start": 2977.28,
      "duration": 5.24
    },
    {
      "text": "so this paper demonstrated that not",
      "start": 2980.44,
      "duration": 4.72
    },
    {
      "text": "masking actually benefits the llm",
      "start": 2982.52,
      "duration": 5.4
    },
    {
      "text": "performance so it's not yet finalized",
      "start": 2985.16,
      "duration": 4.8
    },
    {
      "text": "which is the best method and it's and",
      "start": 2987.92,
      "duration": 4.04
    },
    {
      "text": "it's a subject for open research which",
      "start": 2989.96,
      "duration": 3.359
    },
    {
      "text": "all of you who are listening to this",
      "start": 2991.96,
      "duration": 3.8
    },
    {
      "text": "lecture can also actively contribute by",
      "start": 2993.319,
      "duration": 5.161
    },
    {
      "text": "testing out master and no masking as I",
      "start": 2995.76,
      "duration": 4.48
    },
    {
      "text": "mentioned it's very easy to mask you",
      "start": 2998.48,
      "duration": 3.72
    },
    {
      "text": "just replace the token IDs of the target",
      "start": 3000.24,
      "duration": 4.72
    },
    {
      "text": "text with minus 100 as is seen over here",
      "start": 3002.2,
      "duration": 4.76
    },
    {
      "text": "on the white",
      "start": 3004.96,
      "duration": 5.159
    },
    {
      "text": "board uh okay so in this implementation",
      "start": 3006.96,
      "duration": 5.32
    },
    {
      "text": "in this series we are not going to apply",
      "start": 3010.119,
      "duration": 4.321
    },
    {
      "text": "the masking and we will leave it as an",
      "start": 3012.28,
      "duration": 4.48
    },
    {
      "text": "optional exercise for you but the more",
      "start": 3014.44,
      "duration": 4.08
    },
    {
      "text": "optional exercises like these which you",
      "start": 3016.76,
      "duration": 3.76
    },
    {
      "text": "perform the more confident you will be",
      "start": 3018.52,
      "duration": 4.039
    },
    {
      "text": "of the subject and the stronger llm",
      "start": 3020.52,
      "duration": 4.039
    },
    {
      "text": "engineer or machine learning engineer",
      "start": 3022.559,
      "duration": 4.921
    },
    {
      "text": "you will become so at many places I'm",
      "start": 3024.559,
      "duration": 4.841
    },
    {
      "text": "introducing these open areas of research",
      "start": 3027.48,
      "duration": 3.8
    },
    {
      "text": "to you so you can even do this research",
      "start": 3029.4,
      "duration": 4.04
    },
    {
      "text": "and publish an impactful paper if you",
      "start": 3031.28,
      "duration": 3.92
    },
    {
      "text": "thoroughly investigate the effect of",
      "start": 3033.44,
      "duration": 5.08
    },
    {
      "text": "masking Target token IDs in instruction",
      "start": 3035.2,
      "duration": 6.879
    },
    {
      "text": "fine tuning awesome right so this brings",
      "start": 3038.52,
      "duration": 5.76
    },
    {
      "text": "us to the end of the lecture where we",
      "start": 3042.079,
      "duration": 5.161
    },
    {
      "text": "covered batching the data set and it",
      "start": 3044.28,
      "duration": 5.2
    },
    {
      "text": "sounds simple batching the data set you",
      "start": 3047.24,
      "duration": 4.119
    },
    {
      "text": "might think what's so complicated in",
      "start": 3049.48,
      "duration": 4.16
    },
    {
      "text": "this but it took me the full lecture to",
      "start": 3051.359,
      "duration": 4.161
    },
    {
      "text": "explain the second part itself which is",
      "start": 3053.64,
      "duration": 4.32
    },
    {
      "text": "batching the data set",
      "start": 3055.52,
      "duration": 4.48
    },
    {
      "text": "and the reason is because this batching",
      "start": 3057.96,
      "duration": 4.8
    },
    {
      "text": "itself involved five detailed steps",
      "start": 3060.0,
      "duration": 4.599
    },
    {
      "text": "which I needed to explain to you in a",
      "start": 3062.76,
      "duration": 4.52
    },
    {
      "text": "lot of detail along with the code so I",
      "start": 3064.599,
      "duration": 5.041
    },
    {
      "text": "hope you I hope you liking the style of",
      "start": 3067.28,
      "duration": 4.64
    },
    {
      "text": "whiteboard Plus Code and I'm trying my",
      "start": 3069.64,
      "duration": 4.24
    },
    {
      "text": "best to explain this to you in as simple",
      "start": 3071.92,
      "duration": 4.72
    },
    {
      "text": "manner as possible in the next lectures",
      "start": 3073.88,
      "duration": 4.679
    },
    {
      "text": "we'll create data loaders then we'll",
      "start": 3076.64,
      "duration": 4.28
    },
    {
      "text": "load the pre-trend llm then will",
      "start": 3078.559,
      "duration": 4.641
    },
    {
      "text": "instruction fine tune the llm inspect",
      "start": 3080.92,
      "duration": 5.48
    },
    {
      "text": "the loss accuracy generate the responses",
      "start": 3083.2,
      "duration": 5.08
    },
    {
      "text": "and and even do an evaluation and score",
      "start": 3086.4,
      "duration": 4.6
    },
    {
      "text": "the responses uh we'll ultimately even",
      "start": 3088.28,
      "duration": 4.6
    },
    {
      "text": "package this into a chat GPT prompt",
      "start": 3091.0,
      "duration": 3.599
    },
    {
      "text": "style so you'll get the feeling that you",
      "start": 3092.88,
      "duration": 3.239
    },
    {
      "text": "have built your own chat",
      "start": 3094.599,
      "duration": 4.441
    },
    {
      "text": "GPT thanks a lot everyone I'm making",
      "start": 3096.119,
      "duration": 4.881
    },
    {
      "text": "this nuts and bols approach of learning",
      "start": 3099.04,
      "duration": 4.0
    },
    {
      "text": "large language models deliberately",
      "start": 3101.0,
      "duration": 3.72
    },
    {
      "text": "because I feel that's the strongest way",
      "start": 3103.04,
      "duration": 3.92
    },
    {
      "text": "to build machine learning Engineers",
      "start": 3104.72,
      "duration": 4.28
    },
    {
      "text": "rather than just doing applications",
      "start": 3106.96,
      "duration": 3.72
    },
    {
      "text": "without understanding the basics",
      "start": 3109.0,
      "duration": 3.799
    },
    {
      "text": "foundations are the most important",
      "start": 3110.68,
      "duration": 3.8
    },
    {
      "text": "thanks a lot everyone and I look forward",
      "start": 3112.799,
      "duration": 5.721
    },
    {
      "text": "to seeing you in the next lecture",
      "start": 3114.48,
      "duration": 4.04
    }
  ],
  "full_text": "[Music] hello everyone and welcome to this lecture in the build large language models from scratch Series today we are going to continue with the instruction fine tuning Hands-On example which we started in the previous lecture and today we are going to look at organizing data into training batches let's first quickly recap what we saw in the previous lecture in the previous lecture we started looking at in instruction fine tuning so we saw that pre-trained llms are good at text completion but they really struggle with following Specific Instructions those instructions can be fix the grammar in the text or convert this text into passive voice so pre-trained llms struggle with these and that's why we need to do the process of instruction fine tuning in the process of instruction fine tuning we need to provide the llm with a data set and the data set consists of a list of instruction and the desired responses which we want so here's the data set which we will actually use this is a data set which consists of 1100 instruction and output pairs so you can see a sample instruction output pair such as translate the following sentence into French that's the instruction the input is where is the nearest restaurant and the output is the French translation similarly here the instruction is rewrite the following sentence so that it is in active voice the input is the cake was baked by Sarah and the output is Sarah baked the cake in some instruction output pairs there is no input so for example convert the active sentence to passive and then the sentence is given in the instruction itself so that does not have an input and we have the output directly then what is the capital of Indonesia that does not have an input but directly we have the output the capital of Indonesia is Jakarta so these are the instru ction and the response pairs which we give as the data to pre-train a large language model so first thing which we did in yesterday's lecture is that we downloaded the data set and then we formatted the data set what does it mean formatting we converted the data set into a format which is called as alpaka prompt style so remember when I showed you that when we looked at the data set it had these three keys or these three values instruction input and output it turns out that there is a specific way this needs to be converted into a prompt before we feed it to the large language model for fine tuning and that prompt is called as the alpaka prompt style so Stanford maintains this Stanford alpaka repository where they give you instructions on when you have the instruction response pairs such as what we have here how can you convert them into a prompt so here is the specific formatting for the alpaka style prompt where you when you have an instruction input and response The Prompt needs to be constructed like this below is an instruction that describes a task paired with an input that provides further context Write a response that appropriately completes the request and then you mention the instruction and give the instruction which you had in your data set then you mention the input give the input which you have in the data set and then you collect the response and show the output which you had uh in the data set so what we did in the previous lecture actually was that we we used this alpaka prompt style we took our data set and we converted this data set into an alpaka prompt style and then we partition the data set into training testing and validation we use 85% for training 10% for testing and the remaining 5% for validation so let me show you the code which we used in the previous lecture in this part of the code what we did is we prepared the data set so we made an API call to this URL we downloaded the data set and here you can see there are 1100 entries we printed the 50th example in this data set we printed the 999th example of this data set and we saw that they matched what we actually have in the data set this confirmed that the data set was loaded correctly then we wrote a function called format input to convert the instructions into the alpaka prompt format this format input function takes the entry and the entry is essentially the instruction input output pairs like these and then it converts it into the alpaka prompt so here we have the instruction plus the input and then we append the response as an output so then here you can see that whenever you have an entry such as this when you pass it into the model when you pass it into the format input you have the first line then you have the instruction and then the input and then you have to append this model input to the desired response and then you have the entire prompt like this so then this is the alpaka style prompt which has this first first sentence then it has the instruction it has the input and it has the response the large language model will be trained with these prompts which are then constructed for all of the instruction input output pairs which we have in the data set all 1100 pairs right so now okay one last thing is this splitting the data set into training testing and validation so here you can see that we have used 85% for training 10% for testing and the remaining 5% for validation you can print out the training set length which is 935 out of the 1100 pairs the validation set is 55 and the test set is 110 out of the 1100 pairs awesome now what we are going to do in today's lecture is we are going to come to step number two and step number two is batching the data set this is a bit more complicated than what we have seen before and that's why I'm dedicating this entire lecture to this there are number of finer and subtle things which you need to understand in this lecture and that will really help you to understand the details of the fine tuning process so please pay close attention to whatever I'm showing on the Whiteboard and then I'll show you the entire process through code what does it mean batching the data set well it means that let's say we have uh one data which is like this so I have taken a screenshot here and then I'll bring it to my whiteboard okay so we have one data like this which is the instruction input output uh and we have actually let me use the prompt itself because the data will be converted into a prompt right so let's say for the First Data I have a prompt like this um right and uh for the second data I have a prompt like this for the third data let's say I have a prompt like this now when we have data batches we need to convert this we need to convert all of the data set into a batch so let's say I'm having a batch which has three data samples so let's say this is my batch and this is the data sample number one this is the data sample number two and here is the data sample number three right what I want to do now is that I want to have I want to convert the first data set into a numerical representation so that it forms the first row the second data set needs to be converted into a numerical representation so that it forms the second row and the third data set needs to be converted into a numerical representation so that it forms the third row this whole lecture is about how we are going to construct this numerical representation so that the size of the first row is the same as the size of the second row and it's the same as the size of the third row and when I say size I mean the number of columns which I'm highlighting right now how will we make sure that these prompts which are different for different instruction output pairs right their length may also be different so for example The Prompt which is constructed for this pair will be very different than the prompt which is for this the length also will be different so how do I batch it into these kind of numerical representations and we are going to follow a sequential workflow for this and I'm first I'm going to explain to you the entire workflow on the Whiteboard and then I will take you through code earlier I was thinking I'll show you on the Whiteboard take you through code show you on the Whiteboard take you through code and cycle this multiple times but I found that for this lecture in particular it's much better if you first have a visual understanding of the entire flow have it as a mind map and then you can follow the code as I'm going through it okay so first we have the data and we'll format it using the prompt template using the alpaka prompt template and then the formatted data looks something like this right the first step to convert the data into a numerical representation as I mentioned over here is to tokenize the data so we are going to use a tokenizer and that tokenizer is going to be the bite pair encoder which is used by open a what this tokenizer does is that it takes a sentence and converts it into a bunch of token IDs right so that's the first step for every so let's say this is the prompt below is an instruction that describes a task Write a response that appropriately con completes the request let's say there is an instruction there is an input and there is a response I'm going to take this prompt and I'm going to convert this entire prompt into token IDs that's going to be my first step right so I have written it in a lot of detail over here so let's say if this is the let's say if this is the data set which you have then you convert it into the alpaka prompt style and then it starts looking like this and then it then you convert it into a bunch of token idies using the tick token Library using the tick token library right uh and this you do for the first instruction input output data then you do the same for the second you convert it into the alpaka format and then you convert it into a bunch of token IDs and you do the same process for all of these um instruction input output data which you have in your data set that's the first step but you see the problem here the number of token IDs which are present let's say the number of token IDs which are present over here will be different than the number of token IDs which are present over here because of course the length of the prompt might differ right so if you look at this this first prompt it looks longer than the second prompt so the number of token IDs in the first prompt are greater than the number of token IDs in the second prompt but as you but as I told you over here we need the numerical representation of all the samples in one batch so let's say sample one and Sample two are in one batch we need the number of columns which is the numerical representation or the number of token IDs we need the number of token IDs for each sample to be same so I'm just mentioning it over here we need the number of token IDs for each sample to be same so how do we make sure that the number of token IDs of all samples are the same and that brings us to The Next Step so until now we saw the tokenization the next step is that we are going to adjust the length of every uh a set of token IDs and we are going to pad them with tokens so that the length of all uh numerical representations is exactly the same the way we are going to do this is as has been shown over here so let's say if we have the first batch and the token IDs for the first input are 0 1 2 3 4 the token IDs for the second input in this batch are five and six the token IDs for the third input is 7 8 and N this is one batch now take a look at the length of the token ID here the length of the token ID is five here it's two and here it's three so it's not the same right so what we'll do is that in each batch we'll find that sequence which has the longest number of longest length so this clearly has the longest length so we keep it as it is but the remaining ones we pad them with these tokens so that their length becomes equal to the longest length so for example the input two has only two tokens five and six so we P three additional tokens 50256 uh similarly the third input has three token ID 7 8 and N we pad it with two additional token IDs now if you see because of this padding procedure which is done the length of all of these three is equal is the same and there are five token IDs in all the numerical representation that is exactly what I want now you may be thinking what's the 50256 that's the end of text token so gpt2 uh if you look at gpt2 it has a vocabulary size of 5257 um so 50257 is the vocabulary size so that means the first token has a token ID of zero and the last token has a token ID of 50256 and this last token which has the token ID of 50256 corresponds to the end of text token so it conveys that one particular text sample has ended and it's the start of a new text sample so we are just appending it with the end of text because it does not mean anything if we use any other token ID it might be associated with a token so that might confuse the training so that's why we append it with this with this end of text token ID which is 50256 now one thing which we are going to do here is that let's say this is the first batch and when you look at the second batch we are going to implement the exact same procedure we are going to find that token with the largest token length and we are going to append this 50256 to all the remaining uh token sequences so that the length of all becomes the same so each batch we are going to process sequentially and differently in each batch we are first going to find that representation which has the largest number of tokens and to all the other representations we are going to append 50 256 so that the length of uh so that the length of the converted or the token ID number for all of the inputs is equal is the same and then it starts looking like a batch so here you see this definitely looks like a batch because their number of columns are the same and then I can process all of these together in a batch right so that's the step number three we adjust to the same length with padding tokens so we add the end of text tokens to p add the data samples so that in a batch all of the samples have the same length that is the same number of token IDs right The Next Step what we are going to do is that we are going to create Target token IDs which means that uh let me show you what it actually means so let's say if you have an input right um and that's this prompt over here let's say this is the prompt and that has been converted into a bunch of token IDs which look something like this we need some output right which is the true Target which we want to approximate similar to what we did for large language model training the target is constructed by Shifting the input to the right by one so let's say the input is 0 1 2 3 4 right the target will be you forget the first uh entry here and you take the remaining entries and then you add a padding token 50256 so that the length of the Target and the length of the input is equal to are similar to each other or are exactly same rather here is the second input so if the input is 56 50256 50256 50256 uh the way we construct the target is that we get rid of this first index we take all of the remaining in the input and then we pad it with an extra token which is 50256 do keep this in mind this is the most important step in the fine tuning process and sometimes this is also a step which is very hard to grasp so think about what is exactly happening in this so let's say uh if we have a prompt and I'm I'm going to focus on this prompt further now so that I can explain to you why we construct the target like like this when I learned about this for the first time I really did not understand what is actually happening so if we have a if we have a prompt like this right what we are essentially doing in the when we construct the target pair is that let's say why is the target pair shifted to the right by one because we are still doing the next prediction task so if my let's actually let me take the screenshot of this also so that I can explain to you what is going on so if I take a screenshot of this and I'll paste this over here what this input Target sequence means is that if you have the input as zero the output should be equal to 1 if the input is 0 and 1 the target is equal to 2 if the input is 0 1 and 2 the target is equal to 3 if the input is 0 1 2 3 0 1 2 3 the target is equal to 4 and if the input is 0 1 2 3 4 then we are at the end of the sentence you can think about what we are really training the model over here we are training the model that if below is the input then below is is the output if below is is the input below is and is the output similarly as this happens sequentially we'll train the model that if below is an instruction task which describes write a sequ Write a response that appropriately completes the request if this much is the input then this this should be the output then we train the model that if this much is the input this should be the output I agree that there is some redundant training which is happening here because all the training which we really need to do is tell that if this much is the input instruction and the input this is the response which needs to be constructed and the way we are going about this is through this next word prediction task or next token prediction task that's why we shift that's why the target is the input which is shifted to the right by one please keep this in mind this is a bit not easy to understand and it's not intuitive as well when I first learned about instruction find tuning I thought that the input should be input should be this much and the target should be the response right but the input is this full thing the input is this full thing and the target is this full representation just shifted to the right by one so within the input itself we have the input and the output actually that is exactly how llms work we are using the next word predi or the next token prediction task so what this does is that as we are predicting the next token in the sequence of training we reach a stage with this much is the input the output will be the response so through learning how to predict the next token the llm learns to follow the instructions so there's a lot of similarity between instruction fine tuning and the pre-training process itself in the pre-training we did exactly the same thing we had the target shifted by one and it's nonintuitive that in instruction fine tuning the same thing can work but it does work because as the llm learns to predict the next token it learns to take the instruction the input and predict the response okay so I hope you have understood this part about how to create the target token IDs so the way to create the target token IDs is that we shift the inputs by one and we add an additional padding token to indicate that it's the end of the sentence so whenever we shift the input to the right by one we add this extra padding token right so that is how the target token IDs are created for training and the last step is that we'll replace in the Target token IDs this 5256 which is there wherever it is coming we'll replace it with minus 100 and there is a specific way to do this so if you look at this last part which is replace the padding tokens with placeholders so let's say this is my first Target tensor right um or let's look at the second target tensor let's look at 50256 there are four 50256 values right I'll leave this first one because that indicates the end of text and I'll replace all of the remaining with the value of minus 100 similarly if you look at Target three uh it has three 50256 values I'll leave the first one because it symbolizes end of text and I'll replace all of the remaining 50256 with - 100 similarly if you look at Target one now uh this 50256 represents end of text so I won't I won't replace this it remains like this so that is something to keep in mind we don't replace all the 50256 tokon IDS with minus 100 we only replace the we leave out the first one and replace all of the rest with minus 100 so you you might be thinking what is the significance of minus 100 and I'll explain to this in a lot of detail when we come to code but for now just know that it comes from this cross entropy loss ignore index so ignore index so by default the ignore index for pytorch is equal to minus 100 so when we mention minus 100 here it kind of makes sure that when we calculate the loss function all of these token IDs which do not matter at all we have randomly added them to the Target right they are not included in the loss function this makes the training much more efficient and does not unnecessarily include tokens which are not important this first 50256 needed to be retained because it indicates the end of uh end of a particular uh end of a particular text so that is very important but the remaining 50256 token IDs are not needed and we can just replace them with a value of minus 100 awesome so now uh we have understood this whole procedure so let me repeat the whole proced procedure ones so data batching is actually done in five steps and we are going to see all of these five five steps in code shortly the first step is to of course format the data using the promt template then convert the promt template into token IDs now all of these token IDs won't have equal length so the next step after this is to append U the data samples with the padding tokens of 50256 so that in each batch the length of every data sample is the same and that is equal to the sample which has the maximum number of token IDs then what we do is that we create Target token IDs which will be needed because we need to know what the right answer is and the right answer is just the input shifted to the right by one this is a bit counterintuitive but it does work because the llm learns to predict the next token and in that process it learns that here is the instruction here is the input and I have to predict the output and then finally the last thing is we we replace all all the tokens except for the First 50256 with a value of minus 100 and the reason we do this is that we need to exclude these tokens from the training loss and py torch cross entropy loss implementation has the ignore index of minus 100 and that's why we are going to use this minus 100 we'll see this in a lot of detail when we come to code right so I hope you have all understood this process uh this is very important for you to have as a road map because now we are going to go to code code and everything in the code will be much more clearer and easy to understand once you remember this road map which I've introduced over here so let us jump directly right into code right now so as I mentioned before until now we have converted the data set into training testing and validation batches right the training is 85% of the data testing is 10% of the data and the remaining 5% is for validation the next step is to organize the data into training batches as we saw on the Whiteboard first first what we are going to do is that uh we are going to code an instruction data set class what this class does is that it takes in the data set in this format and it converts it first of all into this alpaka style format that's the first step over here if you recall the first step is format data using prom template so we are going to use the format input function which I explained to you at the start of this lecture this format input function over here which takes in the um instruction input output pair like this and converts U that pair into this first sentence instruction and the input and then we need to append the response or the output so here if you look at the code this instruction class data set when we create an instance of this class it first creates an object self. data and assigns the data set uh which is something like this you can think of the data set like this and then what we do is that for each entry in the data set it first applies this format input function to the entry and then appends the response so the full text in the alpaka format is created and then what we do is that as I mentioned in the next step over here we are going to tokenize the formatted data so we uh we first Define a empty list and then we start appending the token IDs to this Mt list so let's say if you have a prompt which looks like this each of the tokens here are conver ConEd into token IDs and then appended to a list so for every prompt corresponding to each input output pair now we have a list of token IDs which is mentioned in the step one over here see here what we are doing is that every prompt we are converting it into token IDs and for that the tokenizer which we are going to use is we also need to pass this to the instruction data set class but it's going to be the tick token Library I'll share the link to this uh in the chat we have had a separate lecture on bite pair encoder in this lecture Series so if you want to understand about this library in detail I highly encourage you to uh watch that lecture right so now what does the instruction class ENT instruction data set class essentially return return well it returns for every uh for every data which is in this format it converts it into the alpaka style prompt and then it returns uh a bunch of token IDs for every entry awesome so uh let's go to the next part now before coming to the next part I just want to show you the end of text token and its corresponding token ID as we had seen on the Whiteboard and it's indeed 5256 so that's why we are using this 50256 token ID because it conveys the end of text okay now as I mentioned what we are going to do here is that we are going to define a custom colate function what this custom colet function does is that the name may sound complex but it actually does a very simple thing it takes the inputs in each data set that's the first thing it takes the input in each batch it finds that input with the maximum length and then it appends the 50256 or pads the 50256 token ID to all other inputs that's the only thing which it is doing so this custom colate draft one it takes the batch so you can think of the batch as coming in this format like this uh and then it has you have to give the padding token ID which is 50256 and the device which is CPU so this function implements four steps first it finds the longest sequence in the batch and then it pads the other sequences so that the length is equal to the longest sequence that's it and then it converts the list of inputs into a tensor and transfers to our Target device which is the CPU so this entire thing is converted into a tensor what Ive marked with this orange color over here that's the function of of the custom colla draft so let's see how it does it the first thing this function does is that it will find the longest sequence in the batch and it will add it by one so let's say if you have these three it if you have these three the longest sequence length is five and then it will add it by one so then it will be six there is a reason why you add it by one and I'll come to that later but after you add it by one what you do is that for every item in the batch you first add a token ID so even for the first one even for for the first item you add this 50256 token that's the first thing which you do and then you pad the 50256 again so that the length is equal to the maximum length and then what you do is you remove uh you remove the extra added token so here essentially what we are doing is that let's say if you have uh I'll actually remove this in the code what we are trying to do is that we add first a 50256 token ID to all of these so even to the first one we add this 50256 token and to the other ones we add the 50256 token then this will be added one 2 three three more times so total it will be added four times and here we'll add it a total of three times right but then you might think why are we adding an extra 50256 token because here we don't need to add 50256 here also we need to add three times here also we need to add it two times so then we get rid of that extra token later the reason we do this EXT extra addition is that it later helps us to create the target token because if you already add an extra token creating the target creating the target is just simple because then you just use this much to create the target as we saw before the target is just the input you remove the first element and then you add 50256 so earlier adding the 50256 token to all of the inputs in this part of the code it's important because it easily helps us to create the target ID uh to create the target for every inputs so essentially what we do is we add an extra 50256 and then get rid of it later and then we pad everything all the other inputs with 50256 so that the length is equal to the maximum token ID length or the that input which has the maximum length so essentially uh when you reach this part of the code every item in the batch so all of these three items essentially will have the same length that's what is happening in the code and ultimately we convert this um into a tensor and the tensor is the input tensor which is returned by this function custom colate draft one now let's see a practical application of this if you have uh these three inputs like this if inputs one has the size of five inputs two has the size of two and inputs three has the size of three the batch you create a batch with these three inputs and then you pass these you pass this batch into the custom Cola draft one now let's see the output as you can see the first input remains unchanged it has five token IDs because those are the maximum length in the second inputs we pad 50256 three times so that the length becomes same as the first input sequence and in the third input we pad 50256 two times so that the length becomes the same as the first two so now you can see we have an input stenor in which every row has the L has five columns awesome so as we can see here all inputs have been ped to the length of the longest input list inputs one which contains five token IDs awesome uh so until now we have reached this stage where we have padded the inputs with the token IDs and now we have to implement the next part of this process the next part is essentially creating Target token IDs for training so until now we have just implemented our first custom colate function to create batches from list of inputs however as you have learned in previous lessons we also need to create batches with the target token IDs right because we need to know what the real answer is the target token IDs are crucial because they represent what we want the model to generate and based on the target token IDs itself we'll get the loss function ultimately so as I explained to you thoroughly on the Whiteboard the way to get the target token ID is just to shift the input uh to the right by one and then add an additional padding token towards the end and that's exactly what we are going to do in the code so if you see in the code until this part it Remains the Same we have the inputs um and they're padded by the 50256 token and now if you see the targets token it just shifted to the right by one so here you see one colon which means that you forget the first entry and you take the remaining entries uh and here you don't even need to add the 50256 token we because we have already added added an extra 5256 token and this is why we add that extra 5 0256 token as I showed you earlier because here you see actually in the first input you don't need to add the 50256 token but if you add it it makes it very easy to create the target um it makes it very easy to create the the target sensor why because you just ignore the first element and take everything from the second element so here you ignore the first element which is zero and take everything from the second element so the target will be 1 2 3 4 4 5256 in the second the target will be 6 50256 50256 50256 and one more 5256 so it's the inputs which are shifted to the right by one so that is how you create the target tensor and then you just return the input tensor and the target tensor so the simplest way to think about this code is that until now we have made sure that the inputs are of the same length due to the padding which we have done and the targets is the inputs which are shifted to the right by one this is the very important process and as I mentioned to you this is the non-intuitive step because uh the true value is just the input which is shifted to the right by one and that is what is nonintuitive you might think that the response needs to be given in the True Value right why is the instruction and input also given in the True Value but it's given because in the next word prediction task the llm automatically learns that when you have the instruction and the input you have to predict the response this was a bit harder for me to explain but I hope you have got this idea um in the code if it's difficult to understand just try imagining it through the visual representation which I showed to you on the Whiteboard you can even try going back as you are learning this lecture to see the Whiteboard explanation before you try to understand the code so there are actually only two things which are happening in this custom colate draft 2 it takes it truncates the last token for the inputs so that everything is of the same length and it shifts the input to the right by one to get the target tens and we can check this now let's say we have these three inputs as before we create a batch of these three inputs and then we call the custom col draft two function on this batch and we'll print the inputs and the targets so let's see as we learned before the inputs is just the first row Remains the Same the second row has three 50256 tokens padded the third row has 250 256 tokens padded and let's look at the Target if you look at the first row of the targets it's the first dra of the inputs you shift to the right by one so you take the remaining four and then you have 50 256 token similarly if you look at the second row of the target it's basically the second row of the input you shift to the right by one which means you take only the remaining four values and then you add an extra 5256 similarly if you take the third row of the targets it's essentially the third input but you shift to the right by one so you ignore the first and you take all the four and then you append an extra 50256 token ID towards the end that's how you get the inputs and the target tensor so the first tensor represents the inputs and the second tensor represents the target awesome now after this step is implemented we come to the next step which is essentially creating uh or replacing the padding tokens with placeholders which means that except for the first 50256 we'll replace all the remaining with minus 100 uh so in the next step we assign a minus 100 placeholder to all the padding tokens this special value allows us to exclude these padding tokens from contributing to the training loss calculation as we saw on the white board um okay so in the following code okay one more thing to mention is that as I told you on the Whiteboard when we replace this 5025 tokens with minus 100 we retain one 50256 token and the reason we retain one end of text token is because it allows the llm to learn when to generate an end of text token in the response to instructions which we use an which we use as an indicator that the generated response is now complete so you need one 5256 token ID to say that or to represent that this is indeed the end of text so now what we have to do is that we have to take this custom colate draft two and then we have to modify it further so most of the function is the same which now I'm calling Custom colate function which takes in my batch my padding token ID and my ignore index so that's minus 100 so what this does now is that until now here the steps are the same you get the inputs and the target sensor but now what you do is that you take the target sensor only and all the indexes except for the first 50256 you replace it with the ignore index so you first create a mask and that mask has all the indexes which has the padding token ID then you ignore the first index which has the padding token ID that's the first 50256 value and then you replace all the remaining ones with ignore index which is minus 100 uh okay so this now creates my input this now creates my input sensor and this creates my Target stenor and these are both returned by this function which is custom colate function okay until now if you note through the Whiteboard what we have implemented is that we have implemented this part of the code where you can see in the figure that uh here so except for the first 50256 we replace all of the remaining 50256 with the value of minus 100 and now we are going to see why we replace the remaining 50256 token IDs with minus 100 so to to see why we replace the remaining token IDs with minus 100 we are going to see some implementations using pytorch but before that let's actually see whether our custom colate function is really working so to test that you take three inputs you have the inputs one as 0 1 2 3 4 You have the inputs two as 5 comma 6 and you have the inputs three as 7 8 and 9 you create a batch with these three inputs similar to the batch we have created before and then you create the inputs and targets based on the custom collate function now if you see the inputs and targets tensor which we had obtained before the inputs and targets tensor which we obtain now is actually exactly the same except that in the Target sensor uh except for the first 50256 all the remaining 50256 values have been replaced with minus 100 this is the only change which has been done there is no changes to the input sensor the only change happens in the Target sensor where except for the first 50256 all the remaining have been replaced with minus 100 and now you can appreciate this code which is the custom colate function where in this 15 to 20 lines of code what we are essentially doing is that we we are implementing all of the steps which we have learned on the Whiteboard we implementing all of the steps over here so essentially we are implementing the we are implementing the padding we are implementing creating Target token IDs we are also implementing replacing the padding tokens with the placeholder value of minus 100 and that's also called ignore index so up till now it's working right but now you might be thinking that the modified colate function works as expected altering the target list by inserting the token ID of minus 100 but what is the logic behind this adjustment why do we replace with minus 100 so let us take a small demonstration uh and I'm going to show you the categorical or the cross entropy loss calculation right and the cross entropy loss calculation is based on a logic sensor and a Target sensor I'm not going into details of this Logics and targets because I have just taken two sample examples but for now you can think of it that the true answer is 0a 1 and the logits predicted are minus minus one and one for the first training example and for the second training example the Logics predictor are Min -.5 and 1.5 so this this is my predicted value and these are the targets to calculate the loss between the prediction and the target we use the cross entropy loss so I'll share the link to the pytorch Cross entropy loss which which calculates this loss between the prediction and the Target and if you print out the loss you'll get it to be 1.1 1269 that's good then what we do is that we add one more additional token ID in the prediction so in the logits two now we have three training examples with three predictions and we have three true answers so then we are using the categorical or the cross entropy function to calculate the loss between the prediction and between the actual value and here if you print out the loss you'll see that the loss is 7936 it differs from the previous loss because we added one more example now what I want to show you is that let's say instead of the targets two being 0 1 and one what if the targets three is 0 1 and minus 100 so the Logics two will remain the same so the logits which are my predictions will remain the same but now the targets are 0 1 and minus 100 now I want to show you a interesting thing even if you add a minus 100 here and if you have a third example when you calculate this new loss you'll see that the loss for this is the same as the first loss which you had obtained so it's almost like adding the third training example made no difference at all so the loss here is 1.1 1269 and if you saw the the loss in the first logits one and targets one that was also 1.12 69 so essentially there was no effect of this third prediction and the reason there was no effect of this third prediction is that the targets had minus 100 that is the effect of ignore index equal to minus 100 so in other words the cross entropy loss function ignored the third entry in the targets three Vector the token ID corresponding to minus 100 so you can try reading replacing the minus 100 with another token and then the loss will not be the same it's only for minus 100 so what's special aboutus 100 that it's ignored by the cross entropy loss well the default setting of the Cross entropy function in pytorch is cross entropy ignore index equal to minus 100 and you can see this over here also if you look at Cross entropy loss formulation you'll see that the ignore index is equal to minus 100 over here that's the default setting of the pytorch Cross entropy loss this means that pytorch ignores all the targets which are labeled with minus 100 so now if my target tensor has minus 100 over here it will ignore all of those predictions corresponding to these indices which have minus 100 in the targets and that's good for us because these anyways don't represent anything meaningful that's why they won't contribute to our loss function um so in this chapter we actually take advantage of this ignore index to ignore the additional end of text padding tokens that we that we use to pad so we we take advantage of this ignore index to ignore the additional end of text tokens that we used to pad the training examples to have the same length in each batch however as I mentioned we kept one 15256 token ID so this I will reiterate again because sometimes students forget this we we we retain this this first 50256 we retain the first 50256 in all of the targets so even if you see in the even if you replace the other 50256 with minus 100 you retain the first one and you retain the first one because it helps the llm to learn to generate end of text tokens and that's an indicator that the response is complete so this is the entire process and this is the entire workflow for implementing the batching in the training data set there are very few videos which explain this batching process in detail but as I say so many times the devil lies in the details so if you directly go to the model training right now I could have shown you that directly without explaining this lecture but there is so much information to learn here this padding tokens this minus 100 then creating Target token IDs which are just shifted uh to the right hand side by one all of this information would have been lost if I directly jumped to the fine tuning process so in the instruction fine tuning creating these batches is very important to learn because as I explained to you there is a specific fstep process to do this first you you have to convert the data to the alpaka prom template then you have to tokenize the formatted data into token ID then you have to append padding tokens to all the tokenized input sequences in in each batch remember we are doing this separately for each batch so that in each batch the length of all input sequences should be the same then we create the target token IDs which is just the input token ID tensor shifted to the right by one and then we replace the padding tokens with uh 50256 withus 100 except for the first 50256 which indicates the end of text okay one last thing which I want to cover is that sometimes some researchers also mask the target token IDs so if this is my this is my prompt and that's tokenized into this right as input IDs in Target we we shift to the right by one right so as I told you here we shift to the right by one so the target tensor is now the in put shifted to the right which means only this part is the target text but now as I mentioned to you before why should I learn all these other things in the Target all I should learn is the response right so what if I mask the entire other thing in the target with the tokens minus 100 so only the response matters to me I want my llm to learn the instruction input and then produce this response why should I keep the instruction and the input as I mentioned here in the Target text isn't that doing unnecessary computations so why don't we replace the token ID is in the Target text with minus 100 so that they are ignored in the loss function calculation and in fact this part is actually still not yet finalized so let me explain this part a bit better in addition to masking out padding tokens it is common to mask out the target token IDs that correspond to the instructions as I mentioned over here this is called mask asking the target token IDs um by masking out the target token IDs that correspond to the instruction the llm cross entropy loss is only calculated for the generated response Target IDs right and by masking out the instruction tokens the model is trained to focus on generating accurate responses rather than memorizing instructions and that helps with overfitting or reducing overfitting so if this instruction is not even present in the Target the llm won't memorize this as the output so it will reduce overfitting now there is still there are some researchers who are trying this but it's not yet confirmed which approach works the best so as is mentioned here currently researchers are divided on whether masking the instructions is universally beneficial during instruction fine tuning for instance a recent paper titled instruction tuning with loss over instructions demonstrated that not masking the instruction benefits the llm performance so let me actually display this paper paper so that you can see see this paper um so this paper is instruction tuning with loss over instructions this paper demonstrated that uh fine tuning with masking is actually not good so this paper demonstrated that not masking actually benefits the llm performance so it's not yet finalized which is the best method and it's and it's a subject for open research which all of you who are listening to this lecture can also actively contribute by testing out master and no masking as I mentioned it's very easy to mask you just replace the token IDs of the target text with minus 100 as is seen over here on the white board uh okay so in this implementation in this series we are not going to apply the masking and we will leave it as an optional exercise for you but the more optional exercises like these which you perform the more confident you will be of the subject and the stronger llm engineer or machine learning engineer you will become so at many places I'm introducing these open areas of research to you so you can even do this research and publish an impactful paper if you thoroughly investigate the effect of masking Target token IDs in instruction fine tuning awesome right so this brings us to the end of the lecture where we covered batching the data set and it sounds simple batching the data set you might think what's so complicated in this but it took me the full lecture to explain the second part itself which is batching the data set and the reason is because this batching itself involved five detailed steps which I needed to explain to you in a lot of detail along with the code so I hope you I hope you liking the style of whiteboard Plus Code and I'm trying my best to explain this to you in as simple manner as possible in the next lectures we'll create data loaders then we'll load the pre-trend llm then will instruction fine tune the llm inspect the loss accuracy generate the responses and and even do an evaluation and score the responses uh we'll ultimately even package this into a chat GPT prompt style so you'll get the feeling that you have built your own chat GPT thanks a lot everyone I'm making this nuts and bols approach of learning large language models deliberately because I feel that's the strongest way to build machine learning Engineers rather than just doing applications without understanding the basics foundations are the most important thanks a lot everyone and I look forward to seeing you in the next lecture"
}