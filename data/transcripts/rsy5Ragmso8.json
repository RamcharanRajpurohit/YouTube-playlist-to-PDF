{
  "video": {
    "video_id": "rsy5Ragmso8",
    "title": "Lecture 7: Code an LLM Tokenizer from Scratch in Python",
    "duration": 4184.0,
    "index": 6
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.12
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.2,
      "duration": 5.12
    },
    {
      "text": "in the build large language models from",
      "start": 8.12,
      "duration": 3.479
    },
    {
      "text": "scratch",
      "start": 10.32,
      "duration": 5.359
    },
    {
      "text": "series my name is Dr Raj dander and I'll",
      "start": 11.599,
      "duration": 6.76
    },
    {
      "text": "be your instructor for this",
      "start": 15.679,
      "duration": 5.6
    },
    {
      "text": "lecture today we are going to cover a",
      "start": 18.359,
      "duration": 7.201
    },
    {
      "text": "very important topic which is called as",
      "start": 21.279,
      "duration": 4.281
    },
    {
      "text": "tokenization basically when we build",
      "start": 26.519,
      "duration": 5.481
    },
    {
      "text": "large language model Model S the data",
      "start": 29.08,
      "duration": 5.2
    },
    {
      "text": "needs to be pre-processed in a certain",
      "start": 32.0,
      "duration": 6.36
    },
    {
      "text": "manner before it is used for",
      "start": 34.28,
      "duration": 7.16
    },
    {
      "text": "pre-training and uh in this data",
      "start": 38.36,
      "duration": 6.12
    },
    {
      "text": "pre-processing pipeline the first step",
      "start": 41.44,
      "duration": 4.88
    },
    {
      "text": "is",
      "start": 44.48,
      "duration": 5.36
    },
    {
      "text": "tokenization at its really basic form",
      "start": 46.32,
      "duration": 5.919
    },
    {
      "text": "tokenization is just the process of",
      "start": 49.84,
      "duration": 5.48
    },
    {
      "text": "breaking down a sentence into individual",
      "start": 52.239,
      "duration": 5.96
    },
    {
      "text": "words but it is a bit more detailed than",
      "start": 55.32,
      "duration": 5.12
    },
    {
      "text": "that the way it is used to act build",
      "start": 58.199,
      "duration": 5.761
    },
    {
      "text": "large language models in today's lecture",
      "start": 60.44,
      "duration": 5.52
    },
    {
      "text": "we are going to understand everything",
      "start": 63.96,
      "duration": 4.36
    },
    {
      "text": "there is to know about",
      "start": 65.96,
      "duration": 4.36
    },
    {
      "text": "tokenization and we will build a",
      "start": 68.32,
      "duration": 4.6
    },
    {
      "text": "tokenizer fully from scratch we will",
      "start": 70.32,
      "duration": 5.119
    },
    {
      "text": "also build an encoder and decoder from",
      "start": 72.92,
      "duration": 5.36
    },
    {
      "text": "scratch and we'll code it out in",
      "start": 75.439,
      "duration": 5.36
    },
    {
      "text": "Python so today will be a Hands-On",
      "start": 78.28,
      "duration": 5.12
    },
    {
      "text": "session if you have a laptop in front of",
      "start": 80.799,
      "duration": 5.28
    },
    {
      "text": "you U that's great so that you can",
      "start": 83.4,
      "duration": 5.6
    },
    {
      "text": "follow along in Google collab or jupyter",
      "start": 86.079,
      "duration": 5.561
    },
    {
      "text": "notebook or V s code to implement the",
      "start": 89.0,
      "duration": 5.439
    },
    {
      "text": "code along with me so let's get started",
      "start": 91.64,
      "duration": 3.88
    },
    {
      "text": "with today's",
      "start": 94.439,
      "duration": 3.64
    },
    {
      "text": "lecture building a large language model",
      "start": 95.52,
      "duration": 5.04
    },
    {
      "text": "usually proceeds in three stages",
      "start": 98.079,
      "duration": 4.201
    },
    {
      "text": "especially if you want to build it from",
      "start": 100.56,
      "duration": 4.839
    },
    {
      "text": "scratch in stage one we have to",
      "start": 102.28,
      "duration": 5.36
    },
    {
      "text": "understand the nuances or basic",
      "start": 105.399,
      "duration": 4.961
    },
    {
      "text": "mechanism of building an llm and that",
      "start": 107.64,
      "duration": 5.439
    },
    {
      "text": "includes data preparation attention",
      "start": 110.36,
      "duration": 6.119
    },
    {
      "text": "mechanism and understanding the llm",
      "start": 113.079,
      "duration": 5.72
    },
    {
      "text": "architecture stage two involves",
      "start": 116.479,
      "duration": 3.92
    },
    {
      "text": "pre-training and building the",
      "start": 118.799,
      "duration": 4.36
    },
    {
      "text": "foundational model so that involves the",
      "start": 120.399,
      "duration": 5.0
    },
    {
      "text": "training Loop model evaluation and",
      "start": 123.159,
      "duration": 5.001
    },
    {
      "text": "loading pre-trained weights and stage",
      "start": 125.399,
      "duration": 5.64
    },
    {
      "text": "three involves finetuning training on",
      "start": 128.16,
      "duration": 5.68
    },
    {
      "text": "smaller very specific data sets to build",
      "start": 131.039,
      "duration": 4.521
    },
    {
      "text": "applications which are actually useful",
      "start": 133.84,
      "duration": 4.36
    },
    {
      "text": "to you such as a classifier or a",
      "start": 135.56,
      "duration": 3.84
    },
    {
      "text": "personal",
      "start": 138.2,
      "duration": 3.52
    },
    {
      "text": "assistant in this lecture we are going",
      "start": 139.4,
      "duration": 5.64
    },
    {
      "text": "to look at the first aspect of stage one",
      "start": 141.72,
      "duration": 5.96
    },
    {
      "text": "which is data preparation and pre data",
      "start": 145.04,
      "duration": 5.44
    },
    {
      "text": "preparation and sampling",
      "start": 147.68,
      "duration": 6.0
    },
    {
      "text": "so uh tokenization comes under the",
      "start": 150.48,
      "duration": 5.36
    },
    {
      "text": "category of data preparation and",
      "start": 153.68,
      "duration": 4.44
    },
    {
      "text": "sampling and that is what we are going",
      "start": 155.84,
      "duration": 4.52
    },
    {
      "text": "to cover in today's lecture you can",
      "start": 158.12,
      "duration": 4.199
    },
    {
      "text": "think of this as the first building",
      "start": 160.36,
      "duration": 4.799
    },
    {
      "text": "block for building a large language",
      "start": 162.319,
      "duration": 6.401
    },
    {
      "text": "model so let's get started with today's",
      "start": 165.159,
      "duration": 5.881
    },
    {
      "text": "lecture so the main question which we",
      "start": 168.72,
      "duration": 5.239
    },
    {
      "text": "are going to answer today is how exactly",
      "start": 171.04,
      "duration": 6.76
    },
    {
      "text": "do you prepare input text for training",
      "start": 173.959,
      "duration": 6.321
    },
    {
      "text": "large language models",
      "start": 177.8,
      "duration": 4.64
    },
    {
      "text": "at the heart of it llms are just neural",
      "start": 180.28,
      "duration": 5.16
    },
    {
      "text": "networks right so you need data the",
      "start": 182.44,
      "duration": 5.56
    },
    {
      "text": "parameters of the llm are optimized and",
      "start": 185.44,
      "duration": 4.2
    },
    {
      "text": "then we have some",
      "start": 188.0,
      "duration": 4.44
    },
    {
      "text": "output the question is that this data",
      "start": 189.64,
      "duration": 4.959
    },
    {
      "text": "which comes in as the input what form",
      "start": 192.44,
      "duration": 4.48
    },
    {
      "text": "should it take how should we prepare the",
      "start": 194.599,
      "duration": 5.321
    },
    {
      "text": "input text we have a huge number of",
      "start": 196.92,
      "duration": 5.12
    },
    {
      "text": "documents right which the llm is trained",
      "start": 199.92,
      "duration": 3.72
    },
    {
      "text": "on we have seen that in previous",
      "start": 202.04,
      "duration": 4.04
    },
    {
      "text": "lectures the llm is usually trained on",
      "start": 203.64,
      "duration": 5.4
    },
    {
      "text": "billions of documents but do we feed the",
      "start": 206.08,
      "duration": 6.439
    },
    {
      "text": "document Direct ly as the input text do",
      "start": 209.04,
      "duration": 5.759
    },
    {
      "text": "we feed the sentences of the document as",
      "start": 212.519,
      "duration": 5.36
    },
    {
      "text": "the input text no it turns out we have",
      "start": 214.799,
      "duration": 5.36
    },
    {
      "text": "to tokenize the document and then feed",
      "start": 217.879,
      "duration": 3.881
    },
    {
      "text": "individual",
      "start": 220.159,
      "duration": 4.16
    },
    {
      "text": "tokens uh there is one more step after",
      "start": 221.76,
      "duration": 4.479
    },
    {
      "text": "this which is called as Vector embeding",
      "start": 224.319,
      "duration": 3.601
    },
    {
      "text": "but we'll have a separate lecture for",
      "start": 226.239,
      "duration": 4.761
    },
    {
      "text": "that today we are only going to look at",
      "start": 227.92,
      "duration": 5.319
    },
    {
      "text": "tokenization so the process of",
      "start": 231.0,
      "duration": 4.72
    },
    {
      "text": "tokenization can be broadly broken down",
      "start": 233.239,
      "duration": 4.28
    },
    {
      "text": "into three",
      "start": 235.72,
      "duration": 4.32
    },
    {
      "text": "steps this is the key takeaway which you",
      "start": 237.519,
      "duration": 4.8
    },
    {
      "text": "should learn from today's lecture the",
      "start": 240.04,
      "duration": 4.919
    },
    {
      "text": "first step is that you have to initially",
      "start": 242.319,
      "duration": 5.241
    },
    {
      "text": "split the text into individual word and",
      "start": 244.959,
      "duration": 6.041
    },
    {
      "text": "subo tokens that is Step number one",
      "start": 247.56,
      "duration": 6.399
    },
    {
      "text": "imagine you have huge amount of text you",
      "start": 251.0,
      "duration": 5.519
    },
    {
      "text": "break it down into individual words",
      "start": 253.959,
      "duration": 5.28
    },
    {
      "text": "that's the first step of tokenization",
      "start": 256.519,
      "duration": 5.161
    },
    {
      "text": "the second step of tokenization is",
      "start": 259.239,
      "duration": 6.24
    },
    {
      "text": "converting these tokens into token",
      "start": 261.68,
      "duration": 6.72
    },
    {
      "text": "IDs and the third step is basically",
      "start": 265.479,
      "duration": 5.521
    },
    {
      "text": "encode these token ID into Vector",
      "start": 268.4,
      "duration": 4.68
    },
    {
      "text": "representation we are not going to look",
      "start": 271.0,
      "duration": 4.24
    },
    {
      "text": "at step number three in today's lecture",
      "start": 273.08,
      "duration": 4.04
    },
    {
      "text": "because it comes also under Vector",
      "start": 275.24,
      "duration": 3.92
    },
    {
      "text": "embedding but essentially we are going",
      "start": 277.12,
      "duration": 4.76
    },
    {
      "text": "to look at step one and step number two",
      "start": 279.16,
      "duration": 5.2
    },
    {
      "text": "so let me repeat step one and step two",
      "start": 281.88,
      "duration": 4.599
    },
    {
      "text": "step one is basically imagine you have a",
      "start": 284.36,
      "duration": 4.24
    },
    {
      "text": "big document which is let's say you're",
      "start": 286.479,
      "duration": 5.081
    },
    {
      "text": "training on Harry Potter books you take",
      "start": 288.6,
      "duration": 6.12
    },
    {
      "text": "this books you divide it into sentences",
      "start": 291.56,
      "duration": 5.0
    },
    {
      "text": "and then you split the text into",
      "start": 294.72,
      "duration": 4.72
    },
    {
      "text": "individual word and subword tokens and",
      "start": 296.56,
      "duration": 6.0
    },
    {
      "text": "then you convert these tokens into token",
      "start": 299.44,
      "duration": 5.039
    },
    {
      "text": "IDs that's",
      "start": 302.56,
      "duration": 4.96
    },
    {
      "text": "the uh two major steps involved in",
      "start": 304.479,
      "duration": 4.961
    },
    {
      "text": "tokenization which we are going to look",
      "start": 307.52,
      "duration": 3.2
    },
    {
      "text": "in today's",
      "start": 309.44,
      "duration": 4.44
    },
    {
      "text": "lecture um so I just want to show you a",
      "start": 310.72,
      "duration": 4.84
    },
    {
      "text": "visual for",
      "start": 313.88,
      "duration": 4.72
    },
    {
      "text": "how the input text is given to the large",
      "start": 315.56,
      "duration": 5.199
    },
    {
      "text": "language model so let's say this is an",
      "start": 318.6,
      "duration": 5.599
    },
    {
      "text": "input text right this is an example",
      "start": 320.759,
      "duration": 5.521
    },
    {
      "text": "remember the first step of tokenization",
      "start": 324.199,
      "duration": 4.84
    },
    {
      "text": "we break this into individual words so",
      "start": 326.28,
      "duration": 5.359
    },
    {
      "text": "the first word is this the second word",
      "start": 329.039,
      "duration": 5.44
    },
    {
      "text": "is is the third word is an and the",
      "start": 331.639,
      "duration": 5.441
    },
    {
      "text": "fourth word is example so this is the",
      "start": 334.479,
      "duration": 5.081
    },
    {
      "text": "tokenized text and this is my step",
      "start": 337.08,
      "duration": 5.36
    },
    {
      "text": "number one right now step number two is",
      "start": 339.56,
      "duration": 4.8
    },
    {
      "text": "we need to convert each of these",
      "start": 342.44,
      "duration": 5.16
    },
    {
      "text": "individual tokens into token IDs so this",
      "start": 344.36,
      "duration": 8.2
    },
    {
      "text": "is the token ID 1 4013 token ID 2 2011",
      "start": 347.6,
      "duration": 8.599
    },
    {
      "text": "token ID 3 which is 302 and token ID 4",
      "start": 352.56,
      "duration": 4.479
    },
    {
      "text": "which is",
      "start": 356.199,
      "duration": 4.801
    },
    {
      "text": "1134 so every word or every token rather",
      "start": 357.039,
      "duration": 7.081
    },
    {
      "text": "has a token ID associated with it I have",
      "start": 361.0,
      "duration": 5.88
    },
    {
      "text": "currently given these numbers randomly",
      "start": 364.12,
      "duration": 5.28
    },
    {
      "text": "but you need to assign a token ID for",
      "start": 366.88,
      "duration": 5.08
    },
    {
      "text": "each word and then the next step after",
      "start": 369.4,
      "duration": 4.72
    },
    {
      "text": "getting these token IDs is to convert",
      "start": 371.96,
      "duration": 3.84
    },
    {
      "text": "these token IDs into something called",
      "start": 374.12,
      "duration": 3.76
    },
    {
      "text": "token embeddings or vector",
      "start": 375.8,
      "duration": 4.239
    },
    {
      "text": "embeddings and then these Vector",
      "start": 377.88,
      "duration": 4.68
    },
    {
      "text": "embeddings are then fed as input data to",
      "start": 380.039,
      "duration": 3.72
    },
    {
      "text": "the",
      "start": 382.56,
      "duration": 4.039
    },
    {
      "text": "GPT or the llm",
      "start": 383.759,
      "duration": 5.361
    },
    {
      "text": "rather so even before coming to the",
      "start": 386.599,
      "duration": 4.6
    },
    {
      "text": "training stage we need to do these many",
      "start": 389.12,
      "duration": 4.919
    },
    {
      "text": "steps of uh pre-processing and today we",
      "start": 391.199,
      "duration": 4.481
    },
    {
      "text": "are essentially going to look at step",
      "start": 394.039,
      "duration": 3.681
    },
    {
      "text": "number one and step number",
      "start": 395.68,
      "duration": 4.56
    },
    {
      "text": "two I hope you have understood the two",
      "start": 397.72,
      "duration": 4.4
    },
    {
      "text": "main steps which we are going to look at",
      "start": 400.24,
      "duration": 2.959
    },
    {
      "text": "in today's",
      "start": 402.12,
      "duration": 3.68
    },
    {
      "text": "lecture so let us start by looking at",
      "start": 403.199,
      "duration": 4.361
    },
    {
      "text": "step number one initially which is",
      "start": 405.8,
      "duration": 4.04
    },
    {
      "text": "tokenizing the text or breaking down the",
      "start": 407.56,
      "duration": 5.68
    },
    {
      "text": "text into individual words and you",
      "start": 409.84,
      "duration": 5.24
    },
    {
      "text": "definitely do need to understand the",
      "start": 413.24,
      "duration": 3.72
    },
    {
      "text": "nuances which are involved in this and",
      "start": 415.08,
      "duration": 3.64
    },
    {
      "text": "that's why I'm going to take you through",
      "start": 416.96,
      "duration": 4.88
    },
    {
      "text": "the entire course process in Python uh",
      "start": 418.72,
      "duration": 4.64
    },
    {
      "text": "we could have just use a pre-built",
      "start": 421.84,
      "duration": 3.6
    },
    {
      "text": "tokenizer but then you won't understand",
      "start": 423.36,
      "duration": 5.279
    },
    {
      "text": "the nuances involved in tokenizing and",
      "start": 425.44,
      "duration": 6.479
    },
    {
      "text": "uh just to give out a a shout out or",
      "start": 428.639,
      "duration": 7.441
    },
    {
      "text": "rather a credits to Sebastian rashka for",
      "start": 431.919,
      "duration": 6.28
    },
    {
      "text": "hosting the this code publicly and I'll",
      "start": 436.08,
      "duration": 5.32
    },
    {
      "text": "be heavily borrowing from this code so",
      "start": 438.199,
      "duration": 5.881
    },
    {
      "text": "let's get started for the purposes of",
      "start": 441.4,
      "duration": 4.639
    },
    {
      "text": "the demonstration today we are going to",
      "start": 444.08,
      "duration": 4.6
    },
    {
      "text": "use a specific data set we are going to",
      "start": 446.039,
      "duration": 5.241
    },
    {
      "text": "use this book book called The Verdict",
      "start": 448.68,
      "duration": 5.44
    },
    {
      "text": "which has been written by Edith Warton",
      "start": 451.28,
      "duration": 5.12
    },
    {
      "text": "so let's look at a few screenshots or",
      "start": 454.12,
      "duration": 4.56
    },
    {
      "text": "let's look at a few data about this book",
      "start": 456.4,
      "duration": 4.28
    },
    {
      "text": "so if you search The Verdict by edit",
      "start": 458.68,
      "duration": 5.239
    },
    {
      "text": "Warton you'll get a image of this book",
      "start": 460.68,
      "duration": 6.519
    },
    {
      "text": "which looks like this uh year of",
      "start": 463.919,
      "duration": 5.601
    },
    {
      "text": "publishing let's look at the year when",
      "start": 467.199,
      "duration": 4.4
    },
    {
      "text": "this book was published so this was I",
      "start": 469.52,
      "duration": 5.32
    },
    {
      "text": "think published in 1908 so almost 120",
      "start": 471.599,
      "duration": 6.0
    },
    {
      "text": "130 years back and it's available for",
      "start": 474.84,
      "duration": 5.6
    },
    {
      "text": "free uh free to download I'll give the",
      "start": 477.599,
      "duration": 4.841
    },
    {
      "text": "link to download this book it's also",
      "start": 480.44,
      "duration": 3.719
    },
    {
      "text": "available in this repository by",
      "start": 482.44,
      "duration": 4.439
    },
    {
      "text": "Sebastian you can just click on this",
      "start": 484.159,
      "duration": 5.401
    },
    {
      "text": "download and then download this book so",
      "start": 486.879,
      "duration": 4.44
    },
    {
      "text": "we are going to look at this book and",
      "start": 489.56,
      "duration": 5.079
    },
    {
      "text": "assume that this is the only uh training",
      "start": 491.319,
      "duration": 5.641
    },
    {
      "text": "data which we have for the purposes of",
      "start": 494.639,
      "duration": 4.761
    },
    {
      "text": "today's lecture but whatever I show you",
      "start": 496.96,
      "duration": 4.679
    },
    {
      "text": "today can be really applied to any book",
      "start": 499.4,
      "duration": 4.16
    },
    {
      "text": "or even a huge number of",
      "start": 501.639,
      "duration": 4.721
    },
    {
      "text": "books I encourage you later after this",
      "start": 503.56,
      "duration": 4.599
    },
    {
      "text": "lecture to try out some different book",
      "start": 506.36,
      "duration": 4.72
    },
    {
      "text": "which is available for free online such",
      "start": 508.159,
      "duration": 4.641
    },
    {
      "text": "as the Harry Potter book or whichever",
      "start": 511.08,
      "duration": 3.639
    },
    {
      "text": "one interests you and apply the same",
      "start": 512.8,
      "duration": 4.119
    },
    {
      "text": "code over",
      "start": 514.719,
      "duration": 4.8
    },
    {
      "text": "there okay so this is the data set which",
      "start": 516.919,
      "duration": 4.8
    },
    {
      "text": "we are going to look at and then what we",
      "start": 519.519,
      "duration": 4.241
    },
    {
      "text": "have to do is basically download and",
      "start": 521.719,
      "duration": 5.081
    },
    {
      "text": "load this data set in Python so let me",
      "start": 523.76,
      "duration": 5.04
    },
    {
      "text": "show you how to do that so now we are",
      "start": 526.8,
      "duration": 5.4
    },
    {
      "text": "moving into uh jupyter coding uh please",
      "start": 528.8,
      "duration": 5.0
    },
    {
      "text": "comment out in the comment section if",
      "start": 532.2,
      "duration": 3.4
    },
    {
      "text": "the code is visible to you if I've",
      "start": 533.8,
      "duration": 3.8
    },
    {
      "text": "zoomed in enough or if I should follow",
      "start": 535.6,
      "duration": 4.44
    },
    {
      "text": "some different style",
      "start": 537.6,
      "duration": 4.72
    },
    {
      "text": "okay so the first step as I mentioned we",
      "start": 540.04,
      "duration": 4.08
    },
    {
      "text": "are going to look at two steps today",
      "start": 542.32,
      "duration": 3.68
    },
    {
      "text": "creating tokens and then converting",
      "start": 544.12,
      "duration": 4.68
    },
    {
      "text": "these tokens into token IDs the first",
      "start": 546.0,
      "duration": 5.72
    },
    {
      "text": "step is creating tokens right so what we",
      "start": 548.8,
      "duration": 4.88
    },
    {
      "text": "are going to do here is that we are",
      "start": 551.72,
      "duration": 5.04
    },
    {
      "text": "going to uh download the verdict. txt",
      "start": 553.68,
      "duration": 5.36
    },
    {
      "text": "file we are going to open it in Python",
      "start": 556.76,
      "duration": 4.56
    },
    {
      "text": "and we are going to read it and we are",
      "start": 559.04,
      "duration": 4.039
    },
    {
      "text": "going to print the total number of",
      "start": 561.32,
      "duration": 4.6
    },
    {
      "text": "characters so I'm just showing this uh",
      "start": 563.079,
      "duration": 4.601
    },
    {
      "text": "when you download it it looks this much",
      "start": 565.92,
      "duration": 5.32
    },
    {
      "text": "it's not that big um and in fact it's",
      "start": 567.68,
      "duration": 5.56
    },
    {
      "text": "even available over",
      "start": 571.24,
      "duration": 4.52
    },
    {
      "text": "here so I have downloaded this book",
      "start": 573.24,
      "duration": 5.8
    },
    {
      "text": "right now and uh in Python you can use",
      "start": 575.76,
      "duration": 5.8
    },
    {
      "text": "this open command so with open the word",
      "start": 579.04,
      "duration": 4.68
    },
    {
      "text": "it DOC text and then R means we are",
      "start": 581.56,
      "duration": 5.719
    },
    {
      "text": "reading this and then raw text is that",
      "start": 583.72,
      "duration": 7.04
    },
    {
      "text": "variable where we are storing whatever",
      "start": 587.279,
      "duration": 6.201
    },
    {
      "text": "uh content python has read from this",
      "start": 590.76,
      "duration": 5.199
    },
    {
      "text": "book so this raw text variable is going",
      "start": 593.48,
      "duration": 4.12
    },
    {
      "text": "to be very important for us because",
      "start": 595.959,
      "duration": 3.601
    },
    {
      "text": "we'll also refer to it later basically",
      "start": 597.6,
      "duration": 3.799
    },
    {
      "text": "raw text is just the entire text which",
      "start": 599.56,
      "duration": 3.959
    },
    {
      "text": "we are reading and then what we are",
      "start": 601.399,
      "duration": 3.761
    },
    {
      "text": "doing here is that we are counting the",
      "start": 603.519,
      "duration": 3.521
    },
    {
      "text": "total number of characters in this raw",
      "start": 605.16,
      "duration": 4.04
    },
    {
      "text": "text so what I'm doing in this part of",
      "start": 607.04,
      "duration": 4.16
    },
    {
      "text": "the code is that I'm printing out the",
      "start": 609.2,
      "duration": 3.759
    },
    {
      "text": "total number of characters and then",
      "start": 611.2,
      "duration": 6.04
    },
    {
      "text": "length raw text right so here you see",
      "start": 612.959,
      "duration": 6.241
    },
    {
      "text": "the print command towards the end raw",
      "start": 617.24,
      "duration": 5.4
    },
    {
      "text": "text colon 99 what does this do this",
      "start": 619.2,
      "duration": 5.92
    },
    {
      "text": "print command prints the total number of",
      "start": 622.64,
      "duration": 5.12
    },
    {
      "text": "characters uh so here so there are two",
      "start": 625.12,
      "duration": 4.6
    },
    {
      "text": "print commands right the first print",
      "start": 627.76,
      "duration": 3.72
    },
    {
      "text": "command essentially prints the total",
      "start": 629.72,
      "duration": 3.72
    },
    {
      "text": "number of characters and the second",
      "start": 631.48,
      "duration": 5.0
    },
    {
      "text": "print command essentially uh prints the",
      "start": 633.44,
      "duration": 5.12
    },
    {
      "text": "first 100 characters of this file for",
      "start": 636.48,
      "duration": 4.24
    },
    {
      "text": "illustration",
      "start": 638.56,
      "duration": 5.16
    },
    {
      "text": "purposes so uh let's run both of these",
      "start": 640.72,
      "duration": 5.2
    },
    {
      "text": "print commands right now I have already",
      "start": 643.72,
      "duration": 4.0
    },
    {
      "text": "run this before but let me click on this",
      "start": 645.92,
      "duration": 4.12
    },
    {
      "text": "run Icon again so okay the result of",
      "start": 647.72,
      "duration": 4.04
    },
    {
      "text": "this first print command is this the",
      "start": 650.04,
      "duration": 6.28
    },
    {
      "text": "total number of characters are 20479",
      "start": 651.76,
      "duration": 7.079
    },
    {
      "text": "20479 and the result of the second print",
      "start": 656.32,
      "duration": 4.199
    },
    {
      "text": "command is is basically we are printing",
      "start": 658.839,
      "duration": 5.161
    },
    {
      "text": "the first uh 100 characters of this file",
      "start": 660.519,
      "duration": 6.081
    },
    {
      "text": "right so the first 100 characters are I",
      "start": 664.0,
      "duration": 7.04
    },
    {
      "text": "had always thought Jack uh gisburn Etc",
      "start": 666.6,
      "duration": 6.679
    },
    {
      "text": "so this is the these are the first 100",
      "start": 671.04,
      "duration": 4.799
    },
    {
      "text": "characters so this indicates that python",
      "start": 673.279,
      "duration": 4.961
    },
    {
      "text": "has been able to successfully read the",
      "start": 675.839,
      "duration": 4.8
    },
    {
      "text": "text which we have downloaded and that's",
      "start": 678.24,
      "duration": 5.08
    },
    {
      "text": "an awesome sign great now let's move",
      "start": 680.639,
      "duration": 5.2
    },
    {
      "text": "further our goal in this lecture is to",
      "start": 683.32,
      "duration": 5.519
    },
    {
      "text": "tokenize all of the characters which are",
      "start": 685.839,
      "duration": 4.201
    },
    {
      "text": "in this",
      "start": 688.839,
      "duration": 4.361
    },
    {
      "text": "book in this text uh in this short story",
      "start": 690.04,
      "duration": 6.28
    },
    {
      "text": "into individual words that we can later",
      "start": 693.2,
      "duration": 5.96
    },
    {
      "text": "turn into embeddings for llm training",
      "start": 696.32,
      "duration": 6.6
    },
    {
      "text": "remember uh embeddings is the last step",
      "start": 699.16,
      "duration": 6.679
    },
    {
      "text": "so before you come to training the llm",
      "start": 702.92,
      "duration": 4.64
    },
    {
      "text": "there is this thing called embedding so",
      "start": 705.839,
      "duration": 3.361
    },
    {
      "text": "the goal in this lecture is to do the",
      "start": 707.56,
      "duration": 4.36
    },
    {
      "text": "tokenization and the token IDs for the",
      "start": 709.2,
      "duration": 5.12
    },
    {
      "text": "entire text which we have read for all",
      "start": 711.92,
      "duration": 4.359
    },
    {
      "text": "of these characters for the",
      "start": 714.32,
      "duration": 4.56
    },
    {
      "text": "20479 characters of this short story the",
      "start": 716.279,
      "duration": 5.201
    },
    {
      "text": "goal is is to convert them into tokens",
      "start": 718.88,
      "duration": 3.92
    },
    {
      "text": "and token",
      "start": 721.48,
      "duration": 3.96
    },
    {
      "text": "IDs so one note which has been added",
      "start": 722.8,
      "duration": 4.8
    },
    {
      "text": "over here is that note that it's common",
      "start": 725.44,
      "duration": 4.24
    },
    {
      "text": "to process millions of Articles and",
      "start": 727.6,
      "duration": 4.4
    },
    {
      "text": "hundreds of thousands of books which is",
      "start": 729.68,
      "duration": 4.64
    },
    {
      "text": "a huge amount of memory when actually",
      "start": 732.0,
      "duration": 4.639
    },
    {
      "text": "working with llms this is very important",
      "start": 734.32,
      "duration": 5.16
    },
    {
      "text": "to note when you actually work with llms",
      "start": 736.639,
      "duration": 5.721
    },
    {
      "text": "you don't just download one book as I",
      "start": 739.48,
      "duration": 4.76
    },
    {
      "text": "showed to you right now we actually work",
      "start": 742.36,
      "duration": 4.76
    },
    {
      "text": "with millions and thousands of books uh",
      "start": 744.24,
      "duration": 5.159
    },
    {
      "text": "which are a huge amount of data uh to be",
      "start": 747.12,
      "duration": 5.44
    },
    {
      "text": "stored on your computer however for",
      "start": 749.399,
      "duration": 5.201
    },
    {
      "text": "educational purposes it's sufficient to",
      "start": 752.56,
      "duration": 4.12
    },
    {
      "text": "work with smaller text samples like a",
      "start": 754.6,
      "duration": 4.64
    },
    {
      "text": "single book and that's why we are using",
      "start": 756.68,
      "duration": 4.56
    },
    {
      "text": "a single book in today's code to",
      "start": 759.24,
      "duration": 3.719
    },
    {
      "text": "demonstrate the idea to",
      "start": 761.24,
      "duration": 4.76
    },
    {
      "text": "you okay so the next question is how can",
      "start": 762.959,
      "duration": 5.401
    },
    {
      "text": "we best split this text to obtain a list",
      "start": 766.0,
      "duration": 5.199
    },
    {
      "text": "of tokens so I encourage you to pause",
      "start": 768.36,
      "duration": 4.64
    },
    {
      "text": "here right now and let's say I give you",
      "start": 771.199,
      "duration": 3.361
    },
    {
      "text": "this problem that you have read this",
      "start": 773.0,
      "duration": 4.24
    },
    {
      "text": "text now your goal is to split this text",
      "start": 774.56,
      "duration": 5.0
    },
    {
      "text": "into individual tokens right how would",
      "start": 777.24,
      "duration": 4.32
    },
    {
      "text": "you do this which python Library would",
      "start": 779.56,
      "duration": 4.2
    },
    {
      "text": "you",
      "start": 781.56,
      "duration": 2.2
    },
    {
      "text": "use uh so let me reveal the answer the",
      "start": 784.88,
      "duration": 4.92
    },
    {
      "text": "python Library which we we will be using",
      "start": 787.639,
      "duration": 5.44
    },
    {
      "text": "for this is called as regular expression",
      "start": 789.8,
      "duration": 6.0
    },
    {
      "text": "re uh so this module Pro provides",
      "start": 793.079,
      "duration": 4.801
    },
    {
      "text": "regular expression matching",
      "start": 795.8,
      "duration": 4.44
    },
    {
      "text": "operations and let's see how basically",
      "start": 797.88,
      "duration": 5.519
    },
    {
      "text": "this works we have to import re and then",
      "start": 800.24,
      "duration": 4.92
    },
    {
      "text": "what we can do is that we can use this",
      "start": 803.399,
      "duration": 5.321
    },
    {
      "text": "library to split uh to split any given",
      "start": 805.16,
      "duration": 5.16
    },
    {
      "text": "text",
      "start": 808.72,
      "duration": 4.479
    },
    {
      "text": "um based on the white spaces in that",
      "start": 810.32,
      "duration": 5.12
    },
    {
      "text": "text or any other characters in the text",
      "start": 813.199,
      "duration": 4.121
    },
    {
      "text": "and I'll explain what this actually",
      "start": 815.44,
      "duration": 5.16
    },
    {
      "text": "means okay",
      "start": 817.32,
      "duration": 3.28
    },
    {
      "text": "so okay so let's say you have a text",
      "start": 824.68,
      "duration": 5.279
    },
    {
      "text": "which looks like this hello world this",
      "start": 827.519,
      "duration": 5.161
    },
    {
      "text": "is a test right and you want to split",
      "start": 829.959,
      "duration": 4.281
    },
    {
      "text": "and you want to split this into",
      "start": 832.68,
      "duration": 4.44
    },
    {
      "text": "individual tokens what I'm going to do",
      "start": 834.24,
      "duration": 5.2
    },
    {
      "text": "here is that from this Library I'm going",
      "start": 837.12,
      "duration": 5.0
    },
    {
      "text": "to use this function re e dos split",
      "start": 839.44,
      "duration": 5.639
    },
    {
      "text": "right and then there is this R and then",
      "start": 842.12,
      "duration": 6.6
    },
    {
      "text": "the symbol is/ S what this basically",
      "start": 845.079,
      "duration": 5.921
    },
    {
      "text": "does is that this slash s means that we",
      "start": 848.72,
      "duration": 4.2
    },
    {
      "text": "split wherever white spaces are",
      "start": 851.0,
      "duration": 5.12
    },
    {
      "text": "encountered right so let me tell you",
      "start": 852.92,
      "duration": 5.08
    },
    {
      "text": "what is meant by white spaces so let's",
      "start": 856.12,
      "duration": 3.959
    },
    {
      "text": "look at this word let's look at this",
      "start": 858.0,
      "duration": 4.6
    },
    {
      "text": "sentence you see there is a white space",
      "start": 860.079,
      "duration": 4.721
    },
    {
      "text": "over here there is a white space over",
      "start": 862.6,
      "duration": 4.56
    },
    {
      "text": "here there is a white space over",
      "start": 864.8,
      "duration": 6.36
    },
    {
      "text": "here here and here so wherever there is",
      "start": 867.16,
      "duration": 6.599
    },
    {
      "text": "a white space what this command will do",
      "start": 871.16,
      "duration": 4.599
    },
    {
      "text": "is that it will split the text into",
      "start": 873.759,
      "duration": 4.801
    },
    {
      "text": "individual tokens and where the split",
      "start": 875.759,
      "duration": 4.64
    },
    {
      "text": "will be wherever the white space is",
      "start": 878.56,
      "duration": 4.0
    },
    {
      "text": "there so it will scan this sentence from",
      "start": 880.399,
      "duration": 5.321
    },
    {
      "text": "left to right now here it encounters the",
      "start": 882.56,
      "duration": 5.56
    },
    {
      "text": "white space so it will split split here",
      "start": 885.72,
      "duration": 4.44
    },
    {
      "text": "then again it will move forward here it",
      "start": 888.12,
      "duration": 3.6
    },
    {
      "text": "encounters the white space so it will",
      "start": 890.16,
      "duration": 3.799
    },
    {
      "text": "split here so let's go from left to",
      "start": 891.72,
      "duration": 4.919
    },
    {
      "text": "right so until hello and comma it has",
      "start": 893.959,
      "duration": 4.8
    },
    {
      "text": "not encountered a white space right so",
      "start": 896.639,
      "duration": 5.041
    },
    {
      "text": "hello and comma is one token and then it",
      "start": 898.759,
      "duration": 5.041
    },
    {
      "text": "encounters a white space so it splits so",
      "start": 901.68,
      "duration": 4.399
    },
    {
      "text": "then it outputs the white space here",
      "start": 903.8,
      "duration": 4.719
    },
    {
      "text": "when you print out the result then it",
      "start": 906.079,
      "duration": 4.68
    },
    {
      "text": "again goes on scanning and here until",
      "start": 908.519,
      "duration": 4.041
    },
    {
      "text": "world and full stop there is no white",
      "start": 910.759,
      "duration": 4.121
    },
    {
      "text": "space so it prints Out World and full",
      "start": 912.56,
      "duration": 4.839
    },
    {
      "text": "stop then after this point there is a",
      "start": 914.88,
      "duration": 5.36
    },
    {
      "text": "white space right see this so then it",
      "start": 917.399,
      "duration": 5.12
    },
    {
      "text": "prints this out then it again keeps on",
      "start": 920.24,
      "duration": 4.24
    },
    {
      "text": "scanning from left to right so till this",
      "start": 922.519,
      "duration": 4.161
    },
    {
      "text": "and comma there is no white space so it",
      "start": 924.48,
      "duration": 4.96
    },
    {
      "text": "prints out this and comma then it prints",
      "start": 926.68,
      "duration": 4.839
    },
    {
      "text": "out another white space over here so it",
      "start": 929.44,
      "duration": 4.04
    },
    {
      "text": "keeps on scanning from left to right and",
      "start": 931.519,
      "duration": 4.8
    },
    {
      "text": "then splits wherever white space is",
      "start": 933.48,
      "duration": 5.159
    },
    {
      "text": "encountered right that is the advantage",
      "start": 936.319,
      "duration": 4.241
    },
    {
      "text": "of using re do",
      "start": 938.639,
      "duration": 4.32
    },
    {
      "text": "split U and then this character",
      "start": 940.56,
      "duration": 4.6
    },
    {
      "text": "indicates where you want to split so",
      "start": 942.959,
      "duration": 3.841
    },
    {
      "text": "here I have indicated that you split",
      "start": 945.16,
      "duration": 4.44
    },
    {
      "text": "wherever you see a white space right so",
      "start": 946.8,
      "duration": 4.479
    },
    {
      "text": "the result of this is a list of",
      "start": 949.6,
      "duration": 3.84
    },
    {
      "text": "individual words white spaces and",
      "start": 951.279,
      "duration": 3.841
    },
    {
      "text": "punctuation characters so see white",
      "start": 953.44,
      "duration": 4.079
    },
    {
      "text": "spaces are also the results of",
      "start": 955.12,
      "duration": 5.92
    },
    {
      "text": "this now uh what we want to do is we",
      "start": 957.519,
      "duration": 7.281
    },
    {
      "text": "want to also uh split commas and periods",
      "start": 961.04,
      "duration": 6.479
    },
    {
      "text": "so here you see the comma is included as",
      "start": 964.8,
      "duration": 4.92
    },
    {
      "text": "part of the word word itself we the full",
      "start": 967.519,
      "duration": 4.361
    },
    {
      "text": "stop is included as part of the world",
      "start": 969.72,
      "duration": 4.119
    },
    {
      "text": "but we want to have comma and full stop",
      "start": 971.88,
      "duration": 4.519
    },
    {
      "text": "also as separate tokens how do we do",
      "start": 973.839,
      "duration": 5.48
    },
    {
      "text": "that it's pretty simple in the re. split",
      "start": 976.399,
      "duration": 5.44
    },
    {
      "text": "command you also include comma and you",
      "start": 979.319,
      "duration": 5.601
    },
    {
      "text": "also include the full stop along with",
      "start": 981.839,
      "duration": 6.041
    },
    {
      "text": "this SLS command and then when you print",
      "start": 984.92,
      "duration": 4.919
    },
    {
      "text": "the result you will see that along with",
      "start": 987.88,
      "duration": 4.68
    },
    {
      "text": "white spaces so white spaces are of",
      "start": 989.839,
      "duration": 5.12
    },
    {
      "text": "course uh printed out because we are",
      "start": 992.56,
      "duration": 5.0
    },
    {
      "text": "splitting on white spaces also but comma",
      "start": 994.959,
      "duration": 4.88
    },
    {
      "text": "is now a separate token full stop is now",
      "start": 997.56,
      "duration": 4.56
    },
    {
      "text": "a separate token which was not the case",
      "start": 999.839,
      "duration": 4.841
    },
    {
      "text": "before hello and comma were one token",
      "start": 1002.12,
      "duration": 5.48
    },
    {
      "text": "but now you'll see hello and comma are",
      "start": 1004.68,
      "duration": 5.44
    },
    {
      "text": "separate tokens awesome so now we have",
      "start": 1007.6,
      "duration": 5.919
    },
    {
      "text": "split based on comma and full stops also",
      "start": 1010.12,
      "duration": 6.36
    },
    {
      "text": "correct now the main another remaining",
      "start": 1013.519,
      "duration": 5.56
    },
    {
      "text": "issue is that our list still includes",
      "start": 1016.48,
      "duration": 4.88
    },
    {
      "text": "space characters right see these the",
      "start": 1019.079,
      "duration": 4.2
    },
    {
      "text": "white space characters are still counted",
      "start": 1021.36,
      "duration": 5.0
    },
    {
      "text": "as tokens we can even remove these red",
      "start": 1023.279,
      "duration": 5.76
    },
    {
      "text": "redundant character safely so here what",
      "start": 1026.36,
      "duration": 4.199
    },
    {
      "text": "we are doing is",
      "start": 1029.039,
      "duration": 4.88
    },
    {
      "text": "that uh you first scan the result scan",
      "start": 1030.559,
      "duration": 4.801
    },
    {
      "text": "each item in the result so we are",
      "start": 1033.919,
      "duration": 4.241
    },
    {
      "text": "looping over each item in the result so",
      "start": 1035.36,
      "duration": 4.88
    },
    {
      "text": "this is the result right so we are going",
      "start": 1038.16,
      "duration": 4.24
    },
    {
      "text": "over each of the individual items in",
      "start": 1040.24,
      "duration": 5.24
    },
    {
      "text": "this result and if item. strip is equal",
      "start": 1042.4,
      "duration": 5.799
    },
    {
      "text": "to true then only we return that item so",
      "start": 1045.48,
      "duration": 5.28
    },
    {
      "text": "if it's a y space then item. strip will",
      "start": 1048.199,
      "duration": 4.921
    },
    {
      "text": "actually be false so white spaces will",
      "start": 1050.76,
      "duration": 4.32
    },
    {
      "text": "not be returned Whenever there is",
      "start": 1053.12,
      "duration": 3.96
    },
    {
      "text": "actually a full world full word like",
      "start": 1055.08,
      "duration": 6.28
    },
    {
      "text": "hello or comma or world or this or full",
      "start": 1057.08,
      "duration": 7.32
    },
    {
      "text": "stop then only item. strip will return",
      "start": 1061.36,
      "duration": 5.28
    },
    {
      "text": "true and those words will be returned in",
      "start": 1064.4,
      "duration": 5.279
    },
    {
      "text": "this result so using this statement uh",
      "start": 1066.64,
      "duration": 5.84
    },
    {
      "text": "loop over item in result and then find",
      "start": 1069.679,
      "duration": 5.321
    },
    {
      "text": "out item. strip so for white spaces",
      "start": 1072.48,
      "duration": 5.76
    },
    {
      "text": "item. strip will anyway be false so that",
      "start": 1075.0,
      "duration": 5.64
    },
    {
      "text": "will not be returned so in this one line",
      "start": 1078.24,
      "duration": 4.16
    },
    {
      "text": "of command we can actually get rid of",
      "start": 1080.64,
      "duration": 3.56
    },
    {
      "text": "white spaces completely from the",
      "start": 1082.4,
      "duration": 4.2
    },
    {
      "text": "sentence and now when we print out the",
      "start": 1084.2,
      "duration": 4.719
    },
    {
      "text": "result we'll have the tokens as hello",
      "start": 1086.6,
      "duration": 5.319
    },
    {
      "text": "comma world full stop this comma is a",
      "start": 1088.919,
      "duration": 5.281
    },
    {
      "text": "test and full stop we won't have white",
      "start": 1091.919,
      "duration": 4.88
    },
    {
      "text": "spaces so one more consideration which I",
      "start": 1094.2,
      "duration": 4.839
    },
    {
      "text": "would like to mention over here is that",
      "start": 1096.799,
      "duration": 4.801
    },
    {
      "text": "removing white spaces are not this is an",
      "start": 1099.039,
      "duration": 5.241
    },
    {
      "text": "important question to actually",
      "start": 1101.6,
      "duration": 6.04
    },
    {
      "text": "discuss because uh when you develop a",
      "start": 1104.28,
      "duration": 5.56
    },
    {
      "text": "simple tokenizer whether we should",
      "start": 1107.64,
      "duration": 4.84
    },
    {
      "text": "encode white spaces as or as separate",
      "start": 1109.84,
      "duration": 5.04
    },
    {
      "text": "characters or just remove them depends",
      "start": 1112.48,
      "duration": 5.96
    },
    {
      "text": "on our application and its uh",
      "start": 1114.88,
      "duration": 5.84
    },
    {
      "text": "requirements so what are the advantages",
      "start": 1118.44,
      "duration": 4.479
    },
    {
      "text": "of removing white spaces removing white",
      "start": 1120.72,
      "duration": 4.439
    },
    {
      "text": "spaces reduces the memory and Computing",
      "start": 1122.919,
      "duration": 4.88
    },
    {
      "text": "requirements which is great however",
      "start": 1125.159,
      "duration": 5.121
    },
    {
      "text": "keeping white spaces can be useful if we",
      "start": 1127.799,
      "duration": 4.601
    },
    {
      "text": "train models that are sensitive to the",
      "start": 1130.28,
      "duration": 5.8
    },
    {
      "text": "exact structure of the text right for",
      "start": 1132.4,
      "duration": 5.56
    },
    {
      "text": "example python code is sensitive to",
      "start": 1136.08,
      "duration": 4.28
    },
    {
      "text": "indentation and spacing so if python",
      "start": 1137.96,
      "duration": 4.56
    },
    {
      "text": "code is used as the data set for",
      "start": 1140.36,
      "duration": 4.24
    },
    {
      "text": "training and a large language model it",
      "start": 1142.52,
      "duration": 4.0
    },
    {
      "text": "makes sense to keep the white spaces",
      "start": 1144.6,
      "duration": 4.079
    },
    {
      "text": "right because the white spaces have some",
      "start": 1146.52,
      "duration": 5.48
    },
    {
      "text": "meaning in in the case of python code as",
      "start": 1148.679,
      "duration": 5.441
    },
    {
      "text": "the training data",
      "start": 1152.0,
      "duration": 5.84
    },
    {
      "text": "set in this case we are going to uh",
      "start": 1154.12,
      "duration": 5.439
    },
    {
      "text": "until now we are going to remove the",
      "start": 1157.84,
      "duration": 4.079
    },
    {
      "text": "white spaces just for Simplicity so that",
      "start": 1159.559,
      "duration": 5.521
    },
    {
      "text": "we have memory advantages but remember",
      "start": 1161.919,
      "duration": 5.361
    },
    {
      "text": "that it's not obvious to always remove",
      "start": 1165.08,
      "duration": 6.8
    },
    {
      "text": "white spaces uh as has been mentioned uh",
      "start": 1167.28,
      "duration": 7.04
    },
    {
      "text": "uh over here if you have python code AS",
      "start": 1171.88,
      "duration": 4.6
    },
    {
      "text": "training data white spaces are important",
      "start": 1174.32,
      "duration": 5.16
    },
    {
      "text": "because indentation matters so several",
      "start": 1176.48,
      "duration": 4.52
    },
    {
      "text": "things need to be considered when you're",
      "start": 1179.48,
      "duration": 3.28
    },
    {
      "text": "building an actual llm whether you",
      "start": 1181.0,
      "duration": 3.84
    },
    {
      "text": "should remove white spaces or whether",
      "start": 1182.76,
      "duration": 5.12
    },
    {
      "text": "you should not remove white",
      "start": 1184.84,
      "duration": 5.48
    },
    {
      "text": "spaces uh that's great so the",
      "start": 1187.88,
      "duration": 4.36
    },
    {
      "text": "tokenization scheme which we deviced",
      "start": 1190.32,
      "duration": 4.56
    },
    {
      "text": "about works well on simple sample text",
      "start": 1192.24,
      "duration": 5.28
    },
    {
      "text": "so we can modify it a bit further",
      "start": 1194.88,
      "duration": 5.84
    },
    {
      "text": "because we also want uh to have question",
      "start": 1197.52,
      "duration": 5.88
    },
    {
      "text": "marks quotation marks and double dashes",
      "start": 1200.72,
      "duration": 4.56
    },
    {
      "text": "as separate characters or separate",
      "start": 1203.4,
      "duration": 4.519
    },
    {
      "text": "tokens currently only full stop and",
      "start": 1205.28,
      "duration": 4.8
    },
    {
      "text": "comma are separate tokens right but we",
      "start": 1207.919,
      "duration": 4.201
    },
    {
      "text": "also want question marks quotation marks",
      "start": 1210.08,
      "duration": 4.44
    },
    {
      "text": "double dashes Etc separate tokens so",
      "start": 1212.12,
      "duration": 4.559
    },
    {
      "text": "let's look at this document again see",
      "start": 1214.52,
      "duration": 4.399
    },
    {
      "text": "there are these double dashes then I'm",
      "start": 1216.679,
      "duration": 3.88
    },
    {
      "text": "sure there are question marks somewhere",
      "start": 1218.919,
      "duration": 3.0
    },
    {
      "text": "there are question marks there are",
      "start": 1220.559,
      "duration": 3.801
    },
    {
      "text": "exclamation marks in this document we",
      "start": 1221.919,
      "duration": 5.24
    },
    {
      "text": "all want them as separate tokens right",
      "start": 1224.36,
      "duration": 4.64
    },
    {
      "text": "and we need to include that in our",
      "start": 1227.159,
      "duration": 4.961
    },
    {
      "text": "tokenization Command and the way we do",
      "start": 1229.0,
      "duration": 5.039
    },
    {
      "text": "this is pretty simple again we use this",
      "start": 1232.12,
      "duration": 5.2
    },
    {
      "text": "re. split and along with comma and along",
      "start": 1234.039,
      "duration": 5.281
    },
    {
      "text": "with full stop we add all of these other",
      "start": 1237.32,
      "duration": 4.239
    },
    {
      "text": "special characters like colon semicolon",
      "start": 1239.32,
      "duration": 4.2
    },
    {
      "text": "question mark underscore",
      "start": 1241.559,
      "duration": 5.201
    },
    {
      "text": "exclamation uh quotation marks bracket",
      "start": 1243.52,
      "duration": 6.8
    },
    {
      "text": "Etc and uh we say that whenever these",
      "start": 1246.76,
      "duration": 6.0
    },
    {
      "text": "characters are encountered also do a",
      "start": 1250.32,
      "duration": 4.08
    },
    {
      "text": "split on these characters so that they",
      "start": 1252.76,
      "duration": 4.399
    },
    {
      "text": "are considered as individual tokens",
      "start": 1254.4,
      "duration": 5.44
    },
    {
      "text": "right um so so now when you take the",
      "start": 1257.159,
      "duration": 5.361
    },
    {
      "text": "sample text hello comma world is this",
      "start": 1259.84,
      "duration": 5.76
    },
    {
      "text": "dash dash a text all of these will be uh",
      "start": 1262.52,
      "duration": 5.159
    },
    {
      "text": "sample tokens this dash dash will also",
      "start": 1265.6,
      "duration": 3.959
    },
    {
      "text": "be a separate token the question mark",
      "start": 1267.679,
      "duration": 4.0
    },
    {
      "text": "will also be a separate token because we",
      "start": 1269.559,
      "duration": 4.6
    },
    {
      "text": "have included that in our split",
      "start": 1271.679,
      "duration": 5.321
    },
    {
      "text": "command and again we'll strip the white",
      "start": 1274.159,
      "duration": 5.481
    },
    {
      "text": "spaces like the same command for item",
      "start": 1277.0,
      "duration": 5.799
    },
    {
      "text": "for item in result if item. strip",
      "start": 1279.64,
      "duration": 4.96
    },
    {
      "text": "remember this was the command we used to",
      "start": 1282.799,
      "duration": 3.76
    },
    {
      "text": "filter out the white spaces or to remove",
      "start": 1284.6,
      "duration": 4.319
    },
    {
      "text": "the white spaces",
      "start": 1286.559,
      "duration": 5.24
    },
    {
      "text": "uh awesome so now these two lines of",
      "start": 1288.919,
      "duration": 6.36
    },
    {
      "text": "code are our tokenization scheme so this",
      "start": 1291.799,
      "duration": 5.441
    },
    {
      "text": "is the two lines of code in which we",
      "start": 1295.279,
      "duration": 4.4
    },
    {
      "text": "have built the tokenizer in this first",
      "start": 1297.24,
      "duration": 4.48
    },
    {
      "text": "line of code we take a sentence and we",
      "start": 1299.679,
      "duration": 4.12
    },
    {
      "text": "split the sentence wherever there is a",
      "start": 1301.72,
      "duration": 4.4
    },
    {
      "text": "white space wherever there is a comma",
      "start": 1303.799,
      "duration": 4.681
    },
    {
      "text": "full stop colon semicolon question mark",
      "start": 1306.12,
      "duration": 4.72
    },
    {
      "text": "underscore exclamation quotation mark or",
      "start": 1308.48,
      "duration": 5.36
    },
    {
      "text": "bracket we split it and then in the",
      "start": 1310.84,
      "duration": 5.52
    },
    {
      "text": "second sentence we just remove the white",
      "start": 1313.84,
      "duration": 5.719
    },
    {
      "text": "spaces and then we print out the result",
      "start": 1316.36,
      "duration": 4.96
    },
    {
      "text": "it's as simple as that this is the",
      "start": 1319.559,
      "duration": 4.321
    },
    {
      "text": "simple tokenization scheme for building",
      "start": 1321.32,
      "duration": 4.08
    },
    {
      "text": "large language models a different",
      "start": 1323.88,
      "duration": 3.399
    },
    {
      "text": "tokenization scheme is used but we'll",
      "start": 1325.4,
      "duration": 3.44
    },
    {
      "text": "come to that in the next lecture it's",
      "start": 1327.279,
      "duration": 4.801
    },
    {
      "text": "called bite pair encoding but for now I",
      "start": 1328.84,
      "duration": 5.28
    },
    {
      "text": "just wanted to give you a sense of what",
      "start": 1332.08,
      "duration": 4.04
    },
    {
      "text": "all nuances need to be considered for",
      "start": 1334.12,
      "duration": 5.12
    },
    {
      "text": "building a tokenizer like this now you",
      "start": 1336.12,
      "duration": 5.679
    },
    {
      "text": "see we have we have tested these two uh",
      "start": 1339.24,
      "duration": 5.039
    },
    {
      "text": "sentences which are our tokenizer scheme",
      "start": 1341.799,
      "duration": 5.041
    },
    {
      "text": "for a sample sentence right now what we",
      "start": 1344.279,
      "duration": 4.441
    },
    {
      "text": "can do is apply the same to two",
      "start": 1346.84,
      "duration": 5.52
    },
    {
      "text": "statements on the entire raw text so we",
      "start": 1348.72,
      "duration": 6.72
    },
    {
      "text": "had the raw text right defined before",
      "start": 1352.36,
      "duration": 6.72
    },
    {
      "text": "let me go to that U so see this was the",
      "start": 1355.44,
      "duration": 6.2
    },
    {
      "text": "raw text which was basically the book",
      "start": 1359.08,
      "duration": 4.32
    },
    {
      "text": "which we had read now we'll use these",
      "start": 1361.64,
      "duration": 5.279
    },
    {
      "text": "two statements to essentially uh convert",
      "start": 1363.4,
      "duration": 4.6
    },
    {
      "text": "the",
      "start": 1366.919,
      "duration": 3.76
    },
    {
      "text": "entire uh raw text into individual",
      "start": 1368.0,
      "duration": 5.12
    },
    {
      "text": "tokens so what we'll do is that we will",
      "start": 1370.679,
      "duration": 4.761
    },
    {
      "text": "split the entire raw text based on these",
      "start": 1373.12,
      "duration": 5.0
    },
    {
      "text": "tokens which we had seen before and then",
      "start": 1375.44,
      "duration": 4.8
    },
    {
      "text": "we will store it in a variable which is",
      "start": 1378.12,
      "duration": 3.24
    },
    {
      "text": "called as",
      "start": 1380.24,
      "duration": 3.16
    },
    {
      "text": "pre-processed so pre-process",
      "start": 1381.36,
      "duration": 4.199
    },
    {
      "text": "pre-processed is essentially a list of",
      "start": 1383.4,
      "duration": 4.399
    },
    {
      "text": "all the tokens and then in the second",
      "start": 1385.559,
      "duration": 4.6
    },
    {
      "text": "statement exactly same to what we saw",
      "start": 1387.799,
      "duration": 4.801
    },
    {
      "text": "before we'll split or we'll get rid of",
      "start": 1390.159,
      "duration": 5.361
    },
    {
      "text": "the white spaces awesome right so now",
      "start": 1392.6,
      "duration": 4.319
    },
    {
      "text": "let us print the length of the",
      "start": 1395.52,
      "duration": 4.68
    },
    {
      "text": "pre-processed so we have 4690 tokens in",
      "start": 1396.919,
      "duration": 6.36
    },
    {
      "text": "the original uh book so if you go to",
      "start": 1400.2,
      "duration": 4.68
    },
    {
      "text": "this book the simplest way to think",
      "start": 1403.279,
      "duration": 3.52
    },
    {
      "text": "about this is this will be one token",
      "start": 1404.88,
      "duration": 4.12
    },
    {
      "text": "height will be one token this inverted",
      "start": 1406.799,
      "duration": 3.681
    },
    {
      "text": "commas will be one token these two",
      "start": 1409.0,
      "duration": 4.36
    },
    {
      "text": "double dashes will be one token so this",
      "start": 1410.48,
      "duration": 4.6
    },
    {
      "text": "entire thing is broken down into",
      "start": 1413.36,
      "duration": 4.0
    },
    {
      "text": "individual words or tokens which are",
      "start": 1415.08,
      "duration": 5.36
    },
    {
      "text": "4690 in number and here we have just",
      "start": 1417.36,
      "duration": 6.04
    },
    {
      "text": "printed out the first 30 of these tokens",
      "start": 1420.44,
      "duration": 6.16
    },
    {
      "text": "and you see I had always thought they",
      "start": 1423.4,
      "duration": 6.08
    },
    {
      "text": "are individual tokens and dash dash",
      "start": 1426.6,
      "duration": 5.319
    },
    {
      "text": "comma they are also counted as tokens",
      "start": 1429.48,
      "duration": 4.679
    },
    {
      "text": "over here because we have uh mentioned",
      "start": 1431.919,
      "duration": 4.201
    },
    {
      "text": "that in this split where all do we want",
      "start": 1434.159,
      "duration": 4.88
    },
    {
      "text": "the split to happen awesome so we have",
      "start": 1436.12,
      "duration": 5.32
    },
    {
      "text": "now got our basic tokenizer working and",
      "start": 1439.039,
      "duration": 4.441
    },
    {
      "text": "we have applied it to the entire short",
      "start": 1441.44,
      "duration": 3.839
    },
    {
      "text": "story which we are considering",
      "start": 1443.48,
      "duration": 4.199
    },
    {
      "text": "considering as the input data",
      "start": 1445.279,
      "duration": 4.921
    },
    {
      "text": "great now we have to come to the Second",
      "start": 1447.679,
      "duration": 5.88
    },
    {
      "text": "Step so we have now finished the we have",
      "start": 1450.2,
      "duration": 5.52
    },
    {
      "text": "now finished the first step which is uh",
      "start": 1453.559,
      "duration": 5.36
    },
    {
      "text": "tokenizing the entire short story and",
      "start": 1455.72,
      "duration": 4.88
    },
    {
      "text": "now we will come to the next step which",
      "start": 1458.919,
      "duration": 4.801
    },
    {
      "text": "is converting tokens into token IDs now",
      "start": 1460.6,
      "duration": 6.799
    },
    {
      "text": "what is very important is that python",
      "start": 1463.72,
      "duration": 6.04
    },
    {
      "text": "works with numbers right so even if you",
      "start": 1467.399,
      "duration": 4.121
    },
    {
      "text": "look at these individual tokens which",
      "start": 1469.76,
      "duration": 3.279
    },
    {
      "text": "you have obtained right now these are",
      "start": 1471.52,
      "duration": 3.96
    },
    {
      "text": "not numbers they are still words we need",
      "start": 1473.039,
      "duration": 5.0
    },
    {
      "text": "to convert them into token IDs so that",
      "start": 1475.48,
      "duration": 5.199
    },
    {
      "text": "they are numerical representations so",
      "start": 1478.039,
      "duration": 5.601
    },
    {
      "text": "let's see how to do that so we have the",
      "start": 1480.679,
      "duration": 4.88
    },
    {
      "text": "complete training data set here for",
      "start": 1483.64,
      "duration": 3.399
    },
    {
      "text": "illustration I have just taken a",
      "start": 1485.559,
      "duration": 4.321
    },
    {
      "text": "sentence and until now we have converted",
      "start": 1487.039,
      "duration": 5.921
    },
    {
      "text": "this sentence into individual tokens now",
      "start": 1489.88,
      "duration": 4.72
    },
    {
      "text": "what we will do is that we'll take the",
      "start": 1492.96,
      "duration": 4.4
    },
    {
      "text": "tokens and convert them into token IDs",
      "start": 1494.6,
      "duration": 4.64
    },
    {
      "text": "and then there is a very specific way of",
      "start": 1497.36,
      "duration": 4.12
    },
    {
      "text": "doing this first we build something",
      "start": 1499.24,
      "duration": 4.76
    },
    {
      "text": "which is called as vocabulary vocabulary",
      "start": 1501.48,
      "duration": 5.24
    },
    {
      "text": "is just all the list of our tokens but",
      "start": 1504.0,
      "duration": 5.799
    },
    {
      "text": "it's sorted in an alphabetical manner so",
      "start": 1506.72,
      "duration": 6.04
    },
    {
      "text": "if the if our data set or the tokens are",
      "start": 1509.799,
      "duration": 7.12
    },
    {
      "text": "the quick brown fox jumps over the lazy",
      "start": 1512.76,
      "duration": 7.519
    },
    {
      "text": "dog if this is the training data set the",
      "start": 1516.919,
      "duration": 5.161
    },
    {
      "text": "vocabulary is the list of tokens in",
      "start": 1520.279,
      "duration": 4.64
    },
    {
      "text": "alphabetical order so Brown comes first",
      "start": 1522.08,
      "duration": 6.24
    },
    {
      "text": "then dog then Fox then jumps then lazy",
      "start": 1524.919,
      "duration": 6.48
    },
    {
      "text": "easy then over then quick and then the",
      "start": 1528.32,
      "duration": 5.599
    },
    {
      "text": "this is my vocabulary for this training",
      "start": 1531.399,
      "duration": 5.081
    },
    {
      "text": "data set eventually we'll have billions",
      "start": 1533.919,
      "duration": 4.88
    },
    {
      "text": "of data sets billions of files so then",
      "start": 1536.48,
      "duration": 4.76
    },
    {
      "text": "the vocabulary will be huge but I'm just",
      "start": 1538.799,
      "duration": 4.841
    },
    {
      "text": "showing a simple representation here for",
      "start": 1541.24,
      "duration": 5.12
    },
    {
      "text": "how vocabulary is constructed vocabulary",
      "start": 1543.64,
      "duration": 4.72
    },
    {
      "text": "is just a list of tokens which is sorted",
      "start": 1546.36,
      "duration": 4.559
    },
    {
      "text": "in alphabetical Manner and then what we",
      "start": 1548.36,
      "duration": 6.0
    },
    {
      "text": "do is that each unique token is mapped",
      "start": 1550.919,
      "duration": 5.681
    },
    {
      "text": "to a unique integer which is called as",
      "start": 1554.36,
      "duration": 5.199
    },
    {
      "text": "the token ID so it's as simple as that",
      "start": 1556.6,
      "duration": 5.12
    },
    {
      "text": "you map these tokens in alphabetical",
      "start": 1559.559,
      "duration": 4.881
    },
    {
      "text": "order and then to each token you assign",
      "start": 1561.72,
      "duration": 5.16
    },
    {
      "text": "a number so since they are arranged in",
      "start": 1564.44,
      "duration": 4.68
    },
    {
      "text": "alphabetical order Brown will be zero so",
      "start": 1566.88,
      "duration": 4.799
    },
    {
      "text": "you start from zero since it's python",
      "start": 1569.12,
      "duration": 5.439
    },
    {
      "text": "dog will be one fox will be two jumps",
      "start": 1571.679,
      "duration": 5.321
    },
    {
      "text": "will be three lazy will be four over",
      "start": 1574.559,
      "duration": 4.641
    },
    {
      "text": "will be five quick will be six and the",
      "start": 1577.0,
      "duration": 5.32
    },
    {
      "text": "will be seven so the vocabulary always",
      "start": 1579.2,
      "duration": 5.32
    },
    {
      "text": "contains unique tokens remember although",
      "start": 1582.32,
      "duration": 4.599
    },
    {
      "text": "the appears two times in the vocabulary",
      "start": 1584.52,
      "duration": 5.2
    },
    {
      "text": "it only comes once so we make a list of",
      "start": 1586.919,
      "duration": 5.161
    },
    {
      "text": "these unique tokens and we assign token",
      "start": 1589.72,
      "duration": 5.04
    },
    {
      "text": "IDs to all of these and it's important",
      "start": 1592.08,
      "duration": 4.0
    },
    {
      "text": "to arrange the vocabulary in",
      "start": 1594.76,
      "duration": 3.08
    },
    {
      "text": "alphabetical order so that assigning",
      "start": 1596.08,
      "duration": 4.719
    },
    {
      "text": "token IDs becomes very easy so the",
      "start": 1597.84,
      "duration": 4.68
    },
    {
      "text": "process of assigning token IDs is",
      "start": 1600.799,
      "duration": 4.161
    },
    {
      "text": "actually very simple each unique just",
      "start": 1602.52,
      "duration": 4.44
    },
    {
      "text": "remember that each unique token is",
      "start": 1604.96,
      "duration": 4.319
    },
    {
      "text": "mapped to a unique integer which is",
      "start": 1606.96,
      "duration": 5.839
    },
    {
      "text": "called as the token ID right so now",
      "start": 1609.279,
      "duration": 5.961
    },
    {
      "text": "let's see how this is implemented in",
      "start": 1612.799,
      "duration": 5.161
    },
    {
      "text": "Python uh so in the previous section we",
      "start": 1615.24,
      "duration": 5.799
    },
    {
      "text": "talk loaned edit worthon short story and",
      "start": 1617.96,
      "duration": 4.959
    },
    {
      "text": "assigned it to a python variable called",
      "start": 1621.039,
      "duration": 4.081
    },
    {
      "text": "pre-processed so remember pre-processed",
      "start": 1622.919,
      "duration": 4.24
    },
    {
      "text": "is the final list which contains all the",
      "start": 1625.12,
      "duration": 5.279
    },
    {
      "text": "tokenized words this these",
      "start": 1627.159,
      "duration": 6.24
    },
    {
      "text": "words uh now what we'll do is now we'll",
      "start": 1630.399,
      "duration": 5.28
    },
    {
      "text": "create a list of all unique tokens and",
      "start": 1633.399,
      "duration": 4.441
    },
    {
      "text": "sort them alphabetically to determine",
      "start": 1635.679,
      "duration": 4.561
    },
    {
      "text": "the vocabulary size exactly what I had",
      "start": 1637.84,
      "duration": 5.04
    },
    {
      "text": "mentioned over here so what we are doing",
      "start": 1640.24,
      "duration": 5.159
    },
    {
      "text": "is that we are taking pre-processed we",
      "start": 1642.88,
      "duration": 4.36
    },
    {
      "text": "are converting it into a set and we are",
      "start": 1645.399,
      "duration": 3.801
    },
    {
      "text": "going to sort it into set so this will",
      "start": 1647.24,
      "duration": 4.559
    },
    {
      "text": "sort it in alphabetical order and then",
      "start": 1649.2,
      "duration": 3.839
    },
    {
      "text": "we are just going to print the",
      "start": 1651.799,
      "duration": 3.281
    },
    {
      "text": "vocabulary size so we can see that the",
      "start": 1653.039,
      "duration": 5.64
    },
    {
      "text": "vocabulary size is 1 130 right remember",
      "start": 1655.08,
      "duration": 6.12
    },
    {
      "text": "the vocabulary only consists of unique",
      "start": 1658.679,
      "duration": 4.761
    },
    {
      "text": "words so the vocabulary size is less",
      "start": 1661.2,
      "duration": 5.599
    },
    {
      "text": "than the number of tokens which are",
      "start": 1663.44,
      "duration": 6.56
    },
    {
      "text": "there uh so now what we can do is after",
      "start": 1666.799,
      "duration": 5.161
    },
    {
      "text": "determining that the vocabulary size is",
      "start": 1670.0,
      "duration": 5.76
    },
    {
      "text": "1130 via the above code we create the",
      "start": 1671.96,
      "duration": 6.599
    },
    {
      "text": "vocabulary itself so when I say you",
      "start": 1675.76,
      "duration": 5.0
    },
    {
      "text": "create the vocabulary remember that a",
      "start": 1678.559,
      "duration": 4.761
    },
    {
      "text": "vocabulary is not just these tokens but",
      "start": 1680.76,
      "duration": 4.399
    },
    {
      "text": "every token needs to be assigned to a",
      "start": 1683.32,
      "duration": 5.28
    },
    {
      "text": "token ID a vocabulary is a is like a",
      "start": 1685.159,
      "duration": 5.921
    },
    {
      "text": "dictionary of tokens and Associated",
      "start": 1688.6,
      "duration": 5.079
    },
    {
      "text": "token IDs so now we have to create that",
      "start": 1691.08,
      "duration": 5.28
    },
    {
      "text": "dictionary right and so it's very simple",
      "start": 1693.679,
      "duration": 5.081
    },
    {
      "text": "you just all words consists of all the",
      "start": 1696.36,
      "duration": 4.159
    },
    {
      "text": "unique words in the vocabulary right in",
      "start": 1698.76,
      "duration": 4.32
    },
    {
      "text": "alphabetical order what you do is that",
      "start": 1700.519,
      "duration": 3.361
    },
    {
      "text": "you",
      "start": 1703.08,
      "duration": 3.479
    },
    {
      "text": "just uh you take all these tokens and",
      "start": 1703.88,
      "duration": 4.639
    },
    {
      "text": "assign an integer value to all these",
      "start": 1706.559,
      "duration": 4.401
    },
    {
      "text": "words that's it which means the token",
      "start": 1708.519,
      "duration": 4.601
    },
    {
      "text": "which comes first that will be zero the",
      "start": 1710.96,
      "duration": 4.16
    },
    {
      "text": "token which comes second its integer",
      "start": 1713.12,
      "duration": 3.799
    },
    {
      "text": "will be one and it will proceed like",
      "start": 1715.12,
      "duration": 4.36
    },
    {
      "text": "that uh let me print this out for you to",
      "start": 1716.919,
      "duration": 4.48
    },
    {
      "text": "show you so our tokens are also",
      "start": 1719.48,
      "duration": 4.319
    },
    {
      "text": "exclamation marks commas Etc right so by",
      "start": 1721.399,
      "duration": 3.841
    },
    {
      "text": "default they are assigned the first",
      "start": 1723.799,
      "duration": 5.36
    },
    {
      "text": "priority so they will be 0 1 2 3 4 5 6",
      "start": 1725.24,
      "duration": 5.6
    },
    {
      "text": "and here you see everything is arranged",
      "start": 1729.159,
      "duration": 5.12
    },
    {
      "text": "in alphabetical order so a will be 11 ah",
      "start": 1730.84,
      "duration": 6.559
    },
    {
      "text": "will be 12 among will be 13 and will be",
      "start": 1734.279,
      "duration": 7.561
    },
    {
      "text": "14 R will be 15 a r RT will be 16 as",
      "start": 1737.399,
      "duration": 8.12
    },
    {
      "text": "will be 17 at will be 18 Etc now all of",
      "start": 1741.84,
      "duration": 6.839
    },
    {
      "text": "these exclamation all of these I would",
      "start": 1745.519,
      "duration": 6.681
    },
    {
      "text": "say uh words are used because this was",
      "start": 1748.679,
      "duration": 6.36
    },
    {
      "text": "written in 1908 this a rrt is not used",
      "start": 1752.2,
      "duration": 5.68
    },
    {
      "text": "as an exclamation typically now but this",
      "start": 1755.039,
      "duration": 5.961
    },
    {
      "text": "book is written in 1908 so it shows up",
      "start": 1757.88,
      "duration": 5.08
    },
    {
      "text": "right so this is the ulary which we have",
      "start": 1761.0,
      "duration": 3.919
    },
    {
      "text": "and we have printed the first 50 items",
      "start": 1762.96,
      "duration": 3.8
    },
    {
      "text": "of this vocabulary so it looks like a",
      "start": 1764.919,
      "duration": 4.48
    },
    {
      "text": "dictionary pretty much and remember that",
      "start": 1766.76,
      "duration": 5.2
    },
    {
      "text": "every element in this dictionary uh",
      "start": 1769.399,
      "duration": 4.801
    },
    {
      "text": "which is a token has a token ID now",
      "start": 1771.96,
      "duration": 4.16
    },
    {
      "text": "associated with it that's it a",
      "start": 1774.2,
      "duration": 3.719
    },
    {
      "text": "vocabulary is as simple as that and the",
      "start": 1776.12,
      "duration": 4.2
    },
    {
      "text": "simplified code for this is for every",
      "start": 1777.919,
      "duration": 4.081
    },
    {
      "text": "token assign an",
      "start": 1780.32,
      "duration": 4.04
    },
    {
      "text": "integer and how do you get the integer",
      "start": 1782.0,
      "duration": 4.919
    },
    {
      "text": "and tokens you just enumerate all words",
      "start": 1784.36,
      "duration": 5.319
    },
    {
      "text": "so this enumerate command in Python what",
      "start": 1786.919,
      "duration": 4.561
    },
    {
      "text": "it does is that it takes all the words",
      "start": 1789.679,
      "duration": 3.761
    },
    {
      "text": "and then it assigns an integer to each",
      "start": 1791.48,
      "duration": 4.64
    },
    {
      "text": "word in alphabetical order so that way",
      "start": 1793.44,
      "duration": 4.64
    },
    {
      "text": "python is actually awesome and you can",
      "start": 1796.12,
      "duration": 4.84
    },
    {
      "text": "write very complex things in in one line",
      "start": 1798.08,
      "duration": 5.52
    },
    {
      "text": "of code actually so this is how you",
      "start": 1800.96,
      "duration": 4.76
    },
    {
      "text": "create the vocabulary and this is how",
      "start": 1803.6,
      "duration": 4.72
    },
    {
      "text": "you assign token IDs to every individual",
      "start": 1805.72,
      "duration": 6.319
    },
    {
      "text": "token right so as we can see based on",
      "start": 1808.32,
      "duration": 6.0
    },
    {
      "text": "the output above the dictionary contains",
      "start": 1812.039,
      "duration": 4.681
    },
    {
      "text": "individual tokens which are associated",
      "start": 1814.32,
      "duration": 5.28
    },
    {
      "text": "with unique in integer labels exactly",
      "start": 1816.72,
      "duration": 5.559
    },
    {
      "text": "what what we saw here the vocabulary",
      "start": 1819.6,
      "duration": 4.319
    },
    {
      "text": "which is a dictionary consists of",
      "start": 1822.279,
      "duration": 3.841
    },
    {
      "text": "individual tokens assigned to a unique",
      "start": 1823.919,
      "duration": 4.88
    },
    {
      "text": "integer label which are tokenized",
      "start": 1826.12,
      "duration": 5.48
    },
    {
      "text": "s now one more thing which I want to",
      "start": 1828.799,
      "duration": 5.641
    },
    {
      "text": "tell you is that currently we converted",
      "start": 1831.6,
      "duration": 6.559
    },
    {
      "text": "the words or tokens into token IDs right",
      "start": 1834.44,
      "duration": 6.119
    },
    {
      "text": "you can think of this process as",
      "start": 1838.159,
      "duration": 6.201
    },
    {
      "text": "encoding now later uh we will also need",
      "start": 1840.559,
      "duration": 5.84
    },
    {
      "text": "a decoder which means that from the",
      "start": 1844.36,
      "duration": 4.52
    },
    {
      "text": "token ID you need to convert the token",
      "start": 1846.399,
      "duration": 6.16
    },
    {
      "text": "ID back to the word because when the llm",
      "start": 1848.88,
      "duration": 5.12
    },
    {
      "text": "gives the output that will be in",
      "start": 1852.559,
      "duration": 3.761
    },
    {
      "text": "numerical form now you need to convert",
      "start": 1854.0,
      "duration": 4.44
    },
    {
      "text": "it in word form so you also need a",
      "start": 1856.32,
      "duration": 5.4
    },
    {
      "text": "mapping from the token ID to the Token",
      "start": 1858.44,
      "duration": 4.92
    },
    {
      "text": "the vocabulary is a dictionary which",
      "start": 1861.72,
      "duration": 3.48
    },
    {
      "text": "gives you a mapping from the token to",
      "start": 1863.36,
      "duration": 3.799
    },
    {
      "text": "the Token ID but we need a reverse",
      "start": 1865.2,
      "duration": 4.079
    },
    {
      "text": "mapping which is called as a",
      "start": 1867.159,
      "duration": 5.161
    },
    {
      "text": "decoder so this is what uh I have",
      "start": 1869.279,
      "duration": 5.441
    },
    {
      "text": "mentioned here later when we want to",
      "start": 1872.32,
      "duration": 5.64
    },
    {
      "text": "create the output of an llm from numbers",
      "start": 1874.72,
      "duration": 5.959
    },
    {
      "text": "back into text we also need a way to",
      "start": 1877.96,
      "duration": 6.319
    },
    {
      "text": "turn token IDs back into text and for",
      "start": 1880.679,
      "duration": 5.561
    },
    {
      "text": "this we create an inverse version of the",
      "start": 1884.279,
      "duration": 4.64
    },
    {
      "text": "vocabulary that Maps token IDs back to",
      "start": 1886.24,
      "duration": 4.919
    },
    {
      "text": "the corresponding text tokens and we'll",
      "start": 1888.919,
      "duration": 3.6
    },
    {
      "text": "see how to do",
      "start": 1891.159,
      "duration": 4.801
    },
    {
      "text": "that for now what we are going to do is",
      "start": 1892.519,
      "duration": 5.0
    },
    {
      "text": "we have understood enough about",
      "start": 1895.96,
      "duration": 3.36
    },
    {
      "text": "tokenization that we are going to",
      "start": 1897.519,
      "duration": 4.28
    },
    {
      "text": "implement a complete tokenizer class in",
      "start": 1899.32,
      "duration": 5.079
    },
    {
      "text": "Python this class will have two methods",
      "start": 1901.799,
      "duration": 4.321
    },
    {
      "text": "it will have an encode method and it",
      "start": 1904.399,
      "duration": 3.52
    },
    {
      "text": "will have a decode method let me show",
      "start": 1906.12,
      "duration": 4.439
    },
    {
      "text": "you what it actually means so this",
      "start": 1907.919,
      "duration": 4.521
    },
    {
      "text": "tokenizer class which we are going to",
      "start": 1910.559,
      "duration": 3.561
    },
    {
      "text": "implement in Python it will have two",
      "start": 1912.44,
      "duration": 4.52
    },
    {
      "text": "methods the first will be the encode",
      "start": 1914.12,
      "duration": 4.559
    },
    {
      "text": "method",
      "start": 1916.96,
      "duration": 4.839
    },
    {
      "text": "and the second will be the decode method",
      "start": 1918.679,
      "duration": 5.36
    },
    {
      "text": "in the encode method what will happen is",
      "start": 1921.799,
      "duration": 4.401
    },
    {
      "text": "that sample text will be converted into",
      "start": 1924.039,
      "duration": 4.321
    },
    {
      "text": "tokens and then tokens will be assigned",
      "start": 1926.2,
      "duration": 4.719
    },
    {
      "text": "token IDs based on the vocabulary",
      "start": 1928.36,
      "duration": 4.799
    },
    {
      "text": "exactly what we saw like over",
      "start": 1930.919,
      "duration": 6.201
    },
    {
      "text": "here in the decode method exactly",
      "start": 1933.159,
      "duration": 5.801
    },
    {
      "text": "reverse things would happen so in the",
      "start": 1937.12,
      "duration": 4.559
    },
    {
      "text": "decode method we start with token IDs we",
      "start": 1938.96,
      "duration": 4.959
    },
    {
      "text": "convert it into individual tokens and",
      "start": 1941.679,
      "duration": 5.401
    },
    {
      "text": "then we get back the sample text so I",
      "start": 1943.919,
      "duration": 4.6
    },
    {
      "text": "hope you understand the difference",
      "start": 1947.08,
      "duration": 3.319
    },
    {
      "text": "between the encode method and the decode",
      "start": 1948.519,
      "duration": 3.841
    },
    {
      "text": "method here because this is the exact",
      "start": 1950.399,
      "duration": 3.52
    },
    {
      "text": "same difference which will show up in",
      "start": 1952.36,
      "duration": 4.0
    },
    {
      "text": "the encoder and decoder block of the",
      "start": 1953.919,
      "duration": 4.88
    },
    {
      "text": "Transformer architecture or rather this",
      "start": 1956.36,
      "duration": 4.199
    },
    {
      "text": "is the this is a good way to understand",
      "start": 1958.799,
      "duration": 4.281
    },
    {
      "text": "that intuition in the encode in the",
      "start": 1960.559,
      "duration": 4.401
    },
    {
      "text": "encode block what we do is we take",
      "start": 1963.08,
      "duration": 4.079
    },
    {
      "text": "sample text we convert it into tokens",
      "start": 1964.96,
      "duration": 4.679
    },
    {
      "text": "and then we convert it into token IDs",
      "start": 1967.159,
      "duration": 4.321
    },
    {
      "text": "that feeds as the training data to the",
      "start": 1969.639,
      "duration": 4.92
    },
    {
      "text": "llm but when the llm gives its output in",
      "start": 1971.48,
      "duration": 5.199
    },
    {
      "text": "the form of token IDs we need to convert",
      "start": 1974.559,
      "duration": 4.041
    },
    {
      "text": "it back to tokens and and then back to",
      "start": 1976.679,
      "duration": 4.441
    },
    {
      "text": "the sample text so that we know what the",
      "start": 1978.6,
      "duration": 4.64
    },
    {
      "text": "output is in terms of",
      "start": 1981.12,
      "duration": 4.12
    },
    {
      "text": "sentences so that's why when we",
      "start": 1983.24,
      "duration": 4.399
    },
    {
      "text": "implement the tokenizer class we need an",
      "start": 1985.24,
      "duration": 5.439
    },
    {
      "text": "encode method uh and also the decode",
      "start": 1987.639,
      "duration": 5.601
    },
    {
      "text": "method so the encode method will take",
      "start": 1990.679,
      "duration": 4.96
    },
    {
      "text": "text as an input and give token IDs as",
      "start": 1993.24,
      "duration": 5.559
    },
    {
      "text": "output the decode method will take IDs",
      "start": 1995.639,
      "duration": 5.801
    },
    {
      "text": "token IDs as input and will give text as",
      "start": 1998.799,
      "duration": 3.641
    },
    {
      "text": "an",
      "start": 2001.44,
      "duration": 4.599
    },
    {
      "text": "output now let's see how to actually",
      "start": 2002.44,
      "duration": 7.199
    },
    {
      "text": "create this simple token izer class in",
      "start": 2006.039,
      "duration": 6.281
    },
    {
      "text": "Python first so there are if you see",
      "start": 2009.639,
      "duration": 4.721
    },
    {
      "text": "three methods the init method which is",
      "start": 2012.32,
      "duration": 3.92
    },
    {
      "text": "called by default when an instance of",
      "start": 2014.36,
      "duration": 4.48
    },
    {
      "text": "this class is created and let us look at",
      "start": 2016.24,
      "duration": 4.279
    },
    {
      "text": "the arguments of the init method it",
      "start": 2018.84,
      "duration": 4.16
    },
    {
      "text": "takes wcab so when you create an",
      "start": 2020.519,
      "duration": 4.561
    },
    {
      "text": "instance of the tokenizer class you have",
      "start": 2023.0,
      "duration": 4.399
    },
    {
      "text": "to pass in the vocabulary right and",
      "start": 2025.08,
      "duration": 4.599
    },
    {
      "text": "remember the vocabulary is nothing but a",
      "start": 2027.399,
      "duration": 6.481
    },
    {
      "text": "mapping from tokens to token IDs right",
      "start": 2029.679,
      "duration": 7.041
    },
    {
      "text": "so then after this uh instance is",
      "start": 2033.88,
      "duration": 5.56
    },
    {
      "text": "created St Str to in which is string to",
      "start": 2036.72,
      "duration": 4.72
    },
    {
      "text": "integer will just be the vocabulary",
      "start": 2039.44,
      "duration": 3.479
    },
    {
      "text": "because the vocabulary is already a",
      "start": 2041.44,
      "duration": 3.8
    },
    {
      "text": "mapping from string to integer or tokens",
      "start": 2042.919,
      "duration": 5.121
    },
    {
      "text": "to integer and then the integer to",
      "start": 2045.24,
      "duration": 5.0
    },
    {
      "text": "string is basically reverse so what you",
      "start": 2048.04,
      "duration": 5.079
    },
    {
      "text": "do is that you take the string and",
      "start": 2050.24,
      "duration": 5.72
    },
    {
      "text": "integer in the vocabulary and then for",
      "start": 2053.119,
      "duration": 6.04
    },
    {
      "text": "every integer you uh mention which token",
      "start": 2055.96,
      "duration": 6.119
    },
    {
      "text": "it is so for S you can think of as the",
      "start": 2059.159,
      "duration": 4.801
    },
    {
      "text": "token and for I you can think of as",
      "start": 2062.079,
      "duration": 4.84
    },
    {
      "text": "token ID so what we do in this int to",
      "start": 2063.96,
      "duration": 6.32
    },
    {
      "text": "string variable variable is that uh so",
      "start": 2066.919,
      "duration": 6.521
    },
    {
      "text": "we take the token and we take the token",
      "start": 2070.28,
      "duration": 5.079
    },
    {
      "text": "ID in the vocabulary and then we just",
      "start": 2073.44,
      "duration": 4.399
    },
    {
      "text": "flip it then we say that for this token",
      "start": 2075.359,
      "duration": 5.28
    },
    {
      "text": "ID this is the particular token remember",
      "start": 2077.839,
      "duration": 4.881
    },
    {
      "text": "this into string will be needed for the",
      "start": 2080.639,
      "duration": 4.121
    },
    {
      "text": "decoder method when we have the token",
      "start": 2082.72,
      "duration": 5.76
    },
    {
      "text": "IDs and we want to convert it back to",
      "start": 2084.76,
      "duration": 7.44
    },
    {
      "text": "tokens so uh in the encode method the",
      "start": 2088.48,
      "duration": 6.359
    },
    {
      "text": "exact same pre-processing steps will",
      "start": 2092.2,
      "duration": 4.24
    },
    {
      "text": "happen as we had seen before for",
      "start": 2094.839,
      "duration": 4.0
    },
    {
      "text": "tokenization what we'll do if some",
      "start": 2096.44,
      "duration": 4.96
    },
    {
      "text": "random text is given to us we will take",
      "start": 2098.839,
      "duration": 4.881
    },
    {
      "text": "that text we will split it we'll split",
      "start": 2101.4,
      "duration": 4.6
    },
    {
      "text": "it based on the comma based on the full",
      "start": 2103.72,
      "duration": 3.96
    },
    {
      "text": "stop based on the colon based on",
      "start": 2106.0,
      "duration": 5.359
    },
    {
      "text": "semicolon Etc into individual tokens and",
      "start": 2107.68,
      "duration": 6.24
    },
    {
      "text": "then we'll get rid of the white spaces",
      "start": 2111.359,
      "duration": 5.121
    },
    {
      "text": "what we had seen before and then these",
      "start": 2113.92,
      "duration": 4.56
    },
    {
      "text": "will be individual tokens right so up",
      "start": 2116.48,
      "duration": 3.76
    },
    {
      "text": "till now we are at this part where we",
      "start": 2118.48,
      "duration": 4.2
    },
    {
      "text": "have converted the sample text into",
      "start": 2120.24,
      "duration": 4.359
    },
    {
      "text": "individual tokens and then we'll convert",
      "start": 2122.68,
      "duration": 4.28
    },
    {
      "text": "these into token IDs so that's the last",
      "start": 2124.599,
      "duration": 4.601
    },
    {
      "text": "step over here after you get the",
      "start": 2126.96,
      "duration": 4.28
    },
    {
      "text": "individual tokens in this list called",
      "start": 2129.2,
      "duration": 4.12
    },
    {
      "text": "pre-processed what you have to do is",
      "start": 2131.24,
      "duration": 4.68
    },
    {
      "text": "that you have to use this St Str to in",
      "start": 2133.32,
      "duration": 4.68
    },
    {
      "text": "dictionary which is basically just the",
      "start": 2135.92,
      "duration": 5.04
    },
    {
      "text": "vocabulary and then you have to uh",
      "start": 2138.0,
      "duration": 6.44
    },
    {
      "text": "assign a token ID for each token so",
      "start": 2140.96,
      "duration": 5.84
    },
    {
      "text": "remember the Str str2 int is basically",
      "start": 2144.44,
      "duration": 4.32
    },
    {
      "text": "just converting tokens to token IDs",
      "start": 2146.8,
      "duration": 3.64
    },
    {
      "text": "using the vocabulary which is passed",
      "start": 2148.76,
      "duration": 4.559
    },
    {
      "text": "into this tokenizer class so you will",
      "start": 2150.44,
      "duration": 6.2
    },
    {
      "text": "just take the uh tokens you'll take the",
      "start": 2153.319,
      "duration": 4.921
    },
    {
      "text": "tokens which are in this pre-process",
      "start": 2156.64,
      "duration": 3.28
    },
    {
      "text": "list and you'll convert them into token",
      "start": 2158.24,
      "duration": 4.879
    },
    {
      "text": "IDs that's the encoder method that's it",
      "start": 2159.92,
      "duration": 4.96
    },
    {
      "text": "in the decoder method what you are",
      "start": 2163.119,
      "duration": 5.041
    },
    {
      "text": "actually doing is that you are uh you",
      "start": 2164.88,
      "duration": 5.6
    },
    {
      "text": "are using this reverse dictionary which",
      "start": 2168.16,
      "duration": 4.439
    },
    {
      "text": "is integer to string which is basically",
      "start": 2170.48,
      "duration": 4.96
    },
    {
      "text": "token ID to token and then you are",
      "start": 2172.599,
      "duration": 5.841
    },
    {
      "text": "converting the token IDs into individual",
      "start": 2175.44,
      "duration": 5.48
    },
    {
      "text": "tokens that's the first thing so let's",
      "start": 2178.44,
      "duration": 4.399
    },
    {
      "text": "see the decode method you first convert",
      "start": 2180.92,
      "duration": 4.679
    },
    {
      "text": "the token IDs into individual tokens and",
      "start": 2182.839,
      "duration": 4.28
    },
    {
      "text": "then what you do is you join these",
      "start": 2185.599,
      "duration": 3.921
    },
    {
      "text": "individual tokens together so this this",
      "start": 2187.119,
      "duration": 5.441
    },
    {
      "text": "join is used so first you convert the",
      "start": 2189.52,
      "duration": 6.12
    },
    {
      "text": "token IDs into tokens and then you join",
      "start": 2192.56,
      "duration": 6.4
    },
    {
      "text": "the individual tokens together that's it",
      "start": 2195.64,
      "duration": 4.92
    },
    {
      "text": "and then here what we are doing is we",
      "start": 2198.96,
      "duration": 3.48
    },
    {
      "text": "are going to replace spaces before the",
      "start": 2200.56,
      "duration": 5.88
    },
    {
      "text": "punctuations so an example here would be",
      "start": 2202.44,
      "duration": 7.12
    },
    {
      "text": "let's say if the tokens let's say in the",
      "start": 2206.44,
      "duration": 8.0
    },
    {
      "text": "decoder if the tokens are the fox",
      "start": 2209.56,
      "duration": 8.36
    },
    {
      "text": "chased and question and full stop right",
      "start": 2214.44,
      "duration": 6.679
    },
    {
      "text": "now if these are the tokens and if we",
      "start": 2217.92,
      "duration": 5.48
    },
    {
      "text": "convert it into sample text by using the",
      "start": 2221.119,
      "duration": 4.561
    },
    {
      "text": "join method the final answer will be the",
      "start": 2223.4,
      "duration": 6.199
    },
    {
      "text": "the the fox chased and full stop now see",
      "start": 2225.68,
      "duration": 5.56
    },
    {
      "text": "the problem here is that there is a",
      "start": 2229.599,
      "duration": 4.961
    },
    {
      "text": "space between the there is a space here",
      "start": 2231.24,
      "duration": 6.879
    },
    {
      "text": "between the full stop and the chased so",
      "start": 2234.56,
      "duration": 6.32
    },
    {
      "text": "we need to get rid of this space so then",
      "start": 2238.119,
      "duration": 6.561
    },
    {
      "text": "the final answer would be the fox",
      "start": 2240.88,
      "duration": 6.76
    },
    {
      "text": "chased and full stop and this is the the",
      "start": 2244.68,
      "duration": 6.08
    },
    {
      "text": "same for question mark Etc so in this",
      "start": 2247.64,
      "duration": 5.64
    },
    {
      "text": "second sentence here what we are",
      "start": 2250.76,
      "duration": 4.12
    },
    {
      "text": "actually doing is that we are getting",
      "start": 2253.28,
      "duration": 3.96
    },
    {
      "text": "rid of all the spaces before the",
      "start": 2254.88,
      "duration": 3.959
    },
    {
      "text": "punctuation so that it becomes a",
      "start": 2257.24,
      "duration": 3.079
    },
    {
      "text": "complete",
      "start": 2258.839,
      "duration": 3.921
    },
    {
      "text": "sentence so this is the decoder method",
      "start": 2260.319,
      "duration": 4.441
    },
    {
      "text": "it's actually very simple we are just",
      "start": 2262.76,
      "duration": 5.96
    },
    {
      "text": "going to uh use the encode method to",
      "start": 2264.76,
      "duration": 7.079
    },
    {
      "text": "convert uh sample text into token IDs",
      "start": 2268.72,
      "duration": 4.639
    },
    {
      "text": "and we are going to use the decode",
      "start": 2271.839,
      "duration": 3.801
    },
    {
      "text": "method to convert token IDs back into",
      "start": 2273.359,
      "duration": 5.081
    },
    {
      "text": "sample text and for that we needed to",
      "start": 2275.64,
      "duration": 4.679
    },
    {
      "text": "write two methods the encode method and",
      "start": 2278.44,
      "duration": 4.2
    },
    {
      "text": "the decode method so remember this",
      "start": 2280.319,
      "duration": 4.441
    },
    {
      "text": "tokenizer class takes the vocabulary",
      "start": 2282.64,
      "duration": 4.959
    },
    {
      "text": "already as an input so that so we",
      "start": 2284.76,
      "duration": 4.96
    },
    {
      "text": "already have the mapping from tokens to",
      "start": 2287.599,
      "duration": 4.121
    },
    {
      "text": "token IDs because that is inherently",
      "start": 2289.72,
      "duration": 4.28
    },
    {
      "text": "present in the vocabulary we just need",
      "start": 2291.72,
      "duration": 4.2
    },
    {
      "text": "to construct a reverse mapping from the",
      "start": 2294.0,
      "duration": 4.119
    },
    {
      "text": "token IDs back to the tokens and then",
      "start": 2295.92,
      "duration": 5.399
    },
    {
      "text": "use that in the decode method that's it",
      "start": 2298.119,
      "duration": 6.161
    },
    {
      "text": "and to convert the text into tokens we",
      "start": 2301.319,
      "duration": 5.841
    },
    {
      "text": "use the same re dos split and item do",
      "start": 2304.28,
      "duration": 5.24
    },
    {
      "text": "string which we had seen earlier in",
      "start": 2307.16,
      "duration": 3.6
    },
    {
      "text": "today's",
      "start": 2309.52,
      "duration": 3.599
    },
    {
      "text": "lecture so this is essentially the",
      "start": 2310.76,
      "duration": 5.48
    },
    {
      "text": "tokenizer class which we have created",
      "start": 2313.119,
      "duration": 6.041
    },
    {
      "text": "awesome now let's move to the next step",
      "start": 2316.24,
      "duration": 4.8
    },
    {
      "text": "so what we can do is that now we can",
      "start": 2319.16,
      "duration": 4.12
    },
    {
      "text": "instantiate a tokenizer object from this",
      "start": 2321.04,
      "duration": 5.64
    },
    {
      "text": "class right and uh tokenize a passage",
      "start": 2323.28,
      "duration": 5.16
    },
    {
      "text": "from the short story which we have",
      "start": 2326.68,
      "duration": 4.439
    },
    {
      "text": "downloaded to test it out in practice so",
      "start": 2328.44,
      "duration": 4.76
    },
    {
      "text": "here you see I'm creating an instance of",
      "start": 2331.119,
      "duration": 4.2
    },
    {
      "text": "this class I have passed the vocabulary",
      "start": 2333.2,
      "duration": 4.52
    },
    {
      "text": "as an input what is this vocabulary this",
      "start": 2335.319,
      "duration": 5.241
    },
    {
      "text": "vocabulary is basically the uh",
      "start": 2337.72,
      "duration": 5.599
    },
    {
      "text": "vocabulary of tokens and token IDs which",
      "start": 2340.56,
      "duration": 5.24
    },
    {
      "text": "we have converted our input text so this",
      "start": 2343.319,
      "duration": 4.52
    },
    {
      "text": "is our input text and we have converted",
      "start": 2345.8,
      "duration": 3.799
    },
    {
      "text": "this into tokens and assigned the token",
      "start": 2347.839,
      "duration": 4.28
    },
    {
      "text": "ID to each this is my vocabulary now",
      "start": 2349.599,
      "duration": 5.72
    },
    {
      "text": "what I'm doing is I'm actually uh",
      "start": 2352.119,
      "duration": 6.041
    },
    {
      "text": "creating an instance of the yeah I'm",
      "start": 2355.319,
      "duration": 4.441
    },
    {
      "text": "creating an instance of the simple",
      "start": 2358.16,
      "duration": 3.64
    },
    {
      "text": "tokenizer version one class which we",
      "start": 2359.76,
      "duration": 4.68
    },
    {
      "text": "have defined over here by passing in",
      "start": 2361.8,
      "duration": 4.88
    },
    {
      "text": "this vocabulary as an input right and",
      "start": 2364.44,
      "duration": 5.679
    },
    {
      "text": "this is then defined as tokenizer great",
      "start": 2366.68,
      "duration": 6.32
    },
    {
      "text": "and uh so the text which I'm going to",
      "start": 2370.119,
      "duration": 5.321
    },
    {
      "text": "pass in now is this it's the last he",
      "start": 2373.0,
      "duration": 4.599
    },
    {
      "text": "painted you know Mrs gburn said with",
      "start": 2375.44,
      "duration": 4.639
    },
    {
      "text": "pardonable pride this is the text and",
      "start": 2377.599,
      "duration": 4.961
    },
    {
      "text": "now I'm going to test out this encode so",
      "start": 2380.079,
      "duration": 4.361
    },
    {
      "text": "remember this encode takes in the text",
      "start": 2382.56,
      "duration": 4.279
    },
    {
      "text": "as an input so you need a text right to",
      "start": 2384.44,
      "duration": 5.2
    },
    {
      "text": "convert into IDs so what the encode",
      "start": 2386.839,
      "duration": 5.321
    },
    {
      "text": "method will do always remember this",
      "start": 2389.64,
      "duration": 5.0
    },
    {
      "text": "schematic always remember the schematic",
      "start": 2392.16,
      "duration": 4.48
    },
    {
      "text": "the encode method will convert the text",
      "start": 2394.64,
      "duration": 5.8
    },
    {
      "text": "into token ID is so this is the text and",
      "start": 2396.64,
      "duration": 6.0
    },
    {
      "text": "then we apply the method tokenizer do",
      "start": 2400.44,
      "duration": 5.2
    },
    {
      "text": "encode and text so what when you print",
      "start": 2402.64,
      "duration": 4.84
    },
    {
      "text": "the IDS you will see these IDs of the",
      "start": 2405.64,
      "duration": 4.4
    },
    {
      "text": "text which means that our encoder has",
      "start": 2407.48,
      "duration": 4.639
    },
    {
      "text": "successfully converted this text into",
      "start": 2410.04,
      "duration": 5.039
    },
    {
      "text": "token IDs that is exactly what we wanted",
      "start": 2412.119,
      "duration": 5.801
    },
    {
      "text": "right so the code above prints the token",
      "start": 2415.079,
      "duration": 4.801
    },
    {
      "text": "IDs next let's see if we can turn these",
      "start": 2417.92,
      "duration": 4.32
    },
    {
      "text": "token IDs back into text so now what",
      "start": 2419.88,
      "duration": 3.84
    },
    {
      "text": "we'll be doing is we'll be using",
      "start": 2422.24,
      "duration": 5.2
    },
    {
      "text": "tokenizer do decode and use these IDs so",
      "start": 2423.72,
      "duration": 7.44
    },
    {
      "text": "tokenizer decode and IDs uh and pass IDs",
      "start": 2427.44,
      "duration": 6.0
    },
    {
      "text": "as an input remember tokenizer do decode",
      "start": 2431.16,
      "duration": 5.48
    },
    {
      "text": "takes the IDS as an input so I'll pass",
      "start": 2433.44,
      "duration": 4.919
    },
    {
      "text": "these IDs which we have printed over",
      "start": 2436.64,
      "duration": 4.04
    },
    {
      "text": "here and let's see whether it recovers",
      "start": 2438.359,
      "duration": 4.641
    },
    {
      "text": "this text so the text which is recovered",
      "start": 2440.68,
      "duration": 4.48
    },
    {
      "text": "is it's the last he painted you know Mrs",
      "start": 2443.0,
      "duration": 4.92
    },
    {
      "text": "gburn said with pardonable pride amazing",
      "start": 2445.16,
      "duration": 4.679
    },
    {
      "text": "right because it's exactly the same text",
      "start": 2447.92,
      "duration": 4.56
    },
    {
      "text": "which we had given uh to the encoder and",
      "start": 2449.839,
      "duration": 5.361
    },
    {
      "text": "now it has been decoded by the decoder",
      "start": 2452.48,
      "duration": 5.04
    },
    {
      "text": "So based on this output above we can see",
      "start": 2455.2,
      "duration": 4.04
    },
    {
      "text": "that the decode method successfully",
      "start": 2457.52,
      "duration": 3.839
    },
    {
      "text": "converted the token IDs back into the",
      "start": 2459.24,
      "duration": 4.8
    },
    {
      "text": "original text which is exactly what we",
      "start": 2461.359,
      "duration": 4.881
    },
    {
      "text": "wanted which means the encoder and",
      "start": 2464.04,
      "duration": 5.36
    },
    {
      "text": "decoder are working right so so far so",
      "start": 2466.24,
      "duration": 5.64
    },
    {
      "text": "good we have implemented a tokenizer",
      "start": 2469.4,
      "duration": 4.52
    },
    {
      "text": "which is capable of tokenizing and DET",
      "start": 2471.88,
      "duration": 4.8
    },
    {
      "text": "tokenizing text based on a snippet from",
      "start": 2473.92,
      "duration": 5.199
    },
    {
      "text": "the training set so this sentence which",
      "start": 2476.68,
      "duration": 4.2
    },
    {
      "text": "we gave to the encoder was from the",
      "start": 2479.119,
      "duration": 4.161
    },
    {
      "text": "training set so we knew that the words",
      "start": 2480.88,
      "duration": 5.12
    },
    {
      "text": "will be in the vocabulary but what if",
      "start": 2483.28,
      "duration": 4.76
    },
    {
      "text": "the sentence which is given is not",
      "start": 2486.0,
      "duration": 5.48
    },
    {
      "text": "present in the vocabulary so we have a",
      "start": 2488.04,
      "duration": 6.76
    },
    {
      "text": "vocabulary here which is 1130 the size",
      "start": 2491.48,
      "duration": 5.72
    },
    {
      "text": "of the vocabulary is 1130 right what if",
      "start": 2494.8,
      "duration": 4.319
    },
    {
      "text": "I give a sentence to en code which is",
      "start": 2497.2,
      "duration": 4.119
    },
    {
      "text": "not present in the vocabulary so let's",
      "start": 2499.119,
      "duration": 4.321
    },
    {
      "text": "try this let's say the text is hello do",
      "start": 2501.319,
      "duration": 6.481
    },
    {
      "text": "you like T now uh hello is probably not",
      "start": 2503.44,
      "duration": 7.159
    },
    {
      "text": "there in this hello is not there in this",
      "start": 2507.8,
      "duration": 4.84
    },
    {
      "text": "short story so let me search hello it's",
      "start": 2510.599,
      "duration": 5.841
    },
    {
      "text": "not there let's see if T is there t is",
      "start": 2512.64,
      "duration": 7.28
    },
    {
      "text": "actually T is there but hello is not",
      "start": 2516.44,
      "duration": 5.919
    },
    {
      "text": "there so hello will not be there in the",
      "start": 2519.92,
      "duration": 5.08
    },
    {
      "text": "vocabulary right so now let let me see",
      "start": 2522.359,
      "duration": 5.801
    },
    {
      "text": "how what's my answer here because now I",
      "start": 2525.0,
      "duration": 5.04
    },
    {
      "text": "have asked the tokenizer to encode",
      "start": 2528.16,
      "duration": 3.4
    },
    {
      "text": "something which is not present in the",
      "start": 2530.04,
      "duration": 3.72
    },
    {
      "text": "vocabulary okay so if I run this you see",
      "start": 2531.56,
      "duration": 4.4
    },
    {
      "text": "you get an error here because hello it",
      "start": 2533.76,
      "duration": 3.8
    },
    {
      "text": "does not know what to do with this word",
      "start": 2535.96,
      "duration": 4.359
    },
    {
      "text": "hello because it is not present in the",
      "start": 2537.56,
      "duration": 5.08
    },
    {
      "text": "vocabulary so the problem is that the",
      "start": 2540.319,
      "duration": 4.201
    },
    {
      "text": "word hello was not used in the verdict",
      "start": 2542.64,
      "duration": 3.88
    },
    {
      "text": "short story and hence it is not",
      "start": 2544.52,
      "duration": 3.12
    },
    {
      "text": "contained in the",
      "start": 2546.52,
      "duration": 3.599
    },
    {
      "text": "vocabulary now this actually highlights",
      "start": 2547.64,
      "duration": 4.52
    },
    {
      "text": "the need to consider large and diverse",
      "start": 2550.119,
      "duration": 3.761
    },
    {
      "text": "training data sets to extend the",
      "start": 2552.16,
      "duration": 4.439
    },
    {
      "text": "vocabulary when working with llms we do",
      "start": 2553.88,
      "duration": 4.239
    },
    {
      "text": "not want this problem right if our",
      "start": 2556.599,
      "duration": 3.72
    },
    {
      "text": "training data set is small and if some",
      "start": 2558.119,
      "duration": 4.401
    },
    {
      "text": "user gives a new word to chat GPT which",
      "start": 2560.319,
      "duration": 4.481
    },
    {
      "text": "it does not even know we don't want our",
      "start": 2562.52,
      "duration": 5.16
    },
    {
      "text": "we don't want an error that's why huge",
      "start": 2564.8,
      "duration": 4.759
    },
    {
      "text": "number of data set is used for training",
      "start": 2567.68,
      "duration": 3.2
    },
    {
      "text": "large language",
      "start": 2569.559,
      "duration": 5.081
    },
    {
      "text": "models in fact llms like chat GPT use a",
      "start": 2570.88,
      "duration": 5.36
    },
    {
      "text": "very special thing they use something",
      "start": 2574.64,
      "duration": 3.88
    },
    {
      "text": "which is called a special context tokens",
      "start": 2576.24,
      "duration": 4.119
    },
    {
      "text": "to deal with Words which might not be",
      "start": 2578.52,
      "duration": 3.599
    },
    {
      "text": "present in the vocabulary so that an",
      "start": 2580.359,
      "duration": 4.841
    },
    {
      "text": "error message is not shown such as this",
      "start": 2582.119,
      "duration": 4.641
    },
    {
      "text": "and we will come to that in the next",
      "start": 2585.2,
      "duration": 3.04
    },
    {
      "text": "section but here I just want to",
      "start": 2586.76,
      "duration": 4.92
    },
    {
      "text": "illustrate that the reason for having a",
      "start": 2588.24,
      "duration": 6.28
    },
    {
      "text": "a large and diverse training data set is",
      "start": 2591.68,
      "duration": 6.96
    },
    {
      "text": "because uh we we want many words to be",
      "start": 2594.52,
      "duration": 6.0
    },
    {
      "text": "present in our vocabulary it does not",
      "start": 2598.64,
      "duration": 3.32
    },
    {
      "text": "make sense to have a smaller and a",
      "start": 2600.52,
      "duration": 3.52
    },
    {
      "text": "shorter vocabulary because what if the",
      "start": 2601.96,
      "duration": 3.879
    },
    {
      "text": "user gives a new word which is not",
      "start": 2604.04,
      "duration": 3.559
    },
    {
      "text": "present in the vocabulary we don't want",
      "start": 2605.839,
      "duration": 4.0
    },
    {
      "text": "that there is a way to deal with this by",
      "start": 2607.599,
      "duration": 3.921
    },
    {
      "text": "using something called special context",
      "start": 2609.839,
      "duration": 3.881
    },
    {
      "text": "tokens so let's look at this in the next",
      "start": 2611.52,
      "duration": 5.24
    },
    {
      "text": "section now let us have a look at the",
      "start": 2613.72,
      "duration": 5.76
    },
    {
      "text": "last section in today's lecture which is",
      "start": 2616.76,
      "duration": 5.0
    },
    {
      "text": "dealing with special context",
      "start": 2619.48,
      "duration": 5.16
    },
    {
      "text": "tokens these content are not usually",
      "start": 2621.76,
      "duration": 5.04
    },
    {
      "text": "covered in other lectures but they are",
      "start": 2624.64,
      "duration": 4.08
    },
    {
      "text": "very important especially since we are",
      "start": 2626.8,
      "duration": 4.559
    },
    {
      "text": "going to be building llms from scratch",
      "start": 2628.72,
      "duration": 5.48
    },
    {
      "text": "we also want to understand how real life",
      "start": 2631.359,
      "duration": 5.841
    },
    {
      "text": "llms work and special context tokens",
      "start": 2634.2,
      "duration": 5.84
    },
    {
      "text": "play an important role so until now we",
      "start": 2637.2,
      "duration": 4.84
    },
    {
      "text": "have implemented a simple tokenizer",
      "start": 2640.04,
      "duration": 4.0
    },
    {
      "text": "right and applied it to a passage from",
      "start": 2642.04,
      "duration": 4.96
    },
    {
      "text": "the training set but the main question",
      "start": 2644.04,
      "duration": 5.0
    },
    {
      "text": "we encountered before is that if there",
      "start": 2647.0,
      "duration": 4.4
    },
    {
      "text": "is a word in the text which is not there",
      "start": 2649.04,
      "duration": 5.68
    },
    {
      "text": "in the vocabulary how do we encode this",
      "start": 2651.4,
      "duration": 6.84
    },
    {
      "text": "word um so we will be implementing some",
      "start": 2654.72,
      "duration": 5.119
    },
    {
      "text": "uh things which are called as special",
      "start": 2658.24,
      "duration": 4.879
    },
    {
      "text": "context tokens what we will do is that",
      "start": 2659.839,
      "duration": 5.361
    },
    {
      "text": "we will modify the tokenizer to handle",
      "start": 2663.119,
      "duration": 4.72
    },
    {
      "text": "unknown words",
      "start": 2665.2,
      "duration": 5.76
    },
    {
      "text": "uh and we will Implement a python class",
      "start": 2667.839,
      "duration": 4.72
    },
    {
      "text": "which is called a simple tokenizer",
      "start": 2670.96,
      "duration": 3.92
    },
    {
      "text": "version two so we have already",
      "start": 2672.559,
      "duration": 4.881
    },
    {
      "text": "implemented uh simple tokenizer version",
      "start": 2674.88,
      "duration": 4.92
    },
    {
      "text": "one but here it did not have the",
      "start": 2677.44,
      "duration": 5.0
    },
    {
      "text": "provision to handle the unknown tokens",
      "start": 2679.8,
      "duration": 7.12
    },
    {
      "text": "so in this version two we will also uh",
      "start": 2682.44,
      "duration": 7.6
    },
    {
      "text": "Implement uh the simple tokenizer",
      "start": 2686.92,
      "duration": 6.439
    },
    {
      "text": "version two to have the provision to",
      "start": 2690.04,
      "duration": 5.559
    },
    {
      "text": "handle the unknown tokens in particular",
      "start": 2693.359,
      "duration": 4.0
    },
    {
      "text": "we will be learning about two main",
      "start": 2695.599,
      "duration": 3.921
    },
    {
      "text": "tokens the first is this this token",
      "start": 2697.359,
      "duration": 4.72
    },
    {
      "text": "which is unknown for an unknown word and",
      "start": 2699.52,
      "duration": 5.24
    },
    {
      "text": "the second token is for end of",
      "start": 2702.079,
      "duration": 7.161
    },
    {
      "text": "text so let me again go back to this uh",
      "start": 2704.76,
      "duration": 6.559
    },
    {
      "text": "whiteboard over here and write a bit",
      "start": 2709.24,
      "duration": 5.76
    },
    {
      "text": "about why we are exactly using these",
      "start": 2711.319,
      "duration": 7.361
    },
    {
      "text": "tokens so uh let's take a sentence which",
      "start": 2715.0,
      "duration": 7.44
    },
    {
      "text": "is the fox chased the",
      "start": 2718.68,
      "duration": 6.399
    },
    {
      "text": "dog let's say this is my sentence right",
      "start": 2722.44,
      "duration": 6.8
    },
    {
      "text": "now and I have tokenized it into the Fox",
      "start": 2725.079,
      "duration": 6.921
    },
    {
      "text": "Chase the dog and here's the vocabulary",
      "start": 2729.24,
      "duration": 4.68
    },
    {
      "text": "so it's arranged in alphabetical order",
      "start": 2732.0,
      "duration": 5.04
    },
    {
      "text": "and there are token IDs right now to",
      "start": 2733.92,
      "duration": 5.159
    },
    {
      "text": "this existing vocabulary we are going to",
      "start": 2737.04,
      "duration": 5.319
    },
    {
      "text": "add two more tokens the first is the Unk",
      "start": 2739.079,
      "duration": 5.681
    },
    {
      "text": "which is unknown and we'll assign a",
      "start": 2742.359,
      "duration": 5.441
    },
    {
      "text": "token ID and then we'll also have end of",
      "start": 2744.76,
      "duration": 6.24
    },
    {
      "text": "text and then we'll assign it a token ID",
      "start": 2747.8,
      "duration": 5.279
    },
    {
      "text": "these two are the last",
      "start": 2751.0,
      "duration": 5.52
    },
    {
      "text": "two uh tokens in the vocabulary so the",
      "start": 2753.079,
      "duration": 5.76
    },
    {
      "text": "token is corresponding to these two will",
      "start": 2756.52,
      "duration": 3.44
    },
    {
      "text": "be the",
      "start": 2758.839,
      "duration": 3.52
    },
    {
      "text": "largest so what will happen is that",
      "start": 2759.96,
      "duration": 4.8
    },
    {
      "text": "let's say if some new sentence or some",
      "start": 2762.359,
      "duration": 5.76
    },
    {
      "text": "new word is given so let's say uh the",
      "start": 2764.76,
      "duration": 7.24
    },
    {
      "text": "fox chased the dog quickly if that is",
      "start": 2768.119,
      "duration": 5.601
    },
    {
      "text": "the",
      "start": 2772.0,
      "duration": 4.68
    },
    {
      "text": "word all these other words like the Fox",
      "start": 2773.72,
      "duration": 4.839
    },
    {
      "text": "Chase the dog will be converted into",
      "start": 2776.68,
      "duration": 5.04
    },
    {
      "text": "token IDs but for quickly it will have a",
      "start": 2778.559,
      "duration": 6.201
    },
    {
      "text": "token ID of 783 which is the token ID of",
      "start": 2781.72,
      "duration": 5.68
    },
    {
      "text": "unknown and why because quickly was not",
      "start": 2784.76,
      "duration": 4.44
    },
    {
      "text": "in the vocabulary our vocabulary only",
      "start": 2787.4,
      "duration": 4.88
    },
    {
      "text": "consisted of chased dog fox and the so",
      "start": 2789.2,
      "duration": 5.8
    },
    {
      "text": "quickly is an unknown word and so it",
      "start": 2792.28,
      "duration": 6.039
    },
    {
      "text": "will receive a token ID which we have",
      "start": 2795.0,
      "duration": 6.16
    },
    {
      "text": "reserved for unknown words and then you",
      "start": 2798.319,
      "duration": 4.881
    },
    {
      "text": "might be asking what about this end of",
      "start": 2801.16,
      "duration": 4.959
    },
    {
      "text": "text so end of text is something which",
      "start": 2803.2,
      "duration": 3.879
    },
    {
      "text": "is a bit",
      "start": 2806.119,
      "duration": 3.281
    },
    {
      "text": "different uh when we are working with",
      "start": 2807.079,
      "duration": 5.721
    },
    {
      "text": "multiple text sources we typically add",
      "start": 2809.4,
      "duration": 6.32
    },
    {
      "text": "end of text token between the text so",
      "start": 2812.8,
      "duration": 5.72
    },
    {
      "text": "let us look at four text sources here",
      "start": 2815.72,
      "duration": 4.56
    },
    {
      "text": "this is the text Source One let's say it",
      "start": 2818.52,
      "duration": 3.88
    },
    {
      "text": "comes from one book let's say this is",
      "start": 2820.28,
      "duration": 3.799
    },
    {
      "text": "the text Source two let's say it comes",
      "start": 2822.4,
      "duration": 4.0
    },
    {
      "text": "from another news article text Source",
      "start": 2824.079,
      "duration": 5.561
    },
    {
      "text": "three which comes from an encyclopedia",
      "start": 2826.4,
      "duration": 5.12
    },
    {
      "text": "and text Source Four let's say which",
      "start": 2829.64,
      "duration": 3.04
    },
    {
      "text": "comes from an",
      "start": 2831.52,
      "duration": 3.599
    },
    {
      "text": "interview let's say these are are our",
      "start": 2832.68,
      "duration": 5.639
    },
    {
      "text": "training sets usually all of these are",
      "start": 2835.119,
      "duration": 5.521
    },
    {
      "text": "not just collected into one giant",
      "start": 2838.319,
      "duration": 4.881
    },
    {
      "text": "document or all of these sentences are",
      "start": 2840.64,
      "duration": 5.24
    },
    {
      "text": "not just stacked up together we usually",
      "start": 2843.2,
      "duration": 5.76
    },
    {
      "text": "after after this initial text is fed as",
      "start": 2845.88,
      "duration": 6.08
    },
    {
      "text": "an input we have this end of text token",
      "start": 2848.96,
      "duration": 5.28
    },
    {
      "text": "which means that the first text has",
      "start": 2851.96,
      "duration": 4.159
    },
    {
      "text": "ended and now the second text has",
      "start": 2854.24,
      "duration": 4.599
    },
    {
      "text": "started after the second text ends then",
      "start": 2856.119,
      "duration": 5.2
    },
    {
      "text": "we again have this end of text token",
      "start": 2858.839,
      "duration": 4.801
    },
    {
      "text": "then the third text starts and after the",
      "start": 2861.319,
      "duration": 4.24
    },
    {
      "text": "third text ends then we again have the",
      "start": 2863.64,
      "duration": 4.08
    },
    {
      "text": "end of text token and then the fourth",
      "start": 2865.559,
      "duration": 6.04
    },
    {
      "text": "text starts so the end of text token is",
      "start": 2867.72,
      "duration": 7.04
    },
    {
      "text": "basically added between the",
      "start": 2871.599,
      "duration": 5.96
    },
    {
      "text": "texts so essentially the end end of text",
      "start": 2874.76,
      "duration": 5.28
    },
    {
      "text": "tokens act as markers signaling the",
      "start": 2877.559,
      "duration": 6.441
    },
    {
      "text": "start or end of a particular",
      "start": 2880.04,
      "duration": 6.72
    },
    {
      "text": "segment this leads to more effective",
      "start": 2884.0,
      "duration": 5.319
    },
    {
      "text": "processing and understanding by the llm",
      "start": 2886.76,
      "duration": 4.44
    },
    {
      "text": "it's very important to add these end of",
      "start": 2889.319,
      "duration": 4.201
    },
    {
      "text": "text tokens because then llm treats the",
      "start": 2891.2,
      "duration": 4.84
    },
    {
      "text": "initial so let's say end of text there",
      "start": 2893.52,
      "duration": 4.36
    },
    {
      "text": "is before the end of text which is text",
      "start": 2896.04,
      "duration": 3.84
    },
    {
      "text": "Source One and after the end of text",
      "start": 2897.88,
      "duration": 4.84
    },
    {
      "text": "which is text Source two if end if end",
      "start": 2899.88,
      "duration": 4.8
    },
    {
      "text": "of text was not there the llm would have",
      "start": 2902.72,
      "duration": 4.399
    },
    {
      "text": "mixed all of this together right uh but",
      "start": 2904.68,
      "duration": 4.879
    },
    {
      "text": "end of text tokens allows the llm to",
      "start": 2907.119,
      "duration": 4.48
    },
    {
      "text": "process the data and understand the data",
      "start": 2909.559,
      "duration": 4.52
    },
    {
      "text": "in a much better Manner and in fact when",
      "start": 2911.599,
      "duration": 5.081
    },
    {
      "text": "GPT was trained the end of text tokens",
      "start": 2914.079,
      "duration": 5.441
    },
    {
      "text": "were used between different text sources",
      "start": 2916.68,
      "duration": 5.28
    },
    {
      "text": "this is very important to note and only",
      "start": 2919.52,
      "duration": 4.0
    },
    {
      "text": "students who try to understand",
      "start": 2921.96,
      "duration": 4.72
    },
    {
      "text": "tokenization in detail know about such",
      "start": 2923.52,
      "duration": 5.559
    },
    {
      "text": "specifics so these are the two tokens",
      "start": 2926.68,
      "duration": 3.8
    },
    {
      "text": "which we will be considering in this",
      "start": 2929.079,
      "duration": 3.881
    },
    {
      "text": "section the first is the unknown token",
      "start": 2930.48,
      "duration": 5.52
    },
    {
      "text": "and the second is the end of text token",
      "start": 2932.96,
      "duration": 5.52
    },
    {
      "text": "so let's read bit here we can modify the",
      "start": 2936.0,
      "duration": 4.799
    },
    {
      "text": "tokenizer to use an unknown token if it",
      "start": 2938.48,
      "duration": 4.4
    },
    {
      "text": "encounters a word that is not part of",
      "start": 2940.799,
      "duration": 5.201
    },
    {
      "text": "the vocabulary great furthermore we also",
      "start": 2942.88,
      "duration": 5.52
    },
    {
      "text": "add a token between unrelated text which",
      "start": 2946.0,
      "duration": 4.44
    },
    {
      "text": "is the end of text",
      "start": 2948.4,
      "duration": 5.199
    },
    {
      "text": "token so for example when training GPT",
      "start": 2950.44,
      "duration": 5.399
    },
    {
      "text": "like llms on in multiple independent",
      "start": 2953.599,
      "duration": 5.441
    },
    {
      "text": "documents it is common to insert a token",
      "start": 2955.839,
      "duration": 5.681
    },
    {
      "text": "before each document or book that",
      "start": 2959.04,
      "duration": 5.519
    },
    {
      "text": "follows a previous text Source basically",
      "start": 2961.52,
      "duration": 5.279
    },
    {
      "text": "this is exactly the end of text token",
      "start": 2964.559,
      "duration": 4.321
    },
    {
      "text": "which has been written over here I'll be",
      "start": 2966.799,
      "duration": 3.961
    },
    {
      "text": "sharing this notebook with all of you so",
      "start": 2968.88,
      "duration": 3.64
    },
    {
      "text": "no need to worry if you miss certain",
      "start": 2970.76,
      "duration": 3.4
    },
    {
      "text": "portion or want to revise certain",
      "start": 2972.52,
      "duration": 4.839
    },
    {
      "text": "portion again now what we'll be doing as",
      "start": 2974.16,
      "duration": 5.639
    },
    {
      "text": "I mentioned over here uh is that we'll",
      "start": 2977.359,
      "duration": 4.44
    },
    {
      "text": "be modifying the vocabulary we'll be",
      "start": 2979.799,
      "duration": 3.401
    },
    {
      "text": "augmenting the",
      "start": 2981.799,
      "duration": 4.52
    },
    {
      "text": "vocabulary and uh how will we modify or",
      "start": 2983.2,
      "duration": 5.28
    },
    {
      "text": "augment the vocabulary we will include",
      "start": 2986.319,
      "duration": 4.441
    },
    {
      "text": "special tokens we will include two",
      "start": 2988.48,
      "duration": 4.359
    },
    {
      "text": "special Tokens The Unknown and the end",
      "start": 2990.76,
      "duration": 4.72
    },
    {
      "text": "of text token and we will add these to",
      "start": 2992.839,
      "duration": 4.881
    },
    {
      "text": "the list of all the unique words that we",
      "start": 2995.48,
      "duration": 3.839
    },
    {
      "text": "created in the vocabulary in the",
      "start": 2997.72,
      "duration": 3.8
    },
    {
      "text": "previous section so let's look at this",
      "start": 2999.319,
      "duration": 4.601
    },
    {
      "text": "pre-processed so remember pre-processed",
      "start": 3001.52,
      "duration": 6.48
    },
    {
      "text": "is the vocabulary uh or U so",
      "start": 3003.92,
      "duration": 7.159
    },
    {
      "text": "preprocessed was our list and then later",
      "start": 3008.0,
      "duration": 5.319
    },
    {
      "text": "we converted this into vocabulary which",
      "start": 3011.079,
      "duration": 4.52
    },
    {
      "text": "was stored in vocab right and remember",
      "start": 3013.319,
      "duration": 5.401
    },
    {
      "text": "the size of the vocab is 1130 and now we",
      "start": 3015.599,
      "duration": 5.081
    },
    {
      "text": "are going to add two more tokens so the",
      "start": 3018.72,
      "duration": 3.359
    },
    {
      "text": "size will be",
      "start": 3020.68,
      "duration": 4.04
    },
    {
      "text": "1132 so let us again start with",
      "start": 3022.079,
      "duration": 5.121
    },
    {
      "text": "pre-processed we will sort this",
      "start": 3024.72,
      "duration": 4.52
    },
    {
      "text": "and then we'll add two tokens so we'll",
      "start": 3027.2,
      "duration": 4.04
    },
    {
      "text": "add the end of text token and then we'll",
      "start": 3029.24,
      "duration": 4.28
    },
    {
      "text": "add the unknown token and that is done",
      "start": 3031.24,
      "duration": 4.04
    },
    {
      "text": "by using the python command which is",
      "start": 3033.52,
      "duration": 4.16
    },
    {
      "text": "called extend so what this does is that",
      "start": 3035.28,
      "duration": 5.16
    },
    {
      "text": "it just adds two more additional entries",
      "start": 3037.68,
      "duration": 5.48
    },
    {
      "text": "to the list the sorted list in this case",
      "start": 3040.44,
      "duration": 4.32
    },
    {
      "text": "and then again we'll write the same",
      "start": 3043.16,
      "duration": 3.56
    },
    {
      "text": "command which we did earlier right we",
      "start": 3044.76,
      "duration": 5.16
    },
    {
      "text": "first enumerate all the tokens in the",
      "start": 3046.72,
      "duration": 6.24
    },
    {
      "text": "pre-processed list and then for each",
      "start": 3049.92,
      "duration": 4.96
    },
    {
      "text": "token we assign an",
      "start": 3052.96,
      "duration": 4.28
    },
    {
      "text": "integer and this enumer it will make",
      "start": 3054.88,
      "duration": 3.84
    },
    {
      "text": "sure that the tokens are anyways",
      "start": 3057.24,
      "duration": 3.839
    },
    {
      "text": "arranged in alphabetical order and then",
      "start": 3058.72,
      "duration": 4.079
    },
    {
      "text": "for each of these token we assign an",
      "start": 3061.079,
      "duration": 3.881
    },
    {
      "text": "integer which is the token ID that's how",
      "start": 3062.799,
      "duration": 4.601
    },
    {
      "text": "we create the vocabulary now if you",
      "start": 3064.96,
      "duration": 4.399
    },
    {
      "text": "print out the length of the vocabulary",
      "start": 3067.4,
      "duration": 4.04
    },
    {
      "text": "you will see that the length of the",
      "start": 3069.359,
      "duration": 3.401
    },
    {
      "text": "vocabulary is",
      "start": 3071.44,
      "duration": 4.72
    },
    {
      "text": "1132 and if you remember without adding",
      "start": 3072.76,
      "duration": 4.96
    },
    {
      "text": "these two tokens the length of the",
      "start": 3076.16,
      "duration": 2.84
    },
    {
      "text": "vocabulary was",
      "start": 3077.72,
      "duration": 4.119
    },
    {
      "text": "1130 so now the length has increased by",
      "start": 3079.0,
      "duration": 4.72
    },
    {
      "text": "two that is great",
      "start": 3081.839,
      "duration": 4.321
    },
    {
      "text": "right uh that makes sense because two",
      "start": 3083.72,
      "duration": 4.839
    },
    {
      "text": "more tokens have been added so the",
      "start": 3086.16,
      "duration": 6.439
    },
    {
      "text": "vocabulary is extended great so based on",
      "start": 3088.559,
      "duration": 6.321
    },
    {
      "text": "the output of the print statement above",
      "start": 3092.599,
      "duration": 4.281
    },
    {
      "text": "the new vocabulary size is I should",
      "start": 3094.88,
      "duration": 4.08
    },
    {
      "text": "mention here actually",
      "start": 3096.88,
      "duration": 4.959
    },
    {
      "text": "1132 uh so this should be 1132 and in",
      "start": 3098.96,
      "duration": 6.68
    },
    {
      "text": "the previous section it was uh 1130 and",
      "start": 3101.839,
      "duration": 6.841
    },
    {
      "text": "let me run this yeah so based on the",
      "start": 3105.64,
      "duration": 4.679
    },
    {
      "text": "output of the print statement about the",
      "start": 3108.68,
      "duration": 4.399
    },
    {
      "text": "new vocabulary size is 1132 and the",
      "start": 3110.319,
      "duration": 4.52
    },
    {
      "text": "vocabulary size in the previous section",
      "start": 3113.079,
      "duration": 4.72
    },
    {
      "text": "was 1130 good",
      "start": 3114.839,
      "duration": 4.76
    },
    {
      "text": "so what we'll do as an additional Quick",
      "start": 3117.799,
      "duration": 3.681
    },
    {
      "text": "Check we'll also print the last five",
      "start": 3119.599,
      "duration": 4.48
    },
    {
      "text": "entries of the updated vocabulary so if",
      "start": 3121.48,
      "duration": 4.48
    },
    {
      "text": "you print out the last five entries in",
      "start": 3124.079,
      "duration": 3.601
    },
    {
      "text": "the updated vocabulary you will see",
      "start": 3125.96,
      "duration": 3.68
    },
    {
      "text": "let's look at the last two entries these",
      "start": 3127.68,
      "duration": 4.32
    },
    {
      "text": "are the end of text and the unknown so",
      "start": 3129.64,
      "duration": 5.56
    },
    {
      "text": "the end of text has a token ID of 1130",
      "start": 3132.0,
      "duration": 5.92
    },
    {
      "text": "and the unknown has a token ID of 1131",
      "start": 3135.2,
      "duration": 5.8
    },
    {
      "text": "awesome so now the vocabulary is updated",
      "start": 3137.92,
      "duration": 4.6
    },
    {
      "text": "and now what we'll be doing is that",
      "start": 3141.0,
      "duration": 4.24
    },
    {
      "text": "we'll be extending the simple tokenizer",
      "start": 3142.52,
      "duration": 5.599
    },
    {
      "text": "class uh this is the version two",
      "start": 3145.24,
      "duration": 4.52
    },
    {
      "text": "basically many things will remain the",
      "start": 3148.119,
      "duration": 3.601
    },
    {
      "text": "same this initialization will remain the",
      "start": 3149.76,
      "duration": 4.2
    },
    {
      "text": "same it will initialize two dictionaries",
      "start": 3151.72,
      "duration": 4.2
    },
    {
      "text": "the string to integer dictionary which",
      "start": 3153.96,
      "duration": 3.96
    },
    {
      "text": "is the vocabulary itself which converts",
      "start": 3155.92,
      "duration": 4.56
    },
    {
      "text": "the tokens into token IDs and the",
      "start": 3157.92,
      "duration": 4.879
    },
    {
      "text": "integer to string dictionary which has",
      "start": 3160.48,
      "duration": 5.0
    },
    {
      "text": "token IDs and then a string mapped to",
      "start": 3162.799,
      "duration": 3.841
    },
    {
      "text": "each token",
      "start": 3165.48,
      "duration": 3.92
    },
    {
      "text": "ID this encode now let's look at this",
      "start": 3166.64,
      "duration": 5.0
    },
    {
      "text": "encode the first two sentences are very",
      "start": 3169.4,
      "duration": 5.0
    },
    {
      "text": "similar to what we saw before we split",
      "start": 3171.64,
      "duration": 6.199
    },
    {
      "text": "on the comma full stop colon semicolon",
      "start": 3174.4,
      "duration": 5.36
    },
    {
      "text": "question mark underscore explanation",
      "start": 3177.839,
      "duration": 4.361
    },
    {
      "text": "quotation and bracket and then we remove",
      "start": 3179.76,
      "duration": 6.12
    },
    {
      "text": "the white spaces using item. strip but",
      "start": 3182.2,
      "duration": 5.96
    },
    {
      "text": "we add one more thing what we add is",
      "start": 3185.88,
      "duration": 5.959
    },
    {
      "text": "that if the item or if the particular",
      "start": 3188.16,
      "duration": 5.76
    },
    {
      "text": "entry is not present in the",
      "start": 3191.839,
      "duration": 4.921
    },
    {
      "text": "vocabulary uh the token which is",
      "start": 3193.92,
      "duration": 5.32
    },
    {
      "text": "assigned to that entry is unknown so",
      "start": 3196.76,
      "duration": 4.28
    },
    {
      "text": "let's say you are scanning the text",
      "start": 3199.24,
      "duration": 3.64
    },
    {
      "text": "right and if you come across a word in",
      "start": 3201.04,
      "duration": 4.16
    },
    {
      "text": "the text which is not in the vocabulary",
      "start": 3202.88,
      "duration": 4.919
    },
    {
      "text": "so if the item is not present in this",
      "start": 3205.2,
      "duration": 4.8
    },
    {
      "text": "string to integer vocabulary which is",
      "start": 3207.799,
      "duration": 5.201
    },
    {
      "text": "the vocabulary which we have passed uh",
      "start": 3210.0,
      "duration": 5.52
    },
    {
      "text": "then it is it should be the unknown",
      "start": 3213.0,
      "duration": 5.04
    },
    {
      "text": "token we replace that with the unknown",
      "start": 3215.52,
      "duration": 4.88
    },
    {
      "text": "token and then we convert all of these",
      "start": 3218.04,
      "duration": 5.12
    },
    {
      "text": "tokens to token IDs so in this step what",
      "start": 3220.4,
      "duration": 4.56
    },
    {
      "text": "will happen is that all the words in the",
      "start": 3223.16,
      "duration": 3.159
    },
    {
      "text": "input text which are not in the",
      "start": 3224.96,
      "duration": 3.0
    },
    {
      "text": "vocabulary will be replaced by the",
      "start": 3226.319,
      "duration": 3.921
    },
    {
      "text": "unknown token and then in this step that",
      "start": 3227.96,
      "duration": 4.2
    },
    {
      "text": "is the main encoder step where the",
      "start": 3230.24,
      "duration": 4.16
    },
    {
      "text": "tokens are converted into token",
      "start": 3232.16,
      "duration": 5.08
    },
    {
      "text": "IDs in the decoder part part everything",
      "start": 3234.4,
      "duration": 5.28
    },
    {
      "text": "will uh almost stay exactly the same",
      "start": 3237.24,
      "duration": 5.8
    },
    {
      "text": "nothing changes we first convert the",
      "start": 3239.68,
      "duration": 5.56
    },
    {
      "text": "token IDs back into tokens and then we",
      "start": 3243.04,
      "duration": 5.079
    },
    {
      "text": "join them together and uh before the",
      "start": 3245.24,
      "duration": 5.2
    },
    {
      "text": "punctuations there are spaces so we get",
      "start": 3248.119,
      "duration": 4.601
    },
    {
      "text": "rid of these spaces and then we return",
      "start": 3250.44,
      "duration": 5.24
    },
    {
      "text": "the decoded text so this is the change",
      "start": 3252.72,
      "duration": 4.839
    },
    {
      "text": "in this tokenizer simple tokenizer",
      "start": 3255.68,
      "duration": 5.399
    },
    {
      "text": "version two the only change is uh adding",
      "start": 3257.559,
      "duration": 5.56
    },
    {
      "text": "this so if some word is not known you",
      "start": 3261.079,
      "duration": 4.76
    },
    {
      "text": "encode it with unknown and then assign",
      "start": 3263.119,
      "duration": 4.401
    },
    {
      "text": "the correspond oning ID to it so if",
      "start": 3265.839,
      "duration": 4.321
    },
    {
      "text": "something is unknown the ID will be",
      "start": 3267.52,
      "duration": 5.24
    },
    {
      "text": "1131 great now let's actually try some",
      "start": 3270.16,
      "duration": 5.08
    },
    {
      "text": "things okay let's say the first text",
      "start": 3272.76,
      "duration": 4.52
    },
    {
      "text": "source which we have is hello do you",
      "start": 3275.24,
      "duration": 4.68
    },
    {
      "text": "like T the second text source which we",
      "start": 3277.28,
      "duration": 4.559
    },
    {
      "text": "have is in The sunl Terraces of the",
      "start": 3279.92,
      "duration": 4.639
    },
    {
      "text": "palace these are two text sources now",
      "start": 3281.839,
      "duration": 5.201
    },
    {
      "text": "what we'll be doing is we will construct",
      "start": 3284.559,
      "duration": 5.361
    },
    {
      "text": "a text which joins these two and we will",
      "start": 3287.04,
      "duration": 5.24
    },
    {
      "text": "add an end of text at the end of the",
      "start": 3289.92,
      "duration": 5.159
    },
    {
      "text": "first text Source why are we doing this",
      "start": 3292.28,
      "duration": 5.36
    },
    {
      "text": "because we saw that this is usually how",
      "start": 3295.079,
      "duration": 5.441
    },
    {
      "text": "it's done in practice in GPT Etc if you",
      "start": 3297.64,
      "duration": 4.52
    },
    {
      "text": "provide one text source and if you have",
      "start": 3300.52,
      "duration": 3.4
    },
    {
      "text": "another text Source you don't just join",
      "start": 3302.16,
      "duration": 3.88
    },
    {
      "text": "them together you split them with this",
      "start": 3303.92,
      "duration": 5.08
    },
    {
      "text": "end of text token so this is what we are",
      "start": 3306.04,
      "duration": 4.84
    },
    {
      "text": "exactly doing we are now constructing a",
      "start": 3309.0,
      "duration": 4.16
    },
    {
      "text": "text which will feed as an input to the",
      "start": 3310.88,
      "duration": 5.6
    },
    {
      "text": "encoder right but we add this end of",
      "start": 3313.16,
      "duration": 6.399
    },
    {
      "text": "text so the text Will which will",
      "start": 3316.48,
      "duration": 5.359
    },
    {
      "text": "essentially feed to the encoder is hello",
      "start": 3319.559,
      "duration": 5.76
    },
    {
      "text": "do you like T end of text in The sunlit",
      "start": 3321.839,
      "duration": 5.681
    },
    {
      "text": "Terraces of the palet s this is the text",
      "start": 3325.319,
      "duration": 5.0
    },
    {
      "text": "which we are feeding to the encoder and",
      "start": 3327.52,
      "duration": 5.0
    },
    {
      "text": "now we are asking the encoder to encode",
      "start": 3330.319,
      "duration": 4.76
    },
    {
      "text": "these into token IDs so let's look at",
      "start": 3332.52,
      "duration": 5.0
    },
    {
      "text": "this sentence in detail this word hello",
      "start": 3335.079,
      "duration": 5.76
    },
    {
      "text": "is not present in our vocabulary so if",
      "start": 3337.52,
      "duration": 6.44
    },
    {
      "text": "if we pass it into this tokenizer it",
      "start": 3340.839,
      "duration": 5.2
    },
    {
      "text": "will hit this this statement where the",
      "start": 3343.96,
      "duration": 3.96
    },
    {
      "text": "word is not present in the vocabulary so",
      "start": 3346.039,
      "duration": 3.441
    },
    {
      "text": "it will replace it with the unknown",
      "start": 3347.92,
      "duration": 5.119
    },
    {
      "text": "token and this end of text again end of",
      "start": 3349.48,
      "duration": 5.079
    },
    {
      "text": "text is actually present in the",
      "start": 3353.039,
      "duration": 2.921
    },
    {
      "text": "vocabulary because we have added it",
      "start": 3354.559,
      "duration": 3.601
    },
    {
      "text": "right now so the token ID corresponding",
      "start": 3355.96,
      "duration": 4.96
    },
    {
      "text": "to end of text will be 1130 and the",
      "start": 3358.16,
      "duration": 4.72
    },
    {
      "text": "token ID corresponding to hello will be",
      "start": 3360.92,
      "duration": 4.24
    },
    {
      "text": "1131 right so let's check if this is",
      "start": 3362.88,
      "duration": 4.84
    },
    {
      "text": "actually true so I'll now run tokenizer",
      "start": 3365.16,
      "duration": 5.959
    },
    {
      "text": "encod text and see the token ID for",
      "start": 3367.72,
      "duration": 6.92
    },
    {
      "text": "hello is 11131 this is exactly what we",
      "start": 3371.119,
      "duration": 6.521
    },
    {
      "text": "had expected right because uh hello is",
      "start": 3374.64,
      "duration": 5.28
    },
    {
      "text": "not present in the main vocabulary so",
      "start": 3377.64,
      "duration": 3.88
    },
    {
      "text": "it's actually an unknown word so it",
      "start": 3379.92,
      "duration": 5.04
    },
    {
      "text": "token ID should be 1131 awesome and then",
      "start": 3381.52,
      "duration": 6.12
    },
    {
      "text": "for end of text let's see the token ID",
      "start": 3384.96,
      "duration": 5.56
    },
    {
      "text": "yeah this this is the token ID for end",
      "start": 3387.64,
      "duration": 4.6
    },
    {
      "text": "of text the token ID is",
      "start": 3390.52,
      "duration": 4.24
    },
    {
      "text": "1130 so now there is no error which is",
      "start": 3392.24,
      "duration": 5.119
    },
    {
      "text": "coming here earlier when we passed this",
      "start": 3394.76,
      "duration": 4.64
    },
    {
      "text": "hello do you like T there was an error",
      "start": 3397.359,
      "duration": 3.561
    },
    {
      "text": "which we are getting that hello is not",
      "start": 3399.4,
      "duration": 4.04
    },
    {
      "text": "present in the vocabulary but now this",
      "start": 3400.92,
      "duration": 4.04
    },
    {
      "text": "error is not coming because we have",
      "start": 3403.44,
      "duration": 3.04
    },
    {
      "text": "taken care of it we have added the",
      "start": 3404.96,
      "duration": 3.399
    },
    {
      "text": "unknown token in the",
      "start": 3406.48,
      "duration": 4.4
    },
    {
      "text": "vocabulary and so now the tokenizer",
      "start": 3408.359,
      "duration": 6.401
    },
    {
      "text": "encodes the text uh in a correct manner",
      "start": 3410.88,
      "duration": 6.04
    },
    {
      "text": "that's awesome so now what we'll be",
      "start": 3414.76,
      "duration": 4.24
    },
    {
      "text": "doing is that we'll be actually using",
      "start": 3416.92,
      "duration": 5.6
    },
    {
      "text": "the decode function now and we'll pass",
      "start": 3419.0,
      "duration": 7.0
    },
    {
      "text": "the encoded text which are the token IDs",
      "start": 3422.52,
      "duration": 6.88
    },
    {
      "text": "these IDs into the tokenizer do decode",
      "start": 3426.0,
      "duration": 5.4
    },
    {
      "text": "so let's see what the tokenizer decode",
      "start": 3429.4,
      "duration": 3.719
    },
    {
      "text": "so when these are passed into the",
      "start": 3431.4,
      "duration": 4.24
    },
    {
      "text": "decoder the decoder is unknown do you",
      "start": 3433.119,
      "duration": 5.48
    },
    {
      "text": "like T end of text in The sunlit",
      "start": 3435.64,
      "duration": 5.159
    },
    {
      "text": "Terraces of the unknown so actually",
      "start": 3438.599,
      "duration": 4.281
    },
    {
      "text": "there are two unknown text here hello is",
      "start": 3440.799,
      "duration": 4.961
    },
    {
      "text": "an unknown and Palace is also an unknown",
      "start": 3442.88,
      "duration": 5.199
    },
    {
      "text": "so the token IDs for both of these are",
      "start": 3445.76,
      "duration": 4.92
    },
    {
      "text": "1131 so when you decode these IDs you",
      "start": 3448.079,
      "duration": 5.76
    },
    {
      "text": "will get unknown do you like T so which",
      "start": 3450.68,
      "duration": 6.24
    },
    {
      "text": "because hello is not known then end of",
      "start": 3453.839,
      "duration": 5.401
    },
    {
      "text": "text and then in The sunl Terraces of",
      "start": 3456.92,
      "duration": 4.76
    },
    {
      "text": "the unknown so here again unknown means",
      "start": 3459.24,
      "duration": 4.16
    },
    {
      "text": "the palace which was not in the",
      "start": 3461.68,
      "duration": 4.08
    },
    {
      "text": "vocabulary so the encoder and decoder",
      "start": 3463.4,
      "duration": 4.84
    },
    {
      "text": "are working perfectly and we are able to",
      "start": 3465.76,
      "duration": 5.2
    },
    {
      "text": "handle the unknown words now uh we are",
      "start": 3468.24,
      "duration": 4.599
    },
    {
      "text": "able to handle the unknown words",
      "start": 3470.96,
      "duration": 3.599
    },
    {
      "text": "actually quite effectively because we",
      "start": 3472.839,
      "duration": 3.96
    },
    {
      "text": "replace them with the unknown token",
      "start": 3474.559,
      "duration": 4.201
    },
    {
      "text": "and the end of text is also being",
      "start": 3476.799,
      "duration": 4.401
    },
    {
      "text": "captured pretty effectively so when we",
      "start": 3478.76,
      "duration": 4.359
    },
    {
      "text": "actually make the text itself we have to",
      "start": 3481.2,
      "duration": 3.919
    },
    {
      "text": "add end of text before we pass it to the",
      "start": 3483.119,
      "duration": 5.321
    },
    {
      "text": "encoder and subsequently to the decoder",
      "start": 3485.119,
      "duration": 5.521
    },
    {
      "text": "So based on comparing the det tokenized",
      "start": 3488.44,
      "duration": 4.639
    },
    {
      "text": "text with the original input text we",
      "start": 3490.64,
      "duration": 5.08
    },
    {
      "text": "know that the training set which is the",
      "start": 3493.079,
      "duration": 5.161
    },
    {
      "text": "verdict book did not actually contain",
      "start": 3495.72,
      "duration": 5.2
    },
    {
      "text": "the words hello and the palace because",
      "start": 3498.24,
      "duration": 4.04
    },
    {
      "text": "both of them are replaced by this",
      "start": 3500.92,
      "duration": 3.52
    },
    {
      "text": "unknown token here so we will not get",
      "start": 3502.28,
      "duration": 4.839
    },
    {
      "text": "errors this way once we take into",
      "start": 3504.44,
      "duration": 6.72
    },
    {
      "text": "account the special uh context tokens",
      "start": 3507.119,
      "duration": 7.92
    },
    {
      "text": "awesome so uh let me just add a last",
      "start": 3511.16,
      "duration": 7.679
    },
    {
      "text": "note about the special context token so",
      "start": 3515.039,
      "duration": 5.161
    },
    {
      "text": "up till now we have discussed",
      "start": 3518.839,
      "duration": 3.441
    },
    {
      "text": "tokenization as an essential step in",
      "start": 3520.2,
      "duration": 5.159
    },
    {
      "text": "processing text as input to the llms",
      "start": 3522.28,
      "duration": 5.16
    },
    {
      "text": "however along with these Special tokens",
      "start": 3525.359,
      "duration": 3.76
    },
    {
      "text": "there are some other tokens which also",
      "start": 3527.44,
      "duration": 4.0
    },
    {
      "text": "researchers consider so this is the",
      "start": 3529.119,
      "duration": 5.96
    },
    {
      "text": "beginning of a sequence so BOS token so",
      "start": 3531.44,
      "duration": 5.44
    },
    {
      "text": "we saw the end of text right some",
      "start": 3535.079,
      "duration": 3.96
    },
    {
      "text": "researchers also consider",
      "start": 3536.88,
      "duration": 5.32
    },
    {
      "text": "BOS so this token marks the start of a",
      "start": 3539.039,
      "duration": 6.08
    },
    {
      "text": "text it signifies to the llm where a",
      "start": 3542.2,
      "duration": 6.159
    },
    {
      "text": "piece of context begins or where a piece",
      "start": 3545.119,
      "duration": 6.321
    },
    {
      "text": "of content begins right then the second",
      "start": 3548.359,
      "duration": 5.921
    },
    {
      "text": "token is eos which is end of sequence",
      "start": 3551.44,
      "duration": 5.24
    },
    {
      "text": "right so this token is positioned at the",
      "start": 3554.28,
      "duration": 4.88
    },
    {
      "text": "end of a text and especially useful",
      "start": 3556.68,
      "duration": 4.679
    },
    {
      "text": "where concatenating multiple unrelated",
      "start": 3559.16,
      "duration": 5.24
    },
    {
      "text": "text it's similar to end of",
      "start": 3561.359,
      "duration": 5.561
    },
    {
      "text": "text uh then the third token which is",
      "start": 3564.4,
      "duration": 4.88
    },
    {
      "text": "important is the padding token so when",
      "start": 3566.92,
      "duration": 4.76
    },
    {
      "text": "training llms with batch sizes larger",
      "start": 3569.28,
      "duration": 5.0
    },
    {
      "text": "than one the batch might contain texts",
      "start": 3571.68,
      "duration": 5.359
    },
    {
      "text": "of varying lengths so to ensure that all",
      "start": 3574.28,
      "duration": 4.6
    },
    {
      "text": "the texts in different batches have the",
      "start": 3577.039,
      "duration": 4.681
    },
    {
      "text": "same length the shorter texts are",
      "start": 3578.88,
      "duration": 5.84
    },
    {
      "text": "extended or padded using the pad token",
      "start": 3581.72,
      "duration": 4.879
    },
    {
      "text": "up to the length of the longest text in",
      "start": 3584.72,
      "duration": 4.16
    },
    {
      "text": "the batch so imagine the different text",
      "start": 3586.599,
      "duration": 4.561
    },
    {
      "text": "is being put in batches to the llm for",
      "start": 3588.88,
      "duration": 4.199
    },
    {
      "text": "parallel processing we'll look at this",
      "start": 3591.16,
      "duration": 4.04
    },
    {
      "text": "in detail so no need to worry about this",
      "start": 3593.079,
      "duration": 4.681
    },
    {
      "text": "right now but but just remember that for",
      "start": 3595.2,
      "duration": 4.879
    },
    {
      "text": "efficient Computing llm parallely",
      "start": 3597.76,
      "duration": 4.88
    },
    {
      "text": "processes the batches so each batch",
      "start": 3600.079,
      "duration": 5.561
    },
    {
      "text": "contains text which have different sizes",
      "start": 3602.64,
      "duration": 5.56
    },
    {
      "text": "the shortest Texs are augmented with",
      "start": 3605.64,
      "duration": 5.919
    },
    {
      "text": "this pad token to match uh their length",
      "start": 3608.2,
      "duration": 6.08
    },
    {
      "text": "with the largest text and now we know",
      "start": 3611.559,
      "duration": 4.52
    },
    {
      "text": "how to add these special tokens right we",
      "start": 3614.28,
      "duration": 3.519
    },
    {
      "text": "just augment the vocabulary with these",
      "start": 3616.079,
      "duration": 4.24
    },
    {
      "text": "tokens and whenever we pass text to the",
      "start": 3617.799,
      "duration": 4.681
    },
    {
      "text": "encoder we we may add things like",
      "start": 3620.319,
      "duration": 4.24
    },
    {
      "text": "beginning of sequence end of text the",
      "start": 3622.48,
      "duration": 4.839
    },
    {
      "text": "pad token Etc",
      "start": 3624.559,
      "duration": 6.201
    },
    {
      "text": "so we only saw uh the unknown and the",
      "start": 3627.319,
      "duration": 5.72
    },
    {
      "text": "end of text special context tokens but",
      "start": 3630.76,
      "duration": 4.319
    },
    {
      "text": "there are three other BOS beginning of",
      "start": 3633.039,
      "duration": 5.641
    },
    {
      "text": "sequence and EOS end of sequence and Pad",
      "start": 3635.079,
      "duration": 7.0
    },
    {
      "text": "which is padding now here I want you to",
      "start": 3638.68,
      "duration": 5.919
    },
    {
      "text": "note that the tokenizer which is used",
      "start": 3642.079,
      "duration": 5.401
    },
    {
      "text": "for GPT models does not need any of",
      "start": 3644.599,
      "duration": 5.44
    },
    {
      "text": "these tokens mentioned above but it only",
      "start": 3647.48,
      "duration": 4.4
    },
    {
      "text": "uses the end of text token for",
      "start": 3650.039,
      "duration": 4.881
    },
    {
      "text": "Simplicity so gpt3 gp4 when they are",
      "start": 3651.88,
      "duration": 6.12
    },
    {
      "text": "trained they don't use BOS uh padding",
      "start": 3654.92,
      "duration": 5.159
    },
    {
      "text": "they don't even use the unknown token",
      "start": 3658.0,
      "duration": 5.2
    },
    {
      "text": "they only use the end of text token for",
      "start": 3660.079,
      "duration": 5.48
    },
    {
      "text": "Simplicity and this I also shown over",
      "start": 3663.2,
      "duration": 5.44
    },
    {
      "text": "here so the end of text which is used is",
      "start": 3665.559,
      "duration": 5.321
    },
    {
      "text": "exactly how it is used in GPT training",
      "start": 3668.64,
      "duration": 3.439
    },
    {
      "text": "as",
      "start": 3670.88,
      "duration": 3.8
    },
    {
      "text": "well okay and the last sentence is that",
      "start": 3672.079,
      "duration": 5.681
    },
    {
      "text": "the tokenizer used for GPT models also",
      "start": 3674.68,
      "duration": 5.28
    },
    {
      "text": "does not use an unknown token for out of",
      "start": 3677.76,
      "duration": 4.4
    },
    {
      "text": "vocabulary words then you might be",
      "start": 3679.96,
      "duration": 4.2
    },
    {
      "text": "thinking how does GPT deal with unknown",
      "start": 3682.16,
      "duration": 4.679
    },
    {
      "text": "tokens right so then there is something",
      "start": 3684.16,
      "duration": 4.52
    },
    {
      "text": "called as the bite pair encoding",
      "start": 3686.839,
      "duration": 4.561
    },
    {
      "text": "tokenizer which GPT models actually use",
      "start": 3688.68,
      "duration": 4.56
    },
    {
      "text": "which breaks down words into subword",
      "start": 3691.4,
      "duration": 4.719
    },
    {
      "text": "units So currently what we have done is",
      "start": 3693.24,
      "duration": 4.92
    },
    {
      "text": "each word is essentially one token right",
      "start": 3696.119,
      "duration": 4.041
    },
    {
      "text": "and each punctuation mark each colon",
      "start": 3698.16,
      "duration": 5.56
    },
    {
      "text": "semicolon is one token but what actually",
      "start": 3700.16,
      "duration": 6.679
    },
    {
      "text": "GPT does for tokenization it it uses",
      "start": 3703.72,
      "duration": 5.319
    },
    {
      "text": "something called bite pair encoding or",
      "start": 3706.839,
      "duration": 5.52
    },
    {
      "text": "BP tokenizer and that automatically",
      "start": 3709.039,
      "duration": 5.24
    },
    {
      "text": "deals with unknown tokens because in",
      "start": 3712.359,
      "duration": 3.881
    },
    {
      "text": "that tokenizer one word is is not",
      "start": 3714.279,
      "duration": 4.401
    },
    {
      "text": "essentially one token but even the words",
      "start": 3716.24,
      "duration": 4.319
    },
    {
      "text": "are broken down into subwords to",
      "start": 3718.68,
      "duration": 4.679
    },
    {
      "text": "generate individual tokens uh we'll come",
      "start": 3720.559,
      "duration": 5.401
    },
    {
      "text": "to this in the next lecture but I wanted",
      "start": 3723.359,
      "duration": 4.76
    },
    {
      "text": "to show this lecture specifically for",
      "start": 3725.96,
      "duration": 4.96
    },
    {
      "text": "showing you how you can do your own",
      "start": 3728.119,
      "duration": 5.2
    },
    {
      "text": "tokenization from scratch I could have",
      "start": 3730.92,
      "duration": 4.159
    },
    {
      "text": "directly showed you bite pair encoding",
      "start": 3733.319,
      "duration": 4.321
    },
    {
      "text": "with GPT users but then that you would",
      "start": 3735.079,
      "duration": 5.801
    },
    {
      "text": "not have appreciated uh if you were to",
      "start": 3737.64,
      "duration": 5.159
    },
    {
      "text": "tokenize yourself how would you do it",
      "start": 3740.88,
      "duration": 4.159
    },
    {
      "text": "from scratch of course we'll look at the",
      "start": 3742.799,
      "duration": 4.921
    },
    {
      "text": "bite pair and coder tokenizer in a lot",
      "start": 3745.039,
      "duration": 4.52
    },
    {
      "text": "of detail in the next lecture to",
      "start": 3747.72,
      "duration": 4.399
    },
    {
      "text": "understand how GPT actually tokenizes",
      "start": 3749.559,
      "duration": 4.401
    },
    {
      "text": "but I wanted to give you this intuitive",
      "start": 3752.119,
      "duration": 3.401
    },
    {
      "text": "feel so that you don't think",
      "start": 3753.96,
      "duration": 4.52
    },
    {
      "text": "tokenization is a hard process I'll be",
      "start": 3755.52,
      "duration": 4.72
    },
    {
      "text": "sharing this code file with all of you",
      "start": 3758.48,
      "duration": 4.0
    },
    {
      "text": "so that you can run this and I actually",
      "start": 3760.24,
      "duration": 4.079
    },
    {
      "text": "highly encourage you to run this for a",
      "start": 3762.48,
      "duration": 4.799
    },
    {
      "text": "different uh book or a different txt",
      "start": 3764.319,
      "duration": 4.881
    },
    {
      "text": "file so that you get the hang of it and",
      "start": 3767.279,
      "duration": 3.641
    },
    {
      "text": "you become better and better and better",
      "start": 3769.2,
      "duration": 3.04
    },
    {
      "text": "at",
      "start": 3770.92,
      "duration": 3.96
    },
    {
      "text": "this um just before concluding the",
      "start": 3772.24,
      "duration": 4.16
    },
    {
      "text": "lecture I want to go through a quick",
      "start": 3774.88,
      "duration": 3.199
    },
    {
      "text": "revision of what all we have learned in",
      "start": 3776.4,
      "duration": 4.159
    },
    {
      "text": "today's lecture so in today's lecture we",
      "start": 3778.079,
      "duration": 4.76
    },
    {
      "text": "essentially looked at this data",
      "start": 3780.559,
      "duration": 4.441
    },
    {
      "text": "preparation and sampling stage in",
      "start": 3782.839,
      "duration": 3.921
    },
    {
      "text": "building large language models from",
      "start": 3785.0,
      "duration": 5.72
    },
    {
      "text": "scratch in particular we took a look at",
      "start": 3786.76,
      "duration": 7.079
    },
    {
      "text": "tokenization and uh the question is that",
      "start": 3790.72,
      "duration": 5.359
    },
    {
      "text": "how do you prepare input text for",
      "start": 3793.839,
      "duration": 4.681
    },
    {
      "text": "training llms and we saw that this is",
      "start": 3796.079,
      "duration": 5.24
    },
    {
      "text": "divided into two steps in step one we",
      "start": 3798.52,
      "duration": 5.16
    },
    {
      "text": "split the text into individual words so",
      "start": 3801.319,
      "duration": 4.8
    },
    {
      "text": "you have a essay or a book or a huge",
      "start": 3803.68,
      "duration": 4.599
    },
    {
      "text": "number of books you split that text into",
      "start": 3806.119,
      "duration": 4.44
    },
    {
      "text": "individual words and then you convert",
      "start": 3808.279,
      "duration": 4.681
    },
    {
      "text": "these tokens into token",
      "start": 3810.559,
      "duration": 5.201
    },
    {
      "text": "IDs now when GPT actually uses",
      "start": 3812.96,
      "duration": 4.92
    },
    {
      "text": "tokenizers it doesn't use individual",
      "start": 3815.76,
      "duration": 4.76
    },
    {
      "text": "words as tokens it even does subwords",
      "start": 3817.88,
      "duration": 4.8
    },
    {
      "text": "and we'll look at that in the next",
      "start": 3820.52,
      "duration": 5.36
    },
    {
      "text": "lecture so if you look at how llms are",
      "start": 3822.68,
      "duration": 5.32
    },
    {
      "text": "actually trained you take the input text",
      "start": 3825.88,
      "duration": 4.6
    },
    {
      "text": "you tokenize it into words you convert",
      "start": 3828.0,
      "duration": 4.76
    },
    {
      "text": "these words into token IDs and then the",
      "start": 3830.48,
      "duration": 4.0
    },
    {
      "text": "later step is we have to convert these",
      "start": 3832.76,
      "duration": 3.519
    },
    {
      "text": "token IDs also into to Vector",
      "start": 3834.48,
      "duration": 3.599
    },
    {
      "text": "representations which we will come to",
      "start": 3836.279,
      "duration": 4.0
    },
    {
      "text": "later but in today's lecture we mostly",
      "start": 3838.079,
      "duration": 4.321
    },
    {
      "text": "looked at step one which is tokenizing",
      "start": 3840.279,
      "duration": 4.28
    },
    {
      "text": "and step two which is converting tokens",
      "start": 3842.4,
      "duration": 5.48
    },
    {
      "text": "into token IDs right so initially what",
      "start": 3844.559,
      "duration": 5.321
    },
    {
      "text": "we did is we downloaded and loaded the",
      "start": 3847.88,
      "duration": 5.6
    },
    {
      "text": "data set in Python we tokenized the",
      "start": 3849.88,
      "duration": 5.8
    },
    {
      "text": "entire data set using Python's regular",
      "start": 3853.48,
      "duration": 5.559
    },
    {
      "text": "expression library and we used re. spit",
      "start": 3855.68,
      "duration": 6.04
    },
    {
      "text": "we first used it used it at White spaces",
      "start": 3859.039,
      "duration": 4.76
    },
    {
      "text": "and then we added colons semicolons",
      "start": 3861.72,
      "duration": 5.16
    },
    {
      "text": "question marks Dash Dash character",
      "start": 3863.799,
      "duration": 6.04
    },
    {
      "text": "Etc because we wanted to split all of",
      "start": 3866.88,
      "duration": 5.56
    },
    {
      "text": "those punctuations and even if you",
      "start": 3869.839,
      "duration": 4.841
    },
    {
      "text": "search the dash dash in this text you'll",
      "start": 3872.44,
      "duration": 4.599
    },
    {
      "text": "see dash dash appear so many times so",
      "start": 3874.68,
      "duration": 4.439
    },
    {
      "text": "even we wanted to split that into",
      "start": 3877.039,
      "duration": 5.681
    },
    {
      "text": "individual tokens awesome so this is",
      "start": 3879.119,
      "duration": 5.48
    },
    {
      "text": "what we did and then what we did is we",
      "start": 3882.72,
      "duration": 4.319
    },
    {
      "text": "maintained the vocabulary we converted",
      "start": 3884.599,
      "duration": 5.601
    },
    {
      "text": "these tokens into token IDs so we saw",
      "start": 3887.039,
      "duration": 5.721
    },
    {
      "text": "that what a vocabulary is that is it's",
      "start": 3890.2,
      "duration": 3.639
    },
    {
      "text": "simply a",
      "start": 3892.76,
      "duration": 3.72
    },
    {
      "text": "dictionary um and it's a mapping from",
      "start": 3893.839,
      "duration": 5.361
    },
    {
      "text": "the tokens into the token IDs so the",
      "start": 3896.48,
      "duration": 5.16
    },
    {
      "text": "tokens are arranged alphabetically and",
      "start": 3899.2,
      "duration": 5.399
    },
    {
      "text": "then each token is mapped to a token ID",
      "start": 3901.64,
      "duration": 5.28
    },
    {
      "text": "as you can see over",
      "start": 3904.599,
      "duration": 5.44
    },
    {
      "text": "here so each unique token is mapped to a",
      "start": 3906.92,
      "duration": 5.8
    },
    {
      "text": "unique integer called token ID and why",
      "start": 3910.039,
      "duration": 5.08
    },
    {
      "text": "are these token IDs necessary because",
      "start": 3912.72,
      "duration": 4.119
    },
    {
      "text": "you will see that these token IDs are",
      "start": 3915.119,
      "duration": 3.521
    },
    {
      "text": "then converted into token embeddings",
      "start": 3916.839,
      "duration": 4.121
    },
    {
      "text": "which are fed as input to the",
      "start": 3918.64,
      "duration": 5.919
    },
    {
      "text": "GPT um great now then after that we",
      "start": 3920.96,
      "duration": 6.319
    },
    {
      "text": "implemented a tokenized class in Python",
      "start": 3924.559,
      "duration": 4.961
    },
    {
      "text": "so that whenever we create a vocabulary",
      "start": 3927.279,
      "duration": 4.161
    },
    {
      "text": "we can create an instance of this",
      "start": 3929.52,
      "duration": 5.2
    },
    {
      "text": "tokenizer class uh but we did not just",
      "start": 3931.44,
      "duration": 5.2
    },
    {
      "text": "Define the encode method in this class",
      "start": 3934.72,
      "duration": 3.68
    },
    {
      "text": "we defined one more method which is the",
      "start": 3936.64,
      "duration": 4.6
    },
    {
      "text": "decode method so what the encode method",
      "start": 3938.4,
      "duration": 4.76
    },
    {
      "text": "did is that whenever sample text is",
      "start": 3941.24,
      "duration": 4.68
    },
    {
      "text": "provided it converted it into tokens and",
      "start": 3943.16,
      "duration": 4.72
    },
    {
      "text": "then converted these tokens into token",
      "start": 3945.92,
      "duration": 4.359
    },
    {
      "text": "IDs great but the decode method did",
      "start": 3947.88,
      "duration": 5.32
    },
    {
      "text": "exactly opposite it took the token IDs",
      "start": 3950.279,
      "duration": 6.441
    },
    {
      "text": "as an input it converted the token ID",
      "start": 3953.2,
      "duration": 7.119
    },
    {
      "text": "into tokenized text and then it recover",
      "start": 3956.72,
      "duration": 6.16
    },
    {
      "text": "the sample text from the tokenized text",
      "start": 3960.319,
      "duration": 4.72
    },
    {
      "text": "why is the decoder important because the",
      "start": 3962.88,
      "duration": 5.199
    },
    {
      "text": "GPT output is also in token IDs so we",
      "start": 3965.039,
      "duration": 4.8
    },
    {
      "text": "need to convert it back to the sample",
      "start": 3968.079,
      "duration": 5.361
    },
    {
      "text": "text to to make sense of what the output",
      "start": 3969.839,
      "duration": 6.28
    },
    {
      "text": "is so the tokenizer class which we",
      "start": 3973.44,
      "duration": 4.399
    },
    {
      "text": "defined had two methods the encode",
      "start": 3976.119,
      "duration": 5.16
    },
    {
      "text": "method and the decode method finally we",
      "start": 3977.839,
      "duration": 6.081
    },
    {
      "text": "saw a problem that if some word is",
      "start": 3981.279,
      "duration": 4.601
    },
    {
      "text": "passed to the encoder which is not",
      "start": 3983.92,
      "duration": 3.56
    },
    {
      "text": "present in the vocabulary then the",
      "start": 3985.88,
      "duration": 4.64
    },
    {
      "text": "encoder throws up an error to avoid this",
      "start": 3987.48,
      "duration": 5.119
    },
    {
      "text": "we need to add special context or",
      "start": 3990.52,
      "duration": 5.079
    },
    {
      "text": "special text tokens to the vocabulary",
      "start": 3992.599,
      "duration": 5.561
    },
    {
      "text": "and uh one such special text token is",
      "start": 3995.599,
      "duration": 4.68
    },
    {
      "text": "unknown so if some word is encountered",
      "start": 3998.16,
      "duration": 4.24
    },
    {
      "text": "which is not known we add the unknown",
      "start": 4000.279,
      "duration": 4.961
    },
    {
      "text": "token uh the second special context text",
      "start": 4002.4,
      "duration": 6.6
    },
    {
      "text": "token which is also used in GPT is end",
      "start": 4005.24,
      "duration": 5.76
    },
    {
      "text": "of text so whenever there are multiple",
      "start": 4009.0,
      "duration": 4.079
    },
    {
      "text": "text sources when we feed the text to",
      "start": 4011.0,
      "duration": 4.119
    },
    {
      "text": "the encoder we can separate them with",
      "start": 4013.079,
      "duration": 5.0
    },
    {
      "text": "the end text tokens we later saw that",
      "start": 4015.119,
      "duration": 4.96
    },
    {
      "text": "many researchers used other to other",
      "start": 4018.079,
      "duration": 3.801
    },
    {
      "text": "special tokens also like beginning of",
      "start": 4020.079,
      "duration": 4.48
    },
    {
      "text": "sequence end of sequence padding",
      "start": 4021.88,
      "duration": 5.479
    },
    {
      "text": "Etc actually when GPT was trained it",
      "start": 4024.559,
      "duration": 5.841
    },
    {
      "text": "only used the end of text uh token to",
      "start": 4027.359,
      "duration": 6.2
    },
    {
      "text": "distinguish between the different text",
      "start": 4030.4,
      "duration": 7.12
    },
    {
      "text": "sources awesome so then we then we saw",
      "start": 4033.559,
      "duration": 6.24
    },
    {
      "text": "that with these Special tokens even if",
      "start": 4037.52,
      "duration": 4.16
    },
    {
      "text": "you are given a new sentence whose words",
      "start": 4039.799,
      "duration": 4.881
    },
    {
      "text": "are not known uh such as the quickly",
      "start": 4041.68,
      "duration": 4.8
    },
    {
      "text": "year which was not known it is just",
      "start": 4044.68,
      "duration": 4.679
    },
    {
      "text": "replaced by the token ID for the unknown",
      "start": 4046.48,
      "duration": 5.319
    },
    {
      "text": "token no error is shown",
      "start": 4049.359,
      "duration": 6.96
    },
    {
      "text": "up uh now we saw that uh GPT did not",
      "start": 4051.799,
      "duration": 6.841
    },
    {
      "text": "actually use the unknown token so let me",
      "start": 4056.319,
      "duration": 4.441
    },
    {
      "text": "take you to the end of this",
      "start": 4058.64,
      "duration": 5.639
    },
    {
      "text": "book end of this notebook so GPT models",
      "start": 4060.76,
      "duration": 5.839
    },
    {
      "text": "did not use the unknown token then you",
      "start": 4064.279,
      "duration": 4.121
    },
    {
      "text": "might be thinking then how did GPT",
      "start": 4066.599,
      "duration": 3.48
    },
    {
      "text": "models deal with Words which are not",
      "start": 4068.4,
      "duration": 3.959
    },
    {
      "text": "known they deal with Words which are not",
      "start": 4070.079,
      "duration": 4.04
    },
    {
      "text": "known by using something called bite",
      "start": 4072.359,
      "duration": 5.161
    },
    {
      "text": "pair encoding in bite pair en en in bite",
      "start": 4074.119,
      "duration": 6.0
    },
    {
      "text": "pair encoding every word is not a token",
      "start": 4077.52,
      "duration": 4.68
    },
    {
      "text": "words themselves are broken down into",
      "start": 4080.119,
      "duration": 4.48
    },
    {
      "text": "subwords and then these subwords are the",
      "start": 4082.2,
      "duration": 4.28
    },
    {
      "text": "token so let's say you have a word which",
      "start": 4084.599,
      "duration": 3.161
    },
    {
      "text": "is",
      "start": 4086.48,
      "duration": 3.72
    },
    {
      "text": "uh which is chased",
      "start": 4087.76,
      "duration": 4.88
    },
    {
      "text": "right in the vocabulary which we have",
      "start": 4090.2,
      "duration": 4.159
    },
    {
      "text": "developed this chased itself is one",
      "start": 4092.64,
      "duration": 4.32
    },
    {
      "text": "token but in bite pair encoding it might",
      "start": 4094.359,
      "duration": 4.641
    },
    {
      "text": "be possible that this chased itself is",
      "start": 4096.96,
      "duration": 5.399
    },
    {
      "text": "broken down into three sub tokens",
      "start": 4099.0,
      "duration": 7.359
    },
    {
      "text": "CH as this is just for example but the",
      "start": 4102.359,
      "duration": 6.521
    },
    {
      "text": "this is what I mean by subw",
      "start": 4106.359,
      "duration": 5.041
    },
    {
      "text": "tokens uh so subword tokens means",
      "start": 4108.88,
      "duration": 4.2
    },
    {
      "text": "breaking down one word itself into",
      "start": 4111.4,
      "duration": 3.879
    },
    {
      "text": "subwords and then using the subwords as",
      "start": 4113.08,
      "duration": 4.36
    },
    {
      "text": "the",
      "start": 4115.279,
      "duration": 2.161
    },
    {
      "text": "tokens uh so GPT models use a bite pair",
      "start": 4117.56,
      "duration": 5.36
    },
    {
      "text": "encoding tokenizer which breaks down",
      "start": 4120.679,
      "duration": 4.12
    },
    {
      "text": "words into subword units and we are",
      "start": 4122.92,
      "duration": 3.759
    },
    {
      "text": "going to cover that in a lot of detail",
      "start": 4124.799,
      "duration": 3.161
    },
    {
      "text": "in the next",
      "start": 4126.679,
      "duration": 4.6
    },
    {
      "text": "lecture so uh thank you so much everyone",
      "start": 4127.96,
      "duration": 5.759
    },
    {
      "text": "for uh sticking with me for this lecture",
      "start": 4131.279,
      "duration": 3.841
    },
    {
      "text": "and it's been a long lecture I think",
      "start": 4133.719,
      "duration": 4.681
    },
    {
      "text": "it's more than 1 hour but uh I think it",
      "start": 4135.12,
      "duration": 5.8
    },
    {
      "text": "is worth it I've not seen this much of a",
      "start": 4138.4,
      "duration": 4.6
    },
    {
      "text": "detailed treatment on tokenization in",
      "start": 4140.92,
      "duration": 4.359
    },
    {
      "text": "any of the lectures many of the lectures",
      "start": 4143.0,
      "duration": 4.08
    },
    {
      "text": "are toy lectures which means they just",
      "start": 4145.279,
      "duration": 3.56
    },
    {
      "text": "give you the basics but then don't show",
      "start": 4147.08,
      "duration": 3.599
    },
    {
      "text": "you things like end of text dealing with",
      "start": 4148.839,
      "duration": 4.96
    },
    {
      "text": "unknown words padding then pite pair",
      "start": 4150.679,
      "duration": 4.881
    },
    {
      "text": "encoding or some things which are",
      "start": 4153.799,
      "duration": 4.161
    },
    {
      "text": "actually used in llms and that is what I",
      "start": 4155.56,
      "duration": 5.88
    },
    {
      "text": "wanted to cover so my lecture style will",
      "start": 4157.96,
      "duration": 5.239
    },
    {
      "text": "be a mix of whiteboard writing on",
      "start": 4161.44,
      "duration": 4.719
    },
    {
      "text": "whiteboard and uh also showing you code",
      "start": 4163.199,
      "duration": 5.841
    },
    {
      "text": "here in Jupiter notebook I'll be sharing",
      "start": 4166.159,
      "duration": 4.68
    },
    {
      "text": "this code file with everyone so you can",
      "start": 4169.04,
      "duration": 3.759
    },
    {
      "text": "run this code and I highly encourage you",
      "start": 4170.839,
      "duration": 4.801
    },
    {
      "text": "to run this code on your own thank you",
      "start": 4172.799,
      "duration": 4.48
    },
    {
      "text": "so much everyone and I look forward to",
      "start": 4175.64,
      "duration": 5.599
    },
    {
      "text": "seeing you in the next lecture",
      "start": 4177.279,
      "duration": 3.96
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch series my name is Dr Raj dander and I'll be your instructor for this lecture today we are going to cover a very important topic which is called as tokenization basically when we build large language model Model S the data needs to be pre-processed in a certain manner before it is used for pre-training and uh in this data pre-processing pipeline the first step is tokenization at its really basic form tokenization is just the process of breaking down a sentence into individual words but it is a bit more detailed than that the way it is used to act build large language models in today's lecture we are going to understand everything there is to know about tokenization and we will build a tokenizer fully from scratch we will also build an encoder and decoder from scratch and we'll code it out in Python so today will be a Hands-On session if you have a laptop in front of you U that's great so that you can follow along in Google collab or jupyter notebook or V s code to implement the code along with me so let's get started with today's lecture building a large language model usually proceeds in three stages especially if you want to build it from scratch in stage one we have to understand the nuances or basic mechanism of building an llm and that includes data preparation attention mechanism and understanding the llm architecture stage two involves pre-training and building the foundational model so that involves the training Loop model evaluation and loading pre-trained weights and stage three involves finetuning training on smaller very specific data sets to build applications which are actually useful to you such as a classifier or a personal assistant in this lecture we are going to look at the first aspect of stage one which is data preparation and pre data preparation and sampling so uh tokenization comes under the category of data preparation and sampling and that is what we are going to cover in today's lecture you can think of this as the first building block for building a large language model so let's get started with today's lecture so the main question which we are going to answer today is how exactly do you prepare input text for training large language models at the heart of it llms are just neural networks right so you need data the parameters of the llm are optimized and then we have some output the question is that this data which comes in as the input what form should it take how should we prepare the input text we have a huge number of documents right which the llm is trained on we have seen that in previous lectures the llm is usually trained on billions of documents but do we feed the document Direct ly as the input text do we feed the sentences of the document as the input text no it turns out we have to tokenize the document and then feed individual tokens uh there is one more step after this which is called as Vector embeding but we'll have a separate lecture for that today we are only going to look at tokenization so the process of tokenization can be broadly broken down into three steps this is the key takeaway which you should learn from today's lecture the first step is that you have to initially split the text into individual word and subo tokens that is Step number one imagine you have huge amount of text you break it down into individual words that's the first step of tokenization the second step of tokenization is converting these tokens into token IDs and the third step is basically encode these token ID into Vector representation we are not going to look at step number three in today's lecture because it comes also under Vector embedding but essentially we are going to look at step one and step number two so let me repeat step one and step two step one is basically imagine you have a big document which is let's say you're training on Harry Potter books you take this books you divide it into sentences and then you split the text into individual word and subword tokens and then you convert these tokens into token IDs that's the uh two major steps involved in tokenization which we are going to look in today's lecture um so I just want to show you a visual for how the input text is given to the large language model so let's say this is an input text right this is an example remember the first step of tokenization we break this into individual words so the first word is this the second word is is the third word is an and the fourth word is example so this is the tokenized text and this is my step number one right now step number two is we need to convert each of these individual tokens into token IDs so this is the token ID 1 4013 token ID 2 2011 token ID 3 which is 302 and token ID 4 which is 1134 so every word or every token rather has a token ID associated with it I have currently given these numbers randomly but you need to assign a token ID for each word and then the next step after getting these token IDs is to convert these token IDs into something called token embeddings or vector embeddings and then these Vector embeddings are then fed as input data to the GPT or the llm rather so even before coming to the training stage we need to do these many steps of uh pre-processing and today we are essentially going to look at step number one and step number two I hope you have understood the two main steps which we are going to look at in today's lecture so let us start by looking at step number one initially which is tokenizing the text or breaking down the text into individual words and you definitely do need to understand the nuances which are involved in this and that's why I'm going to take you through the entire course process in Python uh we could have just use a pre-built tokenizer but then you won't understand the nuances involved in tokenizing and uh just to give out a a shout out or rather a credits to Sebastian rashka for hosting the this code publicly and I'll be heavily borrowing from this code so let's get started for the purposes of the demonstration today we are going to use a specific data set we are going to use this book book called The Verdict which has been written by Edith Warton so let's look at a few screenshots or let's look at a few data about this book so if you search The Verdict by edit Warton you'll get a image of this book which looks like this uh year of publishing let's look at the year when this book was published so this was I think published in 1908 so almost 120 130 years back and it's available for free uh free to download I'll give the link to download this book it's also available in this repository by Sebastian you can just click on this download and then download this book so we are going to look at this book and assume that this is the only uh training data which we have for the purposes of today's lecture but whatever I show you today can be really applied to any book or even a huge number of books I encourage you later after this lecture to try out some different book which is available for free online such as the Harry Potter book or whichever one interests you and apply the same code over there okay so this is the data set which we are going to look at and then what we have to do is basically download and load this data set in Python so let me show you how to do that so now we are moving into uh jupyter coding uh please comment out in the comment section if the code is visible to you if I've zoomed in enough or if I should follow some different style okay so the first step as I mentioned we are going to look at two steps today creating tokens and then converting these tokens into token IDs the first step is creating tokens right so what we are going to do here is that we are going to uh download the verdict. txt file we are going to open it in Python and we are going to read it and we are going to print the total number of characters so I'm just showing this uh when you download it it looks this much it's not that big um and in fact it's even available over here so I have downloaded this book right now and uh in Python you can use this open command so with open the word it DOC text and then R means we are reading this and then raw text is that variable where we are storing whatever uh content python has read from this book so this raw text variable is going to be very important for us because we'll also refer to it later basically raw text is just the entire text which we are reading and then what we are doing here is that we are counting the total number of characters in this raw text so what I'm doing in this part of the code is that I'm printing out the total number of characters and then length raw text right so here you see the print command towards the end raw text colon 99 what does this do this print command prints the total number of characters uh so here so there are two print commands right the first print command essentially prints the total number of characters and the second print command essentially uh prints the first 100 characters of this file for illustration purposes so uh let's run both of these print commands right now I have already run this before but let me click on this run Icon again so okay the result of this first print command is this the total number of characters are 20479 20479 and the result of the second print command is is basically we are printing the first uh 100 characters of this file right so the first 100 characters are I had always thought Jack uh gisburn Etc so this is the these are the first 100 characters so this indicates that python has been able to successfully read the text which we have downloaded and that's an awesome sign great now let's move further our goal in this lecture is to tokenize all of the characters which are in this book in this text uh in this short story into individual words that we can later turn into embeddings for llm training remember uh embeddings is the last step so before you come to training the llm there is this thing called embedding so the goal in this lecture is to do the tokenization and the token IDs for the entire text which we have read for all of these characters for the 20479 characters of this short story the goal is is to convert them into tokens and token IDs so one note which has been added over here is that note that it's common to process millions of Articles and hundreds of thousands of books which is a huge amount of memory when actually working with llms this is very important to note when you actually work with llms you don't just download one book as I showed to you right now we actually work with millions and thousands of books uh which are a huge amount of data uh to be stored on your computer however for educational purposes it's sufficient to work with smaller text samples like a single book and that's why we are using a single book in today's code to demonstrate the idea to you okay so the next question is how can we best split this text to obtain a list of tokens so I encourage you to pause here right now and let's say I give you this problem that you have read this text now your goal is to split this text into individual tokens right how would you do this which python Library would you use uh so let me reveal the answer the python Library which we we will be using for this is called as regular expression re uh so this module Pro provides regular expression matching operations and let's see how basically this works we have to import re and then what we can do is that we can use this library to split uh to split any given text um based on the white spaces in that text or any other characters in the text and I'll explain what this actually means okay so okay so let's say you have a text which looks like this hello world this is a test right and you want to split and you want to split this into individual tokens what I'm going to do here is that from this Library I'm going to use this function re e dos split right and then there is this R and then the symbol is/ S what this basically does is that this slash s means that we split wherever white spaces are encountered right so let me tell you what is meant by white spaces so let's look at this word let's look at this sentence you see there is a white space over here there is a white space over here there is a white space over here here and here so wherever there is a white space what this command will do is that it will split the text into individual tokens and where the split will be wherever the white space is there so it will scan this sentence from left to right now here it encounters the white space so it will split split here then again it will move forward here it encounters the white space so it will split here so let's go from left to right so until hello and comma it has not encountered a white space right so hello and comma is one token and then it encounters a white space so it splits so then it outputs the white space here when you print out the result then it again goes on scanning and here until world and full stop there is no white space so it prints Out World and full stop then after this point there is a white space right see this so then it prints this out then it again keeps on scanning from left to right so till this and comma there is no white space so it prints out this and comma then it prints out another white space over here so it keeps on scanning from left to right and then splits wherever white space is encountered right that is the advantage of using re do split U and then this character indicates where you want to split so here I have indicated that you split wherever you see a white space right so the result of this is a list of individual words white spaces and punctuation characters so see white spaces are also the results of this now uh what we want to do is we want to also uh split commas and periods so here you see the comma is included as part of the word word itself we the full stop is included as part of the world but we want to have comma and full stop also as separate tokens how do we do that it's pretty simple in the re. split command you also include comma and you also include the full stop along with this SLS command and then when you print the result you will see that along with white spaces so white spaces are of course uh printed out because we are splitting on white spaces also but comma is now a separate token full stop is now a separate token which was not the case before hello and comma were one token but now you'll see hello and comma are separate tokens awesome so now we have split based on comma and full stops also correct now the main another remaining issue is that our list still includes space characters right see these the white space characters are still counted as tokens we can even remove these red redundant character safely so here what we are doing is that uh you first scan the result scan each item in the result so we are looping over each item in the result so this is the result right so we are going over each of the individual items in this result and if item. strip is equal to true then only we return that item so if it's a y space then item. strip will actually be false so white spaces will not be returned Whenever there is actually a full world full word like hello or comma or world or this or full stop then only item. strip will return true and those words will be returned in this result so using this statement uh loop over item in result and then find out item. strip so for white spaces item. strip will anyway be false so that will not be returned so in this one line of command we can actually get rid of white spaces completely from the sentence and now when we print out the result we'll have the tokens as hello comma world full stop this comma is a test and full stop we won't have white spaces so one more consideration which I would like to mention over here is that removing white spaces are not this is an important question to actually discuss because uh when you develop a simple tokenizer whether we should encode white spaces as or as separate characters or just remove them depends on our application and its uh requirements so what are the advantages of removing white spaces removing white spaces reduces the memory and Computing requirements which is great however keeping white spaces can be useful if we train models that are sensitive to the exact structure of the text right for example python code is sensitive to indentation and spacing so if python code is used as the data set for training and a large language model it makes sense to keep the white spaces right because the white spaces have some meaning in in the case of python code as the training data set in this case we are going to uh until now we are going to remove the white spaces just for Simplicity so that we have memory advantages but remember that it's not obvious to always remove white spaces uh as has been mentioned uh uh over here if you have python code AS training data white spaces are important because indentation matters so several things need to be considered when you're building an actual llm whether you should remove white spaces or whether you should not remove white spaces uh that's great so the tokenization scheme which we deviced about works well on simple sample text so we can modify it a bit further because we also want uh to have question marks quotation marks and double dashes as separate characters or separate tokens currently only full stop and comma are separate tokens right but we also want question marks quotation marks double dashes Etc separate tokens so let's look at this document again see there are these double dashes then I'm sure there are question marks somewhere there are question marks there are exclamation marks in this document we all want them as separate tokens right and we need to include that in our tokenization Command and the way we do this is pretty simple again we use this re. split and along with comma and along with full stop we add all of these other special characters like colon semicolon question mark underscore exclamation uh quotation marks bracket Etc and uh we say that whenever these characters are encountered also do a split on these characters so that they are considered as individual tokens right um so so now when you take the sample text hello comma world is this dash dash a text all of these will be uh sample tokens this dash dash will also be a separate token the question mark will also be a separate token because we have included that in our split command and again we'll strip the white spaces like the same command for item for item in result if item. strip remember this was the command we used to filter out the white spaces or to remove the white spaces uh awesome so now these two lines of code are our tokenization scheme so this is the two lines of code in which we have built the tokenizer in this first line of code we take a sentence and we split the sentence wherever there is a white space wherever there is a comma full stop colon semicolon question mark underscore exclamation quotation mark or bracket we split it and then in the second sentence we just remove the white spaces and then we print out the result it's as simple as that this is the simple tokenization scheme for building large language models a different tokenization scheme is used but we'll come to that in the next lecture it's called bite pair encoding but for now I just wanted to give you a sense of what all nuances need to be considered for building a tokenizer like this now you see we have we have tested these two uh sentences which are our tokenizer scheme for a sample sentence right now what we can do is apply the same to two statements on the entire raw text so we had the raw text right defined before let me go to that U so see this was the raw text which was basically the book which we had read now we'll use these two statements to essentially uh convert the entire uh raw text into individual tokens so what we'll do is that we will split the entire raw text based on these tokens which we had seen before and then we will store it in a variable which is called as pre-processed so pre-process pre-processed is essentially a list of all the tokens and then in the second statement exactly same to what we saw before we'll split or we'll get rid of the white spaces awesome right so now let us print the length of the pre-processed so we have 4690 tokens in the original uh book so if you go to this book the simplest way to think about this is this will be one token height will be one token this inverted commas will be one token these two double dashes will be one token so this entire thing is broken down into individual words or tokens which are 4690 in number and here we have just printed out the first 30 of these tokens and you see I had always thought they are individual tokens and dash dash comma they are also counted as tokens over here because we have uh mentioned that in this split where all do we want the split to happen awesome so we have now got our basic tokenizer working and we have applied it to the entire short story which we are considering considering as the input data great now we have to come to the Second Step so we have now finished the we have now finished the first step which is uh tokenizing the entire short story and now we will come to the next step which is converting tokens into token IDs now what is very important is that python works with numbers right so even if you look at these individual tokens which you have obtained right now these are not numbers they are still words we need to convert them into token IDs so that they are numerical representations so let's see how to do that so we have the complete training data set here for illustration I have just taken a sentence and until now we have converted this sentence into individual tokens now what we will do is that we'll take the tokens and convert them into token IDs and then there is a very specific way of doing this first we build something which is called as vocabulary vocabulary is just all the list of our tokens but it's sorted in an alphabetical manner so if the if our data set or the tokens are the quick brown fox jumps over the lazy dog if this is the training data set the vocabulary is the list of tokens in alphabetical order so Brown comes first then dog then Fox then jumps then lazy easy then over then quick and then the this is my vocabulary for this training data set eventually we'll have billions of data sets billions of files so then the vocabulary will be huge but I'm just showing a simple representation here for how vocabulary is constructed vocabulary is just a list of tokens which is sorted in alphabetical Manner and then what we do is that each unique token is mapped to a unique integer which is called as the token ID so it's as simple as that you map these tokens in alphabetical order and then to each token you assign a number so since they are arranged in alphabetical order Brown will be zero so you start from zero since it's python dog will be one fox will be two jumps will be three lazy will be four over will be five quick will be six and the will be seven so the vocabulary always contains unique tokens remember although the appears two times in the vocabulary it only comes once so we make a list of these unique tokens and we assign token IDs to all of these and it's important to arrange the vocabulary in alphabetical order so that assigning token IDs becomes very easy so the process of assigning token IDs is actually very simple each unique just remember that each unique token is mapped to a unique integer which is called as the token ID right so now let's see how this is implemented in Python uh so in the previous section we talk loaned edit worthon short story and assigned it to a python variable called pre-processed so remember pre-processed is the final list which contains all the tokenized words this these words uh now what we'll do is now we'll create a list of all unique tokens and sort them alphabetically to determine the vocabulary size exactly what I had mentioned over here so what we are doing is that we are taking pre-processed we are converting it into a set and we are going to sort it into set so this will sort it in alphabetical order and then we are just going to print the vocabulary size so we can see that the vocabulary size is 1 130 right remember the vocabulary only consists of unique words so the vocabulary size is less than the number of tokens which are there uh so now what we can do is after determining that the vocabulary size is 1130 via the above code we create the vocabulary itself so when I say you create the vocabulary remember that a vocabulary is not just these tokens but every token needs to be assigned to a token ID a vocabulary is a is like a dictionary of tokens and Associated token IDs so now we have to create that dictionary right and so it's very simple you just all words consists of all the unique words in the vocabulary right in alphabetical order what you do is that you just uh you take all these tokens and assign an integer value to all these words that's it which means the token which comes first that will be zero the token which comes second its integer will be one and it will proceed like that uh let me print this out for you to show you so our tokens are also exclamation marks commas Etc right so by default they are assigned the first priority so they will be 0 1 2 3 4 5 6 and here you see everything is arranged in alphabetical order so a will be 11 ah will be 12 among will be 13 and will be 14 R will be 15 a r RT will be 16 as will be 17 at will be 18 Etc now all of these exclamation all of these I would say uh words are used because this was written in 1908 this a rrt is not used as an exclamation typically now but this book is written in 1908 so it shows up right so this is the ulary which we have and we have printed the first 50 items of this vocabulary so it looks like a dictionary pretty much and remember that every element in this dictionary uh which is a token has a token ID now associated with it that's it a vocabulary is as simple as that and the simplified code for this is for every token assign an integer and how do you get the integer and tokens you just enumerate all words so this enumerate command in Python what it does is that it takes all the words and then it assigns an integer to each word in alphabetical order so that way python is actually awesome and you can write very complex things in in one line of code actually so this is how you create the vocabulary and this is how you assign token IDs to every individual token right so as we can see based on the output above the dictionary contains individual tokens which are associated with unique in integer labels exactly what what we saw here the vocabulary which is a dictionary consists of individual tokens assigned to a unique integer label which are tokenized s now one more thing which I want to tell you is that currently we converted the words or tokens into token IDs right you can think of this process as encoding now later uh we will also need a decoder which means that from the token ID you need to convert the token ID back to the word because when the llm gives the output that will be in numerical form now you need to convert it in word form so you also need a mapping from the token ID to the Token the vocabulary is a dictionary which gives you a mapping from the token to the Token ID but we need a reverse mapping which is called as a decoder so this is what uh I have mentioned here later when we want to create the output of an llm from numbers back into text we also need a way to turn token IDs back into text and for this we create an inverse version of the vocabulary that Maps token IDs back to the corresponding text tokens and we'll see how to do that for now what we are going to do is we have understood enough about tokenization that we are going to implement a complete tokenizer class in Python this class will have two methods it will have an encode method and it will have a decode method let me show you what it actually means so this tokenizer class which we are going to implement in Python it will have two methods the first will be the encode method and the second will be the decode method in the encode method what will happen is that sample text will be converted into tokens and then tokens will be assigned token IDs based on the vocabulary exactly what we saw like over here in the decode method exactly reverse things would happen so in the decode method we start with token IDs we convert it into individual tokens and then we get back the sample text so I hope you understand the difference between the encode method and the decode method here because this is the exact same difference which will show up in the encoder and decoder block of the Transformer architecture or rather this is the this is a good way to understand that intuition in the encode in the encode block what we do is we take sample text we convert it into tokens and then we convert it into token IDs that feeds as the training data to the llm but when the llm gives its output in the form of token IDs we need to convert it back to tokens and and then back to the sample text so that we know what the output is in terms of sentences so that's why when we implement the tokenizer class we need an encode method uh and also the decode method so the encode method will take text as an input and give token IDs as output the decode method will take IDs token IDs as input and will give text as an output now let's see how to actually create this simple token izer class in Python first so there are if you see three methods the init method which is called by default when an instance of this class is created and let us look at the arguments of the init method it takes wcab so when you create an instance of the tokenizer class you have to pass in the vocabulary right and remember the vocabulary is nothing but a mapping from tokens to token IDs right so then after this uh instance is created St Str to in which is string to integer will just be the vocabulary because the vocabulary is already a mapping from string to integer or tokens to integer and then the integer to string is basically reverse so what you do is that you take the string and integer in the vocabulary and then for every integer you uh mention which token it is so for S you can think of as the token and for I you can think of as token ID so what we do in this int to string variable variable is that uh so we take the token and we take the token ID in the vocabulary and then we just flip it then we say that for this token ID this is the particular token remember this into string will be needed for the decoder method when we have the token IDs and we want to convert it back to tokens so uh in the encode method the exact same pre-processing steps will happen as we had seen before for tokenization what we'll do if some random text is given to us we will take that text we will split it we'll split it based on the comma based on the full stop based on the colon based on semicolon Etc into individual tokens and then we'll get rid of the white spaces what we had seen before and then these will be individual tokens right so up till now we are at this part where we have converted the sample text into individual tokens and then we'll convert these into token IDs so that's the last step over here after you get the individual tokens in this list called pre-processed what you have to do is that you have to use this St Str to in dictionary which is basically just the vocabulary and then you have to uh assign a token ID for each token so remember the Str str2 int is basically just converting tokens to token IDs using the vocabulary which is passed into this tokenizer class so you will just take the uh tokens you'll take the tokens which are in this pre-process list and you'll convert them into token IDs that's the encoder method that's it in the decoder method what you are actually doing is that you are uh you are using this reverse dictionary which is integer to string which is basically token ID to token and then you are converting the token IDs into individual tokens that's the first thing so let's see the decode method you first convert the token IDs into individual tokens and then what you do is you join these individual tokens together so this this join is used so first you convert the token IDs into tokens and then you join the individual tokens together that's it and then here what we are doing is we are going to replace spaces before the punctuations so an example here would be let's say if the tokens let's say in the decoder if the tokens are the fox chased and question and full stop right now if these are the tokens and if we convert it into sample text by using the join method the final answer will be the the the fox chased and full stop now see the problem here is that there is a space between the there is a space here between the full stop and the chased so we need to get rid of this space so then the final answer would be the fox chased and full stop and this is the the same for question mark Etc so in this second sentence here what we are actually doing is that we are getting rid of all the spaces before the punctuation so that it becomes a complete sentence so this is the decoder method it's actually very simple we are just going to uh use the encode method to convert uh sample text into token IDs and we are going to use the decode method to convert token IDs back into sample text and for that we needed to write two methods the encode method and the decode method so remember this tokenizer class takes the vocabulary already as an input so that so we already have the mapping from tokens to token IDs because that is inherently present in the vocabulary we just need to construct a reverse mapping from the token IDs back to the tokens and then use that in the decode method that's it and to convert the text into tokens we use the same re dos split and item do string which we had seen earlier in today's lecture so this is essentially the tokenizer class which we have created awesome now let's move to the next step so what we can do is that now we can instantiate a tokenizer object from this class right and uh tokenize a passage from the short story which we have downloaded to test it out in practice so here you see I'm creating an instance of this class I have passed the vocabulary as an input what is this vocabulary this vocabulary is basically the uh vocabulary of tokens and token IDs which we have converted our input text so this is our input text and we have converted this into tokens and assigned the token ID to each this is my vocabulary now what I'm doing is I'm actually uh creating an instance of the yeah I'm creating an instance of the simple tokenizer version one class which we have defined over here by passing in this vocabulary as an input right and this is then defined as tokenizer great and uh so the text which I'm going to pass in now is this it's the last he painted you know Mrs gburn said with pardonable pride this is the text and now I'm going to test out this encode so remember this encode takes in the text as an input so you need a text right to convert into IDs so what the encode method will do always remember this schematic always remember the schematic the encode method will convert the text into token ID is so this is the text and then we apply the method tokenizer do encode and text so what when you print the IDS you will see these IDs of the text which means that our encoder has successfully converted this text into token IDs that is exactly what we wanted right so the code above prints the token IDs next let's see if we can turn these token IDs back into text so now what we'll be doing is we'll be using tokenizer do decode and use these IDs so tokenizer decode and IDs uh and pass IDs as an input remember tokenizer do decode takes the IDS as an input so I'll pass these IDs which we have printed over here and let's see whether it recovers this text so the text which is recovered is it's the last he painted you know Mrs gburn said with pardonable pride amazing right because it's exactly the same text which we had given uh to the encoder and now it has been decoded by the decoder So based on this output above we can see that the decode method successfully converted the token IDs back into the original text which is exactly what we wanted which means the encoder and decoder are working right so so far so good we have implemented a tokenizer which is capable of tokenizing and DET tokenizing text based on a snippet from the training set so this sentence which we gave to the encoder was from the training set so we knew that the words will be in the vocabulary but what if the sentence which is given is not present in the vocabulary so we have a vocabulary here which is 1130 the size of the vocabulary is 1130 right what if I give a sentence to en code which is not present in the vocabulary so let's try this let's say the text is hello do you like T now uh hello is probably not there in this hello is not there in this short story so let me search hello it's not there let's see if T is there t is actually T is there but hello is not there so hello will not be there in the vocabulary right so now let let me see how what's my answer here because now I have asked the tokenizer to encode something which is not present in the vocabulary okay so if I run this you see you get an error here because hello it does not know what to do with this word hello because it is not present in the vocabulary so the problem is that the word hello was not used in the verdict short story and hence it is not contained in the vocabulary now this actually highlights the need to consider large and diverse training data sets to extend the vocabulary when working with llms we do not want this problem right if our training data set is small and if some user gives a new word to chat GPT which it does not even know we don't want our we don't want an error that's why huge number of data set is used for training large language models in fact llms like chat GPT use a very special thing they use something which is called a special context tokens to deal with Words which might not be present in the vocabulary so that an error message is not shown such as this and we will come to that in the next section but here I just want to illustrate that the reason for having a a large and diverse training data set is because uh we we want many words to be present in our vocabulary it does not make sense to have a smaller and a shorter vocabulary because what if the user gives a new word which is not present in the vocabulary we don't want that there is a way to deal with this by using something called special context tokens so let's look at this in the next section now let us have a look at the last section in today's lecture which is dealing with special context tokens these content are not usually covered in other lectures but they are very important especially since we are going to be building llms from scratch we also want to understand how real life llms work and special context tokens play an important role so until now we have implemented a simple tokenizer right and applied it to a passage from the training set but the main question we encountered before is that if there is a word in the text which is not there in the vocabulary how do we encode this word um so we will be implementing some uh things which are called as special context tokens what we will do is that we will modify the tokenizer to handle unknown words uh and we will Implement a python class which is called a simple tokenizer version two so we have already implemented uh simple tokenizer version one but here it did not have the provision to handle the unknown tokens so in this version two we will also uh Implement uh the simple tokenizer version two to have the provision to handle the unknown tokens in particular we will be learning about two main tokens the first is this this token which is unknown for an unknown word and the second token is for end of text so let me again go back to this uh whiteboard over here and write a bit about why we are exactly using these tokens so uh let's take a sentence which is the fox chased the dog let's say this is my sentence right now and I have tokenized it into the Fox Chase the dog and here's the vocabulary so it's arranged in alphabetical order and there are token IDs right now to this existing vocabulary we are going to add two more tokens the first is the Unk which is unknown and we'll assign a token ID and then we'll also have end of text and then we'll assign it a token ID these two are the last two uh tokens in the vocabulary so the token is corresponding to these two will be the largest so what will happen is that let's say if some new sentence or some new word is given so let's say uh the fox chased the dog quickly if that is the word all these other words like the Fox Chase the dog will be converted into token IDs but for quickly it will have a token ID of 783 which is the token ID of unknown and why because quickly was not in the vocabulary our vocabulary only consisted of chased dog fox and the so quickly is an unknown word and so it will receive a token ID which we have reserved for unknown words and then you might be asking what about this end of text so end of text is something which is a bit different uh when we are working with multiple text sources we typically add end of text token between the text so let us look at four text sources here this is the text Source One let's say it comes from one book let's say this is the text Source two let's say it comes from another news article text Source three which comes from an encyclopedia and text Source Four let's say which comes from an interview let's say these are are our training sets usually all of these are not just collected into one giant document or all of these sentences are not just stacked up together we usually after after this initial text is fed as an input we have this end of text token which means that the first text has ended and now the second text has started after the second text ends then we again have this end of text token then the third text starts and after the third text ends then we again have the end of text token and then the fourth text starts so the end of text token is basically added between the texts so essentially the end end of text tokens act as markers signaling the start or end of a particular segment this leads to more effective processing and understanding by the llm it's very important to add these end of text tokens because then llm treats the initial so let's say end of text there is before the end of text which is text Source One and after the end of text which is text Source two if end if end of text was not there the llm would have mixed all of this together right uh but end of text tokens allows the llm to process the data and understand the data in a much better Manner and in fact when GPT was trained the end of text tokens were used between different text sources this is very important to note and only students who try to understand tokenization in detail know about such specifics so these are the two tokens which we will be considering in this section the first is the unknown token and the second is the end of text token so let's read bit here we can modify the tokenizer to use an unknown token if it encounters a word that is not part of the vocabulary great furthermore we also add a token between unrelated text which is the end of text token so for example when training GPT like llms on in multiple independent documents it is common to insert a token before each document or book that follows a previous text Source basically this is exactly the end of text token which has been written over here I'll be sharing this notebook with all of you so no need to worry if you miss certain portion or want to revise certain portion again now what we'll be doing as I mentioned over here uh is that we'll be modifying the vocabulary we'll be augmenting the vocabulary and uh how will we modify or augment the vocabulary we will include special tokens we will include two special Tokens The Unknown and the end of text token and we will add these to the list of all the unique words that we created in the vocabulary in the previous section so let's look at this pre-processed so remember pre-processed is the vocabulary uh or U so preprocessed was our list and then later we converted this into vocabulary which was stored in vocab right and remember the size of the vocab is 1130 and now we are going to add two more tokens so the size will be 1132 so let us again start with pre-processed we will sort this and then we'll add two tokens so we'll add the end of text token and then we'll add the unknown token and that is done by using the python command which is called extend so what this does is that it just adds two more additional entries to the list the sorted list in this case and then again we'll write the same command which we did earlier right we first enumerate all the tokens in the pre-processed list and then for each token we assign an integer and this enumer it will make sure that the tokens are anyways arranged in alphabetical order and then for each of these token we assign an integer which is the token ID that's how we create the vocabulary now if you print out the length of the vocabulary you will see that the length of the vocabulary is 1132 and if you remember without adding these two tokens the length of the vocabulary was 1130 so now the length has increased by two that is great right uh that makes sense because two more tokens have been added so the vocabulary is extended great so based on the output of the print statement above the new vocabulary size is I should mention here actually 1132 uh so this should be 1132 and in the previous section it was uh 1130 and let me run this yeah so based on the output of the print statement about the new vocabulary size is 1132 and the vocabulary size in the previous section was 1130 good so what we'll do as an additional Quick Check we'll also print the last five entries of the updated vocabulary so if you print out the last five entries in the updated vocabulary you will see let's look at the last two entries these are the end of text and the unknown so the end of text has a token ID of 1130 and the unknown has a token ID of 1131 awesome so now the vocabulary is updated and now what we'll be doing is that we'll be extending the simple tokenizer class uh this is the version two basically many things will remain the same this initialization will remain the same it will initialize two dictionaries the string to integer dictionary which is the vocabulary itself which converts the tokens into token IDs and the integer to string dictionary which has token IDs and then a string mapped to each token ID this encode now let's look at this encode the first two sentences are very similar to what we saw before we split on the comma full stop colon semicolon question mark underscore explanation quotation and bracket and then we remove the white spaces using item. strip but we add one more thing what we add is that if the item or if the particular entry is not present in the vocabulary uh the token which is assigned to that entry is unknown so let's say you are scanning the text right and if you come across a word in the text which is not in the vocabulary so if the item is not present in this string to integer vocabulary which is the vocabulary which we have passed uh then it is it should be the unknown token we replace that with the unknown token and then we convert all of these tokens to token IDs so in this step what will happen is that all the words in the input text which are not in the vocabulary will be replaced by the unknown token and then in this step that is the main encoder step where the tokens are converted into token IDs in the decoder part part everything will uh almost stay exactly the same nothing changes we first convert the token IDs back into tokens and then we join them together and uh before the punctuations there are spaces so we get rid of these spaces and then we return the decoded text so this is the change in this tokenizer simple tokenizer version two the only change is uh adding this so if some word is not known you encode it with unknown and then assign the correspond oning ID to it so if something is unknown the ID will be 1131 great now let's actually try some things okay let's say the first text source which we have is hello do you like T the second text source which we have is in The sunl Terraces of the palace these are two text sources now what we'll be doing is we will construct a text which joins these two and we will add an end of text at the end of the first text Source why are we doing this because we saw that this is usually how it's done in practice in GPT Etc if you provide one text source and if you have another text Source you don't just join them together you split them with this end of text token so this is what we are exactly doing we are now constructing a text which will feed as an input to the encoder right but we add this end of text so the text Will which will essentially feed to the encoder is hello do you like T end of text in The sunlit Terraces of the palet s this is the text which we are feeding to the encoder and now we are asking the encoder to encode these into token IDs so let's look at this sentence in detail this word hello is not present in our vocabulary so if if we pass it into this tokenizer it will hit this this statement where the word is not present in the vocabulary so it will replace it with the unknown token and this end of text again end of text is actually present in the vocabulary because we have added it right now so the token ID corresponding to end of text will be 1130 and the token ID corresponding to hello will be 1131 right so let's check if this is actually true so I'll now run tokenizer encod text and see the token ID for hello is 11131 this is exactly what we had expected right because uh hello is not present in the main vocabulary so it's actually an unknown word so it token ID should be 1131 awesome and then for end of text let's see the token ID yeah this this is the token ID for end of text the token ID is 1130 so now there is no error which is coming here earlier when we passed this hello do you like T there was an error which we are getting that hello is not present in the vocabulary but now this error is not coming because we have taken care of it we have added the unknown token in the vocabulary and so now the tokenizer encodes the text uh in a correct manner that's awesome so now what we'll be doing is that we'll be actually using the decode function now and we'll pass the encoded text which are the token IDs these IDs into the tokenizer do decode so let's see what the tokenizer decode so when these are passed into the decoder the decoder is unknown do you like T end of text in The sunlit Terraces of the unknown so actually there are two unknown text here hello is an unknown and Palace is also an unknown so the token IDs for both of these are 1131 so when you decode these IDs you will get unknown do you like T so which because hello is not known then end of text and then in The sunl Terraces of the unknown so here again unknown means the palace which was not in the vocabulary so the encoder and decoder are working perfectly and we are able to handle the unknown words now uh we are able to handle the unknown words actually quite effectively because we replace them with the unknown token and the end of text is also being captured pretty effectively so when we actually make the text itself we have to add end of text before we pass it to the encoder and subsequently to the decoder So based on comparing the det tokenized text with the original input text we know that the training set which is the verdict book did not actually contain the words hello and the palace because both of them are replaced by this unknown token here so we will not get errors this way once we take into account the special uh context tokens awesome so uh let me just add a last note about the special context token so up till now we have discussed tokenization as an essential step in processing text as input to the llms however along with these Special tokens there are some other tokens which also researchers consider so this is the beginning of a sequence so BOS token so we saw the end of text right some researchers also consider BOS so this token marks the start of a text it signifies to the llm where a piece of context begins or where a piece of content begins right then the second token is eos which is end of sequence right so this token is positioned at the end of a text and especially useful where concatenating multiple unrelated text it's similar to end of text uh then the third token which is important is the padding token so when training llms with batch sizes larger than one the batch might contain texts of varying lengths so to ensure that all the texts in different batches have the same length the shorter texts are extended or padded using the pad token up to the length of the longest text in the batch so imagine the different text is being put in batches to the llm for parallel processing we'll look at this in detail so no need to worry about this right now but but just remember that for efficient Computing llm parallely processes the batches so each batch contains text which have different sizes the shortest Texs are augmented with this pad token to match uh their length with the largest text and now we know how to add these special tokens right we just augment the vocabulary with these tokens and whenever we pass text to the encoder we we may add things like beginning of sequence end of text the pad token Etc so we only saw uh the unknown and the end of text special context tokens but there are three other BOS beginning of sequence and EOS end of sequence and Pad which is padding now here I want you to note that the tokenizer which is used for GPT models does not need any of these tokens mentioned above but it only uses the end of text token for Simplicity so gpt3 gp4 when they are trained they don't use BOS uh padding they don't even use the unknown token they only use the end of text token for Simplicity and this I also shown over here so the end of text which is used is exactly how it is used in GPT training as well okay and the last sentence is that the tokenizer used for GPT models also does not use an unknown token for out of vocabulary words then you might be thinking how does GPT deal with unknown tokens right so then there is something called as the bite pair encoding tokenizer which GPT models actually use which breaks down words into subword units So currently what we have done is each word is essentially one token right and each punctuation mark each colon semicolon is one token but what actually GPT does for tokenization it it uses something called bite pair encoding or BP tokenizer and that automatically deals with unknown tokens because in that tokenizer one word is is not essentially one token but even the words are broken down into subwords to generate individual tokens uh we'll come to this in the next lecture but I wanted to show this lecture specifically for showing you how you can do your own tokenization from scratch I could have directly showed you bite pair encoding with GPT users but then that you would not have appreciated uh if you were to tokenize yourself how would you do it from scratch of course we'll look at the bite pair and coder tokenizer in a lot of detail in the next lecture to understand how GPT actually tokenizes but I wanted to give you this intuitive feel so that you don't think tokenization is a hard process I'll be sharing this code file with all of you so that you can run this and I actually highly encourage you to run this for a different uh book or a different txt file so that you get the hang of it and you become better and better and better at this um just before concluding the lecture I want to go through a quick revision of what all we have learned in today's lecture so in today's lecture we essentially looked at this data preparation and sampling stage in building large language models from scratch in particular we took a look at tokenization and uh the question is that how do you prepare input text for training llms and we saw that this is divided into two steps in step one we split the text into individual words so you have a essay or a book or a huge number of books you split that text into individual words and then you convert these tokens into token IDs now when GPT actually uses tokenizers it doesn't use individual words as tokens it even does subwords and we'll look at that in the next lecture so if you look at how llms are actually trained you take the input text you tokenize it into words you convert these words into token IDs and then the later step is we have to convert these token IDs also into to Vector representations which we will come to later but in today's lecture we mostly looked at step one which is tokenizing and step two which is converting tokens into token IDs right so initially what we did is we downloaded and loaded the data set in Python we tokenized the entire data set using Python's regular expression library and we used re. spit we first used it used it at White spaces and then we added colons semicolons question marks Dash Dash character Etc because we wanted to split all of those punctuations and even if you search the dash dash in this text you'll see dash dash appear so many times so even we wanted to split that into individual tokens awesome so this is what we did and then what we did is we maintained the vocabulary we converted these tokens into token IDs so we saw that what a vocabulary is that is it's simply a dictionary um and it's a mapping from the tokens into the token IDs so the tokens are arranged alphabetically and then each token is mapped to a token ID as you can see over here so each unique token is mapped to a unique integer called token ID and why are these token IDs necessary because you will see that these token IDs are then converted into token embeddings which are fed as input to the GPT um great now then after that we implemented a tokenized class in Python so that whenever we create a vocabulary we can create an instance of this tokenizer class uh but we did not just Define the encode method in this class we defined one more method which is the decode method so what the encode method did is that whenever sample text is provided it converted it into tokens and then converted these tokens into token IDs great but the decode method did exactly opposite it took the token IDs as an input it converted the token ID into tokenized text and then it recover the sample text from the tokenized text why is the decoder important because the GPT output is also in token IDs so we need to convert it back to the sample text to to make sense of what the output is so the tokenizer class which we defined had two methods the encode method and the decode method finally we saw a problem that if some word is passed to the encoder which is not present in the vocabulary then the encoder throws up an error to avoid this we need to add special context or special text tokens to the vocabulary and uh one such special text token is unknown so if some word is encountered which is not known we add the unknown token uh the second special context text token which is also used in GPT is end of text so whenever there are multiple text sources when we feed the text to the encoder we can separate them with the end text tokens we later saw that many researchers used other to other special tokens also like beginning of sequence end of sequence padding Etc actually when GPT was trained it only used the end of text uh token to distinguish between the different text sources awesome so then we then we saw that with these Special tokens even if you are given a new sentence whose words are not known uh such as the quickly year which was not known it is just replaced by the token ID for the unknown token no error is shown up uh now we saw that uh GPT did not actually use the unknown token so let me take you to the end of this book end of this notebook so GPT models did not use the unknown token then you might be thinking then how did GPT models deal with Words which are not known they deal with Words which are not known by using something called bite pair encoding in bite pair en en in bite pair encoding every word is not a token words themselves are broken down into subwords and then these subwords are the token so let's say you have a word which is uh which is chased right in the vocabulary which we have developed this chased itself is one token but in bite pair encoding it might be possible that this chased itself is broken down into three sub tokens CH as this is just for example but the this is what I mean by subw tokens uh so subword tokens means breaking down one word itself into subwords and then using the subwords as the tokens uh so GPT models use a bite pair encoding tokenizer which breaks down words into subword units and we are going to cover that in a lot of detail in the next lecture so uh thank you so much everyone for uh sticking with me for this lecture and it's been a long lecture I think it's more than 1 hour but uh I think it is worth it I've not seen this much of a detailed treatment on tokenization in any of the lectures many of the lectures are toy lectures which means they just give you the basics but then don't show you things like end of text dealing with unknown words padding then pite pair encoding or some things which are actually used in llms and that is what I wanted to cover so my lecture style will be a mix of whiteboard writing on whiteboard and uh also showing you code here in Jupiter notebook I'll be sharing this code file with everyone so you can run this code and I highly encourage you to run this code on your own thank you so much everyone and I look forward to seeing you in the next lecture"
}