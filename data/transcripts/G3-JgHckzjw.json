{
  "video": {
    "video_id": "G3-JgHckzjw",
    "title": "Coding the 124 million parameter GPT-2 model",
    "duration": 3693.0,
    "index": 23
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.04,
      "duration": 8.32
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.48,
      "duration": 5.039
    },
    {
      "text": "in the build large language models from",
      "start": 8.36,
      "duration": 5.239
    },
    {
      "text": "scratch Series today what we are going",
      "start": 10.519,
      "duration": 5.561
    },
    {
      "text": "to do is we are going to construct the",
      "start": 13.599,
      "duration": 5.52
    },
    {
      "text": "entire GPT architecture or put in other",
      "start": 16.08,
      "duration": 5.119
    },
    {
      "text": "words we are going to code the entire",
      "start": 19.119,
      "duration": 4.881
    },
    {
      "text": "GPT model we have been building up to",
      "start": 21.199,
      "duration": 5.121
    },
    {
      "text": "this lecture for a pretty long time now",
      "start": 24.0,
      "duration": 4.84
    },
    {
      "text": "so let me just quickly recap what all",
      "start": 26.32,
      "duration": 3.959
    },
    {
      "text": "we'll be covering in today today's",
      "start": 28.84,
      "duration": 4.12
    },
    {
      "text": "lecture and what we have completed",
      "start": 30.279,
      "duration": 4.921
    },
    {
      "text": "previously so in the previous set of",
      "start": 32.96,
      "duration": 4.56
    },
    {
      "text": "lectures which started around four to",
      "start": 35.2,
      "duration": 4.96
    },
    {
      "text": "five lectures back in this GPT",
      "start": 37.52,
      "duration": 5.199
    },
    {
      "text": "architecture or llm architecture lecture",
      "start": 40.16,
      "duration": 5.559
    },
    {
      "text": "series we started out with a dummy GPT",
      "start": 42.719,
      "duration": 6.121
    },
    {
      "text": "model class so when we started around",
      "start": 45.719,
      "duration": 5.201
    },
    {
      "text": "four to five lectures back we did not",
      "start": 48.84,
      "duration": 5.64
    },
    {
      "text": "have H an understanding of the building",
      "start": 50.92,
      "duration": 6.04
    },
    {
      "text": "blocks of the llm architecture or the",
      "start": 54.48,
      "duration": 5.599
    },
    {
      "text": "GPT architecture slowly we worked our",
      "start": 56.96,
      "duration": 5.16
    },
    {
      "text": "way towards it initially we learned",
      "start": 60.079,
      "duration": 4.561
    },
    {
      "text": "about layer normalization we wrote code",
      "start": 62.12,
      "duration": 5.319
    },
    {
      "text": "for it then we wrote a code for the feed",
      "start": 64.64,
      "duration": 5.119
    },
    {
      "text": "forward neural network along with the J",
      "start": 67.439,
      "duration": 4.68
    },
    {
      "text": "activation then we learned about",
      "start": 69.759,
      "duration": 4.441
    },
    {
      "text": "shortcut connections and wrote code for",
      "start": 72.119,
      "duration": 4.481
    },
    {
      "text": "that and in the previous lecture we also",
      "start": 74.2,
      "duration": 3.959
    },
    {
      "text": "wrote the code for the entire",
      "start": 76.6,
      "duration": 3.879
    },
    {
      "text": "Transformer Block in which the layer",
      "start": 78.159,
      "duration": 4.801
    },
    {
      "text": "normalization the J activation the feed",
      "start": 80.479,
      "duration": 4.441
    },
    {
      "text": "forward neural network and the shortcut",
      "start": 82.96,
      "duration": 5.0
    },
    {
      "text": "connections all come together today what",
      "start": 84.92,
      "duration": 4.68
    },
    {
      "text": "we are going to do is we are going to",
      "start": 87.96,
      "duration": 3.68
    },
    {
      "text": "write code for this final piece of the",
      "start": 89.6,
      "duration": 4.479
    },
    {
      "text": "puzzle which is how Point number one",
      "start": 91.64,
      "duration": 5.4
    },
    {
      "text": "point number two 3 4 5 and six",
      "start": 94.079,
      "duration": 5.08
    },
    {
      "text": "essentially come together to form the",
      "start": 97.04,
      "duration": 3.6
    },
    {
      "text": "entire GPT",
      "start": 99.159,
      "duration": 4.28
    },
    {
      "text": "architecture throughout this these four",
      "start": 100.64,
      "duration": 5.439
    },
    {
      "text": "to five lectures on the GPT architecture",
      "start": 103.439,
      "duration": 5.04
    },
    {
      "text": "we have this or we should have this",
      "start": 106.079,
      "duration": 5.841
    },
    {
      "text": "visual figure in our mind so here is",
      "start": 108.479,
      "duration": 7.401
    },
    {
      "text": "what happens in the GPT architecture we",
      "start": 111.92,
      "duration": 7.28
    },
    {
      "text": "have text which gets tokenized we have",
      "start": 115.88,
      "duration": 7.159
    },
    {
      "text": "the dropout layer over here and then the",
      "start": 119.2,
      "duration": 6.559
    },
    {
      "text": "input embeddings are essentially passed",
      "start": 123.039,
      "duration": 4.841
    },
    {
      "text": "to this blue colored block which is",
      "start": 125.759,
      "duration": 4.64
    },
    {
      "text": "called as the Transformer Block in the",
      "start": 127.88,
      "duration": 4.56
    },
    {
      "text": "previous lecture we coded out all of",
      "start": 130.399,
      "duration": 3.881
    },
    {
      "text": "these individual Elements which I'm",
      "start": 132.44,
      "duration": 3.6
    },
    {
      "text": "marking in yellow right now in the",
      "start": 134.28,
      "duration": 4.36
    },
    {
      "text": "Transformer block and we saw that when",
      "start": 136.04,
      "duration": 4.6
    },
    {
      "text": "you give an input to the Transformer how",
      "start": 138.64,
      "duration": 3.879
    },
    {
      "text": "to code out all of these blocks and how",
      "start": 140.64,
      "duration": 4.08
    },
    {
      "text": "to stack them all together so that we",
      "start": 142.519,
      "duration": 3.201
    },
    {
      "text": "get the",
      "start": 144.72,
      "duration": 3.519
    },
    {
      "text": "output today we are also going to learn",
      "start": 145.72,
      "duration": 5.0
    },
    {
      "text": "about these final two steps which is",
      "start": 148.239,
      "duration": 4.841
    },
    {
      "text": "another layer normalization layer and",
      "start": 150.72,
      "duration": 4.519
    },
    {
      "text": "another linear output layer towards the",
      "start": 153.08,
      "duration": 5.28
    },
    {
      "text": "end and we are going to see how do we",
      "start": 155.239,
      "duration": 5.321
    },
    {
      "text": "postprocess the output from the",
      "start": 158.36,
      "duration": 4.92
    },
    {
      "text": "Transformer to get an output from the",
      "start": 160.56,
      "duration": 5.88
    },
    {
      "text": "entire GPT model that's the main goal",
      "start": 163.28,
      "duration": 6.52
    },
    {
      "text": "today so if you if you look at the",
      "start": 166.44,
      "duration": 5.36
    },
    {
      "text": "overall picture the mass multihead",
      "start": 169.8,
      "duration": 4.68
    },
    {
      "text": "attention is the key component of the",
      "start": 171.8,
      "duration": 5.2
    },
    {
      "text": "Transformer block and the Transformer",
      "start": 174.48,
      "duration": 4.64
    },
    {
      "text": "block is the key component of the entire",
      "start": 177.0,
      "duration": 3.599
    },
    {
      "text": "GPT model",
      "start": 179.12,
      "duration": 3.56
    },
    {
      "text": "we have coded out the Transformer block",
      "start": 180.599,
      "duration": 3.92
    },
    {
      "text": "right that's fine but now we have to",
      "start": 182.68,
      "duration": 4.32
    },
    {
      "text": "code out the entire GPT model so we'll",
      "start": 184.519,
      "duration": 4.481
    },
    {
      "text": "start from the we'll start from the",
      "start": 187.0,
      "duration": 7.599
    },
    {
      "text": "bottom we'll start from uh uh this input",
      "start": 189.0,
      "duration": 8.68
    },
    {
      "text": "IDs or rather input text we'll tokenize",
      "start": 194.599,
      "duration": 7.881
    },
    {
      "text": "it then we will uh integrate the Dropout",
      "start": 197.68,
      "duration": 6.76
    },
    {
      "text": "and then we'll integrate the Transformer",
      "start": 202.48,
      "duration": 3.56
    },
    {
      "text": "building block and then we'll also",
      "start": 204.44,
      "duration": 3.2
    },
    {
      "text": "integrate these final two building",
      "start": 206.04,
      "duration": 4.24
    },
    {
      "text": "blocks so you can think of this Le as an",
      "start": 207.64,
      "duration": 4.679
    },
    {
      "text": "assembly lecture where different moving",
      "start": 210.28,
      "duration": 4.319
    },
    {
      "text": "pieces of the GPT architecture will all",
      "start": 212.319,
      "duration": 5.041
    },
    {
      "text": "come together and ultimately you will",
      "start": 214.599,
      "duration": 4.84
    },
    {
      "text": "have a system in which or ultimately",
      "start": 217.36,
      "duration": 4.12
    },
    {
      "text": "you'll develop a model which receives an",
      "start": 219.439,
      "duration": 4.44
    },
    {
      "text": "input so the input is in the form of",
      "start": 221.48,
      "duration": 4.28
    },
    {
      "text": "text tokens and the output from the",
      "start": 223.879,
      "duration": 3.961
    },
    {
      "text": "model looks something like this right",
      "start": 225.76,
      "duration": 4.08
    },
    {
      "text": "now in the next lecture we are going to",
      "start": 227.84,
      "duration": 4.119
    },
    {
      "text": "see how to decode this output to predict",
      "start": 229.84,
      "duration": 4.319
    },
    {
      "text": "the next word so the main goal of the",
      "start": 231.959,
      "duration": 4.84
    },
    {
      "text": "GPT model is to take an input text such",
      "start": 234.159,
      "duration": 5.201
    },
    {
      "text": "as every effort moves you and to predict",
      "start": 236.799,
      "duration": 4.921
    },
    {
      "text": "the next word so we are training the",
      "start": 239.36,
      "duration": 4.879
    },
    {
      "text": "model for the next word prediction task",
      "start": 241.72,
      "duration": 4.68
    },
    {
      "text": "today we are going to take today I'm",
      "start": 244.239,
      "duration": 4.0
    },
    {
      "text": "going to take you all to this stage",
      "start": 246.4,
      "duration": 4.839
    },
    {
      "text": "where we get this final output and in",
      "start": 248.239,
      "duration": 4.961
    },
    {
      "text": "the next lecture we are going to see how",
      "start": 251.239,
      "duration": 4.441
    },
    {
      "text": "to get the next word from this final",
      "start": 253.2,
      "duration": 4.96
    },
    {
      "text": "tensor I'll assure you that the final",
      "start": 255.68,
      "duration": 4.64
    },
    {
      "text": "tensor which we will obtain today can be",
      "start": 258.16,
      "duration": 5.88
    },
    {
      "text": "easily used to decode the next word so",
      "start": 260.32,
      "duration": 5.84
    },
    {
      "text": "the main task today is how to go from",
      "start": 264.04,
      "duration": 4.24
    },
    {
      "text": "the Transformer output to this final",
      "start": 266.16,
      "duration": 4.879
    },
    {
      "text": "output tensor from the GP G PT model and",
      "start": 268.28,
      "duration": 4.759
    },
    {
      "text": "we are also going to take an Hands-On",
      "start": 271.039,
      "duration": 4.321
    },
    {
      "text": "example and show you how the operations",
      "start": 273.039,
      "duration": 5.281
    },
    {
      "text": "Stack Up throughout this entire GPT",
      "start": 275.36,
      "duration": 5.88
    },
    {
      "text": "model okay so when we started this",
      "start": 278.32,
      "duration": 5.56
    },
    {
      "text": "lecture series we actually started with",
      "start": 281.24,
      "duration": 5.2
    },
    {
      "text": "this dummy GPT model which I'm going to",
      "start": 283.88,
      "duration": 4.879
    },
    {
      "text": "show you right now so we started this",
      "start": 286.44,
      "duration": 5.16
    },
    {
      "text": "lecture series with this dummy GPT model",
      "start": 288.759,
      "duration": 5.641
    },
    {
      "text": "class right over here and at that time",
      "start": 291.6,
      "duration": 5.439
    },
    {
      "text": "we had left several aspects blank so we",
      "start": 294.4,
      "duration": 4.96
    },
    {
      "text": "had left the Transformer Transformer",
      "start": 297.039,
      "duration": 5.0
    },
    {
      "text": "block blank we had also left the layer",
      "start": 299.36,
      "duration": 4.839
    },
    {
      "text": "normalization class",
      "start": 302.039,
      "duration": 4.88
    },
    {
      "text": "blank we had coded out some aspects of",
      "start": 304.199,
      "duration": 5.481
    },
    {
      "text": "the forward method but most of this",
      "start": 306.919,
      "duration": 5.56
    },
    {
      "text": "these classes were blank um but that's",
      "start": 309.68,
      "duration": 4.519
    },
    {
      "text": "fine now we are ready to fill up these",
      "start": 312.479,
      "duration": 4.761
    },
    {
      "text": "different aspects so that we can return",
      "start": 314.199,
      "duration": 5.241
    },
    {
      "text": "the",
      "start": 317.24,
      "duration": 5.48
    },
    {
      "text": "output so ultimately we are going to see",
      "start": 319.44,
      "duration": 5.24
    },
    {
      "text": "this entire workflow to today we are",
      "start": 322.72,
      "duration": 4.479
    },
    {
      "text": "going to have the input IDs which will",
      "start": 324.68,
      "duration": 4.44
    },
    {
      "text": "be converted into token embeddings then",
      "start": 327.199,
      "duration": 4.041
    },
    {
      "text": "we'll add positional embeddings We'll",
      "start": 329.12,
      "duration": 4.32
    },
    {
      "text": "add the Dropout layer we'll pass the",
      "start": 331.24,
      "duration": 3.64
    },
    {
      "text": "output of the Dropout through the",
      "start": 333.44,
      "duration": 4.039
    },
    {
      "text": "Transformer block then the output from",
      "start": 334.88,
      "duration": 4.36
    },
    {
      "text": "the Transformer block will be passed",
      "start": 337.479,
      "duration": 3.28
    },
    {
      "text": "through another layer normalization",
      "start": 339.24,
      "duration": 3.56
    },
    {
      "text": "layer and then we'll finally have an",
      "start": 340.759,
      "duration": 4.481
    },
    {
      "text": "output head layer and we'll return the",
      "start": 342.8,
      "duration": 5.239
    },
    {
      "text": "Logics don't worry about understanding",
      "start": 345.24,
      "duration": 4.28
    },
    {
      "text": "all of this right now I'm going to",
      "start": 348.039,
      "duration": 3.28
    },
    {
      "text": "sequentially take you through each of",
      "start": 349.52,
      "duration": 4.799
    },
    {
      "text": "this step by step uh and explain to you",
      "start": 351.319,
      "duration": 5.641
    },
    {
      "text": "what exactly happens in each of these uh",
      "start": 354.319,
      "duration": 4.121
    },
    {
      "text": "these code",
      "start": 356.96,
      "duration": 3.519
    },
    {
      "text": "lines okay okay so what we are",
      "start": 358.44,
      "duration": 3.96
    },
    {
      "text": "essentially going to do is that since we",
      "start": 360.479,
      "duration": 3.801
    },
    {
      "text": "coded this Transformer block last time",
      "start": 362.4,
      "duration": 3.44
    },
    {
      "text": "we can replace this with the actual",
      "start": 364.28,
      "duration": 4.039
    },
    {
      "text": "Transformer code we are also coded this",
      "start": 365.84,
      "duration": 4.4
    },
    {
      "text": "layer normalization Block in one of the",
      "start": 368.319,
      "duration": 3.681
    },
    {
      "text": "previous lectures so we'll replace this",
      "start": 370.24,
      "duration": 4.239
    },
    {
      "text": "with the layer normalization",
      "start": 372.0,
      "duration": 5.88
    },
    {
      "text": "code okay so the goal now we are ready",
      "start": 374.479,
      "duration": 5.601
    },
    {
      "text": "to achieve the goal of today's lecture",
      "start": 377.88,
      "duration": 5.36
    },
    {
      "text": "which is to assemble a fully working",
      "start": 380.08,
      "duration": 4.76
    },
    {
      "text": "version of the",
      "start": 383.24,
      "duration": 4.959
    },
    {
      "text": "original uh 124 million parameter",
      "start": 384.84,
      "duration": 6.359
    },
    {
      "text": "version of gpt2",
      "start": 388.199,
      "duration": 4.801
    },
    {
      "text": "so we are going to do a pretty awesome",
      "start": 391.199,
      "duration": 4.72
    },
    {
      "text": "thing today we are going to take input",
      "start": 393.0,
      "duration": 5.24
    },
    {
      "text": "texts and then we are going to pass them",
      "start": 395.919,
      "duration": 4.801
    },
    {
      "text": "through this entire gpt2 architecture",
      "start": 398.24,
      "duration": 4.64
    },
    {
      "text": "which by the way has 124 million",
      "start": 400.72,
      "duration": 4.479
    },
    {
      "text": "parameters and then we'll get the output",
      "start": 402.88,
      "duration": 4.48
    },
    {
      "text": "answer and all of this we'll be doing on",
      "start": 405.199,
      "duration": 4.4
    },
    {
      "text": "our local computer I'll be sharing the",
      "start": 407.36,
      "duration": 3.88
    },
    {
      "text": "code with you so you'll be able to",
      "start": 409.599,
      "duration": 4.921
    },
    {
      "text": "execute it on your own end that's pretty",
      "start": 411.24,
      "duration": 5.56
    },
    {
      "text": "awesome right you'll be probably running",
      "start": 414.52,
      "duration": 4.64
    },
    {
      "text": "a large scale large language model for",
      "start": 416.8,
      "duration": 4.92
    },
    {
      "text": "the first time on your local machine so",
      "start": 419.16,
      "duration": 4.68
    },
    {
      "text": "you at this moment if you have followed",
      "start": 421.72,
      "duration": 4.36
    },
    {
      "text": "the previous lectures we are going to",
      "start": 423.84,
      "duration": 4.96
    },
    {
      "text": "assemble several components here so",
      "start": 426.08,
      "duration": 4.16
    },
    {
      "text": "people who have followed the previous",
      "start": 428.8,
      "duration": 3.28
    },
    {
      "text": "lectures this lecture is going to be",
      "start": 430.24,
      "duration": 4.16
    },
    {
      "text": "very enriching for you if you have come",
      "start": 432.08,
      "duration": 4.16
    },
    {
      "text": "to this lecture for the first time I've",
      "start": 434.4,
      "duration": 3.6
    },
    {
      "text": "designed it so that you can follow it",
      "start": 436.24,
      "duration": 4.679
    },
    {
      "text": "along but please understand that the",
      "start": 438.0,
      "duration": 4.44
    },
    {
      "text": "value which you will derive from this",
      "start": 440.919,
      "duration": 3.921
    },
    {
      "text": "lecture will be significantly higher",
      "start": 442.44,
      "duration": 4.0
    },
    {
      "text": "once you have also gone through the",
      "start": 444.84,
      "duration": 3.199
    },
    {
      "text": "previous",
      "start": 446.44,
      "duration": 5.039
    },
    {
      "text": "lectures okay uh I hope all of you have",
      "start": 448.039,
      "duration": 5.72
    },
    {
      "text": "this visual map in",
      "start": 451.479,
      "duration": 4.921
    },
    {
      "text": "mind we are going to do all of these",
      "start": 453.759,
      "duration": 4.56
    },
    {
      "text": "steps which are shown in this visual map",
      "start": 456.4,
      "duration": 4.44
    },
    {
      "text": "today and with actual parameters from",
      "start": 458.319,
      "duration": 5.041
    },
    {
      "text": "gpt2 we are going to use around 124",
      "start": 460.84,
      "duration": 5.16
    },
    {
      "text": "million parameters today before I dive",
      "start": 463.36,
      "duration": 5.32
    },
    {
      "text": "into code I want to first show you",
      "start": 466.0,
      "duration": 4.759
    },
    {
      "text": "everything on this whiteboard especially",
      "start": 468.68,
      "duration": 4.72
    },
    {
      "text": "in terms of Dimensions so that you get a",
      "start": 470.759,
      "duration": 5.081
    },
    {
      "text": "clear understanding of what is exactly",
      "start": 473.4,
      "duration": 4.04
    },
    {
      "text": "happening when we are going to move to",
      "start": 475.84,
      "duration": 4.039
    },
    {
      "text": "code I have seen that many students who",
      "start": 477.44,
      "duration": 4.12
    },
    {
      "text": "learn about large language models they",
      "start": 479.879,
      "duration": 3.16
    },
    {
      "text": "are very unclear about how the",
      "start": 481.56,
      "duration": 3.599
    },
    {
      "text": "dimensions work out so what I've",
      "start": 483.039,
      "duration": 3.641
    },
    {
      "text": "actually done is that I've have made",
      "start": 485.159,
      "duration": 3.961
    },
    {
      "text": "this uh flow map over here so I'm just",
      "start": 486.68,
      "duration": 4.6
    },
    {
      "text": "zooming out here right now we are going",
      "start": 489.12,
      "duration": 4.639
    },
    {
      "text": "to going to go through all of this in",
      "start": 491.28,
      "duration": 4.359
    },
    {
      "text": "just a moment but this is the flow map",
      "start": 493.759,
      "duration": 4.481
    },
    {
      "text": "which I'm going to teach you right now",
      "start": 495.639,
      "duration": 4.041
    },
    {
      "text": "and the blocks which you see on the",
      "start": 498.24,
      "duration": 2.959
    },
    {
      "text": "screen they are mostly there to",
      "start": 499.68,
      "duration": 4.04
    },
    {
      "text": "represent the dimensions of the input",
      "start": 501.199,
      "duration": 5.0
    },
    {
      "text": "the dimensions of the output so my goal",
      "start": 503.72,
      "duration": 5.08
    },
    {
      "text": "here is to visually convince you of what",
      "start": 506.199,
      "duration": 5.44
    },
    {
      "text": "exactly is happening in the GPT model so",
      "start": 508.8,
      "duration": 4.599
    },
    {
      "text": "that the code becomes significantly",
      "start": 511.639,
      "duration": 4.441
    },
    {
      "text": "easier the code for this GPT model is",
      "start": 513.399,
      "duration": 4.08
    },
    {
      "text": "actually pretty simple and",
      "start": 516.08,
      "duration": 3.439
    },
    {
      "text": "straightforward the only difficulty",
      "start": 517.479,
      "duration": 4.04
    },
    {
      "text": "which students face is that many tensors",
      "start": 519.519,
      "duration": 4.041
    },
    {
      "text": "come into the picture many dimensions",
      "start": 521.519,
      "duration": 3.721
    },
    {
      "text": "come into the picture and students get",
      "start": 523.56,
      "duration": 4.08
    },
    {
      "text": "confused as to they cannot visualize",
      "start": 525.24,
      "duration": 4.52
    },
    {
      "text": "what's going on and I've have not found",
      "start": 527.64,
      "duration": 3.96
    },
    {
      "text": "too many too much good material out",
      "start": 529.76,
      "duration": 3.36
    },
    {
      "text": "there which takes students through every",
      "start": 531.6,
      "duration": 4.56
    },
    {
      "text": "single step in the GPT model like this",
      "start": 533.12,
      "duration": 5.04
    },
    {
      "text": "uh even in the previous lecture we went",
      "start": 536.16,
      "duration": 3.679
    },
    {
      "text": "through the entire code but we did not",
      "start": 538.16,
      "duration": 4.44
    },
    {
      "text": "see this Hands-On example of let's say",
      "start": 539.839,
      "duration": 4.961
    },
    {
      "text": "if you take a specific input sequence",
      "start": 542.6,
      "duration": 4.0
    },
    {
      "text": "how does that input sequence flow",
      "start": 544.8,
      "duration": 4.32
    },
    {
      "text": "through the different blocks of the GPT",
      "start": 546.6,
      "duration": 4.799
    },
    {
      "text": "model and how do we get the output let's",
      "start": 549.12,
      "duration": 4.959
    },
    {
      "text": "dive into every single detail remember",
      "start": 551.399,
      "duration": 4.88
    },
    {
      "text": "the name of this whole playlist is",
      "start": 554.079,
      "duration": 4.921
    },
    {
      "text": "building llms from scratch we are not",
      "start": 556.279,
      "duration": 5.441
    },
    {
      "text": "going to assume anything I want to teach",
      "start": 559.0,
      "duration": 4.519
    },
    {
      "text": "you the nuts and bolts of how every",
      "start": 561.72,
      "duration": 4.119
    },
    {
      "text": "single line of code works and that's why",
      "start": 563.519,
      "duration": 4.241
    },
    {
      "text": "I have made this effort to construct",
      "start": 565.839,
      "duration": 5.081
    },
    {
      "text": "this visual flowchart okay so let's say",
      "start": 567.76,
      "duration": 5.92
    },
    {
      "text": "the input is every effort moves you",
      "start": 570.92,
      "duration": 4.919
    },
    {
      "text": "right and then we have to make the",
      "start": 573.68,
      "duration": 3.88
    },
    {
      "text": "output prediction which is the",
      "start": 575.839,
      "duration": 4.24
    },
    {
      "text": "prediction of the next word and the next",
      "start": 577.56,
      "duration": 4.44
    },
    {
      "text": "word is forward every effort moves you",
      "start": 580.079,
      "duration": 4.241
    },
    {
      "text": "forward so let's go through a sequence",
      "start": 582.0,
      "duration": 4.32
    },
    {
      "text": "of steps of what exactly happens in the",
      "start": 584.32,
      "duration": 4.68
    },
    {
      "text": "GPT model when this input is given I'm",
      "start": 586.32,
      "duration": 4.36
    },
    {
      "text": "going to switch color right now to a",
      "start": 589.0,
      "duration": 3.64
    },
    {
      "text": "darker color so that you all will see",
      "start": 590.68,
      "duration": 4.64
    },
    {
      "text": "what I'm writing on the board okay so",
      "start": 592.64,
      "duration": 5.72
    },
    {
      "text": "ideally inputs come in batches so what",
      "start": 595.32,
      "duration": 5.16
    },
    {
      "text": "we are going to do is is that let's say",
      "start": 598.36,
      "duration": 4.159
    },
    {
      "text": "when we go to code we'll see that we",
      "start": 600.48,
      "duration": 4.2
    },
    {
      "text": "have two batches and in each batch there",
      "start": 602.519,
      "duration": 5.521
    },
    {
      "text": "are four tokens so the first batch has",
      "start": 604.68,
      "duration": 5.719
    },
    {
      "text": "every effort moves you and the four",
      "start": 608.04,
      "duration": 4.84
    },
    {
      "text": "token IDs corresponding to that and the",
      "start": 610.399,
      "duration": 5.481
    },
    {
      "text": "second batch has token IDs corresponding",
      "start": 612.88,
      "duration": 5.72
    },
    {
      "text": "to the second sentence for the sake of",
      "start": 615.88,
      "duration": 5.48
    },
    {
      "text": "Simplicity right now I'm just going to",
      "start": 618.6,
      "duration": 5.239
    },
    {
      "text": "analyze the first batch and then the",
      "start": 621.36,
      "duration": 4.039
    },
    {
      "text": "same learnings which I'm going to show",
      "start": 623.839,
      "duration": 3.641
    },
    {
      "text": "you can be applied to the second batch",
      "start": 625.399,
      "duration": 4.961
    },
    {
      "text": "as well uh so I'm going I'm getting rid",
      "start": 627.48,
      "duration": 4.479
    },
    {
      "text": "of one dimension which is the batch",
      "start": 630.36,
      "duration": 3.32
    },
    {
      "text": "dimension for now for the sake of",
      "start": 631.959,
      "duration": 4.081
    },
    {
      "text": "simplicity so we are going to focus only",
      "start": 633.68,
      "duration": 4.32
    },
    {
      "text": "on these four words every effort moves",
      "start": 636.04,
      "duration": 4.28
    },
    {
      "text": "you and prediction of the next World the",
      "start": 638.0,
      "duration": 4.639
    },
    {
      "text": "first step is that remember that we have",
      "start": 640.32,
      "duration": 5.12
    },
    {
      "text": "a vocabulary right even when gp22 model",
      "start": 642.639,
      "duration": 5.161
    },
    {
      "text": "was constructed we have vocabulary and",
      "start": 645.44,
      "duration": 6.76
    },
    {
      "text": "the vocabulary size is five U",
      "start": 647.8,
      "duration": 7.039
    },
    {
      "text": "5 I think it is",
      "start": 652.2,
      "duration": 6.48
    },
    {
      "text": "50257 so 50257 is the vocabulary size",
      "start": 654.839,
      "duration": 6.56
    },
    {
      "text": "for GP 2 so the first step is you take",
      "start": 658.68,
      "duration": 5.399
    },
    {
      "text": "every word and you map it to a token ID",
      "start": 661.399,
      "duration": 4.761
    },
    {
      "text": "in the vocabulary every token is mapped",
      "start": 664.079,
      "duration": 5.041
    },
    {
      "text": "to a token ID so these are the four",
      "start": 666.16,
      "duration": 5.44
    },
    {
      "text": "tokens for the sake of Simplicity think",
      "start": 669.12,
      "duration": 4.839
    },
    {
      "text": "of every one token equal to one word",
      "start": 671.6,
      "duration": 3.96
    },
    {
      "text": "that's not at all what's happening in",
      "start": 673.959,
      "duration": 4.601
    },
    {
      "text": "the gpt2 because gpt2 uses bite pair",
      "start": 675.56,
      "duration": 5.76
    },
    {
      "text": "encoding which is a subword tokenizer we",
      "start": 678.56,
      "duration": 5.12
    },
    {
      "text": "have a separate lecture for that uh",
      "start": 681.32,
      "duration": 4.36
    },
    {
      "text": "where you can see that even characters",
      "start": 683.68,
      "duration": 5.64
    },
    {
      "text": "and small subwords can be tokens but",
      "start": 685.68,
      "duration": 5.44
    },
    {
      "text": "just for the sake of Simplicity I'm",
      "start": 689.32,
      "duration": 3.959
    },
    {
      "text": "going to use one token equal to one word",
      "start": 691.12,
      "duration": 4.68
    },
    {
      "text": "interchangeably in today's",
      "start": 693.279,
      "duration": 5.321
    },
    {
      "text": "lecture okay so we are looking at this",
      "start": 695.8,
      "duration": 4.479
    },
    {
      "text": "first batch which has four tokens or",
      "start": 698.6,
      "duration": 3.52
    },
    {
      "text": "four words that's the first step to",
      "start": 700.279,
      "duration": 3.68
    },
    {
      "text": "convert these tokens into these four",
      "start": 702.12,
      "duration": 4.36
    },
    {
      "text": "token IDs awesome the next step is",
      "start": 703.959,
      "duration": 4.521
    },
    {
      "text": "actually to take these token IDs and to",
      "start": 706.48,
      "duration": 4.28
    },
    {
      "text": "convert them into token embedding",
      "start": 708.48,
      "duration": 4.799
    },
    {
      "text": "vectors so this is a key Point here",
      "start": 710.76,
      "duration": 5.04
    },
    {
      "text": "computers can't understand words right",
      "start": 713.279,
      "duration": 4.68
    },
    {
      "text": "and it does not make sense to just have",
      "start": 715.8,
      "duration": 4.36
    },
    {
      "text": "token IDs because because we need to",
      "start": 717.959,
      "duration": 4.641
    },
    {
      "text": "capture the meaning between words dog",
      "start": 720.16,
      "duration": 4.52
    },
    {
      "text": "and puppy are close to each other cat",
      "start": 722.6,
      "duration": 4.32
    },
    {
      "text": "and kitten are closer to each other it",
      "start": 724.68,
      "duration": 4.64
    },
    {
      "text": "turns out that representing words in a",
      "start": 726.92,
      "duration": 5.64
    },
    {
      "text": "vectorial format can help preserve the",
      "start": 729.32,
      "duration": 5.519
    },
    {
      "text": "semantic meaning between words so the",
      "start": 732.56,
      "duration": 4.519
    },
    {
      "text": "first step is to convert every input",
      "start": 734.839,
      "duration": 4.841
    },
    {
      "text": "token into this token embedding vector",
      "start": 737.079,
      "duration": 4.921
    },
    {
      "text": "and to decide an embedding size so we",
      "start": 739.68,
      "duration": 4.399
    },
    {
      "text": "are using an embedding size of",
      "start": 742.0,
      "duration": 4.68
    },
    {
      "text": "768 because that was the embedding size",
      "start": 744.079,
      "duration": 5.12
    },
    {
      "text": "which was used for the smallest gpt2",
      "start": 746.68,
      "duration": 5.519
    },
    {
      "text": "model when it came out so if you see",
      "start": 749.199,
      "duration": 5.281
    },
    {
      "text": "every which is the first token is now",
      "start": 752.199,
      "duration": 5.2
    },
    {
      "text": "encoded as a 768 Dimension Vector over",
      "start": 754.48,
      "duration": 5.84
    },
    {
      "text": "here effort which is the second token is",
      "start": 757.399,
      "duration": 5.921
    },
    {
      "text": "also encoded as a 768 dimensional Vector",
      "start": 760.32,
      "duration": 5.12
    },
    {
      "text": "moves which is the third token is",
      "start": 763.32,
      "duration": 4.959
    },
    {
      "text": "encoded as a 768 dimensional vector and",
      "start": 765.44,
      "duration": 5.04
    },
    {
      "text": "U which is the fourth token is also",
      "start": 768.279,
      "duration": 5.401
    },
    {
      "text": "encoded as a 768 dimensional Vector one",
      "start": 770.48,
      "duration": 4.799
    },
    {
      "text": "point which I want to mention here is",
      "start": 773.68,
      "duration": 4.12
    },
    {
      "text": "that this encoding we do not know what's",
      "start": 775.279,
      "duration": 5.081
    },
    {
      "text": "the best encoding from the start we are",
      "start": 777.8,
      "duration": 4.479
    },
    {
      "text": "initially going to project these vectors",
      "start": 780.36,
      "duration": 4.919
    },
    {
      "text": "randomly in the 768 dimensional space",
      "start": 782.279,
      "duration": 4.881
    },
    {
      "text": "and then we are also going to learn the",
      "start": 785.279,
      "duration": 5.201
    },
    {
      "text": "token embedding parameters in GPT models",
      "start": 787.16,
      "duration": 5.119
    },
    {
      "text": "along with everything else the token",
      "start": 790.48,
      "duration": 3.52
    },
    {
      "text": "embedding all the embedding parameters",
      "start": 792.279,
      "duration": 4.481
    },
    {
      "text": "are learned so right now let's say I",
      "start": 794.0,
      "duration": 4.519
    },
    {
      "text": "told you about cat and kitten right when",
      "start": 796.76,
      "duration": 4.16
    },
    {
      "text": "we start out the vector for cat and the",
      "start": 798.519,
      "duration": 4.041
    },
    {
      "text": "vector for kitten is initialized in",
      "start": 800.92,
      "duration": 3.919
    },
    {
      "text": "random directions but when the model is",
      "start": 802.56,
      "duration": 3.719
    },
    {
      "text": "trained when the embedding vectors are",
      "start": 804.839,
      "duration": 2.961
    },
    {
      "text": "trained they will be closer together",
      "start": 806.279,
      "duration": 4.8
    },
    {
      "text": "they'll be more aligned so right now for",
      "start": 807.8,
      "duration": 6.92
    },
    {
      "text": "every U for the token every for the",
      "start": 811.079,
      "duration": 5.76
    },
    {
      "text": "token effort for the token moves and for",
      "start": 814.72,
      "duration": 5.16
    },
    {
      "text": "the token U we randomly initialize",
      "start": 816.839,
      "duration": 5.761
    },
    {
      "text": "vectors in the 768 dimensional space",
      "start": 819.88,
      "duration": 5.079
    },
    {
      "text": "that's the first step token embedding",
      "start": 822.6,
      "duration": 4.64
    },
    {
      "text": "the second step is that remember along",
      "start": 824.959,
      "duration": 4.281
    },
    {
      "text": "with the semantic meaning of words",
      "start": 827.24,
      "duration": 3.88
    },
    {
      "text": "what's also important to capture is",
      "start": 829.24,
      "duration": 3.64
    },
    {
      "text": "where the word comes in the particular",
      "start": 831.12,
      "duration": 4.76
    },
    {
      "text": "sentence so every comes so every effort",
      "start": 832.88,
      "duration": 4.8
    },
    {
      "text": "moves you and every comes in position",
      "start": 835.88,
      "duration": 4.56
    },
    {
      "text": "one effort comes in position two moves",
      "start": 837.68,
      "duration": 5.079
    },
    {
      "text": "comes in position three and U comes in",
      "start": 840.44,
      "duration": 4.399
    },
    {
      "text": "position number four so along with",
      "start": 842.759,
      "duration": 4.32
    },
    {
      "text": "representing the words themselves as an",
      "start": 844.839,
      "duration": 4.761
    },
    {
      "text": "embedding Vector we also represent every",
      "start": 847.079,
      "duration": 4.721
    },
    {
      "text": "position as an embedding Vector so we",
      "start": 849.6,
      "duration": 4.039
    },
    {
      "text": "are considering four positions here",
      "start": 851.8,
      "duration": 3.8
    },
    {
      "text": "right which also becomes the context",
      "start": 853.639,
      "duration": 4.361
    },
    {
      "text": "size remember the context size is the",
      "start": 855.6,
      "duration": 4.799
    },
    {
      "text": "maximum number of words which can be",
      "start": 858.0,
      "duration": 4.6
    },
    {
      "text": "used to predict the next word in our",
      "start": 860.399,
      "duration": 4.44
    },
    {
      "text": "case the context size is equal to four",
      "start": 862.6,
      "duration": 4.599
    },
    {
      "text": "which means only four positions matter",
      "start": 864.839,
      "duration": 3.601
    },
    {
      "text": "so that's why in the positional",
      "start": 867.199,
      "duration": 3.841
    },
    {
      "text": "embedding we we are going to look at the",
      "start": 868.44,
      "duration": 4.56
    },
    {
      "text": "embedding vectors for four positions",
      "start": 871.04,
      "duration": 4.4
    },
    {
      "text": "position number one has again a 768",
      "start": 873.0,
      "duration": 3.759
    },
    {
      "text": "dimensional",
      "start": 875.44,
      "duration": 4.8
    },
    {
      "text": "Vector position number two has a 768",
      "start": 876.759,
      "duration": 5.961
    },
    {
      "text": "dimensional Vector position number three",
      "start": 880.24,
      "duration": 4.959
    },
    {
      "text": "has a 768 dimensional vector and",
      "start": 882.72,
      "duration": 4.6
    },
    {
      "text": "position number four has a 768",
      "start": 885.199,
      "duration": 4.32
    },
    {
      "text": "dimensional Vector similar to token",
      "start": 887.32,
      "duration": 4.6
    },
    {
      "text": "embedding we actually do not know the",
      "start": 889.519,
      "duration": 5.921
    },
    {
      "text": "embedding values in each of these uh um",
      "start": 891.92,
      "duration": 5.039
    },
    {
      "text": "in each of these positional embedding",
      "start": 895.44,
      "duration": 3.12
    },
    {
      "text": "vectors these embedding values are",
      "start": 896.959,
      "duration": 3.24
    },
    {
      "text": "initialized",
      "start": 898.56,
      "duration": 3.8
    },
    {
      "text": "randomly initially we do not know what",
      "start": 900.199,
      "duration": 4.041
    },
    {
      "text": "these embedding values represent these",
      "start": 902.36,
      "duration": 4.36
    },
    {
      "text": "will be trained as the during the",
      "start": 904.24,
      "duration": 4.64
    },
    {
      "text": "training procedure but the important",
      "start": 906.72,
      "duration": 4.2
    },
    {
      "text": "thing to note is the embedding size the",
      "start": 908.88,
      "duration": 4.04
    },
    {
      "text": "embedding size for every Vector in the",
      "start": 910.92,
      "duration": 4.599
    },
    {
      "text": "positional embedding is 768 and this is",
      "start": 912.92,
      "duration": 6.52
    },
    {
      "text": "the same size as the uh token embedding",
      "start": 915.519,
      "duration": 6.641
    },
    {
      "text": "and the reason for this is in the third",
      "start": 919.44,
      "duration": 4.44
    },
    {
      "text": "step what we are going to do is that we",
      "start": 922.16,
      "duration": 4.359
    },
    {
      "text": "are going to add the token embedding for",
      "start": 923.88,
      "duration": 4.319
    },
    {
      "text": "each token along with the positional",
      "start": 926.519,
      "duration": 4.68
    },
    {
      "text": "embedding so if you go to step number",
      "start": 928.199,
      "duration": 4.841
    },
    {
      "text": "three which is seen on the screen right",
      "start": 931.199,
      "duration": 5.041
    },
    {
      "text": "now let me zoom in further for the first",
      "start": 933.04,
      "duration": 6.2
    },
    {
      "text": "word which is every it's in position one",
      "start": 936.24,
      "duration": 4.44
    },
    {
      "text": "so we are going to take the token",
      "start": 939.24,
      "duration": 3.599
    },
    {
      "text": "embedding for every and we are going to",
      "start": 940.68,
      "duration": 5.0
    },
    {
      "text": "add it uh with the positional embedding",
      "start": 942.839,
      "duration": 5.92
    },
    {
      "text": "for the first position and the result is",
      "start": 945.68,
      "duration": 6.2
    },
    {
      "text": "the input embedding for the first token",
      "start": 948.759,
      "duration": 4.601
    },
    {
      "text": "so since the token embedding and",
      "start": 951.88,
      "duration": 2.879
    },
    {
      "text": "positional embedding have the same",
      "start": 953.36,
      "duration": 3.24
    },
    {
      "text": "dimensions the input embedding also has",
      "start": 954.759,
      "duration": 4.681
    },
    {
      "text": "the dimension of 768 so this is the",
      "start": 956.6,
      "duration": 6.88
    },
    {
      "text": "input embedding for position then effort",
      "start": 959.44,
      "duration": 5.839
    },
    {
      "text": "so when you come to effort which is the",
      "start": 963.48,
      "duration": 3.52
    },
    {
      "text": "second word it's in second position so",
      "start": 965.279,
      "duration": 4.56
    },
    {
      "text": "we take the token embedding uh for the",
      "start": 967.0,
      "duration": 4.519
    },
    {
      "text": "second word and we add it with the",
      "start": 969.839,
      "duration": 3.36
    },
    {
      "text": "second positional embedding for the",
      "start": 971.519,
      "duration": 3.841
    },
    {
      "text": "second position and we get the input",
      "start": 973.199,
      "duration": 4.921
    },
    {
      "text": "embedding for the second token which is",
      "start": 975.36,
      "duration": 4.56
    },
    {
      "text": "effort and here again the embedding size",
      "start": 978.12,
      "duration": 4.04
    },
    {
      "text": "is equal to",
      "start": 979.92,
      "duration": 4.599
    },
    {
      "text": "768 you can see over here which I'm",
      "start": 982.16,
      "duration": 5.119
    },
    {
      "text": "marking right now in purple color the",
      "start": 984.519,
      "duration": 5.281
    },
    {
      "text": "embedding size for the",
      "start": 987.279,
      "duration": 6.201
    },
    {
      "text": "uh second position or the second the",
      "start": 989.8,
      "duration": 6.039
    },
    {
      "text": "input embedding size for the second",
      "start": 993.48,
      "duration": 4.839
    },
    {
      "text": "token is 768 Now we move to the third",
      "start": 995.839,
      "duration": 4.92
    },
    {
      "text": "token which is moves this is in position",
      "start": 998.319,
      "duration": 4.601
    },
    {
      "text": "three so we'll take the token and add",
      "start": 1000.759,
      "duration": 4.161
    },
    {
      "text": "the positional embedding Vector for",
      "start": 1002.92,
      "duration": 4.2
    },
    {
      "text": "position three and that leads to input",
      "start": 1004.92,
      "duration": 5.08
    },
    {
      "text": "embedding Vector for moves which has an",
      "start": 1007.12,
      "duration": 5.56
    },
    {
      "text": "embedding size 768 similarly for the",
      "start": 1010.0,
      "duration": 4.839
    },
    {
      "text": "fourth position which is U we take the",
      "start": 1012.68,
      "duration": 3.8
    },
    {
      "text": "token embedding and add the positional",
      "start": 1014.839,
      "duration": 3.721
    },
    {
      "text": "embedding and we get the input embedding",
      "start": 1016.48,
      "duration": 4.0
    },
    {
      "text": "which has the size of",
      "start": 1018.56,
      "duration": 5.0
    },
    {
      "text": "768 this is Step number three remember",
      "start": 1020.48,
      "duration": 5.079
    },
    {
      "text": "this is a very important step token",
      "start": 1023.56,
      "duration": 3.639
    },
    {
      "text": "embeddings plus positional embeddings",
      "start": 1025.559,
      "duration": 4.48
    },
    {
      "text": "leads to the input embeddings which this",
      "start": 1027.199,
      "duration": 4.681
    },
    {
      "text": "formula I have also written over here",
      "start": 1030.039,
      "duration": 5.121
    },
    {
      "text": "input embedding equal to token",
      "start": 1031.88,
      "duration": 6.52
    },
    {
      "text": "embedding uh yeah input embedding equal",
      "start": 1035.16,
      "duration": 7.08
    },
    {
      "text": "to token embedding plus the positional",
      "start": 1038.4,
      "duration": 6.679
    },
    {
      "text": "embedding okay so that's the step number",
      "start": 1042.24,
      "duration": 4.52
    },
    {
      "text": "three and after these steps are",
      "start": 1045.079,
      "duration": 4.001
    },
    {
      "text": "completed we move to step number four",
      "start": 1046.76,
      "duration": 4.919
    },
    {
      "text": "step number four introduces Dropout so",
      "start": 1049.08,
      "duration": 5.08
    },
    {
      "text": "what happens in Dropout is that until",
      "start": 1051.679,
      "duration": 4.281
    },
    {
      "text": "now we have input embeddings for every",
      "start": 1054.16,
      "duration": 3.84
    },
    {
      "text": "word right we have input embeddings for",
      "start": 1055.96,
      "duration": 3.599
    },
    {
      "text": "every",
      "start": 1058.0,
      "duration": 4.2
    },
    {
      "text": "effort uh",
      "start": 1059.559,
      "duration": 6.321
    },
    {
      "text": "moves and U and the embedding size is a",
      "start": 1062.2,
      "duration": 5.32
    },
    {
      "text": "vector so which means that for every",
      "start": 1065.88,
      "duration": 4.28
    },
    {
      "text": "token we have uh the input embedding of",
      "start": 1067.52,
      "duration": 6.08
    },
    {
      "text": "768 sized Vector in Dropout what happens",
      "start": 1070.16,
      "duration": 5.759
    },
    {
      "text": "is that we randomly turn off some",
      "start": 1073.6,
      "duration": 4.84
    },
    {
      "text": "elements of every uh every input",
      "start": 1075.919,
      "duration": 4.721
    },
    {
      "text": "embedding to zero and that's specified",
      "start": 1078.44,
      "duration": 4.359
    },
    {
      "text": "by the dropout rate so if the dropout",
      "start": 1080.64,
      "duration": 5.279
    },
    {
      "text": "rate is 50% from every embedding",
      "start": 1082.799,
      "duration": 5.88
    },
    {
      "text": "randomly 50% of the elements are turned",
      "start": 1085.919,
      "duration": 4.64
    },
    {
      "text": "off to zero so let's say this might be",
      "start": 1088.679,
      "duration": 4.841
    },
    {
      "text": "turned off this might be turned off so",
      "start": 1090.559,
      "duration": 7.0
    },
    {
      "text": "50% of 768 is around 384 right so around",
      "start": 1093.52,
      "duration": 7.56
    },
    {
      "text": "384 elements of each input embedding are",
      "start": 1097.559,
      "duration": 5.521
    },
    {
      "text": "turned off to zero and this is done for",
      "start": 1101.08,
      "duration": 4.52
    },
    {
      "text": "every token so here I'm just showing the",
      "start": 1103.08,
      "duration": 4.4
    },
    {
      "text": "random Elements which are turned off to",
      "start": 1105.6,
      "duration": 4.24
    },
    {
      "text": "zero remember this is probabilistic so",
      "start": 1107.48,
      "duration": 6.079
    },
    {
      "text": "when I say 50% not exactly half of the",
      "start": 1109.84,
      "duration": 5.839
    },
    {
      "text": "embeddings will be turned off to zero on",
      "start": 1113.559,
      "duration": 4.721
    },
    {
      "text": "an average 50% of the input embeddings",
      "start": 1115.679,
      "duration": 6.201
    },
    {
      "text": "will be turned to zero okay so why is",
      "start": 1118.28,
      "duration": 5.84
    },
    {
      "text": "Dropout implemented the main reason why",
      "start": 1121.88,
      "duration": 4.279
    },
    {
      "text": "Dropout is implemented is to prevent",
      "start": 1124.12,
      "duration": 4.12
    },
    {
      "text": "overfitting improve",
      "start": 1126.159,
      "duration": 4.801
    },
    {
      "text": "generalization uh and this generally",
      "start": 1128.24,
      "duration": 5.0
    },
    {
      "text": "helps a lot the main Dropout technique",
      "start": 1130.96,
      "duration": 3.839
    },
    {
      "text": "was initially implemented in neural",
      "start": 1133.24,
      "duration": 3.72
    },
    {
      "text": "networks to prevent some neurons from",
      "start": 1134.799,
      "duration": 4.721
    },
    {
      "text": "being lazy so during training sometimes",
      "start": 1136.96,
      "duration": 4.32
    },
    {
      "text": "what happens is that some neurons don't",
      "start": 1139.52,
      "duration": 4.12
    },
    {
      "text": "learn anything and they depend on other",
      "start": 1141.28,
      "duration": 4.96
    },
    {
      "text": "neurons and that leads to problems in",
      "start": 1143.64,
      "duration": 4.88
    },
    {
      "text": "generalization so what people do is that",
      "start": 1146.24,
      "duration": 4.88
    },
    {
      "text": "they Implement Dropout layers where",
      "start": 1148.52,
      "duration": 4.76
    },
    {
      "text": "neurons are turned off randomly so the",
      "start": 1151.12,
      "duration": 4.24
    },
    {
      "text": "neurons which were lazy earlier have no",
      "start": 1153.28,
      "duration": 4.519
    },
    {
      "text": "choice but to learn something and that",
      "start": 1155.36,
      "duration": 4.319
    },
    {
      "text": "improves the generalization performance",
      "start": 1157.799,
      "duration": 3.481
    },
    {
      "text": "because every neuron is generally trying",
      "start": 1159.679,
      "duration": 3.561
    },
    {
      "text": "to learn something and that's the",
      "start": 1161.28,
      "duration": 3.96
    },
    {
      "text": "similar case for year also wherever",
      "start": 1163.24,
      "duration": 4.4
    },
    {
      "text": "Dropout is implemented the main reason",
      "start": 1165.24,
      "duration": 4.84
    },
    {
      "text": "for implementing Dropout is to prevent",
      "start": 1167.64,
      "duration": 5.279
    },
    {
      "text": "overfitting or to improve generalization",
      "start": 1170.08,
      "duration": 5.32
    },
    {
      "text": "performance okay so we have seen four",
      "start": 1172.919,
      "duration": 4.841
    },
    {
      "text": "steps up till now let's recap them the",
      "start": 1175.4,
      "duration": 6.24
    },
    {
      "text": "first step is uh token embedding uh",
      "start": 1177.76,
      "duration": 6.2
    },
    {
      "text": "which we saw the second step is",
      "start": 1181.64,
      "duration": 4.88
    },
    {
      "text": "positional embedding which we again saw",
      "start": 1183.96,
      "duration": 5.36
    },
    {
      "text": "the third step is uh input embedding",
      "start": 1186.52,
      "duration": 4.32
    },
    {
      "text": "which is essentially adding the token",
      "start": 1189.32,
      "duration": 3.8
    },
    {
      "text": "embedding plus the positional embedding",
      "start": 1190.84,
      "duration": 4.68
    },
    {
      "text": "and the fourth step is implementing",
      "start": 1193.12,
      "duration": 4.76
    },
    {
      "text": "Dropout now remember up till here the",
      "start": 1195.52,
      "duration": 3.84
    },
    {
      "text": "Transformer block has not been",
      "start": 1197.88,
      "duration": 3.799
    },
    {
      "text": "introduced at all we have still we are",
      "start": 1199.36,
      "duration": 4.16
    },
    {
      "text": "still outside the Transformer block so",
      "start": 1201.679,
      "duration": 4.041
    },
    {
      "text": "let's go to this overall structure of",
      "start": 1203.52,
      "duration": 3.96
    },
    {
      "text": "this Transformer block again to see what",
      "start": 1205.72,
      "duration": 3.92
    },
    {
      "text": "all we have seen up till now so if you",
      "start": 1207.48,
      "duration": 5.559
    },
    {
      "text": "look at this structure until now um let",
      "start": 1209.64,
      "duration": 5.64
    },
    {
      "text": "me zoom in further yeah if you look at",
      "start": 1213.039,
      "duration": 4.081
    },
    {
      "text": "the structure until now we have seen the",
      "start": 1215.28,
      "duration": 4.08
    },
    {
      "text": "four steps which come before here so we",
      "start": 1217.12,
      "duration": 5.039
    },
    {
      "text": "tokenize the text into input IDs then we",
      "start": 1219.36,
      "duration": 4.72
    },
    {
      "text": "add the to then we have the token",
      "start": 1222.159,
      "duration": 3.921
    },
    {
      "text": "embeddings which was the first step then",
      "start": 1224.08,
      "duration": 3.76
    },
    {
      "text": "we add the positional embeddings which",
      "start": 1226.08,
      "duration": 3.36
    },
    {
      "text": "was the second and third steps we get",
      "start": 1227.84,
      "duration": 3.6
    },
    {
      "text": "the input embedding and then we apply",
      "start": 1229.44,
      "duration": 4.52
    },
    {
      "text": "the Dropout so we have seen these four",
      "start": 1231.44,
      "duration": 5.08
    },
    {
      "text": "steps until now we are at this point and",
      "start": 1233.96,
      "duration": 4.64
    },
    {
      "text": "after the Dropout now we will enter the",
      "start": 1236.52,
      "duration": 3.88
    },
    {
      "text": "Transformer Block in which all of these",
      "start": 1238.6,
      "duration": 4.12
    },
    {
      "text": "steps will be performed so let's go to",
      "start": 1240.4,
      "duration": 4.159
    },
    {
      "text": "step number five right now where we'll",
      "start": 1242.72,
      "duration": 3.959
    },
    {
      "text": "be looking at the Transformer",
      "start": 1244.559,
      "duration": 4.561
    },
    {
      "text": "block okay so when we reach the",
      "start": 1246.679,
      "duration": 5.801
    },
    {
      "text": "Transformer this these are the input",
      "start": 1249.12,
      "duration": 5.08
    },
    {
      "text": "embeddings which we have so these are",
      "start": 1252.48,
      "duration": 3.84
    },
    {
      "text": "the input embeddings with Dropout one",
      "start": 1254.2,
      "duration": 3.68
    },
    {
      "text": "thing which I would like you to see is",
      "start": 1256.32,
      "duration": 3.92
    },
    {
      "text": "that the dimensions are being preserved",
      "start": 1257.88,
      "duration": 4.12
    },
    {
      "text": "so when we started this at the first",
      "start": 1260.24,
      "duration": 3.799
    },
    {
      "text": "step these were the same dimensions",
      "start": 1262.0,
      "duration": 3.679
    },
    {
      "text": "right every token had the dimen",
      "start": 1264.039,
      "duration": 3.12
    },
    {
      "text": "embedding dimension of",
      "start": 1265.679,
      "duration": 4.321
    },
    {
      "text": "768 and when we enter the enter the",
      "start": 1267.159,
      "duration": 6.52
    },
    {
      "text": "Transformer block every is still a 768",
      "start": 1270.0,
      "duration": 6.039
    },
    {
      "text": "dimensional Vector effort is still a",
      "start": 1273.679,
      "duration": 5.281
    },
    {
      "text": "7608 dimensional Vector moves is still a",
      "start": 1276.039,
      "duration": 5.441
    },
    {
      "text": "7608 dimensional vector and U is still a",
      "start": 1278.96,
      "duration": 5.36
    },
    {
      "text": "7608 dimensional Vector the one thing",
      "start": 1281.48,
      "duration": 4.84
    },
    {
      "text": "which has not happened is that until now",
      "start": 1284.32,
      "duration": 3.92
    },
    {
      "text": "every Vector only contains meaning about",
      "start": 1286.32,
      "duration": 2.92
    },
    {
      "text": "itself",
      "start": 1288.24,
      "duration": 2.72
    },
    {
      "text": "but we do not know let's say when we are",
      "start": 1289.24,
      "duration": 4.28
    },
    {
      "text": "looking at every how much information",
      "start": 1290.96,
      "duration": 4.52
    },
    {
      "text": "how much attention should be we give to",
      "start": 1293.52,
      "duration": 4.44
    },
    {
      "text": "effort moves and you to predict the next",
      "start": 1295.48,
      "duration": 4.799
    },
    {
      "text": "world that's very important right along",
      "start": 1297.96,
      "duration": 4.24
    },
    {
      "text": "with capturing the semantic meaning",
      "start": 1300.279,
      "duration": 4.081
    },
    {
      "text": "which Vector embedding does it does not",
      "start": 1302.2,
      "duration": 4.56
    },
    {
      "text": "capture the meaning of how every how",
      "start": 1304.36,
      "duration": 4.4
    },
    {
      "text": "let's say each word is related to other",
      "start": 1306.76,
      "duration": 4.44
    },
    {
      "text": "words so let's say when we look at",
      "start": 1308.76,
      "duration": 5.0
    },
    {
      "text": "effort and we want to see when we are",
      "start": 1311.2,
      "duration": 4.24
    },
    {
      "text": "looking at effort how much attention",
      "start": 1313.76,
      "duration": 4.48
    },
    {
      "text": "should we pay to every moves and you",
      "start": 1315.44,
      "duration": 4.76
    },
    {
      "text": "when predicting the next word that",
      "start": 1318.24,
      "duration": 4.08
    },
    {
      "text": "information is not captured and that",
      "start": 1320.2,
      "duration": 4.24
    },
    {
      "text": "will be done through the attention Block",
      "start": 1322.32,
      "duration": 4.719
    },
    {
      "text": "in the Transformer module let's see when",
      "start": 1324.44,
      "duration": 5.0
    },
    {
      "text": "we get to that so now we are at the",
      "start": 1327.039,
      "duration": 4.041
    },
    {
      "text": "stage where we have the input embeddings",
      "start": 1329.44,
      "duration": 3.719
    },
    {
      "text": "with Dropout then we apply the first",
      "start": 1331.08,
      "duration": 3.76
    },
    {
      "text": "layer in the Transformer block which is",
      "start": 1333.159,
      "duration": 4.12
    },
    {
      "text": "the layer normalization so this is also",
      "start": 1334.84,
      "duration": 4.8
    },
    {
      "text": "called as the layer Norm what this layer",
      "start": 1337.279,
      "duration": 4.121
    },
    {
      "text": "normalization will do is that it will",
      "start": 1339.64,
      "duration": 4.08
    },
    {
      "text": "look at every uh every token so let's",
      "start": 1341.4,
      "duration": 4.04
    },
    {
      "text": "say I'm looking at this token",
      "start": 1343.72,
      "duration": 5.76
    },
    {
      "text": "every uh and then U it will look at all",
      "start": 1345.44,
      "duration": 6.8
    },
    {
      "text": "of these values here and it will",
      "start": 1349.48,
      "duration": 4.72
    },
    {
      "text": "normalize the values so that the mean of",
      "start": 1352.24,
      "duration": 4.08
    },
    {
      "text": "these values is equal to zero and the",
      "start": 1354.2,
      "duration": 4.28
    },
    {
      "text": "variance of these values is equal to one",
      "start": 1356.32,
      "duration": 3.8
    },
    {
      "text": "and this will be done for every single",
      "start": 1358.48,
      "duration": 4.0
    },
    {
      "text": "token so after we do it for the token",
      "start": 1360.12,
      "duration": 5.08
    },
    {
      "text": "every we then move to effort so we'll",
      "start": 1362.48,
      "duration": 4.799
    },
    {
      "text": "take the take all of these embedding",
      "start": 1365.2,
      "duration": 3.8
    },
    {
      "text": "values and we will normalize them so",
      "start": 1367.279,
      "duration": 4.201
    },
    {
      "text": "we'll subtract the mean from every value",
      "start": 1369.0,
      "duration": 5.0
    },
    {
      "text": "and divide by uh the square root of",
      "start": 1371.48,
      "duration": 4.64
    },
    {
      "text": "variance and this same procedure",
      "start": 1374.0,
      "duration": 4.12
    },
    {
      "text": "normalization procedure will be done to",
      "start": 1376.12,
      "duration": 4.679
    },
    {
      "text": "move and to you so after the layer",
      "start": 1378.12,
      "duration": 4.439
    },
    {
      "text": "normalization is done when we look at",
      "start": 1380.799,
      "duration": 3.841
    },
    {
      "text": "every token and when we look at the",
      "start": 1382.559,
      "duration": 4.12
    },
    {
      "text": "values present in the embedding we'll",
      "start": 1384.64,
      "duration": 3.56
    },
    {
      "text": "see that the mean of those embedding",
      "start": 1386.679,
      "duration": 4.041
    },
    {
      "text": "values is equal to zero and the variance",
      "start": 1388.2,
      "duration": 4.04
    },
    {
      "text": "of those embedding values is equal to",
      "start": 1390.72,
      "duration": 3.8
    },
    {
      "text": "one for every single token layer",
      "start": 1392.24,
      "duration": 4.039
    },
    {
      "text": "normalization is performed to improve",
      "start": 1394.52,
      "duration": 4.159
    },
    {
      "text": "the stability during the training",
      "start": 1396.279,
      "duration": 5.081
    },
    {
      "text": "procedure okay after layer normalization",
      "start": 1398.679,
      "duration": 4.801
    },
    {
      "text": "is performed the most important step",
      "start": 1401.36,
      "duration": 3.64
    },
    {
      "text": "which is actually the engine of the",
      "start": 1403.48,
      "duration": 4.199
    },
    {
      "text": "Transformer block which is why llms work",
      "start": 1405.0,
      "duration": 5.799
    },
    {
      "text": "so well is this Mass multi-ad attention",
      "start": 1407.679,
      "duration": 5.36
    },
    {
      "text": "what is done in this mod in this step is",
      "start": 1410.799,
      "duration": 4.441
    },
    {
      "text": "that we conver we take the embedding",
      "start": 1413.039,
      "duration": 4.64
    },
    {
      "text": "vectors and we convert them into context",
      "start": 1415.24,
      "duration": 4.76
    },
    {
      "text": "vectors so if you look at the output of",
      "start": 1417.679,
      "duration": 4.321
    },
    {
      "text": "the M multi attention the size of the",
      "start": 1420.0,
      "duration": 4.36
    },
    {
      "text": "output is same so for the word effort",
      "start": 1422.0,
      "duration": 5.039
    },
    {
      "text": "let's say it we still have an embedding",
      "start": 1424.36,
      "duration": 5.439
    },
    {
      "text": "of 768 Dimensions but now this is called",
      "start": 1427.039,
      "duration": 4.961
    },
    {
      "text": "as the context Vector embedding the",
      "start": 1429.799,
      "duration": 4.681
    },
    {
      "text": "reason is called context Vector is that",
      "start": 1432.0,
      "duration": 4.679
    },
    {
      "text": "along with capturing the semantic",
      "start": 1434.48,
      "duration": 4.199
    },
    {
      "text": "meaning of effort which the Vector",
      "start": 1436.679,
      "duration": 4.24
    },
    {
      "text": "embedding already did this context",
      "start": 1438.679,
      "duration": 4.961
    },
    {
      "text": "Vector which exists for effort also",
      "start": 1440.919,
      "duration": 4.481
    },
    {
      "text": "captures the meaning of how much",
      "start": 1443.64,
      "duration": 3.88
    },
    {
      "text": "attention should we give to every how",
      "start": 1445.4,
      "duration": 4.36
    },
    {
      "text": "much attention should we give to moves",
      "start": 1447.52,
      "duration": 4.48
    },
    {
      "text": "and how much attention should we give to",
      "start": 1449.76,
      "duration": 5.6
    },
    {
      "text": "you when we are looking at effort so the",
      "start": 1452.0,
      "duration": 5.72
    },
    {
      "text": "context Vector for every token captures",
      "start": 1455.36,
      "duration": 4.72
    },
    {
      "text": "the meaning of how much attention should",
      "start": 1457.72,
      "duration": 4.319
    },
    {
      "text": "be given to all the other tokens in the",
      "start": 1460.08,
      "duration": 4.479
    },
    {
      "text": "sentence that's why it's called",
      "start": 1462.039,
      "duration": 5.52
    },
    {
      "text": "attention this is by far the most",
      "start": 1464.559,
      "duration": 5.48
    },
    {
      "text": "important step in the entire GPT model",
      "start": 1467.559,
      "duration": 4.801
    },
    {
      "text": "without this part it would llms would",
      "start": 1470.039,
      "duration": 4.64
    },
    {
      "text": "not perform as well this part really",
      "start": 1472.36,
      "duration": 4.0
    },
    {
      "text": "tells us that when we want to predict",
      "start": 1474.679,
      "duration": 3.521
    },
    {
      "text": "the next word which are the important",
      "start": 1476.36,
      "duration": 3.88
    },
    {
      "text": "words to look at what is the meaning",
      "start": 1478.2,
      "duration": 3.88
    },
    {
      "text": "between different words how do different",
      "start": 1480.24,
      "duration": 5.0
    },
    {
      "text": "words attend to each other so the",
      "start": 1482.08,
      "duration": 5.4
    },
    {
      "text": "context Vector which I have written here",
      "start": 1485.24,
      "duration": 3.96
    },
    {
      "text": "it looks very simple right now but we",
      "start": 1487.48,
      "duration": 3.88
    },
    {
      "text": "have devoted five lectures of 1 Hour 1",
      "start": 1489.2,
      "duration": 4.24
    },
    {
      "text": "and a half hour each to understand this",
      "start": 1491.36,
      "duration": 4.48
    },
    {
      "text": "one step uh which appears in the whole",
      "start": 1493.44,
      "duration": 5.119
    },
    {
      "text": "GPT model because here is where the",
      "start": 1495.84,
      "duration": 4.76
    },
    {
      "text": "magic happens here is where we",
      "start": 1498.559,
      "duration": 5.641
    },
    {
      "text": "transfer U Vector embeddings into",
      "start": 1500.6,
      "duration": 5.52
    },
    {
      "text": "context Vector embeddings so we we",
      "start": 1504.2,
      "duration": 4.92
    },
    {
      "text": "contain we capture meaning we capture",
      "start": 1506.12,
      "duration": 5.559
    },
    {
      "text": "context as to how different tokens or",
      "start": 1509.12,
      "duration": 4.919
    },
    {
      "text": "different words are related with each",
      "start": 1511.679,
      "duration": 5.72
    },
    {
      "text": "other awesome so until this step here",
      "start": 1514.039,
      "duration": 5.12
    },
    {
      "text": "again you can see the dimensionality is",
      "start": 1517.399,
      "duration": 4.64
    },
    {
      "text": "mentioned is preserved throughout that's",
      "start": 1519.159,
      "duration": 6.12
    },
    {
      "text": "what I like about the GPT model so for",
      "start": 1522.039,
      "duration": 5.12
    },
    {
      "text": "throughout every step you'll see that we",
      "start": 1525.279,
      "duration": 4.241
    },
    {
      "text": "still have four tokens and the dimension",
      "start": 1527.159,
      "duration": 5.201
    },
    {
      "text": "of each is 768 this really makes the GPT",
      "start": 1529.52,
      "duration": 5.0
    },
    {
      "text": "model scalable it's much easier to add",
      "start": 1532.36,
      "duration": 4.319
    },
    {
      "text": "multiple modules together because",
      "start": 1534.52,
      "duration": 4.48
    },
    {
      "text": "addition of modules does not change the",
      "start": 1536.679,
      "duration": 4.441
    },
    {
      "text": "dimensionality then after multi-head",
      "start": 1539.0,
      "duration": 4.44
    },
    {
      "text": "attention we again have a Dropout",
      "start": 1541.12,
      "duration": 6.559
    },
    {
      "text": "layer which randomly uh turns off",
      "start": 1543.44,
      "duration": 7.44
    },
    {
      "text": "certain um certain context Vector values",
      "start": 1547.679,
      "duration": 6.081
    },
    {
      "text": "to zero so here I have shown the color",
      "start": 1550.88,
      "duration": 6.519
    },
    {
      "text": "red where the values of every we look at",
      "start": 1553.76,
      "duration": 5.159
    },
    {
      "text": "every token and weite randomly switch",
      "start": 1557.399,
      "duration": 3.64
    },
    {
      "text": "off certain values to zero it's the same",
      "start": 1558.919,
      "duration": 4.321
    },
    {
      "text": "Dropout layer which we had seen earlier",
      "start": 1561.039,
      "duration": 3.601
    },
    {
      "text": "then we actually have a shortcut",
      "start": 1563.24,
      "duration": 3.28
    },
    {
      "text": "connection so wherever shortcut",
      "start": 1564.64,
      "duration": 4.6
    },
    {
      "text": "connections are mentioned it means that",
      "start": 1566.52,
      "duration": 5.519
    },
    {
      "text": "the output of this the output of this",
      "start": 1569.24,
      "duration": 6.84
    },
    {
      "text": "layer the output of um the Dropout layer",
      "start": 1572.039,
      "duration": 6.88
    },
    {
      "text": "is added back to the input which we",
      "start": 1576.08,
      "duration": 4.64
    },
    {
      "text": "started with to the",
      "start": 1578.919,
      "duration": 4.36
    },
    {
      "text": "Transformer uh this input the output is",
      "start": 1580.72,
      "duration": 4.679
    },
    {
      "text": "added back and the reason it's done is",
      "start": 1583.279,
      "duration": 4.241
    },
    {
      "text": "because it provides another route for",
      "start": 1585.399,
      "duration": 4.041
    },
    {
      "text": "the gradient to flow and it prevents The",
      "start": 1587.52,
      "duration": 4.68
    },
    {
      "text": "Vanishing gradient problem which means",
      "start": 1589.44,
      "duration": 4.28
    },
    {
      "text": "that the training proceeds in a much",
      "start": 1592.2,
      "duration": 4.68
    },
    {
      "text": "smoother manner so after the shortcut",
      "start": 1593.72,
      "duration": 4.64
    },
    {
      "text": "connection is applied that does not",
      "start": 1596.88,
      "duration": 3.48
    },
    {
      "text": "change the dimension at all we again",
      "start": 1598.36,
      "duration": 4.28
    },
    {
      "text": "apply one more round of layer",
      "start": 1600.36,
      "duration": 4.72
    },
    {
      "text": "normalization so again we look at every",
      "start": 1602.64,
      "duration": 7.039
    },
    {
      "text": "Row the mean is the mean is uh changed",
      "start": 1605.08,
      "duration": 6.92
    },
    {
      "text": "to zero the variance is changed to one",
      "start": 1609.679,
      "duration": 5.0
    },
    {
      "text": "so that's how every values is normalized",
      "start": 1612.0,
      "duration": 4.799
    },
    {
      "text": "we subtract the mean from every value",
      "start": 1614.679,
      "duration": 4.201
    },
    {
      "text": "divide by the standard divide by the",
      "start": 1616.799,
      "duration": 3.641
    },
    {
      "text": "square root of the variance so that",
      "start": 1618.88,
      "duration": 3.2
    },
    {
      "text": "finally when we look at the values",
      "start": 1620.44,
      "duration": 3.479
    },
    {
      "text": "together their mean will be zero and",
      "start": 1622.08,
      "duration": 3.68
    },
    {
      "text": "their variance will be one this is done",
      "start": 1623.919,
      "duration": 4.24
    },
    {
      "text": "for every single token and then after",
      "start": 1625.76,
      "duration": 4.159
    },
    {
      "text": "the layer normalization layer we have a",
      "start": 1628.159,
      "duration": 4.321
    },
    {
      "text": "feed forward neural network so when you",
      "start": 1629.919,
      "duration": 4.36
    },
    {
      "text": "zoom into this network you will see that",
      "start": 1632.48,
      "duration": 3.88
    },
    {
      "text": "this is kind of like an expansion",
      "start": 1634.279,
      "duration": 4.481
    },
    {
      "text": "contraction uh Network where let's say",
      "start": 1636.36,
      "duration": 4.64
    },
    {
      "text": "you have the inputs right so we process",
      "start": 1638.76,
      "duration": 4.36
    },
    {
      "text": "every input step by step here if you",
      "start": 1641.0,
      "duration": 3.72
    },
    {
      "text": "first look at the first token which is",
      "start": 1643.12,
      "duration": 4.76
    },
    {
      "text": "every it's a four dimension it's a 768",
      "start": 1644.72,
      "duration": 7.4
    },
    {
      "text": "dimens token right um if you see the",
      "start": 1647.88,
      "duration": 5.96
    },
    {
      "text": "input which enters the speed forward",
      "start": 1652.12,
      "duration": 4.279
    },
    {
      "text": "neural network is that every token has a",
      "start": 1653.84,
      "duration": 5.439
    },
    {
      "text": "dimension or an embedding size of",
      "start": 1656.399,
      "duration": 5.241
    },
    {
      "text": "768 so what happens in this neural",
      "start": 1659.279,
      "duration": 4.28
    },
    {
      "text": "network is that we look at each token",
      "start": 1661.64,
      "duration": 3.519
    },
    {
      "text": "sequentially so let's say we are looking",
      "start": 1663.559,
      "duration": 3.681
    },
    {
      "text": "at the first token which is every let me",
      "start": 1665.159,
      "duration": 4.36
    },
    {
      "text": "zoom into this neural network a",
      "start": 1667.24,
      "duration": 5.84
    },
    {
      "text": "bit uh yeah so first we project this",
      "start": 1669.519,
      "duration": 6.04
    },
    {
      "text": "input into a higher dimensional space in",
      "start": 1673.08,
      "duration": 4.64
    },
    {
      "text": "fact uh this neural network has one",
      "start": 1675.559,
      "duration": 4.041
    },
    {
      "text": "hidden layer and the number of neurons",
      "start": 1677.72,
      "duration": 5.04
    },
    {
      "text": "in this hidden layer is 4 * 768 which is",
      "start": 1679.6,
      "duration": 6.559
    },
    {
      "text": "four * the embedding Dimension and then",
      "start": 1682.76,
      "duration": 5.36
    },
    {
      "text": "we contract back to the original",
      "start": 1686.159,
      "duration": 4.561
    },
    {
      "text": "Dimension so the output from this neural",
      "start": 1688.12,
      "duration": 5.039
    },
    {
      "text": "network is the same Dimension as the",
      "start": 1690.72,
      "duration": 4.559
    },
    {
      "text": "input but there is this expansion and",
      "start": 1693.159,
      "duration": 4.041
    },
    {
      "text": "contraction which is happening and that",
      "start": 1695.279,
      "duration": 4.12
    },
    {
      "text": "allows a much richer exploration of",
      "start": 1697.2,
      "duration": 4.76
    },
    {
      "text": "parameters that makes our llm much",
      "start": 1699.399,
      "duration": 5.041
    },
    {
      "text": "better at prediction of the next word",
      "start": 1701.96,
      "duration": 4.199
    },
    {
      "text": "since it captures the meaning in a much",
      "start": 1704.44,
      "duration": 3.88
    },
    {
      "text": "better manner so if you look at the",
      "start": 1706.159,
      "duration": 3.801
    },
    {
      "text": "output from this neural network the",
      "start": 1708.32,
      "duration": 3.479
    },
    {
      "text": "output from this neural network is the",
      "start": 1709.96,
      "duration": 4.04
    },
    {
      "text": "same size as the input which entered it",
      "start": 1711.799,
      "duration": 3.961
    },
    {
      "text": "we have four tokens and the embedding",
      "start": 1714.0,
      "duration": 3.559
    },
    {
      "text": "size is equal to",
      "start": 1715.76,
      "duration": 5.2
    },
    {
      "text": "768 uh but this expansion contraction",
      "start": 1717.559,
      "duration": 5.201
    },
    {
      "text": "which you see over here the arrow which",
      "start": 1720.96,
      "duration": 3.719
    },
    {
      "text": "I'm showing here is the expansion the",
      "start": 1722.76,
      "duration": 4.279
    },
    {
      "text": "arrow which I'm showing now is the",
      "start": 1724.679,
      "duration": 5.081
    },
    {
      "text": "contraction that is the key because",
      "start": 1727.039,
      "duration": 4.401
    },
    {
      "text": "since the middle layer has huge number",
      "start": 1729.76,
      "duration": 3.2
    },
    {
      "text": "of neurons it is four times the",
      "start": 1731.44,
      "duration": 3.719
    },
    {
      "text": "embedding Dimension neurons which is 4 *",
      "start": 1732.96,
      "duration": 4.959
    },
    {
      "text": "768 the number of parameters are huge in",
      "start": 1735.159,
      "duration": 4.801
    },
    {
      "text": "this feed forward neural network that",
      "start": 1737.919,
      "duration": 5.281
    },
    {
      "text": "allows for a richer exploration space",
      "start": 1739.96,
      "duration": 5.04
    },
    {
      "text": "after this feed forward neural network",
      "start": 1743.2,
      "duration": 4.04
    },
    {
      "text": "we have another Dropout layer which",
      "start": 1745.0,
      "duration": 4.919
    },
    {
      "text": "randomly uh switches off certain values",
      "start": 1747.24,
      "duration": 4.72
    },
    {
      "text": "to be equal to zero which I have just",
      "start": 1749.919,
      "duration": 5.721
    },
    {
      "text": "shown here again by the red um by the",
      "start": 1751.96,
      "duration": 6.28
    },
    {
      "text": "red color so in every token random",
      "start": 1755.64,
      "duration": 4.72
    },
    {
      "text": "values are set to zero typically one",
      "start": 1758.24,
      "duration": 3.96
    },
    {
      "text": "dropout rate is mentioned at the start",
      "start": 1760.36,
      "duration": 3.36
    },
    {
      "text": "and that's applied everywhere where",
      "start": 1762.2,
      "duration": 3.719
    },
    {
      "text": "these Dropout layers are implemented",
      "start": 1763.72,
      "duration": 4.439
    },
    {
      "text": "then we have another shortcut connection",
      "start": 1765.919,
      "duration": 5.721
    },
    {
      "text": "where we basically add the output",
      "start": 1768.159,
      "duration": 6.88
    },
    {
      "text": "uh after this Dropout to the input when",
      "start": 1771.64,
      "duration": 6.36
    },
    {
      "text": "we entered here so the this is the input",
      "start": 1775.039,
      "duration": 6.721
    },
    {
      "text": "in this second row so this so the input",
      "start": 1778.0,
      "duration": 6.159
    },
    {
      "text": "which was there here is",
      "start": 1781.76,
      "duration": 4.919
    },
    {
      "text": "added uh to the output of the Dropout",
      "start": 1784.159,
      "duration": 4.601
    },
    {
      "text": "layer which we are seeing over here and",
      "start": 1786.679,
      "duration": 4.36
    },
    {
      "text": "that's why it's a shortcut connection",
      "start": 1788.76,
      "duration": 4.48
    },
    {
      "text": "and then the output from the shortcut",
      "start": 1791.039,
      "duration": 4.12
    },
    {
      "text": "connection is our Transformer block",
      "start": 1793.24,
      "duration": 5.08
    },
    {
      "text": "output over here so right now the",
      "start": 1795.159,
      "duration": 4.841
    },
    {
      "text": "Transformer block output which I'm",
      "start": 1798.32,
      "duration": 4.199
    },
    {
      "text": "highlighting is the output after so many",
      "start": 1800.0,
      "duration": 5.0
    },
    {
      "text": "steps in the Transformer block so if you",
      "start": 1802.519,
      "duration": 4.16
    },
    {
      "text": "look at the dimensions of this output",
      "start": 1805.0,
      "duration": 3.48
    },
    {
      "text": "you'll see that every token is again a",
      "start": 1806.679,
      "duration": 4.48
    },
    {
      "text": "768 dimensional Vector over here so the",
      "start": 1808.48,
      "duration": 4.679
    },
    {
      "text": "dimensions are exactly the same as the",
      "start": 1811.159,
      "duration": 5.161
    },
    {
      "text": "input to the Transformer block so let me",
      "start": 1813.159,
      "duration": 5.681
    },
    {
      "text": "zoom out a bit here and you can now try",
      "start": 1816.32,
      "duration": 4.16
    },
    {
      "text": "to appreciate the number of steps which",
      "start": 1818.84,
      "duration": 4.12
    },
    {
      "text": "are involved in the Transformer so if I",
      "start": 1820.48,
      "duration": 4.12
    },
    {
      "text": "zoom out here and if you look at the",
      "start": 1822.96,
      "duration": 4.24
    },
    {
      "text": "right side of the screen along with me",
      "start": 1824.6,
      "duration": 4.959
    },
    {
      "text": "uh let me actually move it to the",
      "start": 1827.2,
      "duration": 5.44
    },
    {
      "text": "center yeah so these are all the steps",
      "start": 1829.559,
      "duration": 4.681
    },
    {
      "text": "which are involved in the Transformer",
      "start": 1832.64,
      "duration": 3.759
    },
    {
      "text": "right now whatever you are seeing on the",
      "start": 1834.24,
      "duration": 6.559
    },
    {
      "text": "screen U let me minimize this",
      "start": 1836.399,
      "duration": 4.4
    },
    {
      "text": "color yeah I hope uh you can see the",
      "start": 1847.84,
      "duration": 4.679
    },
    {
      "text": "entire Transformer workf flow on the",
      "start": 1850.72,
      "duration": 4.64
    },
    {
      "text": "screen right now we have we start with",
      "start": 1852.519,
      "duration": 5.481
    },
    {
      "text": "the layer normalization so I'll show",
      "start": 1855.36,
      "duration": 4.12
    },
    {
      "text": "show this with a different color",
      "start": 1858.0,
      "duration": 3.639
    },
    {
      "text": "probably yeah we start with the layer",
      "start": 1859.48,
      "duration": 5.24
    },
    {
      "text": "normalization then we move to the uh",
      "start": 1861.639,
      "duration": 5.481
    },
    {
      "text": "mask multihead attention then we then we",
      "start": 1864.72,
      "duration": 5.16
    },
    {
      "text": "have a Dropout layer uh then we have a",
      "start": 1867.12,
      "duration": 6.159
    },
    {
      "text": "shortcut connection over",
      "start": 1869.88,
      "duration": 3.399
    },
    {
      "text": "here let me Mark this",
      "start": 1873.36,
      "duration": 3.88
    },
    {
      "text": "line yeah we have a shortcut connection",
      "start": 1881.32,
      "duration": 5.04
    },
    {
      "text": "over here then we have another layer",
      "start": 1884.559,
      "duration": 3.96
    },
    {
      "text": "normalization layer we we have a feed",
      "start": 1886.36,
      "duration": 4.159
    },
    {
      "text": "forward neural network we have a Dropout",
      "start": 1888.519,
      "duration": 3.841
    },
    {
      "text": "layer then we again have a final",
      "start": 1890.519,
      "duration": 3.721
    },
    {
      "text": "shortcut layer which actually takes us",
      "start": 1892.36,
      "duration": 4.0
    },
    {
      "text": "to the Transformer block",
      "start": 1894.24,
      "duration": 5.76
    },
    {
      "text": "output um now let us actually go to",
      "start": 1896.36,
      "duration": 6.439
    },
    {
      "text": "the uh visual diagram again to see",
      "start": 1900.0,
      "duration": 4.639
    },
    {
      "text": "whether we indeed have implemented all",
      "start": 1902.799,
      "duration": 4.36
    },
    {
      "text": "the steps which were mentioned in the",
      "start": 1904.639,
      "duration": 4.561
    },
    {
      "text": "Transformer block so I'm zooming in over",
      "start": 1907.159,
      "duration": 4.441
    },
    {
      "text": "here right now and if you see there are",
      "start": 1909.2,
      "duration": 5.319
    },
    {
      "text": "actually 1 2 3 4 5 6 7even eight",
      "start": 1911.6,
      "duration": 5.559
    },
    {
      "text": "different steps right we start out with",
      "start": 1914.519,
      "duration": 4.601
    },
    {
      "text": "the",
      "start": 1917.159,
      "duration": 3.601
    },
    {
      "text": "again let me switch to a different color",
      "start": 1919.12,
      "duration": 4.0
    },
    {
      "text": "here",
      "start": 1920.76,
      "duration": 2.36
    },
    {
      "text": "yeah yeah we start out with the layer",
      "start": 1927.159,
      "duration": 4.321
    },
    {
      "text": "Norm we go to the multi-ad attention we",
      "start": 1929.44,
      "duration": 3.88
    },
    {
      "text": "go to the Dropout we have the shortcut",
      "start": 1931.48,
      "duration": 4.48
    },
    {
      "text": "connection that's the first block then",
      "start": 1933.32,
      "duration": 4.44
    },
    {
      "text": "we have again the layer normalization",
      "start": 1935.96,
      "duration": 3.36
    },
    {
      "text": "the feed forward neural network the",
      "start": 1937.76,
      "duration": 3.6
    },
    {
      "text": "Dropout and then the last shortcut so",
      "start": 1939.32,
      "duration": 3.599
    },
    {
      "text": "there are eight different steps here",
      "start": 1941.36,
      "duration": 2.799
    },
    {
      "text": "which were the same eight different",
      "start": 1942.919,
      "duration": 5.041
    },
    {
      "text": "steps we implemented in the uh visual",
      "start": 1944.159,
      "duration": 6.24
    },
    {
      "text": "flow map and now there are the final two",
      "start": 1947.96,
      "duration": 4.4
    },
    {
      "text": "steps which are remaining so let's look",
      "start": 1950.399,
      "duration": 4.12
    },
    {
      "text": "at the last two steps of the GPT model",
      "start": 1952.36,
      "duration": 3.679
    },
    {
      "text": "in the visual flow",
      "start": 1954.519,
      "duration": 4.081
    },
    {
      "text": "map so the fifth step was the",
      "start": 1956.039,
      "duration": 4.801
    },
    {
      "text": "Transformer step which we implemented",
      "start": 1958.6,
      "duration": 3.84
    },
    {
      "text": "within the Transformer also there were",
      "start": 1960.84,
      "duration": 4.12
    },
    {
      "text": "eight different steps now we go to the",
      "start": 1962.44,
      "duration": 5.76
    },
    {
      "text": "next block which is",
      "start": 1964.96,
      "duration": 6.24
    },
    {
      "text": "uh uh the next block is another layer of",
      "start": 1968.2,
      "duration": 5.24
    },
    {
      "text": "normalization so the Transformer block",
      "start": 1971.2,
      "duration": 4.8
    },
    {
      "text": "itself has two normalization layers but",
      "start": 1973.44,
      "duration": 4.44
    },
    {
      "text": "we have another normalization layer",
      "start": 1976.0,
      "duration": 4.2
    },
    {
      "text": "after the Transformer block output so",
      "start": 1977.88,
      "duration": 3.799
    },
    {
      "text": "here we have the Transformer block",
      "start": 1980.2,
      "duration": 3.319
    },
    {
      "text": "output and here you can focus on the",
      "start": 1981.679,
      "duration": 5.161
    },
    {
      "text": "dimensions again every vector or every",
      "start": 1983.519,
      "duration": 6.961
    },
    {
      "text": "token has an embedding size of 768 right",
      "start": 1986.84,
      "duration": 5.88
    },
    {
      "text": "again the dimensionality is preserved",
      "start": 1990.48,
      "duration": 3.64
    },
    {
      "text": "and then we apply the layer",
      "start": 1992.72,
      "duration": 3.88
    },
    {
      "text": "normalization so same thing as what the",
      "start": 1994.12,
      "duration": 4.48
    },
    {
      "text": "near layer normalization does is done",
      "start": 1996.6,
      "duration": 5.52
    },
    {
      "text": "here also in every token we normalize",
      "start": 1998.6,
      "duration": 7.0
    },
    {
      "text": "the embedding values so that the mean of",
      "start": 2002.12,
      "duration": 5.48
    },
    {
      "text": "the resultant embedding values is zero",
      "start": 2005.6,
      "duration": 4.16
    },
    {
      "text": "and the variance is equal to one and",
      "start": 2007.6,
      "duration": 4.439
    },
    {
      "text": "this is done for all the four",
      "start": 2009.76,
      "duration": 4.879
    },
    {
      "text": "tokens so again the layer normalization",
      "start": 2012.039,
      "duration": 4.76
    },
    {
      "text": "does not change the dimensions and then",
      "start": 2014.639,
      "duration": 3.76
    },
    {
      "text": "the last layer which we have is the",
      "start": 2016.799,
      "duration": 4.0
    },
    {
      "text": "output head so the output head is a",
      "start": 2018.399,
      "duration": 4.681
    },
    {
      "text": "neural network at the final stage of the",
      "start": 2020.799,
      "duration": 5.321
    },
    {
      "text": "GPT model so the input to the output",
      "start": 2023.08,
      "duration": 6.079
    },
    {
      "text": "head is this input which has four tokens",
      "start": 2026.12,
      "duration": 5.76
    },
    {
      "text": "and each token has a dimension of 768",
      "start": 2029.159,
      "duration": 4.601
    },
    {
      "text": "then what we do is that we pass this",
      "start": 2031.88,
      "duration": 4.159
    },
    {
      "text": "through a neural network whose size is",
      "start": 2033.76,
      "duration": 4.96
    },
    {
      "text": "the embedding dimension multiplied by",
      "start": 2036.039,
      "duration": 4.64
    },
    {
      "text": "the vocabulary Dimension and I'll tell",
      "start": 2038.72,
      "duration": 4.72
    },
    {
      "text": "you why just in a moment but if you take",
      "start": 2040.679,
      "duration": 6.12
    },
    {
      "text": "this input input tensor and if you pass",
      "start": 2043.44,
      "duration": 5.639
    },
    {
      "text": "in through the neural network which has",
      "start": 2046.799,
      "duration": 4.88
    },
    {
      "text": "uh which takes in inputs of 768",
      "start": 2049.079,
      "duration": 4.481
    },
    {
      "text": "dimensions and the output is",
      "start": 2051.679,
      "duration": 6.44
    },
    {
      "text": "50257 the resultant uh output which you",
      "start": 2053.56,
      "duration": 7.279
    },
    {
      "text": "get it's called as the logits Matrix and",
      "start": 2058.119,
      "duration": 5.0
    },
    {
      "text": "this logits Matrix actually has four",
      "start": 2060.839,
      "duration": 6.0
    },
    {
      "text": "rows because we have four tokens every",
      "start": 2063.119,
      "duration": 6.56
    },
    {
      "text": "so every f effort moves you and the",
      "start": 2066.839,
      "duration": 4.721
    },
    {
      "text": "number of columns here is not equal to",
      "start": 2069.679,
      "duration": 4.2
    },
    {
      "text": "768 the number of columns is equal to",
      "start": 2071.56,
      "duration": 4.44
    },
    {
      "text": "the vocabulary size because we are",
      "start": 2073.879,
      "duration": 5.0
    },
    {
      "text": "passing in passing the input through a",
      "start": 2076.0,
      "duration": 5.0
    },
    {
      "text": "neural network whose final output size",
      "start": 2078.879,
      "duration": 6.04
    },
    {
      "text": "is 50257 so for every token we have this",
      "start": 2081.0,
      "duration": 5.76
    },
    {
      "text": "logits which is a",
      "start": 2084.919,
      "duration": 5.76
    },
    {
      "text": "50257 dimensional Vector so for every",
      "start": 2086.76,
      "duration": 6.68
    },
    {
      "text": "there is a 50257 dimensional Vector for",
      "start": 2090.679,
      "duration": 5.361
    },
    {
      "text": "effort there is a 50257 dimensional",
      "start": 2093.44,
      "duration": 5.04
    },
    {
      "text": "Vector similarly for move and U there",
      "start": 2096.04,
      "duration": 4.96
    },
    {
      "text": "are 5 257 dimensional",
      "start": 2098.48,
      "duration": 4.96
    },
    {
      "text": "vectors and the reason here is because",
      "start": 2101.0,
      "duration": 5.16
    },
    {
      "text": "we want to predict the next word right",
      "start": 2103.44,
      "duration": 5.28
    },
    {
      "text": "based on the input so when every is the",
      "start": 2106.16,
      "duration": 4.6
    },
    {
      "text": "input we want to predict what's the next",
      "start": 2108.72,
      "duration": 4.119
    },
    {
      "text": "word and that should be effort so we",
      "start": 2110.76,
      "duration": 4.52
    },
    {
      "text": "look at the vocabulary and we look at",
      "start": 2112.839,
      "duration": 6.0
    },
    {
      "text": "that that token or that column which has",
      "start": 2115.28,
      "duration": 6.28
    },
    {
      "text": "the highest value and that column should",
      "start": 2118.839,
      "duration": 5.0
    },
    {
      "text": "ideally correspond to effort when every",
      "start": 2121.56,
      "duration": 4.88
    },
    {
      "text": "is the input similarly when every effort",
      "start": 2123.839,
      "duration": 4.481
    },
    {
      "text": "is the input we we will look at the",
      "start": 2126.44,
      "duration": 4.28
    },
    {
      "text": "column which has the maximum value and",
      "start": 2128.32,
      "duration": 4.24
    },
    {
      "text": "that column corresponds to certain word",
      "start": 2130.72,
      "duration": 3.84
    },
    {
      "text": "in the vocabulary and that word should",
      "start": 2132.56,
      "duration": 4.36
    },
    {
      "text": "be moves because when every and effort",
      "start": 2134.56,
      "duration": 4.559
    },
    {
      "text": "are the input moves is the output",
      "start": 2136.92,
      "duration": 4.04
    },
    {
      "text": "similarly when we look at every effort",
      "start": 2139.119,
      "duration": 4.201
    },
    {
      "text": "moves as the input we will predict the",
      "start": 2140.96,
      "duration": 4.36
    },
    {
      "text": "next word and that should be U so we",
      "start": 2143.32,
      "duration": 3.56
    },
    {
      "text": "look at that column which has the",
      "start": 2145.32,
      "duration": 3.16
    },
    {
      "text": "highest value and that should hopefully",
      "start": 2146.88,
      "duration": 4.56
    },
    {
      "text": "be U and finally we look at every effort",
      "start": 2148.48,
      "duration": 5.2
    },
    {
      "text": "moves you that's the final prediction",
      "start": 2151.44,
      "duration": 5.44
    },
    {
      "text": "task and then uh when every effort moves",
      "start": 2153.68,
      "duration": 5.2
    },
    {
      "text": "you is the input we look at the final",
      "start": 2156.88,
      "duration": 5.56
    },
    {
      "text": "output and that should hopefully be the",
      "start": 2158.88,
      "duration": 5.959
    },
    {
      "text": "token which corresponds to the next word",
      "start": 2162.44,
      "duration": 4.2
    },
    {
      "text": "which is forward every effort moves you",
      "start": 2164.839,
      "duration": 4.441
    },
    {
      "text": "forward we'll see how to make the next",
      "start": 2166.64,
      "duration": 4.76
    },
    {
      "text": "word prediction in the next class but",
      "start": 2169.28,
      "duration": 4.079
    },
    {
      "text": "for now I just want you to appreciate",
      "start": 2171.4,
      "duration": 3.76
    },
    {
      "text": "that this last step is the only step",
      "start": 2173.359,
      "duration": 4.121
    },
    {
      "text": "where the dimensions of the input change",
      "start": 2175.16,
      "duration": 4.52
    },
    {
      "text": "the dimensions of the input all along",
      "start": 2177.48,
      "duration": 4.0
    },
    {
      "text": "where four which were the number of",
      "start": 2179.68,
      "duration": 4.6
    },
    {
      "text": "tokens multiplied by 768 which was the",
      "start": 2181.48,
      "duration": 5.359
    },
    {
      "text": "embedding Dimension but now the final",
      "start": 2184.28,
      "duration": 4.44
    },
    {
      "text": "output size four which were the number",
      "start": 2186.839,
      "duration": 4.681
    },
    {
      "text": "of tokens multiplied by",
      "start": 2188.72,
      "duration": 5.52
    },
    {
      "text": "50257 and based on this final output",
      "start": 2191.52,
      "duration": 4.28
    },
    {
      "text": "we'll get the next word",
      "start": 2194.24,
      "duration": 4.2
    },
    {
      "text": "prediction now when you look at every",
      "start": 2195.8,
      "duration": 4.44
    },
    {
      "text": "effort moves you the context size is",
      "start": 2198.44,
      "duration": 4.96
    },
    {
      "text": "four whenever we have a context size we",
      "start": 2200.24,
      "duration": 5.48
    },
    {
      "text": "have that many prediction tasks so",
      "start": 2203.4,
      "duration": 4.04
    },
    {
      "text": "remember here there are four tokens",
      "start": 2205.72,
      "duration": 3.2
    },
    {
      "text": "right so we don't only have one",
      "start": 2207.44,
      "duration": 3.56
    },
    {
      "text": "prediction task you might think that the",
      "start": 2208.92,
      "duration": 3.919
    },
    {
      "text": "only prediction task is every effort",
      "start": 2211.0,
      "duration": 3.4
    },
    {
      "text": "moves you is the input and we have to",
      "start": 2212.839,
      "duration": 3.721
    },
    {
      "text": "predict the next word no there are four",
      "start": 2214.4,
      "duration": 3.919
    },
    {
      "text": "prediction tasks the first prediction",
      "start": 2216.56,
      "duration": 4.4
    },
    {
      "text": "task is every is the input then effort",
      "start": 2218.319,
      "duration": 4.52
    },
    {
      "text": "should be the output every effort is the",
      "start": 2220.96,
      "duration": 4.04
    },
    {
      "text": "input movees should be the output every",
      "start": 2222.839,
      "duration": 3.921
    },
    {
      "text": "effort moves should be the input U",
      "start": 2225.0,
      "duration": 3.8
    },
    {
      "text": "should be the output and only the fourth",
      "start": 2226.76,
      "duration": 3.76
    },
    {
      "text": "prediction task is every effort moves",
      "start": 2228.8,
      "duration": 4.24
    },
    {
      "text": "you what's the output so that's why we",
      "start": 2230.52,
      "duration": 4.839
    },
    {
      "text": "have this four rows here because there",
      "start": 2233.04,
      "duration": 5.2
    },
    {
      "text": "are four prediction tasks we'll see in",
      "start": 2235.359,
      "duration": 5.361
    },
    {
      "text": "the next uh next class how to predict",
      "start": 2238.24,
      "duration": 4.599
    },
    {
      "text": "the next word from this output tensor",
      "start": 2240.72,
      "duration": 4.24
    },
    {
      "text": "which we have obtained now one thing to",
      "start": 2242.839,
      "duration": 4.76
    },
    {
      "text": "mention is that uh so here if you you",
      "start": 2244.96,
      "duration": 4.76
    },
    {
      "text": "see the output tensor value will",
      "start": 2247.599,
      "duration": 5.281
    },
    {
      "text": "actually be four so let me write this",
      "start": 2249.72,
      "duration": 5.04
    },
    {
      "text": "down over here the output tensor value",
      "start": 2252.88,
      "duration": 5.8
    },
    {
      "text": "will be four multiplied",
      "start": 2254.76,
      "duration": 3.92
    },
    {
      "text": "by uh the vocabulary size which is uh",
      "start": 2260.2,
      "duration": 5.28
    },
    {
      "text": "five so it's 5",
      "start": 2268.04,
      "duration": 6.16
    },
    {
      "text": "0",
      "start": 2271.2,
      "duration": 3.0
    },
    {
      "text": "2 5 and 7 so that is the fin final",
      "start": 2274.68,
      "duration": 6.159
    },
    {
      "text": "tensor value uh remember one thing which",
      "start": 2277.839,
      "duration": 5.641
    },
    {
      "text": "I have considered here is only one batch",
      "start": 2280.839,
      "duration": 4.961
    },
    {
      "text": "if you have two batches this will be the",
      "start": 2283.48,
      "duration": 4.16
    },
    {
      "text": "final tensor generated for both those",
      "start": 2285.8,
      "duration": 3.799
    },
    {
      "text": "batches so similar similar to every",
      "start": 2287.64,
      "duration": 3.88
    },
    {
      "text": "effort moves you if you have another",
      "start": 2289.599,
      "duration": 5.641
    },
    {
      "text": "sentence of four words such as I like uh",
      "start": 2291.52,
      "duration": 5.64
    },
    {
      "text": "movies and and you have to predict the",
      "start": 2295.24,
      "duration": 3.839
    },
    {
      "text": "next word that's the second batch so",
      "start": 2297.16,
      "duration": 3.8
    },
    {
      "text": "then the output tensor will also include",
      "start": 2299.079,
      "duration": 4.121
    },
    {
      "text": "the batch size so then it will be 2",
      "start": 2300.96,
      "duration": 8.04
    },
    {
      "text": "multiplied by uh 4 by 5 2 57 why this",
      "start": 2303.2,
      "duration": 7.639
    },
    {
      "text": "initial two because the batch size is",
      "start": 2309.0,
      "duration": 4.359
    },
    {
      "text": "equal to two so the First Dimension is",
      "start": 2310.839,
      "duration": 4.641
    },
    {
      "text": "always the batch size the second",
      "start": 2313.359,
      "duration": 3.76
    },
    {
      "text": "dimension is the number of tokens which",
      "start": 2315.48,
      "duration": 5.0
    },
    {
      "text": "we have or the context size uh and the",
      "start": 2317.119,
      "duration": 6.401
    },
    {
      "text": "third dimension is the",
      "start": 2320.48,
      "duration": 5.92
    },
    {
      "text": "50257 which is the vocabulary size so",
      "start": 2323.52,
      "duration": 5.48
    },
    {
      "text": "this is the output tensor Dimension",
      "start": 2326.4,
      "duration": 4.08
    },
    {
      "text": "format which we",
      "start": 2329.0,
      "duration": 3.96
    },
    {
      "text": "have now let me zoom out here and show",
      "start": 2330.48,
      "duration": 4.2
    },
    {
      "text": "you the entire flow map which we have",
      "start": 2332.96,
      "duration": 4.8
    },
    {
      "text": "seen it was a pretty long flow map but I",
      "start": 2334.68,
      "duration": 5.04
    },
    {
      "text": "I hope you all have understood what we",
      "start": 2337.76,
      "duration": 4.04
    },
    {
      "text": "are trying to do over here the reason I",
      "start": 2339.72,
      "duration": 3.52
    },
    {
      "text": "went through this entire flow map",
      "start": 2341.8,
      "duration": 3.2
    },
    {
      "text": "starting from token embedding positional",
      "start": 2343.24,
      "duration": 4.119
    },
    {
      "text": "embedding to input embedding to drop out",
      "start": 2345.0,
      "duration": 5.079
    },
    {
      "text": "to the Transformer block uh then we went",
      "start": 2347.359,
      "duration": 4.76
    },
    {
      "text": "to layer normalization then finally we",
      "start": 2350.079,
      "duration": 4.121
    },
    {
      "text": "went to Output head the reason I showed",
      "start": 2352.119,
      "duration": 4.0
    },
    {
      "text": "you all these things is just so that you",
      "start": 2354.2,
      "duration": 3.399
    },
    {
      "text": "understand dimensions and what's going",
      "start": 2356.119,
      "duration": 3.881
    },
    {
      "text": "on with Dimensions I did not just want",
      "start": 2357.599,
      "duration": 4.281
    },
    {
      "text": "to show you the code because when you",
      "start": 2360.0,
      "duration": 3.839
    },
    {
      "text": "see the code you cannot visualize the",
      "start": 2361.88,
      "duration": 4.439
    },
    {
      "text": "dimensions but now once you have seen",
      "start": 2363.839,
      "duration": 4.441
    },
    {
      "text": "this lecture and when whenever you let's",
      "start": 2366.319,
      "duration": 3.961
    },
    {
      "text": "say you're looking at the layer",
      "start": 2368.28,
      "duration": 3.96
    },
    {
      "text": "normalization part of the Transformer",
      "start": 2370.28,
      "duration": 4.44
    },
    {
      "text": "right you can just visualize what's",
      "start": 2372.24,
      "duration": 5.119
    },
    {
      "text": "happening uh what are the dimensions of",
      "start": 2374.72,
      "duration": 4.399
    },
    {
      "text": "the input to a particular block what are",
      "start": 2377.359,
      "duration": 4.681
    },
    {
      "text": "the dimensions of the output and so so",
      "start": 2379.119,
      "duration": 4.561
    },
    {
      "text": "that will make your learning process",
      "start": 2382.04,
      "duration": 2.559
    },
    {
      "text": "much",
      "start": 2383.68,
      "duration": 4.08
    },
    {
      "text": "easier so now actually uh on the",
      "start": 2384.599,
      "duration": 5.561
    },
    {
      "text": "Whiteboard we have seen how the building",
      "start": 2387.76,
      "duration": 5.079
    },
    {
      "text": "block stack up for the entire GPT model",
      "start": 2390.16,
      "duration": 4.48
    },
    {
      "text": "we started with tokenization input",
      "start": 2392.839,
      "duration": 3.321
    },
    {
      "text": "embedding positional we went to",
      "start": 2394.64,
      "duration": 3.8
    },
    {
      "text": "Transformers and then we saw the final",
      "start": 2396.16,
      "duration": 4.76
    },
    {
      "text": "layer nor norm and then we also saw the",
      "start": 2398.44,
      "duration": 4.399
    },
    {
      "text": "final linear output",
      "start": 2400.92,
      "duration": 5.32
    },
    {
      "text": "layer okay so now once your intuition is",
      "start": 2402.839,
      "duration": 5.28
    },
    {
      "text": "clear once your visual understanding is",
      "start": 2406.24,
      "duration": 3.68
    },
    {
      "text": "clear we are pretty much ready to move",
      "start": 2408.119,
      "duration": 4.921
    },
    {
      "text": "into code so now let us dive into code",
      "start": 2409.92,
      "duration": 5.32
    },
    {
      "text": "and uh see how these different blocks",
      "start": 2413.04,
      "duration": 3.84
    },
    {
      "text": "can be arranged",
      "start": 2415.24,
      "duration": 5.359
    },
    {
      "text": "together to code the entire GPT model",
      "start": 2416.88,
      "duration": 6.16
    },
    {
      "text": "all right so let's jump into code right",
      "start": 2420.599,
      "duration": 5.841
    },
    {
      "text": "now here's the GPT configuration it's",
      "start": 2423.04,
      "duration": 6.039
    },
    {
      "text": "the same configuration is gpt2 which we",
      "start": 2426.44,
      "duration": 6.0
    },
    {
      "text": "are going to uh use when we are going to",
      "start": 2429.079,
      "duration": 5.681
    },
    {
      "text": "look at this entire coding module so",
      "start": 2432.44,
      "duration": 4.2
    },
    {
      "text": "here you will see that the vocabulary",
      "start": 2434.76,
      "duration": 3.92
    },
    {
      "text": "size is equal to",
      "start": 2436.64,
      "duration": 4.52
    },
    {
      "text": "50257 this was actually the vocabulary",
      "start": 2438.68,
      "duration": 5.679
    },
    {
      "text": "size which was implemented when gpt2 was",
      "start": 2441.16,
      "duration": 6.4
    },
    {
      "text": "trained the context length is equal to",
      "start": 2444.359,
      "duration": 6.201
    },
    {
      "text": "1024 the vector embedding Dimension",
      "start": 2447.56,
      "duration": 4.96
    },
    {
      "text": "which we also saw on the Whiteboard that",
      "start": 2450.56,
      "duration": 4.88
    },
    {
      "text": "is equal to 768 the number of attention",
      "start": 2452.52,
      "duration": 6.2
    },
    {
      "text": "heads is equal to 12 the number of",
      "start": 2455.44,
      "duration": 5.639
    },
    {
      "text": "Transformers U which are there which we",
      "start": 2458.72,
      "duration": 4.96
    },
    {
      "text": "are going to implement are equal to 12",
      "start": 2461.079,
      "duration": 4.881
    },
    {
      "text": "the drop rate or the dropout rate is",
      "start": 2463.68,
      "duration": 4.84
    },
    {
      "text": "equal to 0.1 and the query key value",
      "start": 2465.96,
      "duration": 5.52
    },
    {
      "text": "bias is false this is for setting or",
      "start": 2468.52,
      "duration": 5.72
    },
    {
      "text": "initializing the weight metrices for the",
      "start": 2471.48,
      "duration": 4.68
    },
    {
      "text": "queries the keys and the values we don't",
      "start": 2474.24,
      "duration": 5.079
    },
    {
      "text": "need the bias term in that for now you",
      "start": 2476.16,
      "duration": 4.84
    },
    {
      "text": "can focus on a couple of things the",
      "start": 2479.319,
      "duration": 4.121
    },
    {
      "text": "first is the embedding Dimension uh",
      "start": 2481.0,
      "duration": 4.68
    },
    {
      "text": "which is going to stay 7608 throughout",
      "start": 2483.44,
      "duration": 4.28
    },
    {
      "text": "the second is the 502 spice and",
      "start": 2485.68,
      "duration": 3.919
    },
    {
      "text": "vocabulary size these are the same",
      "start": 2487.72,
      "duration": 3.32
    },
    {
      "text": "dimensions which we just saw on the",
      "start": 2489.599,
      "duration": 2.921
    },
    {
      "text": "Whiteboard so you might be able to",
      "start": 2491.04,
      "duration": 4.6
    },
    {
      "text": "relate to them uh 12 is the number of",
      "start": 2492.52,
      "duration": 5.68
    },
    {
      "text": "Dropout uh 12 is the number of",
      "start": 2495.64,
      "duration": 5.36
    },
    {
      "text": "Transformer blocks so in gpt2 when it",
      "start": 2498.2,
      "duration": 4.76
    },
    {
      "text": "was trained they had 12 Transformer",
      "start": 2501.0,
      "duration": 4.119
    },
    {
      "text": "blocks we are also going to use a",
      "start": 2502.96,
      "duration": 4.44
    },
    {
      "text": "similar number of Transformer blocks",
      "start": 2505.119,
      "duration": 4.041
    },
    {
      "text": "then the number of attention heads so",
      "start": 2507.4,
      "duration": 3.679
    },
    {
      "text": "every Transformer block has a multi-ad",
      "start": 2509.16,
      "duration": 4.36
    },
    {
      "text": "attention module right and that module",
      "start": 2511.079,
      "duration": 4.201
    },
    {
      "text": "has a certain number of attention heads",
      "start": 2513.52,
      "duration": 3.92
    },
    {
      "text": "we are going to use that equal to 12",
      "start": 2515.28,
      "duration": 3.6
    },
    {
      "text": "dropout rate is",
      "start": 2517.44,
      "duration": 4.32
    },
    {
      "text": "0.1 I think this much should be enough",
      "start": 2518.88,
      "duration": 5.76
    },
    {
      "text": "to understand the entire code now as we",
      "start": 2521.76,
      "duration": 5.16
    },
    {
      "text": "are going through this code I want you",
      "start": 2524.64,
      "duration": 4.88
    },
    {
      "text": "to just keep this diagram in mind and",
      "start": 2526.92,
      "duration": 4.32
    },
    {
      "text": "the dimensions which we just saw in the",
      "start": 2529.52,
      "duration": 4.079
    },
    {
      "text": "visual flow map everything will follow",
      "start": 2531.24,
      "duration": 4.52
    },
    {
      "text": "from this particular diagram since we",
      "start": 2533.599,
      "duration": 5.161
    },
    {
      "text": "are exactly going to uh code it in a",
      "start": 2535.76,
      "duration": 4.319
    },
    {
      "text": "similar",
      "start": 2538.76,
      "duration": 5.079
    },
    {
      "text": "manner okay so we started out this",
      "start": 2540.079,
      "duration": 6.361
    },
    {
      "text": "lecture series four to five lectures",
      "start": 2543.839,
      "duration": 5.881
    },
    {
      "text": "back with this GPT model class where we",
      "start": 2546.44,
      "duration": 5.6
    },
    {
      "text": "had the forward method but the",
      "start": 2549.72,
      "duration": 4.56
    },
    {
      "text": "Transformer block was not implemented so",
      "start": 2552.04,
      "duration": 4.44
    },
    {
      "text": "this was a dummy Transformer block the",
      "start": 2554.28,
      "duration": 5.319
    },
    {
      "text": "layer normalization was not implemented",
      "start": 2556.48,
      "duration": 5.599
    },
    {
      "text": "this was a dummy layer normalization",
      "start": 2559.599,
      "duration": 4.561
    },
    {
      "text": "what we did over the last three to four",
      "start": 2562.079,
      "duration": 3.721
    },
    {
      "text": "lectures is that we coded the layer",
      "start": 2564.16,
      "duration": 3.679
    },
    {
      "text": "normalization class feed forward neural",
      "start": 2565.8,
      "duration": 4.519
    },
    {
      "text": "network class and also the Transformer",
      "start": 2567.839,
      "duration": 5.0
    },
    {
      "text": "class so here's the layer normalization",
      "start": 2570.319,
      "duration": 5.561
    },
    {
      "text": "class which we had coded given a",
      "start": 2572.839,
      "duration": 6.081
    },
    {
      "text": "particular uh layer now you can think of",
      "start": 2575.88,
      "duration": 4.36
    },
    {
      "text": "when whenever you see layer",
      "start": 2578.92,
      "duration": 3.36
    },
    {
      "text": "normalization remember we have seen the",
      "start": 2580.24,
      "duration": 4.76
    },
    {
      "text": "visual visual flow map now right so you",
      "start": 2582.28,
      "duration": 6.12
    },
    {
      "text": "can uh try to visualize what happens in",
      "start": 2585.0,
      "duration": 5.0
    },
    {
      "text": "the layer normalization layer we have",
      "start": 2588.4,
      "duration": 4.12
    },
    {
      "text": "seen that already so see this is exactly",
      "start": 2590.0,
      "duration": 4.119
    },
    {
      "text": "what happens in a layer normalization",
      "start": 2592.52,
      "duration": 4.2
    },
    {
      "text": "layer we look at every token that has",
      "start": 2594.119,
      "duration": 5.801
    },
    {
      "text": "768 Dimensions right then what we are",
      "start": 2596.72,
      "duration": 5.0
    },
    {
      "text": "doing is that we are going to subtract",
      "start": 2599.92,
      "duration": 3.679
    },
    {
      "text": "the mean and divide by the square root",
      "start": 2601.72,
      "duration": 4.56
    },
    {
      "text": "of the variance so that the resulting",
      "start": 2603.599,
      "duration": 5.041
    },
    {
      "text": "values have a mean of zero and standard",
      "start": 2606.28,
      "duration": 4.6
    },
    {
      "text": "deviation or variance of one we also",
      "start": 2608.64,
      "duration": 3.919
    },
    {
      "text": "have a scale and a shift here which are",
      "start": 2610.88,
      "duration": 4.199
    },
    {
      "text": "trainable parameters then we have the",
      "start": 2612.559,
      "duration": 4.881
    },
    {
      "text": "feed forward block and remember where",
      "start": 2615.079,
      "duration": 4.0
    },
    {
      "text": "the feed forward block comes in the",
      "start": 2617.44,
      "duration": 5.2
    },
    {
      "text": "Transformer architecture it's this",
      "start": 2619.079,
      "duration": 7.161
    },
    {
      "text": "uh it's this feed forward block um let",
      "start": 2622.64,
      "duration": 6.24
    },
    {
      "text": "me actually change my pen color to it's",
      "start": 2626.24,
      "duration": 4.64
    },
    {
      "text": "this feed forward neural network which I",
      "start": 2628.88,
      "duration": 4.08
    },
    {
      "text": "have shown you over here the expansion",
      "start": 2630.88,
      "duration": 4.56
    },
    {
      "text": "contraction feed forward neural network",
      "start": 2632.96,
      "duration": 4.399
    },
    {
      "text": "this is that feed forward block so",
      "start": 2635.44,
      "duration": 3.28
    },
    {
      "text": "you'll see that the input is the",
      "start": 2637.359,
      "duration": 3.24
    },
    {
      "text": "embedding Dimension the layer is four",
      "start": 2638.72,
      "duration": 4.0
    },
    {
      "text": "times the embedding Dimension size and",
      "start": 2640.599,
      "duration": 4.121
    },
    {
      "text": "the output is the embedding Dimension",
      "start": 2642.72,
      "duration": 3.52
    },
    {
      "text": "once you are able to visualize this",
      "start": 2644.72,
      "duration": 3.24
    },
    {
      "text": "you'll be able to easily understand this",
      "start": 2646.24,
      "duration": 3.64
    },
    {
      "text": "code the input is the embedding",
      "start": 2647.96,
      "duration": 4.119
    },
    {
      "text": "Dimension the hidden layer is four times",
      "start": 2649.88,
      "duration": 4.08
    },
    {
      "text": "the embedding Dimension that's the",
      "start": 2652.079,
      "duration": 3.721
    },
    {
      "text": "expansion layer then there is a",
      "start": 2653.96,
      "duration": 3.72
    },
    {
      "text": "contraction layer from the four times",
      "start": 2655.8,
      "duration": 3.92
    },
    {
      "text": "embedding Dimension to the embedding",
      "start": 2657.68,
      "duration": 4.6
    },
    {
      "text": "Dimension and there is a Jou activation",
      "start": 2659.72,
      "duration": 4.839
    },
    {
      "text": "function we spent a whole lecture on the",
      "start": 2662.28,
      "duration": 4.039
    },
    {
      "text": "J activation because it's a bit",
      "start": 2664.559,
      "duration": 3.04
    },
    {
      "text": "different than the",
      "start": 2666.319,
      "duration": 3.8
    },
    {
      "text": "it's smoother on the negative side I",
      "start": 2667.599,
      "duration": 4.401
    },
    {
      "text": "think I also have a plot for the J",
      "start": 2670.119,
      "duration": 3.561
    },
    {
      "text": "activation over here so let me quickly",
      "start": 2672.0,
      "duration": 6.24
    },
    {
      "text": "show you that um if I can find it yeah",
      "start": 2673.68,
      "duration": 6.48
    },
    {
      "text": "so on the right hand side you can see",
      "start": 2678.24,
      "duration": 4.96
    },
    {
      "text": "railu it's zero for negative inputs for",
      "start": 2680.16,
      "duration": 5.439
    },
    {
      "text": "the J it's not zero and also it's",
      "start": 2683.2,
      "duration": 4.8
    },
    {
      "text": "differentiable at x equal to Z that's",
      "start": 2685.599,
      "duration": 4.48
    },
    {
      "text": "why in llm training generally the J",
      "start": 2688.0,
      "duration": 5.8
    },
    {
      "text": "activation uh is used so we coded the",
      "start": 2690.079,
      "duration": 6.28
    },
    {
      "text": "layer normalization class the j class",
      "start": 2693.8,
      "duration": 5.039
    },
    {
      "text": "and the feed forward class and then in",
      "start": 2696.359,
      "duration": 4.24
    },
    {
      "text": "the last lecture we coded the entire",
      "start": 2698.839,
      "duration": 4.801
    },
    {
      "text": "Transformer block class itself so here",
      "start": 2700.599,
      "duration": 5.48
    },
    {
      "text": "we had the so the eight steps in the",
      "start": 2703.64,
      "duration": 4.8
    },
    {
      "text": "Transformer if you remember take a look",
      "start": 2706.079,
      "duration": 6.52
    },
    {
      "text": "at these eight steps which we saw um let",
      "start": 2708.44,
      "duration": 6.399
    },
    {
      "text": "me zoom out here further so these are",
      "start": 2712.599,
      "duration": 3.76
    },
    {
      "text": "the eight steps which we saw in the",
      "start": 2714.839,
      "duration": 3.76
    },
    {
      "text": "Transformer we start with input",
      "start": 2716.359,
      "duration": 4.96
    },
    {
      "text": "embeddings add a Dropout layer",
      "start": 2718.599,
      "duration": 5.52
    },
    {
      "text": "normalization then uh we go to the mass",
      "start": 2721.319,
      "duration": 5.401
    },
    {
      "text": "multi attention then drop out and then",
      "start": 2724.119,
      "duration": 4.2
    },
    {
      "text": "the shortcut connections that's the",
      "start": 2726.72,
      "duration": 3.96
    },
    {
      "text": "first of these eight steps in the that's",
      "start": 2728.319,
      "duration": 4.0
    },
    {
      "text": "the first four of these eight steps in",
      "start": 2730.68,
      "duration": 4.08
    },
    {
      "text": "the Transformer block then what we do is",
      "start": 2732.319,
      "duration": 4.681
    },
    {
      "text": "that then we again have a layer",
      "start": 2734.76,
      "duration": 4.28
    },
    {
      "text": "normalization feed forward neural",
      "start": 2737.0,
      "duration": 5.16
    },
    {
      "text": "network with J then Dropout and then",
      "start": 2739.04,
      "duration": 4.84
    },
    {
      "text": "another set of shortcut connections",
      "start": 2742.16,
      "duration": 4.199
    },
    {
      "text": "these are the last four steps so overall",
      "start": 2743.88,
      "duration": 3.88
    },
    {
      "text": "there are these eight steps which are",
      "start": 2746.359,
      "duration": 2.881
    },
    {
      "text": "happening in the Transformer block",
      "start": 2747.76,
      "duration": 3.72
    },
    {
      "text": "architecture and it's the same eight",
      "start": 2749.24,
      "duration": 4.48
    },
    {
      "text": "steps which we wrote down when we wrote",
      "start": 2751.48,
      "duration": 5.599
    },
    {
      "text": "the code for the Transformer block class",
      "start": 2753.72,
      "duration": 5.359
    },
    {
      "text": "these are the first four steps and it",
      "start": 2757.079,
      "duration": 4.881
    },
    {
      "text": "ends with the shortcut connection and",
      "start": 2759.079,
      "duration": 4.641
    },
    {
      "text": "these are the last four steps and here",
      "start": 2761.96,
      "duration": 3.04
    },
    {
      "text": "again it ends with the shortcut",
      "start": 2763.72,
      "duration": 4.119
    },
    {
      "text": "connection the main point which we also",
      "start": 2765.0,
      "duration": 4.48
    },
    {
      "text": "recognized when we implemented the",
      "start": 2767.839,
      "duration": 3.361
    },
    {
      "text": "Transformer block class is that the",
      "start": 2769.48,
      "duration": 4.2
    },
    {
      "text": "dimensions are preserved the dimensions",
      "start": 2771.2,
      "duration": 4.84
    },
    {
      "text": "of the input to the Transformer are the",
      "start": 2773.68,
      "duration": 4.56
    },
    {
      "text": "same as the dimensions of the output",
      "start": 2776.04,
      "duration": 5.6
    },
    {
      "text": "from the Transformer block awesome now",
      "start": 2778.24,
      "duration": 5.16
    },
    {
      "text": "we have all the knowledge and now we are",
      "start": 2781.64,
      "duration": 3.84
    },
    {
      "text": "ready to code the entire GPT",
      "start": 2783.4,
      "duration": 4.04
    },
    {
      "text": "architecture so first first let's look",
      "start": 2785.48,
      "duration": 5.24
    },
    {
      "text": "at the forward method um which takes in",
      "start": 2787.44,
      "duration": 6.04
    },
    {
      "text": "the input and remember the input looks",
      "start": 2790.72,
      "duration": 4.32
    },
    {
      "text": "something like this let me show you how",
      "start": 2793.48,
      "duration": 3.76
    },
    {
      "text": "the input actually looks like the input",
      "start": 2795.04,
      "duration": 4.039
    },
    {
      "text": "might look something like this where",
      "start": 2797.24,
      "duration": 4.24
    },
    {
      "text": "here we are showing two batches and each",
      "start": 2799.079,
      "duration": 5.681
    },
    {
      "text": "batch has let's say four token IDs uh so",
      "start": 2801.48,
      "duration": 5.28
    },
    {
      "text": "the number of rows here in this tensor",
      "start": 2804.76,
      "duration": 4.24
    },
    {
      "text": "are equal to the number of batches and",
      "start": 2806.76,
      "duration": 3.88
    },
    {
      "text": "the number of columns are the sequence",
      "start": 2809.0,
      "duration": 3.8
    },
    {
      "text": "length or the number of tokens which we",
      "start": 2810.64,
      "duration": 4.16
    },
    {
      "text": "are considering that's why if you see",
      "start": 2812.8,
      "duration": 3.519
    },
    {
      "text": "the number of rows here in the input",
      "start": 2814.8,
      "duration": 4.68
    },
    {
      "text": "shape are size number of columns are",
      "start": 2816.319,
      "duration": 4.841
    },
    {
      "text": "essentially the sequence length or the",
      "start": 2819.48,
      "duration": 2.599
    },
    {
      "text": "number of",
      "start": 2821.16,
      "duration": 4.08
    },
    {
      "text": "tokens whenever now I'm explaining this",
      "start": 2822.079,
      "duration": 5.04
    },
    {
      "text": "next whole part you can keep that flow",
      "start": 2825.24,
      "duration": 3.64
    },
    {
      "text": "map in mind which I showed you on the",
      "start": 2827.119,
      "duration": 4.081
    },
    {
      "text": "Whiteboard and think about the flow",
      "start": 2828.88,
      "duration": 5.0
    },
    {
      "text": "which we had for every effort moves you",
      "start": 2831.2,
      "duration": 5.56
    },
    {
      "text": "so first we convert the tokens into",
      "start": 2833.88,
      "duration": 5.76
    },
    {
      "text": "token IDs and then we convert the token",
      "start": 2836.76,
      "duration": 4.88
    },
    {
      "text": "IDs into embedding vectors that's what's",
      "start": 2839.64,
      "duration": 3.32
    },
    {
      "text": "done in this token",
      "start": 2841.64,
      "duration": 4.84
    },
    {
      "text": "embedding uh so we initialize an",
      "start": 2842.96,
      "duration": 8.32
    },
    {
      "text": "embedding layer from the pytorch module",
      "start": 2846.48,
      "duration": 7.359
    },
    {
      "text": "and then from this embedding layer we",
      "start": 2851.28,
      "duration": 4.36
    },
    {
      "text": "get the embedding token embedding",
      "start": 2853.839,
      "duration": 4.321
    },
    {
      "text": "vectors for the input IDs and then",
      "start": 2855.64,
      "duration": 3.959
    },
    {
      "text": "similarly we get the positional",
      "start": 2858.16,
      "duration": 3.36
    },
    {
      "text": "embedding vectors for the four positions",
      "start": 2859.599,
      "duration": 4.361
    },
    {
      "text": "so see sequence length is equal to four",
      "start": 2861.52,
      "duration": 4.28
    },
    {
      "text": "so we get the positional embedding",
      "start": 2863.96,
      "duration": 3.92
    },
    {
      "text": "vectors for the four positions and we",
      "start": 2865.8,
      "duration": 3.519
    },
    {
      "text": "add the token embedding to the",
      "start": 2867.88,
      "duration": 3.6
    },
    {
      "text": "positional embeddings so whenever you",
      "start": 2869.319,
      "duration": 3.881
    },
    {
      "text": "try to make sense of Dimensions here",
      "start": 2871.48,
      "duration": 3.76
    },
    {
      "text": "always think of one batch at a time when",
      "start": 2873.2,
      "duration": 3.84
    },
    {
      "text": "you think of one batch things will be",
      "start": 2875.24,
      "duration": 3.839
    },
    {
      "text": "simplified because one batch only has",
      "start": 2877.04,
      "duration": 4.519
    },
    {
      "text": "four tokens let's say so when you look",
      "start": 2879.079,
      "duration": 5.721
    },
    {
      "text": "at this step the output of this step uh",
      "start": 2881.559,
      "duration": 5.881
    },
    {
      "text": "you can visualize the output exactly",
      "start": 2884.8,
      "duration": 4.759
    },
    {
      "text": "from the flow map which we had seen so",
      "start": 2887.44,
      "duration": 5.119
    },
    {
      "text": "let me take you to that yeah look at the",
      "start": 2889.559,
      "duration": 4.881
    },
    {
      "text": "first step over here which was titled as",
      "start": 2892.559,
      "duration": 4.241
    },
    {
      "text": "positional embedding when you look at",
      "start": 2894.44,
      "duration": 6.24
    },
    {
      "text": "this code the token embeddings these are",
      "start": 2896.8,
      "duration": 5.6
    },
    {
      "text": "essentially",
      "start": 2900.68,
      "duration": 5.679
    },
    {
      "text": "uh these are essentially this kind of a",
      "start": 2902.4,
      "duration": 5.8
    },
    {
      "text": "tensor where for every token you have",
      "start": 2906.359,
      "duration": 5.121
    },
    {
      "text": "the 768 dimensional Vector similarly you",
      "start": 2908.2,
      "duration": 5.919
    },
    {
      "text": "can think of the same visualization for",
      "start": 2911.48,
      "duration": 4.599
    },
    {
      "text": "positional embeddings and when you look",
      "start": 2914.119,
      "duration": 4.081
    },
    {
      "text": "at this x equal to token embeddings plus",
      "start": 2916.079,
      "duration": 5.201
    },
    {
      "text": "positional embeddings you can uh you can",
      "start": 2918.2,
      "duration": 5.44
    },
    {
      "text": "try to visualize this this part which we",
      "start": 2921.28,
      "duration": 4.76
    },
    {
      "text": "saw we added the token embeddings to the",
      "start": 2923.64,
      "duration": 5.32
    },
    {
      "text": "positional embeddings right over",
      "start": 2926.04,
      "duration": 5.84
    },
    {
      "text": "here so for every token we added the",
      "start": 2928.96,
      "duration": 4.359
    },
    {
      "text": "token embeddings and the positional",
      "start": 2931.88,
      "duration": 3.719
    },
    {
      "text": "embeddings to get the input embeddings",
      "start": 2933.319,
      "duration": 4.081
    },
    {
      "text": "that's exactly what I'm I've written in",
      "start": 2935.599,
      "duration": 4.161
    },
    {
      "text": "the code over here and then after this",
      "start": 2937.4,
      "duration": 5.24
    },
    {
      "text": "part we go to the next steps which were",
      "start": 2939.76,
      "duration": 5.16
    },
    {
      "text": "which were the same in the visual flow",
      "start": 2942.64,
      "duration": 3.52
    },
    {
      "text": "so the next",
      "start": 2944.92,
      "duration": 3.679
    },
    {
      "text": "step Next Step",
      "start": 2946.16,
      "duration": 5.399
    },
    {
      "text": "was addition of the Dropout which was",
      "start": 2948.599,
      "duration": 4.841
    },
    {
      "text": "step number four over here I'm showing",
      "start": 2951.559,
      "duration": 4.24
    },
    {
      "text": "that with a star right now that was the",
      "start": 2953.44,
      "duration": 4.6
    },
    {
      "text": "step number four so Dropout layer",
      "start": 2955.799,
      "duration": 6.361
    },
    {
      "text": "followed by the Transformer followed by",
      "start": 2958.04,
      "duration": 6.92
    },
    {
      "text": "the Transformer block and then the last",
      "start": 2962.16,
      "duration": 4.639
    },
    {
      "text": "two layers were another layer",
      "start": 2964.96,
      "duration": 3.68
    },
    {
      "text": "normalization layer which which was step",
      "start": 2966.799,
      "duration": 5.28
    },
    {
      "text": "number six here this was the second last",
      "start": 2968.64,
      "duration": 5.199
    },
    {
      "text": "layer which was another layer of",
      "start": 2972.079,
      "duration": 3.921
    },
    {
      "text": "normalization and then the final layer",
      "start": 2973.839,
      "duration": 3.561
    },
    {
      "text": "was output",
      "start": 2976.0,
      "duration": 3.839
    },
    {
      "text": "head so after you get the input",
      "start": 2977.4,
      "duration": 3.959
    },
    {
      "text": "embeddings there are four things you",
      "start": 2979.839,
      "duration": 4.24
    },
    {
      "text": "apply Dropout layer you apply the",
      "start": 2981.359,
      "duration": 5.161
    },
    {
      "text": "Transformer blocks you have the another",
      "start": 2984.079,
      "duration": 4.441
    },
    {
      "text": "layer of normalization and then you",
      "start": 2986.52,
      "duration": 4.0
    },
    {
      "text": "apply the output head that's",
      "start": 2988.52,
      "duration": 6.0
    },
    {
      "text": "it now if you see the Transformer blocks",
      "start": 2990.52,
      "duration": 5.599
    },
    {
      "text": "we are chaining different Transformer",
      "start": 2994.52,
      "duration": 4.12
    },
    {
      "text": "blocks together together based on the",
      "start": 2996.119,
      "duration": 4.521
    },
    {
      "text": "number of layers so in the configuration",
      "start": 2998.64,
      "duration": 3.679
    },
    {
      "text": "we have seen that the number of layers",
      "start": 3000.64,
      "duration": 3.719
    },
    {
      "text": "is equal to 12 right in this",
      "start": 3002.319,
      "duration": 3.441
    },
    {
      "text": "configuration we have seen that the",
      "start": 3004.359,
      "duration": 5.0
    },
    {
      "text": "number of layers is equal to 12 so uh we",
      "start": 3005.76,
      "duration": 5.92
    },
    {
      "text": "are actually chaining 12 Transformer",
      "start": 3009.359,
      "duration": 4.841
    },
    {
      "text": "blocks together using NN do sequential",
      "start": 3011.68,
      "duration": 4.679
    },
    {
      "text": "this NN do sequential is a tensor flow",
      "start": 3014.2,
      "duration": 4.48
    },
    {
      "text": "or pytorch module which allows us to",
      "start": 3016.359,
      "duration": 5.361
    },
    {
      "text": "chain um different neural network blocks",
      "start": 3018.68,
      "duration": 5.08
    },
    {
      "text": "together so we have chained 12",
      "start": 3021.72,
      "duration": 4.68
    },
    {
      "text": "Transformer blocks so when you see the",
      "start": 3023.76,
      "duration": 3.599
    },
    {
      "text": "cell",
      "start": 3026.4,
      "duration": 3.08
    },
    {
      "text": "trf blocks it just looks like one line",
      "start": 3027.359,
      "duration": 4.361
    },
    {
      "text": "of code right but there are actually 12",
      "start": 3029.48,
      "duration": 4.079
    },
    {
      "text": "Transformer blocks chain together here",
      "start": 3031.72,
      "duration": 4.52
    },
    {
      "text": "and in each Transformer block there are",
      "start": 3033.559,
      "duration": 4.201
    },
    {
      "text": "eight different steps which are being",
      "start": 3036.24,
      "duration": 3.64
    },
    {
      "text": "performed so it's a huge number of",
      "start": 3037.76,
      "duration": 3.839
    },
    {
      "text": "operations being performed in this one",
      "start": 3039.88,
      "duration": 3.32
    },
    {
      "text": "simple line of",
      "start": 3041.599,
      "duration": 4.52
    },
    {
      "text": "code that's it actually if you think of",
      "start": 3043.2,
      "duration": 4.159
    },
    {
      "text": "the building blocks if you have",
      "start": 3046.119,
      "duration": 3.321
    },
    {
      "text": "understood the Transformer block if you",
      "start": 3047.359,
      "duration": 4.24
    },
    {
      "text": "have understood the layer normalization",
      "start": 3049.44,
      "duration": 4.28
    },
    {
      "text": "class uh if you have understood the",
      "start": 3051.599,
      "duration": 3.681
    },
    {
      "text": "dimensions with respect to the Token",
      "start": 3053.72,
      "duration": 3.599
    },
    {
      "text": "embeddings positional embeddings",
      "start": 3055.28,
      "duration": 3.559
    },
    {
      "text": "that's all what",
      "start": 3057.319,
      "duration": 3.321
    },
    {
      "text": "the",
      "start": 3058.839,
      "duration": 5.041
    },
    {
      "text": "um Transformer or what the GPT model",
      "start": 3060.64,
      "duration": 5.8
    },
    {
      "text": "class is actually doing it takes in the",
      "start": 3063.88,
      "duration": 4.32
    },
    {
      "text": "input and then the outputs are the",
      "start": 3066.44,
      "duration": 4.639
    },
    {
      "text": "logits which we saw on the Whiteboard so",
      "start": 3068.2,
      "duration": 4.52
    },
    {
      "text": "the dimensions of the logits if you",
      "start": 3071.079,
      "duration": 2.961
    },
    {
      "text": "remember let me take you to the",
      "start": 3072.72,
      "duration": 3.839
    },
    {
      "text": "Whiteboard once more so if you go to the",
      "start": 3074.04,
      "duration": 4.559
    },
    {
      "text": "Whiteboard and if I zoom into this part",
      "start": 3076.559,
      "duration": 4.76
    },
    {
      "text": "of the if I zoom into this part of the",
      "start": 3078.599,
      "duration": 4.72
    },
    {
      "text": "Whiteboard right now you'll see that the",
      "start": 3081.319,
      "duration": 4.24
    },
    {
      "text": "output of the GPT model is this output",
      "start": 3083.319,
      "duration": 4.8
    },
    {
      "text": "Logics and the shape of these logits is",
      "start": 3085.559,
      "duration": 4.8
    },
    {
      "text": "for each batch we have the number of",
      "start": 3088.119,
      "duration": 4.841
    },
    {
      "text": "tokens uh in this case those were four",
      "start": 3090.359,
      "duration": 4.401
    },
    {
      "text": "tokens and the number of columns are",
      "start": 3092.96,
      "duration": 4.24
    },
    {
      "text": "equal to the vocabulary size and then",
      "start": 3094.76,
      "duration": 3.92
    },
    {
      "text": "the First Dimension is the number of",
      "start": 3097.2,
      "duration": 3.639
    },
    {
      "text": "batches which we",
      "start": 3098.68,
      "duration": 5.0
    },
    {
      "text": "have okay so now what we can actually do",
      "start": 3100.839,
      "duration": 5.601
    },
    {
      "text": "is that we can take a simple input batch",
      "start": 3103.68,
      "duration": 4.159
    },
    {
      "text": "and then we can pass in through this",
      "start": 3106.44,
      "duration": 3.04
    },
    {
      "text": "entire GPT",
      "start": 3107.839,
      "duration": 4.081
    },
    {
      "text": "model uh so what we are doing here is",
      "start": 3109.48,
      "duration": 4.359
    },
    {
      "text": "that we are taking an input batch which",
      "start": 3111.92,
      "duration": 4.08
    },
    {
      "text": "has every which has two batches and each",
      "start": 3113.839,
      "duration": 4.601
    },
    {
      "text": "batch has two tokens what we are doing",
      "start": 3116.0,
      "duration": 4.119
    },
    {
      "text": "is that first we create an instance of",
      "start": 3118.44,
      "duration": 3.919
    },
    {
      "text": "the GPT model and pass in the model",
      "start": 3120.119,
      "duration": 4.881
    },
    {
      "text": "configuration and then we create this",
      "start": 3122.359,
      "duration": 6.0
    },
    {
      "text": "object so we create an instance of model",
      "start": 3125.0,
      "duration": 5.44
    },
    {
      "text": "uh which Returns the output so if you",
      "start": 3128.359,
      "duration": 3.841
    },
    {
      "text": "print out the input batch you'll see",
      "start": 3130.44,
      "duration": 3.639
    },
    {
      "text": "this and if you print out the output",
      "start": 3132.2,
      "duration": 4.24
    },
    {
      "text": "batch let's look at one batch currently",
      "start": 3134.079,
      "duration": 3.881
    },
    {
      "text": "if you look at one batch you will see",
      "start": 3136.44,
      "duration": 3.44
    },
    {
      "text": "that there are four tokens and each",
      "start": 3137.96,
      "duration": 3.879
    },
    {
      "text": "token has number of columns equal to the",
      "start": 3139.88,
      "duration": 3.6
    },
    {
      "text": "vocabulary size which is",
      "start": 3141.839,
      "duration": 3.921
    },
    {
      "text": "50257 this is exactly what we had seen",
      "start": 3143.48,
      "duration": 4.599
    },
    {
      "text": "in the output logic right so if you see",
      "start": 3145.76,
      "duration": 4.64
    },
    {
      "text": "in one batch there are four tokens four",
      "start": 3148.079,
      "duration": 5.801
    },
    {
      "text": "rows and there are 50257 columns this is",
      "start": 3150.4,
      "duration": 5.28
    },
    {
      "text": "exactly the same thing which is",
      "start": 3153.88,
      "duration": 3.8
    },
    {
      "text": "happening for the second batch also",
      "start": 3155.68,
      "duration": 3.879
    },
    {
      "text": "there are four tokens here and there are",
      "start": 3157.68,
      "duration": 4.6
    },
    {
      "text": "50257 columns because the vocabulary",
      "start": 3159.559,
      "duration": 3.881
    },
    {
      "text": "size is",
      "start": 3162.28,
      "duration": 4.0
    },
    {
      "text": "50257 so as we can see the output tensor",
      "start": 3163.44,
      "duration": 6.6
    },
    {
      "text": "has the shape uh two batches four tokens",
      "start": 3166.28,
      "duration": 7.0
    },
    {
      "text": "in each batch and 50257 columns since we",
      "start": 3170.04,
      "duration": 5.36
    },
    {
      "text": "passed in two input text and four tokens",
      "start": 3173.28,
      "duration": 4.24
    },
    {
      "text": "each the last Dimension",
      "start": 3175.4,
      "duration": 4.8
    },
    {
      "text": "50257 corresponds to the vocabulary size",
      "start": 3177.52,
      "duration": 5.319
    },
    {
      "text": "of the tokenizer in the next class we",
      "start": 3180.2,
      "duration": 4.72
    },
    {
      "text": "are going to see how to convert these",
      "start": 3182.839,
      "duration": 4.881
    },
    {
      "text": "50257 dimensional output token vectors",
      "start": 3184.92,
      "duration": 4.96
    },
    {
      "text": "back to tokens and how to predict the",
      "start": 3187.72,
      "duration": 5.16
    },
    {
      "text": "next word but for now you can see that",
      "start": 3189.88,
      "duration": 5.239
    },
    {
      "text": "the trans the GPT model class which we",
      "start": 3192.88,
      "duration": 5.0
    },
    {
      "text": "have constructed is working fine and",
      "start": 3195.119,
      "duration": 4.48
    },
    {
      "text": "there are a huge number of parameters",
      "start": 3197.88,
      "duration": 3.28
    },
    {
      "text": "which we are actually dealing with here",
      "start": 3199.599,
      "duration": 3.72
    },
    {
      "text": "this small piece of code over here has",
      "start": 3201.16,
      "duration": 4.32
    },
    {
      "text": "more than 100 million parameters can you",
      "start": 3203.319,
      "duration": 3.881
    },
    {
      "text": "imagine that",
      "start": 3205.48,
      "duration": 3.2
    },
    {
      "text": "there are just a huge number of",
      "start": 3207.2,
      "duration": 3.119
    },
    {
      "text": "parameters because there are 12",
      "start": 3208.68,
      "duration": 3.159
    },
    {
      "text": "Transformer blocks chained together",
      "start": 3210.319,
      "duration": 2.641
    },
    {
      "text": "there are eight steps in each",
      "start": 3211.839,
      "duration": 5.0
    },
    {
      "text": "Transformer block then uh and remember",
      "start": 3212.96,
      "duration": 5.68
    },
    {
      "text": "there is that expansion contraction",
      "start": 3216.839,
      "duration": 4.52
    },
    {
      "text": "layer in each Transformer block that",
      "start": 3218.64,
      "duration": 4.8
    },
    {
      "text": "that layer itself has a huge number of",
      "start": 3221.359,
      "duration": 4.44
    },
    {
      "text": "parameters we have parameters for token",
      "start": 3223.44,
      "duration": 6.0
    },
    {
      "text": "embedding positional embedding uh Etc so",
      "start": 3225.799,
      "duration": 5.961
    },
    {
      "text": "totally all of the parameters add up we",
      "start": 3229.44,
      "duration": 4.359
    },
    {
      "text": "can actually print out the number of",
      "start": 3231.76,
      "duration": 4.359
    },
    {
      "text": "parameters so what we can do is that we",
      "start": 3233.799,
      "duration": 4.361
    },
    {
      "text": "can have have total params and we can do",
      "start": 3236.119,
      "duration": 5.24
    },
    {
      "text": "p. numl what p. numl will do is that it",
      "start": 3238.16,
      "duration": 5.919
    },
    {
      "text": "will actually print out uh so using the",
      "start": 3241.359,
      "duration": 5.081
    },
    {
      "text": "numl method short for number of elements",
      "start": 3244.079,
      "duration": 3.921
    },
    {
      "text": "we can collect the total number of",
      "start": 3246.44,
      "duration": 3.24
    },
    {
      "text": "parameters in the model's parameter",
      "start": 3248.0,
      "duration": 4.0
    },
    {
      "text": "tensors so if you print out the total",
      "start": 3249.68,
      "duration": 4.04
    },
    {
      "text": "number of parameters you will see that",
      "start": 3252.0,
      "duration": 4.44
    },
    {
      "text": "the number of parameters is equal to 163",
      "start": 3253.72,
      "duration": 6.079
    },
    {
      "text": "million huge number of parameters right",
      "start": 3256.44,
      "duration": 5.639
    },
    {
      "text": "and all of this is running on our local",
      "start": 3259.799,
      "duration": 4.841
    },
    {
      "text": "machine so you might be thinking earlier",
      "start": 3262.079,
      "duration": 4.961
    },
    {
      "text": "we spoke of initializing a 12 4 million",
      "start": 3264.64,
      "duration": 5.12
    },
    {
      "text": "parameter GPT model right so then why is",
      "start": 3267.04,
      "duration": 4.92
    },
    {
      "text": "the actual number of parameters 163",
      "start": 3269.76,
      "duration": 4.799
    },
    {
      "text": "million why is it so",
      "start": 3271.96,
      "duration": 5.44
    },
    {
      "text": "higher so this actually relates back to",
      "start": 3274.559,
      "duration": 4.641
    },
    {
      "text": "a concept which is called weight time",
      "start": 3277.4,
      "duration": 3.959
    },
    {
      "text": "that is used in the original gpt2",
      "start": 3279.2,
      "duration": 4.2
    },
    {
      "text": "architecture so which means that the",
      "start": 3281.359,
      "duration": 4.361
    },
    {
      "text": "original gpt2 architecture is reusing",
      "start": 3283.4,
      "duration": 4.48
    },
    {
      "text": "the weights from the token embedding",
      "start": 3285.72,
      "duration": 5.32
    },
    {
      "text": "layer in its output layer so if you go",
      "start": 3287.88,
      "duration": 6.36
    },
    {
      "text": "back to the Whiteboard right now yeah in",
      "start": 3291.04,
      "duration": 4.96
    },
    {
      "text": "the schematic which we saw over here",
      "start": 3294.24,
      "duration": 3.599
    },
    {
      "text": "there is an output layer right there is",
      "start": 3296.0,
      "duration": 3.559
    },
    {
      "text": "an output layer towards the end and",
      "start": 3297.839,
      "duration": 3.28
    },
    {
      "text": "there is a token embedding layer over",
      "start": 3299.559,
      "duration": 5.161
    },
    {
      "text": "here when GPT model when gpt2 model was",
      "start": 3301.119,
      "duration": 5.72
    },
    {
      "text": "constructed the parameters which were",
      "start": 3304.72,
      "duration": 3.96
    },
    {
      "text": "used for the token embedding layer were",
      "start": 3306.839,
      "duration": 3.601
    },
    {
      "text": "the same parameters which were used in",
      "start": 3308.68,
      "duration": 3.919
    },
    {
      "text": "the linear output",
      "start": 3310.44,
      "duration": 5.6
    },
    {
      "text": "layer and that's why uh the total number",
      "start": 3312.599,
      "duration": 5.681
    },
    {
      "text": "of parameters was less in our case right",
      "start": 3316.04,
      "duration": 4.559
    },
    {
      "text": "now we did not reuse the parameters so",
      "start": 3318.28,
      "duration": 4.039
    },
    {
      "text": "when we print out the parameters they",
      "start": 3320.599,
      "duration": 4.921
    },
    {
      "text": "say 163 million uh but in gpt2",
      "start": 3322.319,
      "duration": 5.48
    },
    {
      "text": "architecture they are reusing the weight",
      "start": 3325.52,
      "duration": 4.079
    },
    {
      "text": "so we can actually print this out so in",
      "start": 3327.799,
      "duration": 3.52
    },
    {
      "text": "our case what we can do is that we can",
      "start": 3329.599,
      "duration": 4.2
    },
    {
      "text": "print out the embedding layer shape of",
      "start": 3331.319,
      "duration": 4.76
    },
    {
      "text": "the token embedding layer and we can see",
      "start": 3333.799,
      "duration": 4.921
    },
    {
      "text": "that um the embedding layer shape is",
      "start": 3336.079,
      "duration": 5.921
    },
    {
      "text": "50257 comma 768 and we can also print",
      "start": 3338.72,
      "duration": 5.52
    },
    {
      "text": "out the output layer shape and these",
      "start": 3342.0,
      "duration": 4.52
    },
    {
      "text": "both have exactly the same shape so",
      "start": 3344.24,
      "duration": 4.24
    },
    {
      "text": "that's why in gpt2 model they just",
      "start": 3346.52,
      "duration": 4.799
    },
    {
      "text": "reused this these",
      "start": 3348.48,
      "duration": 5.72
    },
    {
      "text": "parameters um so what we can actually do",
      "start": 3351.319,
      "duration": 6.841
    },
    {
      "text": "is that we can um let we can remove we",
      "start": 3354.2,
      "duration": 5.76
    },
    {
      "text": "can remove the output layer parameter",
      "start": 3358.16,
      "duration": 3.8
    },
    {
      "text": "count from the total parameter count and",
      "start": 3359.96,
      "duration": 4.119
    },
    {
      "text": "check the number of parameters so in the",
      "start": 3361.96,
      "duration": 4.76
    },
    {
      "text": "output layer parameter parameters are",
      "start": 3364.079,
      "duration": 4.48
    },
    {
      "text": "removed the total number of parameters",
      "start": 3366.72,
      "duration": 4.04
    },
    {
      "text": "comes up to be exactly 124 million",
      "start": 3368.559,
      "duration": 4.401
    },
    {
      "text": "parameters which is the same size of the",
      "start": 3370.76,
      "duration": 6.039
    },
    {
      "text": "gpt2 model so weight Ty actually reduces",
      "start": 3372.96,
      "duration": 6.119
    },
    {
      "text": "the overall memory footprint from 163",
      "start": 3376.799,
      "duration": 4.921
    },
    {
      "text": "million we go to 124 million and it also",
      "start": 3379.079,
      "duration": 4.641
    },
    {
      "text": "reduces the computational complexity of",
      "start": 3381.72,
      "duration": 4.879
    },
    {
      "text": "the model however using separate token",
      "start": 3383.72,
      "duration": 4.839
    },
    {
      "text": "embedding and output layers results in",
      "start": 3386.599,
      "duration": 4.2
    },
    {
      "text": "better training and model performance",
      "start": 3388.559,
      "duration": 4.0
    },
    {
      "text": "that's why when in our code which we are",
      "start": 3390.799,
      "duration": 3.961
    },
    {
      "text": "writing we are using separate layers in",
      "start": 3392.559,
      "duration": 4.721
    },
    {
      "text": "our GPT model implementation and this is",
      "start": 3394.76,
      "duration": 5.44
    },
    {
      "text": "true for modern llms as well weight time",
      "start": 3397.28,
      "duration": 5.64
    },
    {
      "text": "is good for reducing the memory",
      "start": 3400.2,
      "duration": 5.08
    },
    {
      "text": "footprint and reducing the computational",
      "start": 3402.92,
      "duration": 5.119
    },
    {
      "text": "time and complexity but to get a better",
      "start": 3405.28,
      "duration": 5.079
    },
    {
      "text": "model training and performance it's good",
      "start": 3408.039,
      "duration": 4.0
    },
    {
      "text": "not to reuse the",
      "start": 3410.359,
      "duration": 4.361
    },
    {
      "text": "parameters um we can also print out the",
      "start": 3412.039,
      "duration": 6.201
    },
    {
      "text": "space which is taken by our model",
      "start": 3414.72,
      "duration": 5.8
    },
    {
      "text": "so we can compute the memory",
      "start": 3418.24,
      "duration": 4.119
    },
    {
      "text": "requirements of the 163 million",
      "start": 3420.52,
      "duration": 3.519
    },
    {
      "text": "parameters so the total size of the",
      "start": 3422.359,
      "duration": 3.801
    },
    {
      "text": "model it turns out it's around 600",
      "start": 3424.039,
      "duration": 5.441
    },
    {
      "text": "megabytes of space so in conclusion by",
      "start": 3426.16,
      "duration": 5.439
    },
    {
      "text": "calculating the memory requirements for",
      "start": 3429.48,
      "duration": 4.359
    },
    {
      "text": "the 163 million million parameters in",
      "start": 3431.599,
      "duration": 5.681
    },
    {
      "text": "our gpt2 model um and assuming that each",
      "start": 3433.839,
      "duration": 5.561
    },
    {
      "text": "parameter is a 32-bit float taking up to",
      "start": 3437.28,
      "duration": 3.88
    },
    {
      "text": "four bytes that's why we have multiplied",
      "start": 3439.4,
      "duration": 3.719
    },
    {
      "text": "by four over here we have assumed that",
      "start": 3441.16,
      "duration": 3.879
    },
    {
      "text": "each parameter takes around four bytes",
      "start": 3443.119,
      "duration": 3.48
    },
    {
      "text": "so we multiplied the total number of",
      "start": 3445.039,
      "duration": 4.481
    },
    {
      "text": "parameters by four to get the total",
      "start": 3446.599,
      "duration": 6.121
    },
    {
      "text": "size that 620 MB which illustrates the",
      "start": 3449.52,
      "duration": 5.24
    },
    {
      "text": "relatively large storage capacity",
      "start": 3452.72,
      "duration": 4.04
    },
    {
      "text": "required to accommodate even relatively",
      "start": 3454.76,
      "duration": 5.079
    },
    {
      "text": "small llms so gpt2 is relatively small",
      "start": 3456.76,
      "duration": 5.64
    },
    {
      "text": "120 million compared to there are now",
      "start": 3459.839,
      "duration": 4.72
    },
    {
      "text": "models which have more than b billion",
      "start": 3462.4,
      "duration": 5.08
    },
    {
      "text": "parameters imagine how much space such a",
      "start": 3464.559,
      "duration": 4.921
    },
    {
      "text": "model will take on your device it's",
      "start": 3467.48,
      "duration": 4.559
    },
    {
      "text": "impossible to run extremely large models",
      "start": 3469.48,
      "duration": 4.4
    },
    {
      "text": "on local devices and that's why we need",
      "start": 3472.039,
      "duration": 4.361
    },
    {
      "text": "GPU access",
      "start": 3473.88,
      "duration": 4.76
    },
    {
      "text": "so in this lecture today we implemented",
      "start": 3476.4,
      "duration": 4.76
    },
    {
      "text": "the GPT model architecture and saw that",
      "start": 3478.64,
      "duration": 4.88
    },
    {
      "text": "it outputs numerical tensors right so",
      "start": 3481.16,
      "duration": 4.399
    },
    {
      "text": "you might be curious how do we go from",
      "start": 3483.52,
      "duration": 4.559
    },
    {
      "text": "that numerical tensor output into the",
      "start": 3485.559,
      "duration": 4.201
    },
    {
      "text": "next word prediction",
      "start": 3488.079,
      "duration": 4.72
    },
    {
      "text": "task uh so that we'll be doing next in",
      "start": 3489.76,
      "duration": 4.319
    },
    {
      "text": "the next lecture we are going to",
      "start": 3492.799,
      "duration": 3.32
    },
    {
      "text": "generate text from the output tensor",
      "start": 3494.079,
      "duration": 4.121
    },
    {
      "text": "which we have obtained today today what",
      "start": 3496.119,
      "duration": 4.081
    },
    {
      "text": "I want to what I want you to appreciate",
      "start": 3498.2,
      "duration": 5.159
    },
    {
      "text": "is just within 10 lines of or even 15 20",
      "start": 3500.2,
      "duration": 5.48
    },
    {
      "text": "lines of code we have actually run a",
      "start": 3503.359,
      "duration": 4.281
    },
    {
      "text": "very power powerful large language model",
      "start": 3505.68,
      "duration": 4.439
    },
    {
      "text": "completely on our laptop and we buil",
      "start": 3507.64,
      "duration": 4.36
    },
    {
      "text": "this model from scratch this model had",
      "start": 3510.119,
      "duration": 5.041
    },
    {
      "text": "more than 160 million parameters it took",
      "start": 3512.0,
      "duration": 6.119
    },
    {
      "text": "600 megabytes of space on our machine",
      "start": 3515.16,
      "duration": 5.36
    },
    {
      "text": "indicating the size required by these",
      "start": 3518.119,
      "duration": 4.72
    },
    {
      "text": "models and I want you to appreciate that",
      "start": 3520.52,
      "duration": 4.12
    },
    {
      "text": "once you understood the visual flow map",
      "start": 3522.839,
      "duration": 4.321
    },
    {
      "text": "I showed you on the Whiteboard it's just",
      "start": 3524.64,
      "duration": 4.719
    },
    {
      "text": "eight lines of code but to understand",
      "start": 3527.16,
      "duration": 3.679
    },
    {
      "text": "these eight lines of codes we need to",
      "start": 3529.359,
      "duration": 3.44
    },
    {
      "text": "put in a lot of effort because there is",
      "start": 3530.839,
      "duration": 4.881
    },
    {
      "text": "a lot of theory and lot of intuition",
      "start": 3532.799,
      "duration": 4.8
    },
    {
      "text": "behind every single line of code over",
      "start": 3535.72,
      "duration": 5.16
    },
    {
      "text": "here but once you get that it just these",
      "start": 3537.599,
      "duration": 5.281
    },
    {
      "text": "uh stacking of the token embeddings",
      "start": 3540.88,
      "duration": 3.479
    },
    {
      "text": "positional embeddings Dropout layer",
      "start": 3542.88,
      "duration": 3.08
    },
    {
      "text": "Transformer blocks another layer",
      "start": 3544.359,
      "duration": 4.321
    },
    {
      "text": "normalization and the output output head",
      "start": 3545.96,
      "duration": 5.72
    },
    {
      "text": "that's it and then we directly get the",
      "start": 3548.68,
      "duration": 5.96
    },
    {
      "text": "Logics um I hope you are following these",
      "start": 3551.68,
      "duration": 4.879
    },
    {
      "text": "lectures I know this lecture is also",
      "start": 3554.64,
      "duration": 4.52
    },
    {
      "text": "become a bit long but it is essential to",
      "start": 3556.559,
      "duration": 4.881
    },
    {
      "text": "show you the theory as well as the code",
      "start": 3559.16,
      "duration": 3.919
    },
    {
      "text": "in today's lecture I could have directly",
      "start": 3561.44,
      "duration": 3.359
    },
    {
      "text": "showed you this code which was only this",
      "start": 3563.079,
      "duration": 4.161
    },
    {
      "text": "much but then at every step you would",
      "start": 3564.799,
      "duration": 4.441
    },
    {
      "text": "not have visualized the dimensions what",
      "start": 3567.24,
      "duration": 3.72
    },
    {
      "text": "is exactly happening can we take a",
      "start": 3569.24,
      "duration": 3.2
    },
    {
      "text": "specific example and see what is",
      "start": 3570.96,
      "duration": 3.8
    },
    {
      "text": "happening for that example and now I've",
      "start": 3572.44,
      "duration": 3.919
    },
    {
      "text": "given you a visual understanding",
      "start": 3574.76,
      "duration": 3.64
    },
    {
      "text": "intuitive understanding as well as a",
      "start": 3576.359,
      "duration": 4.48
    },
    {
      "text": "coding understanding in the code I",
      "start": 3578.4,
      "duration": 5.36
    },
    {
      "text": "written uh the description of what what",
      "start": 3580.839,
      "duration": 5.441
    },
    {
      "text": "is happening in every layer for example",
      "start": 3583.76,
      "duration": 4.279
    },
    {
      "text": "here you will see that the forward",
      "start": 3586.28,
      "duration": 4.519
    },
    {
      "text": "method takes a batch of input tokens",
      "start": 3588.039,
      "duration": 4.52
    },
    {
      "text": "computes their embeddings applies the",
      "start": 3590.799,
      "duration": 3.24
    },
    {
      "text": "positional embeddings passes the",
      "start": 3592.559,
      "duration": 3.681
    },
    {
      "text": "sequence through the Transformer block",
      "start": 3594.039,
      "duration": 4.08
    },
    {
      "text": "normalizes the final output and then",
      "start": 3596.24,
      "duration": 4.119
    },
    {
      "text": "computes the logits representing the",
      "start": 3598.119,
      "duration": 5.801
    },
    {
      "text": "next tokens unnormalized",
      "start": 3600.359,
      "duration": 3.561
    },
    {
      "text": "probabilities we will convert these",
      "start": 3604.88,
      "duration": 5.919
    },
    {
      "text": "Logics into tokens and text outputs in",
      "start": 3607.96,
      "duration": 5.32
    },
    {
      "text": "the next class but whenever this when",
      "start": 3610.799,
      "duration": 4.201
    },
    {
      "text": "you access this code file Ive also",
      "start": 3613.28,
      "duration": 4.24
    },
    {
      "text": "written a number of such comments and",
      "start": 3615.0,
      "duration": 4.24
    },
    {
      "text": "paragraphs here which you can read and",
      "start": 3617.52,
      "duration": 5.039
    },
    {
      "text": "try to understand the code for yourself",
      "start": 3619.24,
      "duration": 5.52
    },
    {
      "text": "one thing which I will very very highly",
      "start": 3622.559,
      "duration": 4.841
    },
    {
      "text": "suggest is that",
      "start": 3624.76,
      "duration": 4.2
    },
    {
      "text": "whatever I written on the Whiteboard",
      "start": 3627.4,
      "duration": 3.6
    },
    {
      "text": "right when I was I was myself writing",
      "start": 3628.96,
      "duration": 4.52
    },
    {
      "text": "the flow map of the Transformer on the",
      "start": 3631.0,
      "duration": 4.4
    },
    {
      "text": "Whiteboard this this flow map I",
      "start": 3633.48,
      "duration": 4.119
    },
    {
      "text": "understood that my understanding also",
      "start": 3635.4,
      "duration": 5.28
    },
    {
      "text": "became very clear if I just ran the code",
      "start": 3637.599,
      "duration": 5.401
    },
    {
      "text": "my understanding was not that strong but",
      "start": 3640.68,
      "duration": 5.0
    },
    {
      "text": "when I wrote every single step of the",
      "start": 3643.0,
      "duration": 5.52
    },
    {
      "text": "code on a whiteboard um and when I",
      "start": 3645.68,
      "duration": 6.48
    },
    {
      "text": "created a visual flow map uh which I",
      "start": 3648.52,
      "duration": 5.559
    },
    {
      "text": "demonstrated to you in today's lecture",
      "start": 3652.16,
      "duration": 3.919
    },
    {
      "text": "such kind of visual flow map which I'm",
      "start": 3654.079,
      "duration": 3.76
    },
    {
      "text": "showing right now on the screen it",
      "start": 3656.079,
      "duration": 3.441
    },
    {
      "text": "really strengthened my understanding and",
      "start": 3657.839,
      "duration": 4.081
    },
    {
      "text": "it made me confident about this subject",
      "start": 3659.52,
      "duration": 4.48
    },
    {
      "text": "if you can also write this similar flow",
      "start": 3661.92,
      "duration": 4.36
    },
    {
      "text": "map on your notebook or on the",
      "start": 3664.0,
      "duration": 4.119
    },
    {
      "text": "Whiteboard which you are using it will",
      "start": 3666.28,
      "duration": 4.36
    },
    {
      "text": "tremendously improve your understanding",
      "start": 3668.119,
      "duration": 4.041
    },
    {
      "text": "thanks a lot everyone I hope you are",
      "start": 3670.64,
      "duration": 3.8
    },
    {
      "text": "enjoying from this lectures next lecture",
      "start": 3672.16,
      "duration": 3.959
    },
    {
      "text": "will probably be the last lecture in the",
      "start": 3674.44,
      "duration": 4.56
    },
    {
      "text": "GPT architecture series uh where we'll",
      "start": 3676.119,
      "duration": 4.641
    },
    {
      "text": "generate text from the final output",
      "start": 3679.0,
      "duration": 3.52
    },
    {
      "text": "tensor which we have obtained today",
      "start": 3680.76,
      "duration": 4.079
    },
    {
      "text": "thanks a lot everyone and I look forward",
      "start": 3682.52,
      "duration": 6.559
    },
    {
      "text": "uh to seeing you in the next lecture",
      "start": 3684.839,
      "duration": 4.24
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch Series today what we are going to do is we are going to construct the entire GPT architecture or put in other words we are going to code the entire GPT model we have been building up to this lecture for a pretty long time now so let me just quickly recap what all we'll be covering in today today's lecture and what we have completed previously so in the previous set of lectures which started around four to five lectures back in this GPT architecture or llm architecture lecture series we started out with a dummy GPT model class so when we started around four to five lectures back we did not have H an understanding of the building blocks of the llm architecture or the GPT architecture slowly we worked our way towards it initially we learned about layer normalization we wrote code for it then we wrote a code for the feed forward neural network along with the J activation then we learned about shortcut connections and wrote code for that and in the previous lecture we also wrote the code for the entire Transformer Block in which the layer normalization the J activation the feed forward neural network and the shortcut connections all come together today what we are going to do is we are going to write code for this final piece of the puzzle which is how Point number one point number two 3 4 5 and six essentially come together to form the entire GPT architecture throughout this these four to five lectures on the GPT architecture we have this or we should have this visual figure in our mind so here is what happens in the GPT architecture we have text which gets tokenized we have the dropout layer over here and then the input embeddings are essentially passed to this blue colored block which is called as the Transformer Block in the previous lecture we coded out all of these individual Elements which I'm marking in yellow right now in the Transformer block and we saw that when you give an input to the Transformer how to code out all of these blocks and how to stack them all together so that we get the output today we are also going to learn about these final two steps which is another layer normalization layer and another linear output layer towards the end and we are going to see how do we postprocess the output from the Transformer to get an output from the entire GPT model that's the main goal today so if you if you look at the overall picture the mass multihead attention is the key component of the Transformer block and the Transformer block is the key component of the entire GPT model we have coded out the Transformer block right that's fine but now we have to code out the entire GPT model so we'll start from the we'll start from the bottom we'll start from uh uh this input IDs or rather input text we'll tokenize it then we will uh integrate the Dropout and then we'll integrate the Transformer building block and then we'll also integrate these final two building blocks so you can think of this Le as an assembly lecture where different moving pieces of the GPT architecture will all come together and ultimately you will have a system in which or ultimately you'll develop a model which receives an input so the input is in the form of text tokens and the output from the model looks something like this right now in the next lecture we are going to see how to decode this output to predict the next word so the main goal of the GPT model is to take an input text such as every effort moves you and to predict the next word so we are training the model for the next word prediction task today we are going to take today I'm going to take you all to this stage where we get this final output and in the next lecture we are going to see how to get the next word from this final tensor I'll assure you that the final tensor which we will obtain today can be easily used to decode the next word so the main task today is how to go from the Transformer output to this final output tensor from the GP G PT model and we are also going to take an Hands-On example and show you how the operations Stack Up throughout this entire GPT model okay so when we started this lecture series we actually started with this dummy GPT model which I'm going to show you right now so we started this lecture series with this dummy GPT model class right over here and at that time we had left several aspects blank so we had left the Transformer Transformer block blank we had also left the layer normalization class blank we had coded out some aspects of the forward method but most of this these classes were blank um but that's fine now we are ready to fill up these different aspects so that we can return the output so ultimately we are going to see this entire workflow to today we are going to have the input IDs which will be converted into token embeddings then we'll add positional embeddings We'll add the Dropout layer we'll pass the output of the Dropout through the Transformer block then the output from the Transformer block will be passed through another layer normalization layer and then we'll finally have an output head layer and we'll return the Logics don't worry about understanding all of this right now I'm going to sequentially take you through each of this step by step uh and explain to you what exactly happens in each of these uh these code lines okay okay so what we are essentially going to do is that since we coded this Transformer block last time we can replace this with the actual Transformer code we are also coded this layer normalization Block in one of the previous lectures so we'll replace this with the layer normalization code okay so the goal now we are ready to achieve the goal of today's lecture which is to assemble a fully working version of the original uh 124 million parameter version of gpt2 so we are going to do a pretty awesome thing today we are going to take input texts and then we are going to pass them through this entire gpt2 architecture which by the way has 124 million parameters and then we'll get the output answer and all of this we'll be doing on our local computer I'll be sharing the code with you so you'll be able to execute it on your own end that's pretty awesome right you'll be probably running a large scale large language model for the first time on your local machine so you at this moment if you have followed the previous lectures we are going to assemble several components here so people who have followed the previous lectures this lecture is going to be very enriching for you if you have come to this lecture for the first time I've designed it so that you can follow it along but please understand that the value which you will derive from this lecture will be significantly higher once you have also gone through the previous lectures okay uh I hope all of you have this visual map in mind we are going to do all of these steps which are shown in this visual map today and with actual parameters from gpt2 we are going to use around 124 million parameters today before I dive into code I want to first show you everything on this whiteboard especially in terms of Dimensions so that you get a clear understanding of what is exactly happening when we are going to move to code I have seen that many students who learn about large language models they are very unclear about how the dimensions work out so what I've actually done is that I've have made this uh flow map over here so I'm just zooming out here right now we are going to going to go through all of this in just a moment but this is the flow map which I'm going to teach you right now and the blocks which you see on the screen they are mostly there to represent the dimensions of the input the dimensions of the output so my goal here is to visually convince you of what exactly is happening in the GPT model so that the code becomes significantly easier the code for this GPT model is actually pretty simple and straightforward the only difficulty which students face is that many tensors come into the picture many dimensions come into the picture and students get confused as to they cannot visualize what's going on and I've have not found too many too much good material out there which takes students through every single step in the GPT model like this uh even in the previous lecture we went through the entire code but we did not see this Hands-On example of let's say if you take a specific input sequence how does that input sequence flow through the different blocks of the GPT model and how do we get the output let's dive into every single detail remember the name of this whole playlist is building llms from scratch we are not going to assume anything I want to teach you the nuts and bolts of how every single line of code works and that's why I have made this effort to construct this visual flowchart okay so let's say the input is every effort moves you right and then we have to make the output prediction which is the prediction of the next word and the next word is forward every effort moves you forward so let's go through a sequence of steps of what exactly happens in the GPT model when this input is given I'm going to switch color right now to a darker color so that you all will see what I'm writing on the board okay so ideally inputs come in batches so what we are going to do is is that let's say when we go to code we'll see that we have two batches and in each batch there are four tokens so the first batch has every effort moves you and the four token IDs corresponding to that and the second batch has token IDs corresponding to the second sentence for the sake of Simplicity right now I'm just going to analyze the first batch and then the same learnings which I'm going to show you can be applied to the second batch as well uh so I'm going I'm getting rid of one dimension which is the batch dimension for now for the sake of simplicity so we are going to focus only on these four words every effort moves you and prediction of the next World the first step is that remember that we have a vocabulary right even when gp22 model was constructed we have vocabulary and the vocabulary size is five U 5 I think it is 50257 so 50257 is the vocabulary size for GP 2 so the first step is you take every word and you map it to a token ID in the vocabulary every token is mapped to a token ID so these are the four tokens for the sake of Simplicity think of every one token equal to one word that's not at all what's happening in the gpt2 because gpt2 uses bite pair encoding which is a subword tokenizer we have a separate lecture for that uh where you can see that even characters and small subwords can be tokens but just for the sake of Simplicity I'm going to use one token equal to one word interchangeably in today's lecture okay so we are looking at this first batch which has four tokens or four words that's the first step to convert these tokens into these four token IDs awesome the next step is actually to take these token IDs and to convert them into token embedding vectors so this is a key Point here computers can't understand words right and it does not make sense to just have token IDs because because we need to capture the meaning between words dog and puppy are close to each other cat and kitten are closer to each other it turns out that representing words in a vectorial format can help preserve the semantic meaning between words so the first step is to convert every input token into this token embedding vector and to decide an embedding size so we are using an embedding size of 768 because that was the embedding size which was used for the smallest gpt2 model when it came out so if you see every which is the first token is now encoded as a 768 Dimension Vector over here effort which is the second token is also encoded as a 768 dimensional Vector moves which is the third token is encoded as a 768 dimensional vector and U which is the fourth token is also encoded as a 768 dimensional Vector one point which I want to mention here is that this encoding we do not know what's the best encoding from the start we are initially going to project these vectors randomly in the 768 dimensional space and then we are also going to learn the token embedding parameters in GPT models along with everything else the token embedding all the embedding parameters are learned so right now let's say I told you about cat and kitten right when we start out the vector for cat and the vector for kitten is initialized in random directions but when the model is trained when the embedding vectors are trained they will be closer together they'll be more aligned so right now for every U for the token every for the token effort for the token moves and for the token U we randomly initialize vectors in the 768 dimensional space that's the first step token embedding the second step is that remember along with the semantic meaning of words what's also important to capture is where the word comes in the particular sentence so every comes so every effort moves you and every comes in position one effort comes in position two moves comes in position three and U comes in position number four so along with representing the words themselves as an embedding Vector we also represent every position as an embedding Vector so we are considering four positions here right which also becomes the context size remember the context size is the maximum number of words which can be used to predict the next word in our case the context size is equal to four which means only four positions matter so that's why in the positional embedding we we are going to look at the embedding vectors for four positions position number one has again a 768 dimensional Vector position number two has a 768 dimensional Vector position number three has a 768 dimensional vector and position number four has a 768 dimensional Vector similar to token embedding we actually do not know the embedding values in each of these uh um in each of these positional embedding vectors these embedding values are initialized randomly initially we do not know what these embedding values represent these will be trained as the during the training procedure but the important thing to note is the embedding size the embedding size for every Vector in the positional embedding is 768 and this is the same size as the uh token embedding and the reason for this is in the third step what we are going to do is that we are going to add the token embedding for each token along with the positional embedding so if you go to step number three which is seen on the screen right now let me zoom in further for the first word which is every it's in position one so we are going to take the token embedding for every and we are going to add it uh with the positional embedding for the first position and the result is the input embedding for the first token so since the token embedding and positional embedding have the same dimensions the input embedding also has the dimension of 768 so this is the input embedding for position then effort so when you come to effort which is the second word it's in second position so we take the token embedding uh for the second word and we add it with the second positional embedding for the second position and we get the input embedding for the second token which is effort and here again the embedding size is equal to 768 you can see over here which I'm marking right now in purple color the embedding size for the uh second position or the second the input embedding size for the second token is 768 Now we move to the third token which is moves this is in position three so we'll take the token and add the positional embedding Vector for position three and that leads to input embedding Vector for moves which has an embedding size 768 similarly for the fourth position which is U we take the token embedding and add the positional embedding and we get the input embedding which has the size of 768 this is Step number three remember this is a very important step token embeddings plus positional embeddings leads to the input embeddings which this formula I have also written over here input embedding equal to token embedding uh yeah input embedding equal to token embedding plus the positional embedding okay so that's the step number three and after these steps are completed we move to step number four step number four introduces Dropout so what happens in Dropout is that until now we have input embeddings for every word right we have input embeddings for every effort uh moves and U and the embedding size is a vector so which means that for every token we have uh the input embedding of 768 sized Vector in Dropout what happens is that we randomly turn off some elements of every uh every input embedding to zero and that's specified by the dropout rate so if the dropout rate is 50% from every embedding randomly 50% of the elements are turned off to zero so let's say this might be turned off this might be turned off so 50% of 768 is around 384 right so around 384 elements of each input embedding are turned off to zero and this is done for every token so here I'm just showing the random Elements which are turned off to zero remember this is probabilistic so when I say 50% not exactly half of the embeddings will be turned off to zero on an average 50% of the input embeddings will be turned to zero okay so why is Dropout implemented the main reason why Dropout is implemented is to prevent overfitting improve generalization uh and this generally helps a lot the main Dropout technique was initially implemented in neural networks to prevent some neurons from being lazy so during training sometimes what happens is that some neurons don't learn anything and they depend on other neurons and that leads to problems in generalization so what people do is that they Implement Dropout layers where neurons are turned off randomly so the neurons which were lazy earlier have no choice but to learn something and that improves the generalization performance because every neuron is generally trying to learn something and that's the similar case for year also wherever Dropout is implemented the main reason for implementing Dropout is to prevent overfitting or to improve generalization performance okay so we have seen four steps up till now let's recap them the first step is uh token embedding uh which we saw the second step is positional embedding which we again saw the third step is uh input embedding which is essentially adding the token embedding plus the positional embedding and the fourth step is implementing Dropout now remember up till here the Transformer block has not been introduced at all we have still we are still outside the Transformer block so let's go to this overall structure of this Transformer block again to see what all we have seen up till now so if you look at this structure until now um let me zoom in further yeah if you look at the structure until now we have seen the four steps which come before here so we tokenize the text into input IDs then we add the to then we have the token embeddings which was the first step then we add the positional embeddings which was the second and third steps we get the input embedding and then we apply the Dropout so we have seen these four steps until now we are at this point and after the Dropout now we will enter the Transformer Block in which all of these steps will be performed so let's go to step number five right now where we'll be looking at the Transformer block okay so when we reach the Transformer this these are the input embeddings which we have so these are the input embeddings with Dropout one thing which I would like you to see is that the dimensions are being preserved so when we started this at the first step these were the same dimensions right every token had the dimen embedding dimension of 768 and when we enter the enter the Transformer block every is still a 768 dimensional Vector effort is still a 7608 dimensional Vector moves is still a 7608 dimensional vector and U is still a 7608 dimensional Vector the one thing which has not happened is that until now every Vector only contains meaning about itself but we do not know let's say when we are looking at every how much information how much attention should be we give to effort moves and you to predict the next world that's very important right along with capturing the semantic meaning which Vector embedding does it does not capture the meaning of how every how let's say each word is related to other words so let's say when we look at effort and we want to see when we are looking at effort how much attention should we pay to every moves and you when predicting the next word that information is not captured and that will be done through the attention Block in the Transformer module let's see when we get to that so now we are at the stage where we have the input embeddings with Dropout then we apply the first layer in the Transformer block which is the layer normalization so this is also called as the layer Norm what this layer normalization will do is that it will look at every uh every token so let's say I'm looking at this token every uh and then U it will look at all of these values here and it will normalize the values so that the mean of these values is equal to zero and the variance of these values is equal to one and this will be done for every single token so after we do it for the token every we then move to effort so we'll take the take all of these embedding values and we will normalize them so we'll subtract the mean from every value and divide by uh the square root of variance and this same procedure normalization procedure will be done to move and to you so after the layer normalization is done when we look at every token and when we look at the values present in the embedding we'll see that the mean of those embedding values is equal to zero and the variance of those embedding values is equal to one for every single token layer normalization is performed to improve the stability during the training procedure okay after layer normalization is performed the most important step which is actually the engine of the Transformer block which is why llms work so well is this Mass multi-ad attention what is done in this mod in this step is that we conver we take the embedding vectors and we convert them into context vectors so if you look at the output of the M multi attention the size of the output is same so for the word effort let's say it we still have an embedding of 768 Dimensions but now this is called as the context Vector embedding the reason is called context Vector is that along with capturing the semantic meaning of effort which the Vector embedding already did this context Vector which exists for effort also captures the meaning of how much attention should we give to every how much attention should we give to moves and how much attention should we give to you when we are looking at effort so the context Vector for every token captures the meaning of how much attention should be given to all the other tokens in the sentence that's why it's called attention this is by far the most important step in the entire GPT model without this part it would llms would not perform as well this part really tells us that when we want to predict the next word which are the important words to look at what is the meaning between different words how do different words attend to each other so the context Vector which I have written here it looks very simple right now but we have devoted five lectures of 1 Hour 1 and a half hour each to understand this one step uh which appears in the whole GPT model because here is where the magic happens here is where we transfer U Vector embeddings into context Vector embeddings so we we contain we capture meaning we capture context as to how different tokens or different words are related with each other awesome so until this step here again you can see the dimensionality is mentioned is preserved throughout that's what I like about the GPT model so for throughout every step you'll see that we still have four tokens and the dimension of each is 768 this really makes the GPT model scalable it's much easier to add multiple modules together because addition of modules does not change the dimensionality then after multi-head attention we again have a Dropout layer which randomly uh turns off certain um certain context Vector values to zero so here I have shown the color red where the values of every we look at every token and weite randomly switch off certain values to zero it's the same Dropout layer which we had seen earlier then we actually have a shortcut connection so wherever shortcut connections are mentioned it means that the output of this the output of this layer the output of um the Dropout layer is added back to the input which we started with to the Transformer uh this input the output is added back and the reason it's done is because it provides another route for the gradient to flow and it prevents The Vanishing gradient problem which means that the training proceeds in a much smoother manner so after the shortcut connection is applied that does not change the dimension at all we again apply one more round of layer normalization so again we look at every Row the mean is the mean is uh changed to zero the variance is changed to one so that's how every values is normalized we subtract the mean from every value divide by the standard divide by the square root of the variance so that finally when we look at the values together their mean will be zero and their variance will be one this is done for every single token and then after the layer normalization layer we have a feed forward neural network so when you zoom into this network you will see that this is kind of like an expansion contraction uh Network where let's say you have the inputs right so we process every input step by step here if you first look at the first token which is every it's a four dimension it's a 768 dimens token right um if you see the input which enters the speed forward neural network is that every token has a dimension or an embedding size of 768 so what happens in this neural network is that we look at each token sequentially so let's say we are looking at the first token which is every let me zoom into this neural network a bit uh yeah so first we project this input into a higher dimensional space in fact uh this neural network has one hidden layer and the number of neurons in this hidden layer is 4 * 768 which is four * the embedding Dimension and then we contract back to the original Dimension so the output from this neural network is the same Dimension as the input but there is this expansion and contraction which is happening and that allows a much richer exploration of parameters that makes our llm much better at prediction of the next word since it captures the meaning in a much better manner so if you look at the output from this neural network the output from this neural network is the same size as the input which entered it we have four tokens and the embedding size is equal to 768 uh but this expansion contraction which you see over here the arrow which I'm showing here is the expansion the arrow which I'm showing now is the contraction that is the key because since the middle layer has huge number of neurons it is four times the embedding Dimension neurons which is 4 * 768 the number of parameters are huge in this feed forward neural network that allows for a richer exploration space after this feed forward neural network we have another Dropout layer which randomly uh switches off certain values to be equal to zero which I have just shown here again by the red um by the red color so in every token random values are set to zero typically one dropout rate is mentioned at the start and that's applied everywhere where these Dropout layers are implemented then we have another shortcut connection where we basically add the output uh after this Dropout to the input when we entered here so the this is the input in this second row so this so the input which was there here is added uh to the output of the Dropout layer which we are seeing over here and that's why it's a shortcut connection and then the output from the shortcut connection is our Transformer block output over here so right now the Transformer block output which I'm highlighting is the output after so many steps in the Transformer block so if you look at the dimensions of this output you'll see that every token is again a 768 dimensional Vector over here so the dimensions are exactly the same as the input to the Transformer block so let me zoom out a bit here and you can now try to appreciate the number of steps which are involved in the Transformer so if I zoom out here and if you look at the right side of the screen along with me uh let me actually move it to the center yeah so these are all the steps which are involved in the Transformer right now whatever you are seeing on the screen U let me minimize this color yeah I hope uh you can see the entire Transformer workf flow on the screen right now we have we start with the layer normalization so I'll show show this with a different color probably yeah we start with the layer normalization then we move to the uh mask multihead attention then we then we have a Dropout layer uh then we have a shortcut connection over here let me Mark this line yeah we have a shortcut connection over here then we have another layer normalization layer we we have a feed forward neural network we have a Dropout layer then we again have a final shortcut layer which actually takes us to the Transformer block output um now let us actually go to the uh visual diagram again to see whether we indeed have implemented all the steps which were mentioned in the Transformer block so I'm zooming in over here right now and if you see there are actually 1 2 3 4 5 6 7even eight different steps right we start out with the again let me switch to a different color here yeah yeah we start out with the layer Norm we go to the multi-ad attention we go to the Dropout we have the shortcut connection that's the first block then we have again the layer normalization the feed forward neural network the Dropout and then the last shortcut so there are eight different steps here which were the same eight different steps we implemented in the uh visual flow map and now there are the final two steps which are remaining so let's look at the last two steps of the GPT model in the visual flow map so the fifth step was the Transformer step which we implemented within the Transformer also there were eight different steps now we go to the next block which is uh uh the next block is another layer of normalization so the Transformer block itself has two normalization layers but we have another normalization layer after the Transformer block output so here we have the Transformer block output and here you can focus on the dimensions again every vector or every token has an embedding size of 768 right again the dimensionality is preserved and then we apply the layer normalization so same thing as what the near layer normalization does is done here also in every token we normalize the embedding values so that the mean of the resultant embedding values is zero and the variance is equal to one and this is done for all the four tokens so again the layer normalization does not change the dimensions and then the last layer which we have is the output head so the output head is a neural network at the final stage of the GPT model so the input to the output head is this input which has four tokens and each token has a dimension of 768 then what we do is that we pass this through a neural network whose size is the embedding dimension multiplied by the vocabulary Dimension and I'll tell you why just in a moment but if you take this input input tensor and if you pass in through the neural network which has uh which takes in inputs of 768 dimensions and the output is 50257 the resultant uh output which you get it's called as the logits Matrix and this logits Matrix actually has four rows because we have four tokens every so every f effort moves you and the number of columns here is not equal to 768 the number of columns is equal to the vocabulary size because we are passing in passing the input through a neural network whose final output size is 50257 so for every token we have this logits which is a 50257 dimensional Vector so for every there is a 50257 dimensional Vector for effort there is a 50257 dimensional Vector similarly for move and U there are 5 257 dimensional vectors and the reason here is because we want to predict the next word right based on the input so when every is the input we want to predict what's the next word and that should be effort so we look at the vocabulary and we look at that that token or that column which has the highest value and that column should ideally correspond to effort when every is the input similarly when every effort is the input we we will look at the column which has the maximum value and that column corresponds to certain word in the vocabulary and that word should be moves because when every and effort are the input moves is the output similarly when we look at every effort moves as the input we will predict the next word and that should be U so we look at that column which has the highest value and that should hopefully be U and finally we look at every effort moves you that's the final prediction task and then uh when every effort moves you is the input we look at the final output and that should hopefully be the token which corresponds to the next word which is forward every effort moves you forward we'll see how to make the next word prediction in the next class but for now I just want you to appreciate that this last step is the only step where the dimensions of the input change the dimensions of the input all along where four which were the number of tokens multiplied by 768 which was the embedding Dimension but now the final output size four which were the number of tokens multiplied by 50257 and based on this final output we'll get the next word prediction now when you look at every effort moves you the context size is four whenever we have a context size we have that many prediction tasks so remember here there are four tokens right so we don't only have one prediction task you might think that the only prediction task is every effort moves you is the input and we have to predict the next word no there are four prediction tasks the first prediction task is every is the input then effort should be the output every effort is the input movees should be the output every effort moves should be the input U should be the output and only the fourth prediction task is every effort moves you what's the output so that's why we have this four rows here because there are four prediction tasks we'll see in the next uh next class how to predict the next word from this output tensor which we have obtained now one thing to mention is that uh so here if you you see the output tensor value will actually be four so let me write this down over here the output tensor value will be four multiplied by uh the vocabulary size which is uh five so it's 5 0 2 5 and 7 so that is the fin final tensor value uh remember one thing which I have considered here is only one batch if you have two batches this will be the final tensor generated for both those batches so similar similar to every effort moves you if you have another sentence of four words such as I like uh movies and and you have to predict the next word that's the second batch so then the output tensor will also include the batch size so then it will be 2 multiplied by uh 4 by 5 2 57 why this initial two because the batch size is equal to two so the First Dimension is always the batch size the second dimension is the number of tokens which we have or the context size uh and the third dimension is the 50257 which is the vocabulary size so this is the output tensor Dimension format which we have now let me zoom out here and show you the entire flow map which we have seen it was a pretty long flow map but I I hope you all have understood what we are trying to do over here the reason I went through this entire flow map starting from token embedding positional embedding to input embedding to drop out to the Transformer block uh then we went to layer normalization then finally we went to Output head the reason I showed you all these things is just so that you understand dimensions and what's going on with Dimensions I did not just want to show you the code because when you see the code you cannot visualize the dimensions but now once you have seen this lecture and when whenever you let's say you're looking at the layer normalization part of the Transformer right you can just visualize what's happening uh what are the dimensions of the input to a particular block what are the dimensions of the output and so so that will make your learning process much easier so now actually uh on the Whiteboard we have seen how the building block stack up for the entire GPT model we started with tokenization input embedding positional we went to Transformers and then we saw the final layer nor norm and then we also saw the final linear output layer okay so now once your intuition is clear once your visual understanding is clear we are pretty much ready to move into code so now let us dive into code and uh see how these different blocks can be arranged together to code the entire GPT model all right so let's jump into code right now here's the GPT configuration it's the same configuration is gpt2 which we are going to uh use when we are going to look at this entire coding module so here you will see that the vocabulary size is equal to 50257 this was actually the vocabulary size which was implemented when gpt2 was trained the context length is equal to 1024 the vector embedding Dimension which we also saw on the Whiteboard that is equal to 768 the number of attention heads is equal to 12 the number of Transformers U which are there which we are going to implement are equal to 12 the drop rate or the dropout rate is equal to 0.1 and the query key value bias is false this is for setting or initializing the weight metrices for the queries the keys and the values we don't need the bias term in that for now you can focus on a couple of things the first is the embedding Dimension uh which is going to stay 7608 throughout the second is the 502 spice and vocabulary size these are the same dimensions which we just saw on the Whiteboard so you might be able to relate to them uh 12 is the number of Dropout uh 12 is the number of Transformer blocks so in gpt2 when it was trained they had 12 Transformer blocks we are also going to use a similar number of Transformer blocks then the number of attention heads so every Transformer block has a multi-ad attention module right and that module has a certain number of attention heads we are going to use that equal to 12 dropout rate is 0.1 I think this much should be enough to understand the entire code now as we are going through this code I want you to just keep this diagram in mind and the dimensions which we just saw in the visual flow map everything will follow from this particular diagram since we are exactly going to uh code it in a similar manner okay so we started out this lecture series four to five lectures back with this GPT model class where we had the forward method but the Transformer block was not implemented so this was a dummy Transformer block the layer normalization was not implemented this was a dummy layer normalization what we did over the last three to four lectures is that we coded the layer normalization class feed forward neural network class and also the Transformer class so here's the layer normalization class which we had coded given a particular uh layer now you can think of when whenever you see layer normalization remember we have seen the visual visual flow map now right so you can uh try to visualize what happens in the layer normalization layer we have seen that already so see this is exactly what happens in a layer normalization layer we look at every token that has 768 Dimensions right then what we are doing is that we are going to subtract the mean and divide by the square root of the variance so that the resulting values have a mean of zero and standard deviation or variance of one we also have a scale and a shift here which are trainable parameters then we have the feed forward block and remember where the feed forward block comes in the Transformer architecture it's this uh it's this feed forward block um let me actually change my pen color to it's this feed forward neural network which I have shown you over here the expansion contraction feed forward neural network this is that feed forward block so you'll see that the input is the embedding Dimension the layer is four times the embedding Dimension size and the output is the embedding Dimension once you are able to visualize this you'll be able to easily understand this code the input is the embedding Dimension the hidden layer is four times the embedding Dimension that's the expansion layer then there is a contraction layer from the four times embedding Dimension to the embedding Dimension and there is a Jou activation function we spent a whole lecture on the J activation because it's a bit different than the it's smoother on the negative side I think I also have a plot for the J activation over here so let me quickly show you that um if I can find it yeah so on the right hand side you can see railu it's zero for negative inputs for the J it's not zero and also it's differentiable at x equal to Z that's why in llm training generally the J activation uh is used so we coded the layer normalization class the j class and the feed forward class and then in the last lecture we coded the entire Transformer block class itself so here we had the so the eight steps in the Transformer if you remember take a look at these eight steps which we saw um let me zoom out here further so these are the eight steps which we saw in the Transformer we start with input embeddings add a Dropout layer normalization then uh we go to the mass multi attention then drop out and then the shortcut connections that's the first of these eight steps in the that's the first four of these eight steps in the Transformer block then what we do is that then we again have a layer normalization feed forward neural network with J then Dropout and then another set of shortcut connections these are the last four steps so overall there are these eight steps which are happening in the Transformer block architecture and it's the same eight steps which we wrote down when we wrote the code for the Transformer block class these are the first four steps and it ends with the shortcut connection and these are the last four steps and here again it ends with the shortcut connection the main point which we also recognized when we implemented the Transformer block class is that the dimensions are preserved the dimensions of the input to the Transformer are the same as the dimensions of the output from the Transformer block awesome now we have all the knowledge and now we are ready to code the entire GPT architecture so first first let's look at the forward method um which takes in the input and remember the input looks something like this let me show you how the input actually looks like the input might look something like this where here we are showing two batches and each batch has let's say four token IDs uh so the number of rows here in this tensor are equal to the number of batches and the number of columns are the sequence length or the number of tokens which we are considering that's why if you see the number of rows here in the input shape are size number of columns are essentially the sequence length or the number of tokens whenever now I'm explaining this next whole part you can keep that flow map in mind which I showed you on the Whiteboard and think about the flow which we had for every effort moves you so first we convert the tokens into token IDs and then we convert the token IDs into embedding vectors that's what's done in this token embedding uh so we initialize an embedding layer from the pytorch module and then from this embedding layer we get the embedding token embedding vectors for the input IDs and then similarly we get the positional embedding vectors for the four positions so see sequence length is equal to four so we get the positional embedding vectors for the four positions and we add the token embedding to the positional embeddings so whenever you try to make sense of Dimensions here always think of one batch at a time when you think of one batch things will be simplified because one batch only has four tokens let's say so when you look at this step the output of this step uh you can visualize the output exactly from the flow map which we had seen so let me take you to that yeah look at the first step over here which was titled as positional embedding when you look at this code the token embeddings these are essentially uh these are essentially this kind of a tensor where for every token you have the 768 dimensional Vector similarly you can think of the same visualization for positional embeddings and when you look at this x equal to token embeddings plus positional embeddings you can uh you can try to visualize this this part which we saw we added the token embeddings to the positional embeddings right over here so for every token we added the token embeddings and the positional embeddings to get the input embeddings that's exactly what I'm I've written in the code over here and then after this part we go to the next steps which were which were the same in the visual flow so the next step Next Step was addition of the Dropout which was step number four over here I'm showing that with a star right now that was the step number four so Dropout layer followed by the Transformer followed by the Transformer block and then the last two layers were another layer normalization layer which which was step number six here this was the second last layer which was another layer of normalization and then the final layer was output head so after you get the input embeddings there are four things you apply Dropout layer you apply the Transformer blocks you have the another layer of normalization and then you apply the output head that's it now if you see the Transformer blocks we are chaining different Transformer blocks together together based on the number of layers so in the configuration we have seen that the number of layers is equal to 12 right in this configuration we have seen that the number of layers is equal to 12 so uh we are actually chaining 12 Transformer blocks together using NN do sequential this NN do sequential is a tensor flow or pytorch module which allows us to chain um different neural network blocks together so we have chained 12 Transformer blocks so when you see the cell trf blocks it just looks like one line of code right but there are actually 12 Transformer blocks chain together here and in each Transformer block there are eight different steps which are being performed so it's a huge number of operations being performed in this one simple line of code that's it actually if you think of the building blocks if you have understood the Transformer block if you have understood the layer normalization class uh if you have understood the dimensions with respect to the Token embeddings positional embeddings that's all what the um Transformer or what the GPT model class is actually doing it takes in the input and then the outputs are the logits which we saw on the Whiteboard so the dimensions of the logits if you remember let me take you to the Whiteboard once more so if you go to the Whiteboard and if I zoom into this part of the if I zoom into this part of the Whiteboard right now you'll see that the output of the GPT model is this output Logics and the shape of these logits is for each batch we have the number of tokens uh in this case those were four tokens and the number of columns are equal to the vocabulary size and then the First Dimension is the number of batches which we have okay so now what we can actually do is that we can take a simple input batch and then we can pass in through this entire GPT model uh so what we are doing here is that we are taking an input batch which has every which has two batches and each batch has two tokens what we are doing is that first we create an instance of the GPT model and pass in the model configuration and then we create this object so we create an instance of model uh which Returns the output so if you print out the input batch you'll see this and if you print out the output batch let's look at one batch currently if you look at one batch you will see that there are four tokens and each token has number of columns equal to the vocabulary size which is 50257 this is exactly what we had seen in the output logic right so if you see in one batch there are four tokens four rows and there are 50257 columns this is exactly the same thing which is happening for the second batch also there are four tokens here and there are 50257 columns because the vocabulary size is 50257 so as we can see the output tensor has the shape uh two batches four tokens in each batch and 50257 columns since we passed in two input text and four tokens each the last Dimension 50257 corresponds to the vocabulary size of the tokenizer in the next class we are going to see how to convert these 50257 dimensional output token vectors back to tokens and how to predict the next word but for now you can see that the trans the GPT model class which we have constructed is working fine and there are a huge number of parameters which we are actually dealing with here this small piece of code over here has more than 100 million parameters can you imagine that there are just a huge number of parameters because there are 12 Transformer blocks chained together there are eight steps in each Transformer block then uh and remember there is that expansion contraction layer in each Transformer block that that layer itself has a huge number of parameters we have parameters for token embedding positional embedding uh Etc so totally all of the parameters add up we can actually print out the number of parameters so what we can do is that we can have have total params and we can do p. numl what p. numl will do is that it will actually print out uh so using the numl method short for number of elements we can collect the total number of parameters in the model's parameter tensors so if you print out the total number of parameters you will see that the number of parameters is equal to 163 million huge number of parameters right and all of this is running on our local machine so you might be thinking earlier we spoke of initializing a 12 4 million parameter GPT model right so then why is the actual number of parameters 163 million why is it so higher so this actually relates back to a concept which is called weight time that is used in the original gpt2 architecture so which means that the original gpt2 architecture is reusing the weights from the token embedding layer in its output layer so if you go back to the Whiteboard right now yeah in the schematic which we saw over here there is an output layer right there is an output layer towards the end and there is a token embedding layer over here when GPT model when gpt2 model was constructed the parameters which were used for the token embedding layer were the same parameters which were used in the linear output layer and that's why uh the total number of parameters was less in our case right now we did not reuse the parameters so when we print out the parameters they say 163 million uh but in gpt2 architecture they are reusing the weight so we can actually print this out so in our case what we can do is that we can print out the embedding layer shape of the token embedding layer and we can see that um the embedding layer shape is 50257 comma 768 and we can also print out the output layer shape and these both have exactly the same shape so that's why in gpt2 model they just reused this these parameters um so what we can actually do is that we can um let we can remove we can remove the output layer parameter count from the total parameter count and check the number of parameters so in the output layer parameter parameters are removed the total number of parameters comes up to be exactly 124 million parameters which is the same size of the gpt2 model so weight Ty actually reduces the overall memory footprint from 163 million we go to 124 million and it also reduces the computational complexity of the model however using separate token embedding and output layers results in better training and model performance that's why when in our code which we are writing we are using separate layers in our GPT model implementation and this is true for modern llms as well weight time is good for reducing the memory footprint and reducing the computational time and complexity but to get a better model training and performance it's good not to reuse the parameters um we can also print out the space which is taken by our model so we can compute the memory requirements of the 163 million parameters so the total size of the model it turns out it's around 600 megabytes of space so in conclusion by calculating the memory requirements for the 163 million million parameters in our gpt2 model um and assuming that each parameter is a 32-bit float taking up to four bytes that's why we have multiplied by four over here we have assumed that each parameter takes around four bytes so we multiplied the total number of parameters by four to get the total size that 620 MB which illustrates the relatively large storage capacity required to accommodate even relatively small llms so gpt2 is relatively small 120 million compared to there are now models which have more than b billion parameters imagine how much space such a model will take on your device it's impossible to run extremely large models on local devices and that's why we need GPU access so in this lecture today we implemented the GPT model architecture and saw that it outputs numerical tensors right so you might be curious how do we go from that numerical tensor output into the next word prediction task uh so that we'll be doing next in the next lecture we are going to generate text from the output tensor which we have obtained today today what I want to what I want you to appreciate is just within 10 lines of or even 15 20 lines of code we have actually run a very power powerful large language model completely on our laptop and we buil this model from scratch this model had more than 160 million parameters it took 600 megabytes of space on our machine indicating the size required by these models and I want you to appreciate that once you understood the visual flow map I showed you on the Whiteboard it's just eight lines of code but to understand these eight lines of codes we need to put in a lot of effort because there is a lot of theory and lot of intuition behind every single line of code over here but once you get that it just these uh stacking of the token embeddings positional embeddings Dropout layer Transformer blocks another layer normalization and the output output head that's it and then we directly get the Logics um I hope you are following these lectures I know this lecture is also become a bit long but it is essential to show you the theory as well as the code in today's lecture I could have directly showed you this code which was only this much but then at every step you would not have visualized the dimensions what is exactly happening can we take a specific example and see what is happening for that example and now I've given you a visual understanding intuitive understanding as well as a coding understanding in the code I written uh the description of what what is happening in every layer for example here you will see that the forward method takes a batch of input tokens computes their embeddings applies the positional embeddings passes the sequence through the Transformer block normalizes the final output and then computes the logits representing the next tokens unnormalized probabilities we will convert these Logics into tokens and text outputs in the next class but whenever this when you access this code file Ive also written a number of such comments and paragraphs here which you can read and try to understand the code for yourself one thing which I will very very highly suggest is that whatever I written on the Whiteboard right when I was I was myself writing the flow map of the Transformer on the Whiteboard this this flow map I understood that my understanding also became very clear if I just ran the code my understanding was not that strong but when I wrote every single step of the code on a whiteboard um and when I created a visual flow map uh which I demonstrated to you in today's lecture such kind of visual flow map which I'm showing right now on the screen it really strengthened my understanding and it made me confident about this subject if you can also write this similar flow map on your notebook or on the Whiteboard which you are using it will tremendously improve your understanding thanks a lot everyone I hope you are enjoying from this lectures next lecture will probably be the last lecture in the GPT architecture series uh where we'll generate text from the final output tensor which we have obtained today thanks a lot everyone and I look forward uh to seeing you in the next lecture"
}