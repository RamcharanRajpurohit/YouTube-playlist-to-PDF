{
  "video": {
    "video_id": "UjdRN80c6p8",
    "title": "Lecture 15: Coding the self attention mechanism with key, query and value matrices",
    "duration": 4748.0,
    "index": 14
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 7.72
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.16,
      "duration": 4.84
    },
    {
      "text": "in the build large language models from",
      "start": 7.72,
      "duration": 5.72
    },
    {
      "text": "scratch Series today I'm very excited",
      "start": 10.0,
      "duration": 6.0
    },
    {
      "text": "for this particular lecture because",
      "start": 13.44,
      "duration": 3.999
    },
    {
      "text": "today we are going to look at",
      "start": 16.0,
      "duration": 3.76
    },
    {
      "text": "implementing a self attention mechanism",
      "start": 17.439,
      "duration": 5.08
    },
    {
      "text": "with trainable weights we are going to",
      "start": 19.76,
      "duration": 5.679
    },
    {
      "text": "look at Key query and value and we'll",
      "start": 22.519,
      "duration": 4.801
    },
    {
      "text": "also see why this self attention",
      "start": 25.439,
      "duration": 4.561
    },
    {
      "text": "mechanism is also called as scaled do",
      "start": 27.32,
      "duration": 3.72
    },
    {
      "text": "product",
      "start": 30.0,
      "duration": 3.8
    },
    {
      "text": "attention so we are now moving very",
      "start": 31.04,
      "duration": 4.6
    },
    {
      "text": "closer and closer to the actual",
      "start": 33.8,
      "duration": 3.8
    },
    {
      "text": "attention mechanism which is implemented",
      "start": 35.64,
      "duration": 5.52
    },
    {
      "text": "in LMS such as GPT today's lecture will",
      "start": 37.6,
      "duration": 5.799
    },
    {
      "text": "be a great combination of",
      "start": 41.16,
      "duration": 5.32
    },
    {
      "text": "mathematics uh Theory intuition and also",
      "start": 43.399,
      "duration": 5.921
    },
    {
      "text": "coding I really enjoyed learning so much",
      "start": 46.48,
      "duration": 4.52
    },
    {
      "text": "about this lecture and preparing the",
      "start": 49.32,
      "duration": 3.68
    },
    {
      "text": "lecture material and I've condensed all",
      "start": 51.0,
      "duration": 4.359
    },
    {
      "text": "the information in today's video so",
      "start": 53.0,
      "duration": 4.96
    },
    {
      "text": "let's get started before we get started",
      "start": 55.359,
      "duration": 4.241
    },
    {
      "text": "I want to quickly touch upon what we",
      "start": 57.96,
      "duration": 4.279
    },
    {
      "text": "covered in the previous lecture in the",
      "start": 59.6,
      "duration": 5.0
    },
    {
      "text": "previous lecture we implemented a self",
      "start": 62.239,
      "duration": 4.56
    },
    {
      "text": "attention mechanism without trainable",
      "start": 64.6,
      "duration": 5.4
    },
    {
      "text": "weights so this is the uh sentence which",
      "start": 66.799,
      "duration": 5.161
    },
    {
      "text": "we looked at the sentence which we",
      "start": 70.0,
      "duration": 5.0
    },
    {
      "text": "looked at was your journey starts with",
      "start": 71.96,
      "duration": 6.56
    },
    {
      "text": "one step and we saw how to convert the",
      "start": 75.0,
      "duration": 5.68
    },
    {
      "text": "embedding Vector the vector embedding",
      "start": 78.52,
      "duration": 4.88
    },
    {
      "text": "for every single token into a context",
      "start": 80.68,
      "duration": 4.68
    },
    {
      "text": "Vector for every single",
      "start": 83.4,
      "duration": 5.359
    },
    {
      "text": "token and uh let me take you through the",
      "start": 85.36,
      "duration": 5.92
    },
    {
      "text": "steps we implemented to do that and I'll",
      "start": 88.759,
      "duration": 3.761
    },
    {
      "text": "go to the figure which really",
      "start": 91.28,
      "duration": 3.519
    },
    {
      "text": "illustrates everything yeah so what we",
      "start": 92.52,
      "duration": 4.36
    },
    {
      "text": "did essentially in the last lecture was",
      "start": 94.799,
      "duration": 5.0
    },
    {
      "text": "that we broke down the initial sentence",
      "start": 96.88,
      "duration": 5.199
    },
    {
      "text": "into the embedding vectors which were",
      "start": 99.799,
      "duration": 4.041
    },
    {
      "text": "three-dimensional input embedding",
      "start": 102.079,
      "duration": 4.961
    },
    {
      "text": "vectors and then we looked at queries so",
      "start": 103.84,
      "duration": 5.68
    },
    {
      "text": "we took the example of let's say Journey",
      "start": 107.04,
      "duration": 4.64
    },
    {
      "text": "which is the second query your journey",
      "start": 109.52,
      "duration": 5.279
    },
    {
      "text": "begins with one step and then for each",
      "start": 111.68,
      "duration": 7.32
    },
    {
      "text": "such query we found the attention scores",
      "start": 114.799,
      "duration": 5.881
    },
    {
      "text": "with respect to the input embedding",
      "start": 119.0,
      "duration": 5.0
    },
    {
      "text": "Vector so for the first word there is a",
      "start": 120.68,
      "duration": 6.24
    },
    {
      "text": "attention score between the first word",
      "start": 124.0,
      "duration": 5.2
    },
    {
      "text": "and the query for the second word there",
      "start": 126.92,
      "duration": 4.56
    },
    {
      "text": "is a attention score similarly for the",
      "start": 129.2,
      "duration": 4.759
    },
    {
      "text": "last word there is a attention score",
      "start": 131.48,
      "duration": 5.24
    },
    {
      "text": "this attention score actually quantifies",
      "start": 133.959,
      "duration": 5.721
    },
    {
      "text": "how much importance should be given to",
      "start": 136.72,
      "duration": 5.44
    },
    {
      "text": "each word when we look at the query",
      "start": 139.68,
      "duration": 3.6
    },
    {
      "text": "which is",
      "start": 142.16,
      "duration": 3.32
    },
    {
      "text": "Journey and then based on these",
      "start": 143.28,
      "duration": 4.12
    },
    {
      "text": "attention scores we found the attention",
      "start": 145.48,
      "duration": 3.88
    },
    {
      "text": "weights the difference between the",
      "start": 147.4,
      "duration": 4.0
    },
    {
      "text": "attention weights is that attention",
      "start": 149.36,
      "duration": 4.32
    },
    {
      "text": "weights sum up to one so attention",
      "start": 151.4,
      "duration": 4.479
    },
    {
      "text": "scores and attention weights intuitively",
      "start": 153.68,
      "duration": 3.839
    },
    {
      "text": "mean the same thing the encode",
      "start": 155.879,
      "duration": 4.161
    },
    {
      "text": "information about how much the query",
      "start": 157.519,
      "duration": 5.241
    },
    {
      "text": "vector and the input embedding Vector",
      "start": 160.04,
      "duration": 5.479
    },
    {
      "text": "are related to each other and attention",
      "start": 162.76,
      "duration": 4.479
    },
    {
      "text": "weights are normalized which means they",
      "start": 165.519,
      "duration": 4.561
    },
    {
      "text": "sum up to one the way we computed the",
      "start": 167.239,
      "duration": 4.761
    },
    {
      "text": "attention scores is by implementing a",
      "start": 170.08,
      "duration": 3.28
    },
    {
      "text": "DOT product",
      "start": 172.0,
      "duration": 3.319
    },
    {
      "text": "operation so what we did is we",
      "start": 173.36,
      "duration": 3.84
    },
    {
      "text": "implemented a DOT product between the",
      "start": 175.319,
      "duration": 4.521
    },
    {
      "text": "query vector and the input Vector so let",
      "start": 177.2,
      "duration": 7.399
    },
    {
      "text": "let me show this to you um in figure so",
      "start": 179.84,
      "duration": 7.6
    },
    {
      "text": "that you have some reference to compare",
      "start": 184.599,
      "duration": 5.081
    },
    {
      "text": "yeah so what we did essentially was we",
      "start": 187.44,
      "duration": 5.64
    },
    {
      "text": "had this uh Journey which is the query",
      "start": 189.68,
      "duration": 5.6
    },
    {
      "text": "Vector to find the attention score we",
      "start": 193.08,
      "duration": 3.96
    },
    {
      "text": "found the dot product of this Vector",
      "start": 195.28,
      "duration": 4.28
    },
    {
      "text": "with all the other vectors and that give",
      "start": 197.04,
      "duration": 4.279
    },
    {
      "text": "the attention scores then we normalize",
      "start": 199.56,
      "duration": 3.64
    },
    {
      "text": "the attention scores to give the to get",
      "start": 201.319,
      "duration": 4.121
    },
    {
      "text": "the attention weights and then finally",
      "start": 203.2,
      "duration": 4.28
    },
    {
      "text": "we use the attention weights to find the",
      "start": 205.44,
      "duration": 4.439
    },
    {
      "text": "context Vector so here is the context",
      "start": 207.48,
      "duration": 4.56
    },
    {
      "text": "Vector for Journey and similarly we",
      "start": 209.879,
      "duration": 4.161
    },
    {
      "text": "found context Vector for all the other",
      "start": 212.04,
      "duration": 5.199
    },
    {
      "text": "vectors for all the other input tokens",
      "start": 214.04,
      "duration": 5.759
    },
    {
      "text": "so uh the steps which we implemented in",
      "start": 217.239,
      "duration": 5.56
    },
    {
      "text": "the last class can be summed up in three",
      "start": 219.799,
      "duration": 5.201
    },
    {
      "text": "uh categories first we computed the",
      "start": 222.799,
      "duration": 4.561
    },
    {
      "text": "attention scores for that we computed",
      "start": 225.0,
      "duration": 4.319
    },
    {
      "text": "the dot product between the inputs and",
      "start": 227.36,
      "duration": 4.32
    },
    {
      "text": "the query then we computed attention",
      "start": 229.319,
      "duration": 4.161
    },
    {
      "text": "weights which were normalized attention",
      "start": 231.68,
      "duration": 4.199
    },
    {
      "text": "scores and then we computed context",
      "start": 233.48,
      "duration": 4.52
    },
    {
      "text": "vectors so context vectors are",
      "start": 235.879,
      "duration": 4.881
    },
    {
      "text": "essentially the weighted sum of the",
      "start": 238.0,
      "duration": 6.079
    },
    {
      "text": "attention weights and the input vectors",
      "start": 240.76,
      "duration": 5.52
    },
    {
      "text": "so here's a figure which explains how we",
      "start": 244.079,
      "duration": 4.601
    },
    {
      "text": "found the context Vector so we found the",
      "start": 246.28,
      "duration": 5.319
    },
    {
      "text": "attention weights for the given query",
      "start": 248.68,
      "duration": 4.72
    },
    {
      "text": "and then let's say the first attention",
      "start": 251.599,
      "duration": 3.761
    },
    {
      "text": "weight was multiplied by the first input",
      "start": 253.4,
      "duration": 4.0
    },
    {
      "text": "Vector the second attention weight was",
      "start": 255.36,
      "duration": 4.04
    },
    {
      "text": "multiplied by the second input Vector",
      "start": 257.4,
      "duration": 3.76
    },
    {
      "text": "similarly the last attention weight was",
      "start": 259.4,
      "duration": 4.16
    },
    {
      "text": "multiplied by the last input vector and",
      "start": 261.16,
      "duration": 5.16
    },
    {
      "text": "we added all of these vectors to give us",
      "start": 263.56,
      "duration": 6.6
    },
    {
      "text": "the context Vector for that given query",
      "start": 266.32,
      "duration": 5.96
    },
    {
      "text": "in a similar manner we found the context",
      "start": 270.16,
      "duration": 4.36
    },
    {
      "text": "Vector for all the other queries in the",
      "start": 272.28,
      "duration": 4.16
    },
    {
      "text": "given",
      "start": 274.52,
      "duration": 4.679
    },
    {
      "text": "sentence so this is what we implemented",
      "start": 276.44,
      "duration": 4.64
    },
    {
      "text": "in the last lecture and we did not have",
      "start": 279.199,
      "duration": 4.241
    },
    {
      "text": "any trainable weights we did not train",
      "start": 281.08,
      "duration": 4.6
    },
    {
      "text": "anything in the last lecture everything",
      "start": 283.44,
      "duration": 4.199
    },
    {
      "text": "was fixed the attention scores was",
      "start": 285.68,
      "duration": 4.64
    },
    {
      "text": "calculated using the dot product um the",
      "start": 287.639,
      "duration": 4.601
    },
    {
      "text": "attention weights was just normalization",
      "start": 290.32,
      "duration": 3.36
    },
    {
      "text": "and the context Vector was just",
      "start": 292.24,
      "duration": 5.36
    },
    {
      "text": "summation of the uh attention weights",
      "start": 293.68,
      "duration": 6.44
    },
    {
      "text": "multiplied by the corresponding input",
      "start": 297.6,
      "duration": 4.96
    },
    {
      "text": "Vector today we are going to look at a",
      "start": 300.12,
      "duration": 4.88
    },
    {
      "text": "more real life situation which is",
      "start": 302.56,
      "duration": 4.6
    },
    {
      "text": "actually implemented and we are going to",
      "start": 305.0,
      "duration": 4.479
    },
    {
      "text": "consider trainable weights if you have",
      "start": 307.16,
      "duration": 4.44
    },
    {
      "text": "not seen the previous lecture I highly",
      "start": 309.479,
      "duration": 3.881
    },
    {
      "text": "recommend you to go through the previous",
      "start": 311.6,
      "duration": 3.599
    },
    {
      "text": "lecture you'll appreciate the current",
      "start": 313.36,
      "duration": 4.88
    },
    {
      "text": "lecture much much more okay so in this",
      "start": 315.199,
      "duration": 4.72
    },
    {
      "text": "section we are going to learn about the",
      "start": 318.24,
      "duration": 4.0
    },
    {
      "text": "self attention mechanism which is used",
      "start": 319.919,
      "duration": 4.801
    },
    {
      "text": "in the original Transformer architecture",
      "start": 322.24,
      "duration": 5.239
    },
    {
      "text": "the GPT models and most other popular",
      "start": 324.72,
      "duration": 5.44
    },
    {
      "text": "large language models the self attention",
      "start": 327.479,
      "duration": 4.961
    },
    {
      "text": "mechanism is also called as scaled do",
      "start": 330.16,
      "duration": 4.319
    },
    {
      "text": "product attention and in this lecture we",
      "start": 332.44,
      "duration": 4.4
    },
    {
      "text": "are going to see why this name comes",
      "start": 334.479,
      "duration": 4.921
    },
    {
      "text": "into the picture and where this name is",
      "start": 336.84,
      "duration": 3.639
    },
    {
      "text": "derived",
      "start": 339.4,
      "duration": 3.56
    },
    {
      "text": "from I'm following this particular",
      "start": 340.479,
      "duration": 5.241
    },
    {
      "text": "sequence in the attention Series in the",
      "start": 342.96,
      "duration": 4.72
    },
    {
      "text": "last lecture we covered simplified self",
      "start": 345.72,
      "duration": 3.919
    },
    {
      "text": "attention in today's lecture we are",
      "start": 347.68,
      "duration": 3.799
    },
    {
      "text": "going to cover self attention with",
      "start": 349.639,
      "duration": 3.84
    },
    {
      "text": "trainable weights in the next lecture",
      "start": 351.479,
      "duration": 3.881
    },
    {
      "text": "we'll look at causal attention and in",
      "start": 353.479,
      "duration": 3.801
    },
    {
      "text": "the final lecture we look at multi-head",
      "start": 355.36,
      "duration": 4.44
    },
    {
      "text": "attention as I explained in the previous",
      "start": 357.28,
      "duration": 5.0
    },
    {
      "text": "lecture also it's impossible to cover",
      "start": 359.8,
      "duration": 4.72
    },
    {
      "text": "attention in one lecture that's why I",
      "start": 362.28,
      "duration": 4.319
    },
    {
      "text": "have designed these extensive lectures",
      "start": 364.52,
      "duration": 4.0
    },
    {
      "text": "to teach you the concept in a very",
      "start": 366.599,
      "duration": 4.761
    },
    {
      "text": "proper manner it might get a bit complex",
      "start": 368.52,
      "duration": 4.64
    },
    {
      "text": "at times but if you understand the",
      "start": 371.36,
      "duration": 4.959
    },
    {
      "text": "concept you will Master Transformers",
      "start": 373.16,
      "duration": 5.319
    },
    {
      "text": "because this is the heart this is the",
      "start": 376.319,
      "duration": 4.841
    },
    {
      "text": "engine of Transformers and I'll show",
      "start": 378.479,
      "duration": 4.921
    },
    {
      "text": "everything from scratch right up to the",
      "start": 381.16,
      "duration": 4.2
    },
    {
      "text": "last dot product I'll multiply all",
      "start": 383.4,
      "duration": 4.56
    },
    {
      "text": "matrices directly in front of you so",
      "start": 385.36,
      "duration": 3.72
    },
    {
      "text": "that you understand matrix",
      "start": 387.96,
      "duration": 2.28
    },
    {
      "text": "multiplication",
      "start": 389.08,
      "duration": 2.92
    },
    {
      "text": "it's very important to do things on a",
      "start": 390.24,
      "duration": 3.32
    },
    {
      "text": "whiteboard because then the",
      "start": 392.0,
      "duration": 4.199
    },
    {
      "text": "understanding is improved much",
      "start": 393.56,
      "duration": 6.12
    },
    {
      "text": "more okay so what we want to do in",
      "start": 396.199,
      "duration": 5.84
    },
    {
      "text": "today's lecture is that we want to",
      "start": 399.68,
      "duration": 4.639
    },
    {
      "text": "compute the context Vector for every",
      "start": 402.039,
      "duration": 4.72
    },
    {
      "text": "given input token so the objective of",
      "start": 404.319,
      "duration": 4.44
    },
    {
      "text": "today's lecture is the same as the",
      "start": 406.759,
      "duration": 4.321
    },
    {
      "text": "objective was in the last lecture",
      "start": 408.759,
      "duration": 4.801
    },
    {
      "text": "remember what we did previously in the",
      "start": 411.08,
      "duration": 4.6
    },
    {
      "text": "previous lecture we found we took this",
      "start": 413.56,
      "duration": 4.039
    },
    {
      "text": "sentence your journey starts with one",
      "start": 415.68,
      "duration": 4.44
    },
    {
      "text": "step we we had the input embedding",
      "start": 417.599,
      "duration": 4.681
    },
    {
      "text": "Vector for each of these tokens and then",
      "start": 420.12,
      "duration": 4.12
    },
    {
      "text": "we found a context Vector for each of",
      "start": 422.28,
      "duration": 4.639
    },
    {
      "text": "these tokens so this graph here shows",
      "start": 424.24,
      "duration": 4.519
    },
    {
      "text": "the input embedding Vector for every",
      "start": 426.919,
      "duration": 5.521
    },
    {
      "text": "token and the red the journey context it",
      "start": 428.759,
      "duration": 6.081
    },
    {
      "text": "shows the context Vector for Journey",
      "start": 432.44,
      "duration": 4.159
    },
    {
      "text": "similarly we found the context Vector",
      "start": 434.84,
      "duration": 4.799
    },
    {
      "text": "for all the other input vectors to",
      "start": 436.599,
      "duration": 5.241
    },
    {
      "text": "refresh your understanding the context",
      "start": 439.639,
      "duration": 4.201
    },
    {
      "text": "Vector can be thought of as an enriched",
      "start": 441.84,
      "duration": 4.28
    },
    {
      "text": "input embedding Vector so if you look at",
      "start": 443.84,
      "duration": 5.16
    },
    {
      "text": "the word journe here the embedding",
      "start": 446.12,
      "duration": 5.519
    },
    {
      "text": "Vector for Journey uh the embedding",
      "start": 449.0,
      "duration": 5.479
    },
    {
      "text": "Vector for Journey Only in encapsulates",
      "start": 451.639,
      "duration": 5.521
    },
    {
      "text": "or encodes the semantic meaning but it",
      "start": 454.479,
      "duration": 4.961
    },
    {
      "text": "really contains no information about how",
      "start": 457.16,
      "duration": 4.24
    },
    {
      "text": "that word Journey relates to the other",
      "start": 459.44,
      "duration": 4.439
    },
    {
      "text": "words right the context Vector for",
      "start": 461.4,
      "duration": 4.519
    },
    {
      "text": "Journey on the other hand has more",
      "start": 463.879,
      "duration": 4.04
    },
    {
      "text": "information it not only contains the",
      "start": 465.919,
      "duration": 4.4
    },
    {
      "text": "meaning of Journey but it also contains",
      "start": 467.919,
      "duration": 5.441
    },
    {
      "text": "how Journey relates to step your with",
      "start": 470.319,
      "duration": 6.121
    },
    {
      "text": "and one that's why the context Vector is",
      "start": 473.36,
      "duration": 4.72
    },
    {
      "text": "thought of as an enriched embedding",
      "start": 476.44,
      "duration": 3.439
    },
    {
      "text": "vector",
      "start": 478.08,
      "duration": 4.36
    },
    {
      "text": "awesome so today what we are going to do",
      "start": 479.879,
      "duration": 4.641
    },
    {
      "text": "is we are going to introduce weight",
      "start": 482.44,
      "duration": 4.8
    },
    {
      "text": "matrices which are eventually optimized",
      "start": 484.52,
      "duration": 4.84
    },
    {
      "text": "when the large language model is",
      "start": 487.24,
      "duration": 4.72
    },
    {
      "text": "trained now these trainable weight",
      "start": 489.36,
      "duration": 5.2
    },
    {
      "text": "matrices are very crucial because the",
      "start": 491.96,
      "duration": 4.4
    },
    {
      "text": "model then learns to produce good",
      "start": 494.56,
      "duration": 4.8
    },
    {
      "text": "context vectors in the last lecture we",
      "start": 496.36,
      "duration": 5.959
    },
    {
      "text": "just uh looked at context vectors by",
      "start": 499.36,
      "duration": 4.679
    },
    {
      "text": "essentially taking the dot product to",
      "start": 502.319,
      "duration": 3.44
    },
    {
      "text": "get the attention weights that's it",
      "start": 504.039,
      "duration": 4.241
    },
    {
      "text": "right we did not train anything but",
      "start": 505.759,
      "duration": 4.12
    },
    {
      "text": "we'll see how these train weight",
      "start": 508.28,
      "duration": 4.319
    },
    {
      "text": "matrices are constructed and once these",
      "start": 509.879,
      "duration": 4.52
    },
    {
      "text": "weight matrices are trained the model",
      "start": 512.599,
      "duration": 3.88
    },
    {
      "text": "can learn to produce good context Vector",
      "start": 514.399,
      "duration": 3.281
    },
    {
      "text": "for every",
      "start": 516.479,
      "duration": 4.641
    },
    {
      "text": "token so at the heart of this trainable",
      "start": 517.68,
      "duration": 7.0
    },
    {
      "text": "weight matrices are three terminologies",
      "start": 521.12,
      "duration": 6.64
    },
    {
      "text": "query key and",
      "start": 524.68,
      "duration": 5.839
    },
    {
      "text": "value let me repeat that again we are",
      "start": 527.76,
      "duration": 4.44
    },
    {
      "text": "going to implement the self attention",
      "start": 530.519,
      "duration": 3.961
    },
    {
      "text": "mechanism step by step by introducing",
      "start": 532.2,
      "duration": 4.88
    },
    {
      "text": "three trainable weight matrices the",
      "start": 534.48,
      "duration": 4.359
    },
    {
      "text": "first is called the weight Matrix for",
      "start": 537.08,
      "duration": 4.16
    },
    {
      "text": "query the second is called weight Matrix",
      "start": 538.839,
      "duration": 5.24
    },
    {
      "text": "for key and the third is called the",
      "start": 541.24,
      "duration": 5.76
    },
    {
      "text": "weight Matrix for Value so these three",
      "start": 544.079,
      "duration": 4.961
    },
    {
      "text": "terminologies will show up again and",
      "start": 547.0,
      "duration": 6.0
    },
    {
      "text": "again and again query key and value and",
      "start": 549.04,
      "duration": 5.52
    },
    {
      "text": "let me show you a diagram which",
      "start": 553.0,
      "duration": 4.0
    },
    {
      "text": "illustrates what these terminologies",
      "start": 554.56,
      "duration": 3.6
    },
    {
      "text": "actually",
      "start": 557.0,
      "duration": 5.56
    },
    {
      "text": "mean so uh here I have mentioned here",
      "start": 558.16,
      "duration": 7.4
    },
    {
      "text": "step number one step number one which we",
      "start": 562.56,
      "duration": 5.719
    },
    {
      "text": "are going to learn in today's lecture is",
      "start": 565.56,
      "duration": 5.44
    },
    {
      "text": "how to convert in input embeddings which",
      "start": 568.279,
      "duration": 5.321
    },
    {
      "text": "are the input vectors into key query and",
      "start": 571.0,
      "duration": 4.959
    },
    {
      "text": "value vectors remember the goal here is",
      "start": 573.6,
      "duration": 4.12
    },
    {
      "text": "the same as the last lecture we want to",
      "start": 575.959,
      "duration": 3.841
    },
    {
      "text": "get from the input embeddings to context",
      "start": 577.72,
      "duration": 4.4
    },
    {
      "text": "embeddings for every token but there are",
      "start": 579.8,
      "duration": 4.52
    },
    {
      "text": "number of steps to be done and the first",
      "start": 582.12,
      "duration": 3.88
    },
    {
      "text": "step here is to convert the input",
      "start": 584.32,
      "duration": 3.92
    },
    {
      "text": "embeddings into key query and value",
      "start": 586.0,
      "duration": 4.88
    },
    {
      "text": "vectors let's see what we mean by that",
      "start": 588.24,
      "duration": 6.08
    },
    {
      "text": "and how to get these key query and value",
      "start": 590.88,
      "duration": 6.079
    },
    {
      "text": "vectors okay so here are my",
      "start": 594.32,
      "duration": 6.32
    },
    {
      "text": "inputs your journey start with one step",
      "start": 596.959,
      "duration": 7.361
    },
    {
      "text": "right these are my uh six inputs and",
      "start": 600.64,
      "duration": 5.6
    },
    {
      "text": "I've represented these six inputs as",
      "start": 604.32,
      "duration": 3.6
    },
    {
      "text": "threedimensional vectors which you can",
      "start": 606.24,
      "duration": 3.88
    },
    {
      "text": "also see in the graph below so if you",
      "start": 607.92,
      "duration": 5.32
    },
    {
      "text": "look at the input Matrix the first row",
      "start": 610.12,
      "duration": 5.08
    },
    {
      "text": "of this Matrix represents the three",
      "start": 613.24,
      "duration": 5.76
    },
    {
      "text": "dimensional Vector for y the word Y the",
      "start": 615.2,
      "duration": 5.96
    },
    {
      "text": "second row of this Matrix represents the",
      "start": 619.0,
      "duration": 4.12
    },
    {
      "text": "three-dimensional Vector for Journey",
      "start": 621.16,
      "duration": 4.6
    },
    {
      "text": "which can also be plotted here similarly",
      "start": 623.12,
      "duration": 4.56
    },
    {
      "text": "the last row represents the",
      "start": 625.76,
      "duration": 4.199
    },
    {
      "text": "three-dimensional Vector for the input",
      "start": 627.68,
      "duration": 5.719
    },
    {
      "text": "word or the input token step right this",
      "start": 629.959,
      "duration": 6.081
    },
    {
      "text": "is how the input embeddings are that's",
      "start": 633.399,
      "duration": 5.481
    },
    {
      "text": "given to us now the next step to",
      "start": 636.04,
      "duration": 5.44
    },
    {
      "text": "construct the query key and value Matrix",
      "start": 638.88,
      "duration": 5.92
    },
    {
      "text": "is to look at three trainable weight",
      "start": 641.48,
      "duration": 5.88
    },
    {
      "text": "Matrix the first trainable weight Matrix",
      "start": 644.8,
      "duration": 5.2
    },
    {
      "text": "is called as the query weight Matrix the",
      "start": 647.36,
      "duration": 4.52
    },
    {
      "text": "second trainable weight Matrix is called",
      "start": 650.0,
      "duration": 4.8
    },
    {
      "text": "as the key weight Matrix and the third",
      "start": 651.88,
      "duration": 5.399
    },
    {
      "text": "trainable weight Matrix is the value",
      "start": 654.8,
      "duration": 5.44
    },
    {
      "text": "weight Matrix so what we going to do is",
      "start": 657.279,
      "duration": 5.601
    },
    {
      "text": "that let's look at these weight matrices",
      "start": 660.24,
      "duration": 4.08
    },
    {
      "text": "uh and let's also focus on the",
      "start": 662.88,
      "duration": 4.04
    },
    {
      "text": "dimensions here so the input has",
      "start": 664.32,
      "duration": 4.84
    },
    {
      "text": "Dimensions 6x3 because there are six",
      "start": 666.92,
      "duration": 5.159
    },
    {
      "text": "rows one row for each input token and",
      "start": 669.16,
      "duration": 4.88
    },
    {
      "text": "why three because the dimension Vector",
      "start": 672.079,
      "duration": 4.56
    },
    {
      "text": "Dimension size is three now let's look",
      "start": 674.04,
      "duration": 7.84
    },
    {
      "text": "at uh uh the query key and value",
      "start": 676.639,
      "duration": 8.521
    },
    {
      "text": "trainable weight matrices so this W key",
      "start": 681.88,
      "duration": 9.16
    },
    {
      "text": "W uh WQ W K and W V these three matrices",
      "start": 685.16,
      "duration": 7.4
    },
    {
      "text": "which I've written over here I've",
      "start": 691.04,
      "duration": 3.68
    },
    {
      "text": "initialized them with some random values",
      "start": 692.56,
      "duration": 3.48
    },
    {
      "text": "but these are the ones which are",
      "start": 694.72,
      "duration": 3.64
    },
    {
      "text": "actually trained we do not know these",
      "start": 696.04,
      "duration": 3.799
    },
    {
      "text": "parameters so we initialize them",
      "start": 698.36,
      "duration": 4.24
    },
    {
      "text": "randomly and then train them so to get",
      "start": 699.839,
      "duration": 4.721
    },
    {
      "text": "context vectors later which we'll see in",
      "start": 702.6,
      "duration": 3.6
    },
    {
      "text": "today's lecture these are the ones which",
      "start": 704.56,
      "duration": 2.6
    },
    {
      "text": "are",
      "start": 706.2,
      "duration": 4.36
    },
    {
      "text": "optimized now what these uh matrices do",
      "start": 707.16,
      "duration": 6.28
    },
    {
      "text": "is actually they project the inputs into",
      "start": 710.56,
      "duration": 5.36
    },
    {
      "text": "a different dimension space let me tell",
      "start": 713.44,
      "duration": 4.32
    },
    {
      "text": "you what I mean by that so first let's",
      "start": 715.92,
      "duration": 4.68
    },
    {
      "text": "focus on the qu query Matrix so if you",
      "start": 717.76,
      "duration": 4.48
    },
    {
      "text": "look at the query Matrix and if you see",
      "start": 720.6,
      "duration": 3.72
    },
    {
      "text": "the dimensions it's",
      "start": 722.24,
      "duration": 6.279
    },
    {
      "text": "3x2 uh so it has three rows and two",
      "start": 724.32,
      "duration": 7.48
    },
    {
      "text": "columns so if you multiply the inputs if",
      "start": 728.519,
      "duration": 5.801
    },
    {
      "text": "you multiply the input Matrix with the",
      "start": 731.8,
      "duration": 5.039
    },
    {
      "text": "query weight Matrix what you will get is",
      "start": 734.32,
      "duration": 4.72
    },
    {
      "text": "the resultant Matrix which is called as",
      "start": 736.839,
      "duration": 4.401
    },
    {
      "text": "the queries so this is the queries",
      "start": 739.04,
      "duration": 5.96
    },
    {
      "text": "Matrix and it's a 6x2 matrix so what has",
      "start": 741.24,
      "duration": 5.64
    },
    {
      "text": "been done essentially is that each row",
      "start": 745.0,
      "duration": 4.639
    },
    {
      "text": "here still corresponds to the individual",
      "start": 746.88,
      "duration": 4.399
    },
    {
      "text": "words so the first row corresponds to",
      "start": 749.639,
      "duration": 3.56
    },
    {
      "text": "your the second row corresponds to",
      "start": 751.279,
      "duration": 3.92
    },
    {
      "text": "Journey the third row corresponds to",
      "start": 753.199,
      "duration": 5.961
    },
    {
      "text": "begins with one and",
      "start": 755.199,
      "duration": 6.281
    },
    {
      "text": "step but you can see here that the",
      "start": 759.16,
      "duration": 4.88
    },
    {
      "text": "dimension has been changed usually when",
      "start": 761.48,
      "duration": 4.84
    },
    {
      "text": "we train GPT the dimension is preserved",
      "start": 764.04,
      "duration": 4.28
    },
    {
      "text": "but here I'm just illustrating that the",
      "start": 766.32,
      "duration": 3.56
    },
    {
      "text": "dimensions can be changed when you",
      "start": 768.32,
      "duration": 5.4
    },
    {
      "text": "multiply with the weight query Matrix so",
      "start": 769.88,
      "duration": 5.68
    },
    {
      "text": "the simplest way to think of the query",
      "start": 773.72,
      "duration": 4.119
    },
    {
      "text": "Matrix and all the other weight matrices",
      "start": 775.56,
      "duration": 4.639
    },
    {
      "text": "is the transformation from let's say a",
      "start": 777.839,
      "duration": 4.24
    },
    {
      "text": "threedimensional space into in this case",
      "start": 780.199,
      "duration": 4.0
    },
    {
      "text": "a two- dimensional",
      "start": 782.079,
      "duration": 5.56
    },
    {
      "text": "space uh so when we multiply the input",
      "start": 784.199,
      "duration": 6.44
    },
    {
      "text": "Matrix with the query weight Matrix we",
      "start": 787.639,
      "duration": 5.401
    },
    {
      "text": "get the queries Matrix which is a 6x2",
      "start": 790.639,
      "duration": 6.841
    },
    {
      "text": "matrix okay so now you can think of each",
      "start": 793.04,
      "duration": 6.64
    },
    {
      "text": "row as corresponding to the",
      "start": 797.48,
      "duration": 4.44
    },
    {
      "text": "corresponding input token your journey",
      "start": 799.68,
      "duration": 4.0
    },
    {
      "text": "begins with one",
      "start": 801.92,
      "duration": 5.64
    },
    {
      "text": "step the second weight Matrix is the key",
      "start": 803.68,
      "duration": 6.279
    },
    {
      "text": "weight Matrix and that's all also 3x2",
      "start": 807.56,
      "duration": 4.719
    },
    {
      "text": "Matrix and very similar to what we did",
      "start": 809.959,
      "duration": 4.12
    },
    {
      "text": "for the queries we'll multiply the",
      "start": 812.279,
      "duration": 5.961
    },
    {
      "text": "inputs with the keys weight Matrix and",
      "start": 814.079,
      "duration": 6.641
    },
    {
      "text": "then finally we'll have the keys Matrix",
      "start": 818.24,
      "duration": 5.08
    },
    {
      "text": "which is again a",
      "start": 820.72,
      "duration": 5.679
    },
    {
      "text": "6x2 we'll see what these different uh",
      "start": 823.32,
      "duration": 4.8
    },
    {
      "text": "matrices mean what's the meaning of",
      "start": 826.399,
      "duration": 4.36
    },
    {
      "text": "query key and values but for now let's",
      "start": 828.12,
      "duration": 4.68
    },
    {
      "text": "just look at the mathematical details of",
      "start": 830.759,
      "duration": 4.76
    },
    {
      "text": "the implementation right and similarly",
      "start": 832.8,
      "duration": 5.08
    },
    {
      "text": "to get the values to get the values",
      "start": 835.519,
      "duration": 5.201
    },
    {
      "text": "Matrix we have to multiply the inputs",
      "start": 837.88,
      "duration": 5.44
    },
    {
      "text": "with the weight Matrix for values and",
      "start": 840.72,
      "duration": 5.08
    },
    {
      "text": "the weight Matrix for values is also 3x2",
      "start": 843.32,
      "duration": 4.959
    },
    {
      "text": "Matrix and when we multiply the inputs",
      "start": 845.8,
      "duration": 5.399
    },
    {
      "text": "with values we again get a 6x2 so the",
      "start": 848.279,
      "duration": 4.68
    },
    {
      "text": "way to interpret",
      "start": 851.199,
      "duration": 5.161
    },
    {
      "text": "the uh query key and the value is that",
      "start": 852.959,
      "duration": 5.841
    },
    {
      "text": "every row of the query key and value",
      "start": 856.36,
      "duration": 5.719
    },
    {
      "text": "essentially represents one token and a",
      "start": 858.8,
      "duration": 5.92
    },
    {
      "text": "representation for that token so",
      "start": 862.079,
      "duration": 4.88
    },
    {
      "text": "henceforth after we get the queries key",
      "start": 864.72,
      "duration": 4.0
    },
    {
      "text": "and values we are not going to look at",
      "start": 866.959,
      "duration": 4.041
    },
    {
      "text": "the inut embeddings again the input",
      "start": 868.72,
      "duration": 4.08
    },
    {
      "text": "embeddings have been transformed into",
      "start": 871.0,
      "duration": 5.16
    },
    {
      "text": "three ve into three matrices the query",
      "start": 872.8,
      "duration": 7.159
    },
    {
      "text": "the query Matrix the key Matrix and the",
      "start": 876.16,
      "duration": 6.0
    },
    {
      "text": "value Matrix and remember this",
      "start": 879.959,
      "duration": 4.8
    },
    {
      "text": "transformation is not fixed the key the",
      "start": 882.16,
      "duration": 4.4
    },
    {
      "text": "key to these Transformations are these",
      "start": 884.759,
      "duration": 5.801
    },
    {
      "text": "three weight matrices WQ w k and WV the",
      "start": 886.56,
      "duration": 6.199
    },
    {
      "text": "parameters of these weight matrices are",
      "start": 890.56,
      "duration": 4.24
    },
    {
      "text": "to be optimized later that's why these",
      "start": 892.759,
      "duration": 4.44
    },
    {
      "text": "are called as the trainable weight",
      "start": 894.8,
      "duration": 5.52
    },
    {
      "text": "matrices for now just think that what we",
      "start": 897.199,
      "duration": 4.681
    },
    {
      "text": "have done in this first step is we have",
      "start": 900.32,
      "duration": 3.6
    },
    {
      "text": "taken the input Matrix and we have",
      "start": 901.88,
      "duration": 4.399
    },
    {
      "text": "converted the input Matrix into three",
      "start": 903.92,
      "duration": 5.159
    },
    {
      "text": "other Matrix matrices queries keys and",
      "start": 906.279,
      "duration": 4.761
    },
    {
      "text": "values and the way we have done that",
      "start": 909.079,
      "duration": 4.44
    },
    {
      "text": "conversion is by multiplication of the",
      "start": 911.04,
      "duration": 5.239
    },
    {
      "text": "input embedding with the trainable query",
      "start": 913.519,
      "duration": 4.88
    },
    {
      "text": "Matrix with the trainable key Matrix and",
      "start": 916.279,
      "duration": 4.601
    },
    {
      "text": "the trainable value Matrix and so we",
      "start": 918.399,
      "duration": 4.44
    },
    {
      "text": "have these three queries keys and values",
      "start": 920.88,
      "duration": 4.319
    },
    {
      "text": "Matrix which is constructed so you might",
      "start": 922.839,
      "duration": 4.481
    },
    {
      "text": "be thinking why do we have these three",
      "start": 925.199,
      "duration": 4.241
    },
    {
      "text": "and how do we get the attention scores",
      "start": 927.32,
      "duration": 4.24
    },
    {
      "text": "how do we get the context vectors don't",
      "start": 929.44,
      "duration": 4.28
    },
    {
      "text": "worry we'll come to all of that but",
      "start": 931.56,
      "duration": 4.36
    },
    {
      "text": "first let's go to code and let's try to",
      "start": 933.72,
      "duration": 5.08
    },
    {
      "text": "implement uh all of",
      "start": 935.92,
      "duration": 5.12
    },
    {
      "text": "these okay so I'm going to take you",
      "start": 938.8,
      "duration": 4.2
    },
    {
      "text": "through code right now so the first",
      "start": 941.04,
      "duration": 4.599
    },
    {
      "text": "thing is to construct the inputs so",
      "start": 943.0,
      "duration": 4.88
    },
    {
      "text": "we'll have as I mentioned the inputs is",
      "start": 945.639,
      "duration": 5.56
    },
    {
      "text": "essentially a matrix which has six rows",
      "start": 947.88,
      "duration": 5.079
    },
    {
      "text": "uh it has six rows and it has three",
      "start": 951.199,
      "duration": 3.76
    },
    {
      "text": "columns so that's what I'm going to",
      "start": 952.959,
      "duration": 3.841
    },
    {
      "text": "Define here the inputs is a tensor with",
      "start": 954.959,
      "duration": 4.641
    },
    {
      "text": "six rows and three columns so each row",
      "start": 956.8,
      "duration": 4.719
    },
    {
      "text": "corresponds to a particular word let's",
      "start": 959.6,
      "duration": 3.52
    },
    {
      "text": "say journey is a three-dimensional",
      "start": 961.519,
      "duration": 3.641
    },
    {
      "text": "Vector so I'm going to run this right",
      "start": 963.12,
      "duration": 3.839
    },
    {
      "text": "now and you'll see that this block has",
      "start": 965.16,
      "duration": 6.039
    },
    {
      "text": "been run and what we are going to do is",
      "start": 966.959,
      "duration": 5.921
    },
    {
      "text": "that the next thing what we are going to",
      "start": 971.199,
      "duration": 4.961
    },
    {
      "text": "do is initialize the query key and the",
      "start": 972.88,
      "duration": 5.959
    },
    {
      "text": "value query key and the value Matrix",
      "start": 976.16,
      "duration": 4.72
    },
    {
      "text": "query key and the value weight Matrix",
      "start": 978.839,
      "duration": 4.321
    },
    {
      "text": "right and for that we need to define the",
      "start": 980.88,
      "duration": 5.72
    },
    {
      "text": "dimensions right so uh here the",
      "start": 983.16,
      "duration": 6.679
    },
    {
      "text": "dimensions are 3x2 the three has to be",
      "start": 986.6,
      "duration": 6.28
    },
    {
      "text": "equal to the vector dimension of the",
      "start": 989.839,
      "duration": 5.321
    },
    {
      "text": "input so that has to match because we",
      "start": 992.88,
      "duration": 4.319
    },
    {
      "text": "are doing a matrix multiplication here",
      "start": 995.16,
      "duration": 4.359
    },
    {
      "text": "so for all of these three weight query",
      "start": 997.199,
      "duration": 5.12
    },
    {
      "text": "key and value weight Matrix the first",
      "start": 999.519,
      "duration": 4.841
    },
    {
      "text": "value of the dimension three has to",
      "start": 1002.319,
      "duration": 3.721
    },
    {
      "text": "match the vector Dimension but the",
      "start": 1004.36,
      "duration": 3.52
    },
    {
      "text": "second dimension can be",
      "start": 1006.04,
      "duration": 4.479
    },
    {
      "text": "anything so uh if you when you",
      "start": 1007.88,
      "duration": 4.639
    },
    {
      "text": "initialize the query key and the value",
      "start": 1010.519,
      "duration": 5.481
    },
    {
      "text": "we are initializing random values and",
      "start": 1012.519,
      "duration": 5.961
    },
    {
      "text": "the shape of the Matrix is D in comma D",
      "start": 1016.0,
      "duration": 3.8
    },
    {
      "text": "out",
      "start": 1018.48,
      "duration": 3.359
    },
    {
      "text": "remember D in is just the shape of the",
      "start": 1019.8,
      "duration": 4.12
    },
    {
      "text": "input so we are just looking at one",
      "start": 1021.839,
      "duration": 3.761
    },
    {
      "text": "particular Vector of the input that's",
      "start": 1023.92,
      "duration": 3.96
    },
    {
      "text": "the vector Dimension three so that's",
      "start": 1025.6,
      "duration": 5.8
    },
    {
      "text": "also the first uh argument of the shape",
      "start": 1027.88,
      "duration": 6.079
    },
    {
      "text": "of the key query and value Matrix this",
      "start": 1031.4,
      "duration": 4.399
    },
    {
      "text": "exactly what we saw here this first",
      "start": 1033.959,
      "duration": 5.72
    },
    {
      "text": "argument has to be same as the uh Vector",
      "start": 1035.799,
      "duration": 5.321
    },
    {
      "text": "Dimension which is",
      "start": 1039.679,
      "duration": 4.201
    },
    {
      "text": "three and the out Dimension we are",
      "start": 1041.12,
      "duration": 5.559
    },
    {
      "text": "choosing to be two in this case so note",
      "start": 1043.88,
      "duration": 4.76
    },
    {
      "text": "that in GPT like models the input and",
      "start": 1046.679,
      "duration": 3.601
    },
    {
      "text": "out output dimensions are usually the",
      "start": 1048.64,
      "duration": 4.32
    },
    {
      "text": "same but for illustration purposes we",
      "start": 1050.28,
      "duration": 4.36
    },
    {
      "text": "are choosing the input Dimension is",
      "start": 1052.96,
      "duration": 3.4
    },
    {
      "text": "three and the output Dimension is two",
      "start": 1054.64,
      "duration": 4.84
    },
    {
      "text": "over here so these are the query key and",
      "start": 1056.36,
      "duration": 5.48
    },
    {
      "text": "the value weight matrices and each",
      "start": 1059.48,
      "duration": 4.12
    },
    {
      "text": "element in these weight matrices has",
      "start": 1061.84,
      "duration": 4.44
    },
    {
      "text": "been initialized in a random manner so",
      "start": 1063.6,
      "duration": 5.28
    },
    {
      "text": "you can actually see tor. nn. parameter",
      "start": 1066.28,
      "duration": 4.56
    },
    {
      "text": "and you can see the documentation for",
      "start": 1068.88,
      "duration": 5.52
    },
    {
      "text": "this uh yeah so what so you can see what",
      "start": 1070.84,
      "duration": 7.6
    },
    {
      "text": "it does it's uh so it's a kind of tensor",
      "start": 1074.4,
      "duration": 5.6
    },
    {
      "text": "that is to to be considered a module",
      "start": 1078.44,
      "duration": 3.68
    },
    {
      "text": "parameter and inside this you can pass",
      "start": 1080.0,
      "duration": 4.12
    },
    {
      "text": "some things so what we are doing is that",
      "start": 1082.12,
      "duration": 4.039
    },
    {
      "text": "we are passing the parameters to be",
      "start": 1084.12,
      "duration": 4.36
    },
    {
      "text": "random values with the shape of D in",
      "start": 1086.159,
      "duration": 5.361
    },
    {
      "text": "comma D out which means that the Matrix",
      "start": 1088.48,
      "duration": 6.319
    },
    {
      "text": "shape for query key and value will be 3",
      "start": 1091.52,
      "duration": 5.96
    },
    {
      "text": "comma 2 so you can print out the",
      "start": 1094.799,
      "duration": 4.76
    },
    {
      "text": "trainable weight matrices so this is the",
      "start": 1097.48,
      "duration": 4.36
    },
    {
      "text": "trainable weight Matrix for query it's a",
      "start": 1099.559,
      "duration": 4.761
    },
    {
      "text": "three three row by two column tensor",
      "start": 1101.84,
      "duration": 4.199
    },
    {
      "text": "this is the trainable weight Matrix for",
      "start": 1104.32,
      "duration": 3.96
    },
    {
      "text": "key it's a three row and two column",
      "start": 1106.039,
      "duration": 3.281
    },
    {
      "text": "tensor",
      "start": 1108.28,
      "duration": 2.759
    },
    {
      "text": "and this is the trainable weight Matrix",
      "start": 1109.32,
      "duration": 4.0
    },
    {
      "text": "for Value this again is a three row and",
      "start": 1111.039,
      "duration": 4.12
    },
    {
      "text": "two column tensor",
      "start": 1113.32,
      "duration": 4.28
    },
    {
      "text": "awesome uh some minor things is that we",
      "start": 1115.159,
      "duration": 4.321
    },
    {
      "text": "are setting this requires grad right now",
      "start": 1117.6,
      "duration": 5.4
    },
    {
      "text": "to false but remember that uh later we",
      "start": 1119.48,
      "duration": 5.48
    },
    {
      "text": "are going to train the values in these",
      "start": 1123.0,
      "duration": 4.36
    },
    {
      "text": "matrices through back propagation so at",
      "start": 1124.96,
      "duration": 3.68
    },
    {
      "text": "that time we'll need to compute",
      "start": 1127.36,
      "duration": 3.08
    },
    {
      "text": "gradients and at that time we'll set the",
      "start": 1128.64,
      "duration": 4.68
    },
    {
      "text": "requires grad to be equal to True great",
      "start": 1130.44,
      "duration": 5.4
    },
    {
      "text": "so now we have computed the key query",
      "start": 1133.32,
      "duration": 6.88
    },
    {
      "text": "and value um key Quant value weight",
      "start": 1135.84,
      "duration": 6.36
    },
    {
      "text": "trainable weight Matrix right now we",
      "start": 1140.2,
      "duration": 3.52
    },
    {
      "text": "have to do the multiplication of the",
      "start": 1142.2,
      "duration": 5.16
    },
    {
      "text": "inputs with these Matrix to get the",
      "start": 1143.72,
      "duration": 7.52
    },
    {
      "text": "queries key and values uh final Matrix",
      "start": 1147.36,
      "duration": 6.319
    },
    {
      "text": "so to do that first what we can do is we",
      "start": 1151.24,
      "duration": 4.52
    },
    {
      "text": "can look at an individual element so",
      "start": 1153.679,
      "duration": 4.48
    },
    {
      "text": "let's say we look at this second element",
      "start": 1155.76,
      "duration": 5.0
    },
    {
      "text": "which is Journey and let's say we want",
      "start": 1158.159,
      "duration": 5.0
    },
    {
      "text": "to First convert this second element",
      "start": 1160.76,
      "duration": 5.96
    },
    {
      "text": "journey into its corresponding",
      "start": 1163.159,
      "duration": 6.081
    },
    {
      "text": "query uh let me show you how we can do",
      "start": 1166.72,
      "duration": 5.92
    },
    {
      "text": "that but first let me rub um some of the",
      "start": 1169.24,
      "duration": 7.12
    },
    {
      "text": "things here so that I can easily show",
      "start": 1172.64,
      "duration": 6.6
    },
    {
      "text": "you how we get the key query and value",
      "start": 1176.36,
      "duration": 5.0
    },
    {
      "text": "Matrix for",
      "start": 1179.24,
      "duration": 4.919
    },
    {
      "text": "Journey okay so what we are doing here",
      "start": 1181.36,
      "duration": 4.799
    },
    {
      "text": "is that we are focusing our",
      "start": 1184.159,
      "duration": 6.361
    },
    {
      "text": "attention um also let me change the",
      "start": 1186.159,
      "duration": 4.361
    },
    {
      "text": "color yeah so we are focusing our our",
      "start": 1192.799,
      "duration": 4.441
    },
    {
      "text": "attention on this",
      "start": 1195.4,
      "duration": 4.24
    },
    {
      "text": "second um",
      "start": 1197.24,
      "duration": 5.24
    },
    {
      "text": "input Vector which is Journey and then",
      "start": 1199.64,
      "duration": 5.72
    },
    {
      "text": "we want to get the query key and value",
      "start": 1202.48,
      "duration": 5.04
    },
    {
      "text": "Vector for Journey So currently the",
      "start": 1205.36,
      "duration": 3.88
    },
    {
      "text": "input Vector is a three-dimensional",
      "start": 1207.52,
      "duration": 3.279
    },
    {
      "text": "Vector now we need to get the",
      "start": 1209.24,
      "duration": 3.799
    },
    {
      "text": "two-dimensional Vector so the way to get",
      "start": 1210.799,
      "duration": 3.961
    },
    {
      "text": "that is look at these queries keys and",
      "start": 1213.039,
      "duration": 4.161
    },
    {
      "text": "values so the second row here will be",
      "start": 1214.76,
      "duration": 4.88
    },
    {
      "text": "the corresponding query Vector for",
      "start": 1217.2,
      "duration": 4.92
    },
    {
      "text": "Journey the second row here will be the",
      "start": 1219.64,
      "duration": 5.24
    },
    {
      "text": "corresponding key Vector for journey and",
      "start": 1222.12,
      "duration": 4.28
    },
    {
      "text": "the second row here will be the",
      "start": 1224.88,
      "duration": 4.08
    },
    {
      "text": "corresponding value Vector for journey",
      "start": 1226.4,
      "duration": 4.6
    },
    {
      "text": "so the way you get the this second",
      "start": 1228.96,
      "duration": 4.24
    },
    {
      "text": "element is just you take",
      "start": 1231.0,
      "duration": 4.48
    },
    {
      "text": "the uh you take the",
      "start": 1233.2,
      "duration": 6.08
    },
    {
      "text": "journey and uh then you are going to uh",
      "start": 1235.48,
      "duration": 6.84
    },
    {
      "text": "dot product it with",
      "start": 1239.28,
      "duration": 6.0
    },
    {
      "text": "this uh weight Matrix q and similarly",
      "start": 1242.32,
      "duration": 6.0
    },
    {
      "text": "for key and similarly for Value so this",
      "start": 1245.28,
      "duration": 5.32
    },
    {
      "text": "is what we are going to see in code",
      "start": 1248.32,
      "duration": 3.2
    },
    {
      "text": "right",
      "start": 1250.6,
      "duration": 3.76
    },
    {
      "text": "now um instead of directly showing you",
      "start": 1251.52,
      "duration": 4.72
    },
    {
      "text": "the matrix multiplication first I wanted",
      "start": 1254.36,
      "duration": 3.88
    },
    {
      "text": "to show it to you for each individual",
      "start": 1256.24,
      "duration": 3.28
    },
    {
      "text": "element",
      "start": 1258.24,
      "duration": 3.12
    },
    {
      "text": "so remember this if you look at each",
      "start": 1259.52,
      "duration": 3.48
    },
    {
      "text": "individual element this is one row and",
      "start": 1261.36,
      "duration": 3.6
    },
    {
      "text": "three columns so we can multiply it with",
      "start": 1263.0,
      "duration": 3.919
    },
    {
      "text": "the query three row two column and that",
      "start": 1264.96,
      "duration": 3.88
    },
    {
      "text": "will give us a one row two column which",
      "start": 1266.919,
      "duration": 4.24
    },
    {
      "text": "is the query Vector for Journey",
      "start": 1268.84,
      "duration": 4.88
    },
    {
      "text": "similarly for keys and values this is",
      "start": 1271.159,
      "duration": 4.76
    },
    {
      "text": "what I'm going to show you in code right",
      "start": 1273.72,
      "duration": 5.28
    },
    {
      "text": "now so uh let's look at the second",
      "start": 1275.919,
      "duration": 5.88
    },
    {
      "text": "element X2 and X2 has been defined",
      "start": 1279.0,
      "duration": 5.679
    },
    {
      "text": "earlier as inputs accessed and the index",
      "start": 1281.799,
      "duration": 4.76
    },
    {
      "text": "is one so since python has a zero",
      "start": 1284.679,
      "duration": 3.961
    },
    {
      "text": "indexing system inputs one will",
      "start": 1286.559,
      "duration": 5.561
    },
    {
      "text": "essentially be the input for Journey so",
      "start": 1288.64,
      "duration": 4.96
    },
    {
      "text": "that is defined by",
      "start": 1292.12,
      "duration": 4.679
    },
    {
      "text": "xor2 so we are going to uh find the",
      "start": 1293.6,
      "duration": 5.88
    },
    {
      "text": "query corresponding to this xor2 by",
      "start": 1296.799,
      "duration": 5.321
    },
    {
      "text": "multiplying it with the weight Matrix",
      "start": 1299.48,
      "duration": 5.24
    },
    {
      "text": "for query we are going to find the key",
      "start": 1302.12,
      "duration": 5.12
    },
    {
      "text": "for the journey vector by multiplying it",
      "start": 1304.72,
      "duration": 4.76
    },
    {
      "text": "with the key Matrix and we are going to",
      "start": 1307.24,
      "duration": 4.48
    },
    {
      "text": "find the value for the journey vector by",
      "start": 1309.48,
      "duration": 4.079
    },
    {
      "text": "multiplying it with the value weight",
      "start": 1311.72,
      "duration": 5.12
    },
    {
      "text": "Matrix and here you can see we get 4306",
      "start": 1313.559,
      "duration": 7.401
    },
    {
      "text": "and 1.45 51 which is the query Vector",
      "start": 1316.84,
      "duration": 6.92
    },
    {
      "text": "for uh so I'm just printing the query",
      "start": 1320.96,
      "duration": 5.12
    },
    {
      "text": "here so this is the query Vector for",
      "start": 1323.76,
      "duration": 4.64
    },
    {
      "text": "Journey and let's see whether it matches",
      "start": 1326.08,
      "duration": 4.079
    },
    {
      "text": "so the query Vector for Journey was",
      "start": 1328.4,
      "duration": 5.56
    },
    {
      "text": "indeed 43 and 1.45 here if you can",
      "start": 1330.159,
      "duration": 6.52
    },
    {
      "text": "see uh what I'm showing in the color",
      "start": 1333.96,
      "duration": 4.76
    },
    {
      "text": "right now and that exactly matches what",
      "start": 1336.679,
      "duration": 4.281
    },
    {
      "text": "we have in code awesome so we are",
      "start": 1338.72,
      "duration": 4.6
    },
    {
      "text": "currently moving in the right direction",
      "start": 1340.96,
      "duration": 4.079
    },
    {
      "text": "as we can see based on the output the",
      "start": 1343.32,
      "duration": 3.599
    },
    {
      "text": "result is a two dimensional t two",
      "start": 1345.039,
      "duration": 4.081
    },
    {
      "text": "dimensional vector",
      "start": 1346.919,
      "duration": 3.88
    },
    {
      "text": "now what we can do is that we can",
      "start": 1349.12,
      "duration": 5.439
    },
    {
      "text": "actually obtain the keys and values uh",
      "start": 1350.799,
      "duration": 6.36
    },
    {
      "text": "and queries for all the different inputs",
      "start": 1354.559,
      "duration": 4.36
    },
    {
      "text": "this is exactly what we had written over",
      "start": 1357.159,
      "duration": 3.76
    },
    {
      "text": "here once we get the trainable weight",
      "start": 1358.919,
      "duration": 4.041
    },
    {
      "text": "Matrix we can just multiply the inputs",
      "start": 1360.919,
      "duration": 5.281
    },
    {
      "text": "with these weight Matrix and get the get",
      "start": 1362.96,
      "duration": 5.8
    },
    {
      "text": "the queries Matrix get the keys",
      "start": 1366.2,
      "duration": 5.599
    },
    {
      "text": "Matrix uh get the keys",
      "start": 1368.76,
      "duration": 6.36
    },
    {
      "text": "Matrix and get the values Matrix so this",
      "start": 1371.799,
      "duration": 5.601
    },
    {
      "text": "is what I'm going to show to you in code",
      "start": 1375.12,
      "duration": 5.32
    },
    {
      "text": "right now",
      "start": 1377.4,
      "duration": 3.04
    },
    {
      "text": "okay so to get the keys Matrix we just",
      "start": 1380.76,
      "duration": 5.2
    },
    {
      "text": "multiply the inputs with the weight",
      "start": 1383.799,
      "duration": 4.88
    },
    {
      "text": "Matrix for keys to get the values Matrix",
      "start": 1385.96,
      "duration": 4.319
    },
    {
      "text": "we just multiply the inputs with the",
      "start": 1388.679,
      "duration": 3.761
    },
    {
      "text": "weight Matrix for values and to get the",
      "start": 1390.279,
      "duration": 3.921
    },
    {
      "text": "queries Matrix we just multiply the",
      "start": 1392.44,
      "duration": 4.64
    },
    {
      "text": "inputs with the weight Matrix for query",
      "start": 1394.2,
      "duration": 5.04
    },
    {
      "text": "and uh let me just run this right now so",
      "start": 1397.08,
      "duration": 3.92
    },
    {
      "text": "if you run this you can just print out",
      "start": 1399.24,
      "duration": 4.0
    },
    {
      "text": "the shape of the keys values and queries",
      "start": 1401.0,
      "duration": 4.48
    },
    {
      "text": "and as expected it's 6 by two so we have",
      "start": 1403.24,
      "duration": 5.28
    },
    {
      "text": "six rows and two columns uh why six",
      "start": 1405.48,
      "duration": 5.72
    },
    {
      "text": "because there are six input tokens your",
      "start": 1408.52,
      "duration": 4.92
    },
    {
      "text": "journey begins with one step and for",
      "start": 1411.2,
      "duration": 3.88
    },
    {
      "text": "each input token we have a twood",
      "start": 1413.44,
      "duration": 3.76
    },
    {
      "text": "dimensional key Vector two dimensional",
      "start": 1415.08,
      "duration": 4.92
    },
    {
      "text": "value vector and two dimensional query",
      "start": 1417.2,
      "duration": 5.2
    },
    {
      "text": "Vector so as we can tell from the",
      "start": 1420.0,
      "duration": 4.36
    },
    {
      "text": "outputs we have successfully projected",
      "start": 1422.4,
      "duration": 4.8
    },
    {
      "text": "the six input tokens from a 3D input",
      "start": 1424.36,
      "duration": 5.0
    },
    {
      "text": "embedding space from a 2d embedding",
      "start": 1427.2,
      "duration": 4.719
    },
    {
      "text": "space for the keys values and",
      "start": 1429.36,
      "duration": 5.04
    },
    {
      "text": "queries okay so if you have understood",
      "start": 1431.919,
      "duration": 3.921
    },
    {
      "text": "up till now you have essentially",
      "start": 1434.4,
      "duration": 3.159
    },
    {
      "text": "understood the first part of today's",
      "start": 1435.84,
      "duration": 4.0
    },
    {
      "text": "lecture and and the first part was how",
      "start": 1437.559,
      "duration": 5.0
    },
    {
      "text": "to convert input embeddings how to",
      "start": 1439.84,
      "duration": 5.04
    },
    {
      "text": "convert input embeddings into key query",
      "start": 1442.559,
      "duration": 4.921
    },
    {
      "text": "and value vectors awesome now we are",
      "start": 1444.88,
      "duration": 6.72
    },
    {
      "text": "ready to move to step number two and uh",
      "start": 1447.48,
      "duration": 5.679
    },
    {
      "text": "before that let me just show you a",
      "start": 1451.6,
      "duration": 4.04
    },
    {
      "text": "schematic of what we have done until now",
      "start": 1453.159,
      "duration": 4.681
    },
    {
      "text": "so what we have done is that we have the",
      "start": 1455.64,
      "duration": 4.12
    },
    {
      "text": "inputs right and we have converted the",
      "start": 1457.84,
      "duration": 3.439
    },
    {
      "text": "inputs into their embedding the",
      "start": 1459.76,
      "duration": 3.32
    },
    {
      "text": "threedimensional embedding",
      "start": 1461.279,
      "duration": 4.801
    },
    {
      "text": "vectors then what we did was we had a",
      "start": 1463.08,
      "duration": 5.68
    },
    {
      "text": "key query and value and we multip IED",
      "start": 1466.08,
      "duration": 6.599
    },
    {
      "text": "every input with the key query and the",
      "start": 1468.76,
      "duration": 6.12
    },
    {
      "text": "value so here I'm showing keys and",
      "start": 1472.679,
      "duration": 4.561
    },
    {
      "text": "values so you can multiply every input",
      "start": 1474.88,
      "duration": 5.08
    },
    {
      "text": "with the keys queries and value to get",
      "start": 1477.24,
      "duration": 5.559
    },
    {
      "text": "the uh key query and value Matrix for",
      "start": 1479.96,
      "duration": 4.88
    },
    {
      "text": "every single input",
      "start": 1482.799,
      "duration": 4.12
    },
    {
      "text": "embedding that's what we have done until",
      "start": 1484.84,
      "duration": 4.199
    },
    {
      "text": "now essentially every input embedding",
      "start": 1486.919,
      "duration": 3.76
    },
    {
      "text": "Vector has been multiplied by with the",
      "start": 1489.039,
      "duration": 4.041
    },
    {
      "text": "key query and value trainable weight",
      "start": 1490.679,
      "duration": 5.561
    },
    {
      "text": "Matrix to get the final key query and",
      "start": 1493.08,
      "duration": 5.76
    },
    {
      "text": "value matrices for all the inputs so for",
      "start": 1496.24,
      "duration": 4.96
    },
    {
      "text": "the rest of the lecture imagine that we",
      "start": 1498.84,
      "duration": 4.199
    },
    {
      "text": "don't have the input embeddings at all",
      "start": 1501.2,
      "duration": 4.04
    },
    {
      "text": "we will only deal with the key query and",
      "start": 1503.039,
      "duration": 5.24
    },
    {
      "text": "value matrices so now let's move to the",
      "start": 1505.24,
      "duration": 5.24
    },
    {
      "text": "next step in the next",
      "start": 1508.279,
      "duration": 5.64
    },
    {
      "text": "step uh we have to compute the attention",
      "start": 1510.48,
      "duration": 6.079
    },
    {
      "text": "scores what is meant by attention scores",
      "start": 1513.919,
      "duration": 4.521
    },
    {
      "text": "we have to essentially compute that if",
      "start": 1516.559,
      "duration": 4.441
    },
    {
      "text": "you are given a query U let's say if you",
      "start": 1518.44,
      "duration": 6.119
    },
    {
      "text": "are given a particular query uh how does",
      "start": 1521.0,
      "duration": 8.48
    },
    {
      "text": "the other Keys attend to the that query",
      "start": 1524.559,
      "duration": 8.24
    },
    {
      "text": "let me explain this to you so let's say",
      "start": 1529.48,
      "duration": 6.36
    },
    {
      "text": "uh we have the query for the",
      "start": 1532.799,
      "duration": 5.721
    },
    {
      "text": "second word which is Journey and this is",
      "start": 1535.84,
      "duration": 4.28
    },
    {
      "text": "the query which is a two-dimensional",
      "start": 1538.52,
      "duration": 4.24
    },
    {
      "text": "Vector now we have to find out how does",
      "start": 1540.12,
      "duration": 4.84
    },
    {
      "text": "this query attend to the keys for the",
      "start": 1542.76,
      "duration": 4.56
    },
    {
      "text": "different input words so you can think",
      "start": 1544.96,
      "duration": 5.56
    },
    {
      "text": "of the keys right now as just individual",
      "start": 1547.32,
      "duration": 5.12
    },
    {
      "text": "tokens like what we did in the previous",
      "start": 1550.52,
      "duration": 3.96
    },
    {
      "text": "class remember in the previous class the",
      "start": 1552.44,
      "duration": 4.8
    },
    {
      "text": "query was just the just that particular",
      "start": 1554.48,
      "duration": 5.199
    },
    {
      "text": "token we did not have a separate Vector",
      "start": 1557.24,
      "duration": 5.0
    },
    {
      "text": "for query we just the journey itself was",
      "start": 1559.679,
      "duration": 5.0
    },
    {
      "text": "the query the journey token itself was",
      "start": 1562.24,
      "duration": 5.0
    },
    {
      "text": "the query so if ever you get confused in",
      "start": 1564.679,
      "duration": 4.201
    },
    {
      "text": "terms of intuition just think of the",
      "start": 1567.24,
      "duration": 4.24
    },
    {
      "text": "query as being the token itself although",
      "start": 1568.88,
      "duration": 4.32
    },
    {
      "text": "it's a bit different so what we are",
      "start": 1571.48,
      "duration": 4.6
    },
    {
      "text": "essentially doing is finding how that",
      "start": 1573.2,
      "duration": 4.479
    },
    {
      "text": "particular query so now we are looking",
      "start": 1576.08,
      "duration": 3.44
    },
    {
      "text": "at query number two which is related to",
      "start": 1577.679,
      "duration": 4.401
    },
    {
      "text": "journey and we want to see how the other",
      "start": 1579.52,
      "duration": 5.759
    },
    {
      "text": "words attend to Journey which means that",
      "start": 1582.08,
      "duration": 5.28
    },
    {
      "text": "when I'm predicting the next word how",
      "start": 1585.279,
      "duration": 4.601
    },
    {
      "text": "much in importance should I give to your",
      "start": 1587.36,
      "duration": 3.919
    },
    {
      "text": "how much importance should I give to",
      "start": 1589.88,
      "duration": 3.12
    },
    {
      "text": "Journey how much importance should I",
      "start": 1591.279,
      "duration": 5.041
    },
    {
      "text": "give to with one and step so my query is",
      "start": 1593.0,
      "duration": 5.12
    },
    {
      "text": "Journey and I'm going to look at how",
      "start": 1596.32,
      "duration": 3.64
    },
    {
      "text": "much importance I need to give to the",
      "start": 1598.12,
      "duration": 4.52
    },
    {
      "text": "other words so that's why we need to",
      "start": 1599.96,
      "duration": 6.0
    },
    {
      "text": "find the attention scores between the",
      "start": 1602.64,
      "duration": 5.24
    },
    {
      "text": "query and the",
      "start": 1605.96,
      "duration": 4.959
    },
    {
      "text": "key remember this intuition is very very",
      "start": 1607.88,
      "duration": 4.84
    },
    {
      "text": "important right now what we are",
      "start": 1610.919,
      "duration": 3.601
    },
    {
      "text": "essentially doing is that we are finding",
      "start": 1612.72,
      "duration": 3.72
    },
    {
      "text": "the attention score between the query",
      "start": 1614.52,
      "duration": 4.2
    },
    {
      "text": "and the key in the previous leure we",
      "start": 1616.44,
      "duration": 4.04
    },
    {
      "text": "just found the attention score between",
      "start": 1618.72,
      "duration": 4.0
    },
    {
      "text": "the input embedding vector and the other",
      "start": 1620.48,
      "duration": 3.96
    },
    {
      "text": "embedding vectors by taking a DOT",
      "start": 1622.72,
      "duration": 4.64
    },
    {
      "text": "product but now remember we don't have",
      "start": 1624.44,
      "duration": 4.64
    },
    {
      "text": "the input embeding space at all we are",
      "start": 1627.36,
      "duration": 3.84
    },
    {
      "text": "in an abstract space we are in the query",
      "start": 1629.08,
      "duration": 5.12
    },
    {
      "text": "key and value space so we are going to",
      "start": 1631.2,
      "duration": 6.28
    },
    {
      "text": "find how the query number two attends to",
      "start": 1634.2,
      "duration": 5.359
    },
    {
      "text": "the different key vectors and we are",
      "start": 1637.48,
      "duration": 4.28
    },
    {
      "text": "going to find it in the exact same",
      "start": 1639.559,
      "duration": 4.201
    },
    {
      "text": "manner as we did in the last class",
      "start": 1641.76,
      "duration": 3.799
    },
    {
      "text": "remember the mathematical operation",
      "start": 1643.76,
      "duration": 3.279
    },
    {
      "text": "which helps us to find whether two",
      "start": 1645.559,
      "duration": 3.521
    },
    {
      "text": "vectors are aligned or not",
      "start": 1647.039,
      "duration": 4.161
    },
    {
      "text": "that is the dot product operation so",
      "start": 1649.08,
      "duration": 5.12
    },
    {
      "text": "let's say if this is my query vector and",
      "start": 1651.2,
      "duration": 6.64
    },
    {
      "text": "if uh let me show the key Vector in some",
      "start": 1654.2,
      "duration": 6.04
    },
    {
      "text": "other color so let's say this is my",
      "start": 1657.84,
      "duration": 3.559
    },
    {
      "text": "query",
      "start": 1660.24,
      "duration": 3.919
    },
    {
      "text": "vector and let's say this is my key",
      "start": 1661.399,
      "duration": 5.12
    },
    {
      "text": "Vector these two vectors are very much",
      "start": 1664.159,
      "duration": 4.081
    },
    {
      "text": "aligned with each other right so the dot",
      "start": 1666.519,
      "duration": 4.52
    },
    {
      "text": "product will be maximum so it says that",
      "start": 1668.24,
      "duration": 4.48
    },
    {
      "text": "when I look at the query I should",
      "start": 1671.039,
      "duration": 4.561
    },
    {
      "text": "probably pay more attention to this key",
      "start": 1672.72,
      "duration": 5.48
    },
    {
      "text": "Vector whereas let's look at another key",
      "start": 1675.6,
      "duration": 4.319
    },
    {
      "text": "key Vector now which is like this so",
      "start": 1678.2,
      "duration": 3.199
    },
    {
      "text": "let's say there is another key Vector",
      "start": 1679.919,
      "duration": 3.921
    },
    {
      "text": "which is like this so now if you look at",
      "start": 1681.399,
      "duration": 4.4
    },
    {
      "text": "the query and this second key Vector",
      "start": 1683.84,
      "duration": 3.8
    },
    {
      "text": "they have a 90\u00b0 angle with each other",
      "start": 1685.799,
      "duration": 4.281
    },
    {
      "text": "they are not at all aligned which means",
      "start": 1687.64,
      "duration": 4.68
    },
    {
      "text": "that when you look at that query you",
      "start": 1690.08,
      "duration": 4.4
    },
    {
      "text": "should not pay attention to this green",
      "start": 1692.32,
      "duration": 5.199
    },
    {
      "text": "key over here and that is encapsulated",
      "start": 1694.48,
      "duration": 5.079
    },
    {
      "text": "by the dot product if you find the dot",
      "start": 1697.519,
      "duration": 3.481
    },
    {
      "text": "product between the yellow vector and",
      "start": 1699.559,
      "duration": 3.521
    },
    {
      "text": "the green Vector they have a 90\u00b0 angles",
      "start": 1701.0,
      "duration": 4.519
    },
    {
      "text": "the dot product will be zero that's what",
      "start": 1703.08,
      "duration": 4.719
    },
    {
      "text": "we are going to do now we are going to",
      "start": 1705.519,
      "duration": 4.16
    },
    {
      "text": "find the the attention scores between",
      "start": 1707.799,
      "duration": 4.0
    },
    {
      "text": "the particular query and all the other",
      "start": 1709.679,
      "duration": 5.641
    },
    {
      "text": "Keys remember every every query will",
      "start": 1711.799,
      "duration": 5.24
    },
    {
      "text": "have an attention score with all the",
      "start": 1715.32,
      "duration": 4.04
    },
    {
      "text": "other keys so for example if you look at",
      "start": 1717.039,
      "duration": 4.48
    },
    {
      "text": "query number two for Journey it will",
      "start": 1719.36,
      "duration": 4.12
    },
    {
      "text": "have an attention score with key number",
      "start": 1721.519,
      "duration": 4.64
    },
    {
      "text": "one it will have an attention score with",
      "start": 1723.48,
      "duration": 5.36
    },
    {
      "text": "the key number two and similarly it will",
      "start": 1726.159,
      "duration": 4.681
    },
    {
      "text": "have an attention score for the final",
      "start": 1728.84,
      "duration": 5.12
    },
    {
      "text": "key uh which is Step so this is what we",
      "start": 1730.84,
      "duration": 4.52
    },
    {
      "text": "are going to do",
      "start": 1733.96,
      "duration": 3.8
    },
    {
      "text": "next and let me show this to you in a",
      "start": 1735.36,
      "duration": 4.799
    },
    {
      "text": "picture tutorial representation right",
      "start": 1737.76,
      "duration": 4.84
    },
    {
      "text": "now okay so the way we are going to do",
      "start": 1740.159,
      "duration": 4.64
    },
    {
      "text": "this is by initially only focusing on",
      "start": 1742.6,
      "duration": 4.84
    },
    {
      "text": "the queries and keys okay so let's say",
      "start": 1744.799,
      "duration": 5.081
    },
    {
      "text": "we look at so this is our queries Matrix",
      "start": 1747.44,
      "duration": 4.68
    },
    {
      "text": "which is a 6x2 and I'm going to now",
      "start": 1749.88,
      "duration": 4.039
    },
    {
      "text": "focus on the second row of this because",
      "start": 1752.12,
      "duration": 3.919
    },
    {
      "text": "I'm going to look at Journey I'm going",
      "start": 1753.919,
      "duration": 4.401
    },
    {
      "text": "to look at the word Journey so the query",
      "start": 1756.039,
      "duration": 4.36
    },
    {
      "text": "for the word journey is the second row",
      "start": 1758.32,
      "duration": 4.52
    },
    {
      "text": "right now what I actually want to do is",
      "start": 1760.399,
      "duration": 5.801
    },
    {
      "text": "I want to find the uh dot product",
      "start": 1762.84,
      "duration": 5.64
    },
    {
      "text": "between this query and all the other key",
      "start": 1766.2,
      "duration": 3.8
    },
    {
      "text": "so I want to find the dot product",
      "start": 1768.48,
      "duration": 3.84
    },
    {
      "text": "between this query and the first row",
      "start": 1770.0,
      "duration": 5.6
    },
    {
      "text": "with the second row with the third row",
      "start": 1772.32,
      "duration": 6.359
    },
    {
      "text": "with the fourth row with the fifth row",
      "start": 1775.6,
      "duration": 6.679
    },
    {
      "text": "and with the sixth row so to essentially",
      "start": 1778.679,
      "duration": 6.6
    },
    {
      "text": "find the attention score for the second",
      "start": 1782.279,
      "duration": 5.481
    },
    {
      "text": "query all we need to do is we need to",
      "start": 1785.279,
      "duration": 5.24
    },
    {
      "text": "take the that particular row and we need",
      "start": 1787.76,
      "duration": 4.6
    },
    {
      "text": "to find the dot product with all the",
      "start": 1790.519,
      "duration": 4.201
    },
    {
      "text": "other rows of the keys so we'll have six",
      "start": 1792.36,
      "duration": 3.4
    },
    {
      "text": "attention",
      "start": 1794.72,
      "duration": 3.72
    },
    {
      "text": "scores and that those attention scores",
      "start": 1795.76,
      "duration": 4.36
    },
    {
      "text": "contain the information that when you",
      "start": 1798.44,
      "duration": 4.16
    },
    {
      "text": "look at the query for Journey how much",
      "start": 1800.12,
      "duration": 3.88
    },
    {
      "text": "importance should be given to other",
      "start": 1802.6,
      "duration": 3.72
    },
    {
      "text": "words like your journey begins with one",
      "start": 1804.0,
      "duration": 4.36
    },
    {
      "text": "step so this is what we are going to",
      "start": 1806.32,
      "duration": 5.0
    },
    {
      "text": "implement in code right now so I'm going",
      "start": 1808.36,
      "duration": 5.84
    },
    {
      "text": "to look at the keys of",
      "start": 1811.32,
      "duration": 7.239
    },
    {
      "text": "one um so I'm going to look at this keys",
      "start": 1814.2,
      "duration": 6.52
    },
    {
      "text": "so Keys 2 is keys of one which means the",
      "start": 1818.559,
      "duration": 3.761
    },
    {
      "text": "key for",
      "start": 1820.72,
      "duration": 3.959
    },
    {
      "text": "Journey uh which I have highlighted over",
      "start": 1822.32,
      "duration": 5.92
    },
    {
      "text": "here so this this is actually my key",
      "start": 1824.679,
      "duration": 5.401
    },
    {
      "text": "Keys",
      "start": 1828.24,
      "duration": 4.64
    },
    {
      "text": "underscore",
      "start": 1830.08,
      "duration": 6.52
    },
    {
      "text": "2 so the keys uncr 2 in the code is the",
      "start": 1832.88,
      "duration": 5.519
    },
    {
      "text": "keys for",
      "start": 1836.6,
      "duration": 4.36
    },
    {
      "text": "Journey uh and then what I'm going to do",
      "start": 1838.399,
      "duration": 6.4
    },
    {
      "text": "is I'm going to uh take the dot product",
      "start": 1840.96,
      "duration": 8.36
    },
    {
      "text": "between actually before that let",
      "start": 1844.799,
      "duration": 4.521
    },
    {
      "text": "me",
      "start": 1849.919,
      "duration": 6.401
    },
    {
      "text": "right yeah so let me correct that a bit",
      "start": 1852.799,
      "duration": 5.961
    },
    {
      "text": "so this actually is queries",
      "start": 1856.32,
      "duration": 5.239
    },
    {
      "text": "so this is actually queries unroll 2 so",
      "start": 1858.76,
      "duration": 6.68
    },
    {
      "text": "query unroll 2 is is this query so let",
      "start": 1861.559,
      "duration": 5.84
    },
    {
      "text": "me reframe that",
      "start": 1865.44,
      "duration": 6.959
    },
    {
      "text": "again uh so here what I have is",
      "start": 1867.399,
      "duration": 5.0
    },
    {
      "text": "query query uncore",
      "start": 1874.36,
      "duration": 7.08
    },
    {
      "text": "2 and to find the attention score for",
      "start": 1878.639,
      "duration": 4.681
    },
    {
      "text": "this query I'm going to take a DOT",
      "start": 1881.44,
      "duration": 3.68
    },
    {
      "text": "product between the second query and the",
      "start": 1883.32,
      "duration": 4.959
    },
    {
      "text": "keys Matrix so this is exactly what has",
      "start": 1885.12,
      "duration": 5.919
    },
    {
      "text": "been done in the in the code here so to",
      "start": 1888.279,
      "duration": 6.28
    },
    {
      "text": "find the attention score between the",
      "start": 1891.039,
      "duration": 6.0
    },
    {
      "text": "query 2 which is the query for Journey",
      "start": 1894.559,
      "duration": 3.96
    },
    {
      "text": "we are going to take a DOT product",
      "start": 1897.039,
      "duration": 3.64
    },
    {
      "text": "between the query and the keys transpose",
      "start": 1898.519,
      "duration": 3.64
    },
    {
      "text": "why are we taking a transpose here",
      "start": 1900.679,
      "duration": 2.96
    },
    {
      "text": "because look at the dimensions always",
      "start": 1902.159,
      "duration": 4.041
    },
    {
      "text": "look at the dimensions uh the dimensions",
      "start": 1903.639,
      "duration": 5.201
    },
    {
      "text": "for query is that it's one row and it's",
      "start": 1906.2,
      "duration": 5.719
    },
    {
      "text": "uh it's one row and it's two columns so",
      "start": 1908.84,
      "duration": 4.88
    },
    {
      "text": "we cannot directly multiply it with the",
      "start": 1911.919,
      "duration": 3.921
    },
    {
      "text": "keys because the keys has six rows and",
      "start": 1913.72,
      "duration": 4.919
    },
    {
      "text": "two columns whereas keys transpose we",
      "start": 1915.84,
      "duration": 5.36
    },
    {
      "text": "have two rows and six columns and that",
      "start": 1918.639,
      "duration": 5.481
    },
    {
      "text": "can be multiplied so then Keys transpose",
      "start": 1921.2,
      "duration": 4.8
    },
    {
      "text": "will be 2x 6 so let me write that",
      "start": 1924.12,
      "duration": 4.159
    },
    {
      "text": "Dimension down so Keys transpose will be",
      "start": 1926.0,
      "duration": 5.72
    },
    {
      "text": "2x 6 and if you multiply 1X two Matrix",
      "start": 1928.279,
      "duration": 5.88
    },
    {
      "text": "with 2x 6 you'll get 1X 6 so you'll get",
      "start": 1931.72,
      "duration": 4.559
    },
    {
      "text": "six attention scores for all the",
      "start": 1934.159,
      "duration": 5.0
    },
    {
      "text": "different uh for the six input tokens",
      "start": 1936.279,
      "duration": 5.0
    },
    {
      "text": "your journey begins with one step so",
      "start": 1939.159,
      "duration": 4.201
    },
    {
      "text": "that sounds correct so this is what we",
      "start": 1941.279,
      "duration": 4.24
    },
    {
      "text": "are going to do over here so the",
      "start": 1943.36,
      "duration": 3.96
    },
    {
      "text": "attention scores two which is the",
      "start": 1945.519,
      "duration": 4.241
    },
    {
      "text": "attention score for Journey is just a",
      "start": 1947.32,
      "duration": 4.199
    },
    {
      "text": "matrix product between the query for",
      "start": 1949.76,
      "duration": 3.48
    },
    {
      "text": "Journey multiplied with the keys",
      "start": 1951.519,
      "duration": 3.961
    },
    {
      "text": "transpose so you get the six attention",
      "start": 1953.24,
      "duration": 5.12
    },
    {
      "text": "scores here so as you can see the first",
      "start": 1955.48,
      "duration": 6.0
    },
    {
      "text": "attention score is the encodes",
      "start": 1958.36,
      "duration": 5.52
    },
    {
      "text": "information about how much Journey",
      "start": 1961.48,
      "duration": 5.0
    },
    {
      "text": "attends to your the second attention",
      "start": 1963.88,
      "duration": 5.08
    },
    {
      "text": "score encodes information about how much",
      "start": 1966.48,
      "duration": 4.84
    },
    {
      "text": "Journey attends to Journey the third",
      "start": 1968.96,
      "duration": 4.24
    },
    {
      "text": "attention score encodes information",
      "start": 1971.32,
      "duration": 3.44
    },
    {
      "text": "about how much Journey attends with",
      "start": 1973.2,
      "duration": 4.12
    },
    {
      "text": "width similarly the last attention score",
      "start": 1974.76,
      "duration": 4.32
    },
    {
      "text": "encodes information about how Journey",
      "start": 1977.32,
      "duration": 4.479
    },
    {
      "text": "attends with step so the second score is",
      "start": 1979.08,
      "duration": 4.079
    },
    {
      "text": "the highest because of course journey",
      "start": 1981.799,
      "duration": 3.161
    },
    {
      "text": "and journey will be intuitively more",
      "start": 1983.159,
      "duration": 3.961
    },
    {
      "text": "aligned to each other but remember these",
      "start": 1984.96,
      "duration": 3.76
    },
    {
      "text": "scores don't mean anything right now",
      "start": 1987.12,
      "duration": 3.36
    },
    {
      "text": "because we have not trained any of the",
      "start": 1988.72,
      "duration": 2.679
    },
    {
      "text": "weight",
      "start": 1990.48,
      "duration": 3.079
    },
    {
      "text": "matrices these scores will only mean",
      "start": 1991.399,
      "duration": 3.801
    },
    {
      "text": "something when you train the weight",
      "start": 1993.559,
      "duration": 4.0
    },
    {
      "text": "matrices so ideally if you have a long",
      "start": 1995.2,
      "duration": 4.56
    },
    {
      "text": "paragraph and and if journey and step",
      "start": 1997.559,
      "duration": 4.0
    },
    {
      "text": "are more related to each other in that",
      "start": 1999.76,
      "duration": 4.519
    },
    {
      "text": "paragraph after the matrices are trained",
      "start": 2001.559,
      "duration": 4.521
    },
    {
      "text": "this last value which is the attention",
      "start": 2004.279,
      "duration": 3.921
    },
    {
      "text": "score between journey and step has to be",
      "start": 2006.08,
      "duration": 3.28
    },
    {
      "text": "the",
      "start": 2008.2,
      "duration": 3.719
    },
    {
      "text": "highest so up till now what I showed you",
      "start": 2009.36,
      "duration": 5.039
    },
    {
      "text": "is how to find the attention score the",
      "start": 2011.919,
      "duration": 4.201
    },
    {
      "text": "six values of the attention score for",
      "start": 2014.399,
      "duration": 4.441
    },
    {
      "text": "one query but now what if you want to",
      "start": 2016.12,
      "duration": 4.32
    },
    {
      "text": "find the attention score for all the",
      "start": 2018.84,
      "duration": 3.559
    },
    {
      "text": "other queries for the first query second",
      "start": 2020.44,
      "duration": 4.119
    },
    {
      "text": "third fourth fifth sixth the simplest",
      "start": 2022.399,
      "duration": 4.4
    },
    {
      "text": "way to do that is just for the second",
      "start": 2024.559,
      "duration": 3.761
    },
    {
      "text": "query you just multiplied it with the",
      "start": 2026.799,
      "duration": 3.401
    },
    {
      "text": "keys transpose right and similarly",
      "start": 2028.32,
      "duration": 4.04
    },
    {
      "text": "you'll do for all queries so why don't",
      "start": 2030.2,
      "duration": 4.12
    },
    {
      "text": "you just do a matrix multiplication so",
      "start": 2032.36,
      "duration": 3.84
    },
    {
      "text": "you multiply the queries Matrix with",
      "start": 2034.32,
      "duration": 4.68
    },
    {
      "text": "keys transpose",
      "start": 2036.2,
      "duration": 4.56
    },
    {
      "text": "and I've shown this over here for your",
      "start": 2039.0,
      "duration": 4.2
    },
    {
      "text": "reference so this is the query's Matrix",
      "start": 2040.76,
      "duration": 4.799
    },
    {
      "text": "which is 6x2 and the keys transpose is",
      "start": 2043.2,
      "duration": 5.679
    },
    {
      "text": "2x6 so of course 6x2 can be multiplied",
      "start": 2045.559,
      "duration": 5.481
    },
    {
      "text": "with the 2x6 Matrix and ultimately",
      "start": 2048.879,
      "duration": 4.401
    },
    {
      "text": "you'll get a matrix like this which is a",
      "start": 2051.04,
      "duration": 5.359
    },
    {
      "text": "6x6 Matrix and that is our attention",
      "start": 2053.28,
      "duration": 5.639
    },
    {
      "text": "score Matrix now what does this",
      "start": 2056.399,
      "duration": 5.2
    },
    {
      "text": "attention score Matrix symbolize so the",
      "start": 2058.919,
      "duration": 6.16
    },
    {
      "text": "first row is contains the attention",
      "start": 2061.599,
      "duration": 6.121
    },
    {
      "text": "scores between the first query and all",
      "start": 2065.079,
      "duration": 5.04
    },
    {
      "text": "all the other Keys the second row",
      "start": 2067.72,
      "duration": 4.24
    },
    {
      "text": "contains the attention scores between",
      "start": 2070.119,
      "duration": 4.28
    },
    {
      "text": "the second query and all the other Keys",
      "start": 2071.96,
      "duration": 4.639
    },
    {
      "text": "similarly the last row contains the",
      "start": 2074.399,
      "duration": 4.28
    },
    {
      "text": "attention score between the last query",
      "start": 2076.599,
      "duration": 3.361
    },
    {
      "text": "and all the other",
      "start": 2078.679,
      "duration": 3.601
    },
    {
      "text": "Keys that's the simple meaning of the",
      "start": 2079.96,
      "duration": 4.6
    },
    {
      "text": "attention scores and I'm again going to",
      "start": 2082.28,
      "duration": 4.0
    },
    {
      "text": "do this in code right now to get the",
      "start": 2084.56,
      "duration": 4.279
    },
    {
      "text": "attention scores I just take the matrix",
      "start": 2086.28,
      "duration": 4.48
    },
    {
      "text": "multiplication of queries with keys",
      "start": 2088.839,
      "duration": 5.32
    },
    {
      "text": "transpose and then you get the 6x6 uh",
      "start": 2090.76,
      "duration": 5.68
    },
    {
      "text": "attention scores Matrix that's it so we",
      "start": 2094.159,
      "duration": 4.161
    },
    {
      "text": "have calculated the attention score",
      "start": 2096.44,
      "duration": 4.2
    },
    {
      "text": "between every query with respect to all",
      "start": 2098.32,
      "duration": 4.08
    },
    {
      "text": "the other Keys",
      "start": 2100.64,
      "duration": 4.64
    },
    {
      "text": "awesome uh this is the second step and",
      "start": 2102.4,
      "duration": 4.719
    },
    {
      "text": "we have still not yet got to the context",
      "start": 2105.28,
      "duration": 3.92
    },
    {
      "text": "Vector so the first step was to convert",
      "start": 2107.119,
      "duration": 3.761
    },
    {
      "text": "the input embedding vectors to the key",
      "start": 2109.2,
      "duration": 4.0
    },
    {
      "text": "query and value the Second Step was to",
      "start": 2110.88,
      "duration": 5.0
    },
    {
      "text": "use the key and the query to get to the",
      "start": 2113.2,
      "duration": 5.04
    },
    {
      "text": "attention scores now the problem with",
      "start": 2115.88,
      "duration": 3.92
    },
    {
      "text": "these attention scores is that they are",
      "start": 2118.24,
      "duration": 3.96
    },
    {
      "text": "not interpretable right ideally I want",
      "start": 2119.8,
      "duration": 4.36
    },
    {
      "text": "to be able to let's say if I look at",
      "start": 2122.2,
      "duration": 6.32
    },
    {
      "text": "this second uh this second row which are",
      "start": 2124.16,
      "duration": 6.04
    },
    {
      "text": "the attention scores for the query",
      "start": 2128.52,
      "duration": 3.88
    },
    {
      "text": "journey I want to be able to make the",
      "start": 2130.2,
      "duration": 4.919
    },
    {
      "text": "statements like okay pay 10% attention",
      "start": 2132.4,
      "duration": 5.719
    },
    {
      "text": "to your pay 20% attention to Journey pay",
      "start": 2135.119,
      "duration": 5.881
    },
    {
      "text": "30% attention to step pay 40% attention",
      "start": 2138.119,
      "duration": 5.361
    },
    {
      "text": "to width but I'm not able to make these",
      "start": 2141.0,
      "duration": 4.16
    },
    {
      "text": "interpretable statements because if you",
      "start": 2143.48,
      "duration": 3.24
    },
    {
      "text": "look at all these attention scores they",
      "start": 2145.16,
      "duration": 3.12
    },
    {
      "text": "do not sum up to",
      "start": 2146.72,
      "duration": 4.2
    },
    {
      "text": "one these look like random values that",
      "start": 2148.28,
      "duration": 4.72
    },
    {
      "text": "they are not summing up to one so we",
      "start": 2150.92,
      "duration": 3.679
    },
    {
      "text": "have to do the next step of",
      "start": 2153.0,
      "duration": 3.839
    },
    {
      "text": "normalization so normalization serves",
      "start": 2154.599,
      "duration": 4.52
    },
    {
      "text": "two purposes first it will help make",
      "start": 2156.839,
      "duration": 4.081
    },
    {
      "text": "things interpretable so I can make",
      "start": 2159.119,
      "duration": 3.521
    },
    {
      "text": "statements like okay when the query is",
      "start": 2160.92,
      "duration": 4.919
    },
    {
      "text": "Journey the you pay 20% attention to the",
      "start": 2162.64,
      "duration": 5.4
    },
    {
      "text": "first token 30% attention to the final",
      "start": 2165.839,
      "duration": 4.641
    },
    {
      "text": "token Etc and the second advantage of",
      "start": 2168.04,
      "duration": 4.44
    },
    {
      "text": "normalization is that it helps when we",
      "start": 2170.48,
      "duration": 4.4
    },
    {
      "text": "do back propagation generally in many",
      "start": 2172.48,
      "duration": 4.16
    },
    {
      "text": "machine learning Frameworks it's better",
      "start": 2174.88,
      "duration": 4.12
    },
    {
      "text": "to normalize things so that the scale",
      "start": 2176.64,
      "duration": 5.6
    },
    {
      "text": "stay consistent between zero and one so",
      "start": 2179.0,
      "duration": 5.04
    },
    {
      "text": "the third step what we are going to do",
      "start": 2182.24,
      "duration": 5.359
    },
    {
      "text": "is that we are going to compute uh the",
      "start": 2184.04,
      "duration": 5.16
    },
    {
      "text": "is terminology which is called as",
      "start": 2187.599,
      "duration": 3.801
    },
    {
      "text": "attention weights so up till now we have",
      "start": 2189.2,
      "duration": 5.159
    },
    {
      "text": "calculated attention scores that's fine",
      "start": 2191.4,
      "duration": 4.64
    },
    {
      "text": "now we are going to just normalize the",
      "start": 2194.359,
      "duration": 4.281
    },
    {
      "text": "attention weights uh so that the",
      "start": 2196.04,
      "duration": 5.559
    },
    {
      "text": "attention scores in each row sum up to",
      "start": 2198.64,
      "duration": 4.92
    },
    {
      "text": "one so there is a difference between",
      "start": 2201.599,
      "duration": 3.76
    },
    {
      "text": "attention scores and weights the meaning",
      "start": 2203.56,
      "duration": 3.64
    },
    {
      "text": "is the same but attention weights sum up",
      "start": 2205.359,
      "duration": 2.96
    },
    {
      "text": "to one they are",
      "start": 2207.2,
      "duration": 3.48
    },
    {
      "text": "normalized in the previous lecture what",
      "start": 2208.319,
      "duration": 4.161
    },
    {
      "text": "we did for normalization is that we",
      "start": 2210.68,
      "duration": 3.6
    },
    {
      "text": "looked at each row and we simply took",
      "start": 2212.48,
      "duration": 5.2
    },
    {
      "text": "the soft Max right uh and the soft Max",
      "start": 2214.28,
      "duration": 5.079
    },
    {
      "text": "function actually ensures that all the",
      "start": 2217.68,
      "duration": 4.0
    },
    {
      "text": "elements sum up to one I'm not going to",
      "start": 2219.359,
      "duration": 4.121
    },
    {
      "text": "cover the soft Max implementation in",
      "start": 2221.68,
      "duration": 3.399
    },
    {
      "text": "today's lecture because we have already",
      "start": 2223.48,
      "duration": 3.8
    },
    {
      "text": "seen it in the last lecture in a lot of",
      "start": 2225.079,
      "duration": 4.76
    },
    {
      "text": "detail but remember that softmax just",
      "start": 2227.28,
      "duration": 4.839
    },
    {
      "text": "make sure that all of the these",
      "start": 2229.839,
      "duration": 4.081
    },
    {
      "text": "quantities sum up to one and they lie",
      "start": 2232.119,
      "duration": 4.081
    },
    {
      "text": "between zero and one and they are",
      "start": 2233.92,
      "duration": 5.72
    },
    {
      "text": "positive but actually in today's lecture",
      "start": 2236.2,
      "duration": 5.639
    },
    {
      "text": "before we Implement soft Max there is",
      "start": 2239.64,
      "duration": 4.12
    },
    {
      "text": "one more very important step which is",
      "start": 2241.839,
      "duration": 4.201
    },
    {
      "text": "actually done and I'll come to why this",
      "start": 2243.76,
      "duration": 4.64
    },
    {
      "text": "step is done but remember that",
      "start": 2246.04,
      "duration": 4.279
    },
    {
      "text": "before implementing soft Max what is",
      "start": 2248.4,
      "duration": 4.04
    },
    {
      "text": "done is that all of these values are",
      "start": 2250.319,
      "duration": 4.0
    },
    {
      "text": "taken and they are scaled by something",
      "start": 2252.44,
      "duration": 4.32
    },
    {
      "text": "which is called as square root of the",
      "start": 2254.319,
      "duration": 5.081
    },
    {
      "text": "keys Dimension So currently the",
      "start": 2256.76,
      "duration": 4.96
    },
    {
      "text": "dimension of the keys is a it's two two",
      "start": 2259.4,
      "duration": 4.32
    },
    {
      "text": "Dimension right because remember the",
      "start": 2261.72,
      "duration": 6.399
    },
    {
      "text": "keys queries and the values Matrix we uh",
      "start": 2263.72,
      "duration": 6.44
    },
    {
      "text": "we took the three-dimensional input",
      "start": 2268.119,
      "duration": 3.921
    },
    {
      "text": "embedding and we transformed it into two",
      "start": 2270.16,
      "duration": 4.32
    },
    {
      "text": "dimensional so the dimension of the keys",
      "start": 2272.04,
      "duration": 5.279
    },
    {
      "text": "in this case is two uh so we are going",
      "start": 2274.48,
      "duration": 5.44
    },
    {
      "text": "to to scale everything by square root of",
      "start": 2277.319,
      "duration": 6.441
    },
    {
      "text": "two why two because if you look at the",
      "start": 2279.92,
      "duration": 6.96
    },
    {
      "text": "um let's look at the key Matrix again",
      "start": 2283.76,
      "duration": 5.44
    },
    {
      "text": "yeah this is my key Matrix right now and",
      "start": 2286.88,
      "duration": 5.12
    },
    {
      "text": "if you look at every uh if you look at",
      "start": 2289.2,
      "duration": 5.72
    },
    {
      "text": "every token here it has two Dimensions",
      "start": 2292.0,
      "duration": 5.56
    },
    {
      "text": "right so that's why we are going to uh",
      "start": 2294.92,
      "duration": 4.36
    },
    {
      "text": "we are going to scale by square root of",
      "start": 2297.56,
      "duration": 4.4
    },
    {
      "text": "two and you might be thinking okay this",
      "start": 2299.28,
      "duration": 4.48
    },
    {
      "text": "looks like magic who thought about the",
      "start": 2301.96,
      "duration": 4.0
    },
    {
      "text": "square root why do we do the scaling",
      "start": 2303.76,
      "duration": 3.92
    },
    {
      "text": "there is a very nice reason for that and",
      "start": 2305.96,
      "duration": 3.72
    },
    {
      "text": "I'm going to come to it for now just",
      "start": 2307.68,
      "duration": 3.96
    },
    {
      "text": "remember that after getting the",
      "start": 2309.68,
      "duration": 4.439
    },
    {
      "text": "attention score we are going to scale it",
      "start": 2311.64,
      "duration": 4.64
    },
    {
      "text": "by square root of D and that's why it's",
      "start": 2314.119,
      "duration": 3.921
    },
    {
      "text": "also called as scaled dot product",
      "start": 2316.28,
      "duration": 3.799
    },
    {
      "text": "attention remember we saw at the start",
      "start": 2318.04,
      "duration": 4.2
    },
    {
      "text": "the scaled we are going to scale by",
      "start": 2320.079,
      "duration": 4.601
    },
    {
      "text": "square root of D that that is one of the",
      "start": 2322.24,
      "duration": 4.119
    },
    {
      "text": "reasons why it's called scaled dot",
      "start": 2324.68,
      "duration": 2.72
    },
    {
      "text": "product",
      "start": 2326.359,
      "duration": 3.361
    },
    {
      "text": "attention so we are going to scale it",
      "start": 2327.4,
      "duration": 5.04
    },
    {
      "text": "and then we are going to apply soft Max",
      "start": 2329.72,
      "duration": 5.44
    },
    {
      "text": "so when we scale by square root of two",
      "start": 2332.44,
      "duration": 4.639
    },
    {
      "text": "it leads to this Matrix over here and",
      "start": 2335.16,
      "duration": 4.24
    },
    {
      "text": "then we have apply soft Max so you'll",
      "start": 2337.079,
      "duration": 4.24
    },
    {
      "text": "see that when soft Max activation is",
      "start": 2339.4,
      "duration": 4.199
    },
    {
      "text": "applied and if you look at each row",
      "start": 2341.319,
      "duration": 4.081
    },
    {
      "text": "right now you'll see that they sum up to",
      "start": 2343.599,
      "duration": 4.321
    },
    {
      "text": "one so if I look at the second row right",
      "start": 2345.4,
      "duration": 7.679
    },
    {
      "text": "now uh it corresponds to again um",
      "start": 2347.92,
      "duration": 7.96
    },
    {
      "text": "Journey so I can now confidently make",
      "start": 2353.079,
      "duration": 4.481
    },
    {
      "text": "statements like when the query is",
      "start": 2355.88,
      "duration": 5.08
    },
    {
      "text": "Journey pay 15% attention to your pay",
      "start": 2357.56,
      "duration": 7.0
    },
    {
      "text": "22% attention to Journey itself pay 22%",
      "start": 2360.96,
      "duration": 7.04
    },
    {
      "text": "attention to with to begins pay 30 15%",
      "start": 2364.56,
      "duration": 6.519
    },
    {
      "text": "attention to width pay only 9% attention",
      "start": 2368.0,
      "duration": 6.04
    },
    {
      "text": "to one and pay around 18% attention to",
      "start": 2371.079,
      "duration": 5.24
    },
    {
      "text": "step remember these weights are not",
      "start": 2374.04,
      "duration": 4.16
    },
    {
      "text": "optimized but when they are optimized we",
      "start": 2376.319,
      "duration": 3.8
    },
    {
      "text": "can make interpretable statements like",
      "start": 2378.2,
      "duration": 4.36
    },
    {
      "text": "these all of the rows will sum up to one",
      "start": 2380.119,
      "duration": 4.321
    },
    {
      "text": "you can check these and this is called",
      "start": 2382.56,
      "duration": 4.0
    },
    {
      "text": "as the attention weight Matrix this is",
      "start": 2384.44,
      "duration": 5.08
    },
    {
      "text": "one of the most important step uh in",
      "start": 2386.56,
      "duration": 5.08
    },
    {
      "text": "getting to the context Vector please",
      "start": 2389.52,
      "duration": 4.44
    },
    {
      "text": "remember this step and uh we did two",
      "start": 2391.64,
      "duration": 4.04
    },
    {
      "text": "things here we scaled by the square root",
      "start": 2393.96,
      "duration": 3.639
    },
    {
      "text": "of the dimension and then we implement",
      "start": 2395.68,
      "duration": 4.639
    },
    {
      "text": "soft Max now let's go go to code and",
      "start": 2397.599,
      "duration": 4.681
    },
    {
      "text": "let's implement this in the process",
      "start": 2400.319,
      "duration": 4.201
    },
    {
      "text": "we'll also understand why we do the",
      "start": 2402.28,
      "duration": 4.76
    },
    {
      "text": "scaling by square root of",
      "start": 2404.52,
      "duration": 5.52
    },
    {
      "text": "D",
      "start": 2407.04,
      "duration": 3.0
    },
    {
      "text": "okay yeah so we compute the attention",
      "start": 2410.079,
      "duration": 4.921
    },
    {
      "text": "weights by scaling the attention scores",
      "start": 2412.96,
      "duration": 4.32
    },
    {
      "text": "and using the soft Max function the",
      "start": 2415.0,
      "duration": 4.16
    },
    {
      "text": "difference to earlier when I say earlier",
      "start": 2417.28,
      "duration": 4.28
    },
    {
      "text": "it's the previous lectures is now we",
      "start": 2419.16,
      "duration": 4.48
    },
    {
      "text": "scale the attention scores by dividing",
      "start": 2421.56,
      "duration": 4.0
    },
    {
      "text": "them by the square root of the embedding",
      "start": 2423.64,
      "duration": 4.32
    },
    {
      "text": "dimension of the keys and that embedding",
      "start": 2425.56,
      "duration": 4.559
    },
    {
      "text": "Dimensions is two so we are dividing by",
      "start": 2427.96,
      "duration": 3.44
    },
    {
      "text": "square root of",
      "start": 2430.119,
      "duration": 4.281
    },
    {
      "text": "two so let's do the same thing here so",
      "start": 2431.4,
      "duration": 5.919
    },
    {
      "text": "we have this attention scores 2 so it's",
      "start": 2434.4,
      "duration": 6.32
    },
    {
      "text": "a 6x6 Matrix which we have printed out",
      "start": 2437.319,
      "duration": 5.481
    },
    {
      "text": "here actually currently I'm just",
      "start": 2440.72,
      "duration": 4.0
    },
    {
      "text": "employing this on the attention scores",
      "start": 2442.8,
      "duration": 4.319
    },
    {
      "text": "for the second query okay no problem so",
      "start": 2444.72,
      "duration": 4.639
    },
    {
      "text": "attention scores 2 is the attention",
      "start": 2447.119,
      "duration": 4.561
    },
    {
      "text": "scores for the query 2 which is Journey",
      "start": 2449.359,
      "duration": 4.96
    },
    {
      "text": "so it's actually one row and six columns",
      "start": 2451.68,
      "duration": 4.2
    },
    {
      "text": "so what I'm going to do is that I'm",
      "start": 2454.319,
      "duration": 3.8
    },
    {
      "text": "going to take this attention scores for",
      "start": 2455.88,
      "duration": 4.6
    },
    {
      "text": "Journey I'm going to first divide it by",
      "start": 2458.119,
      "duration": 6.841
    },
    {
      "text": "the square root of the keys Dimension",
      "start": 2460.48,
      "duration": 6.44
    },
    {
      "text": "and then uh what I'm going to do is that",
      "start": 2464.96,
      "duration": 4.159
    },
    {
      "text": "I'm going to apply soft Max the reason",
      "start": 2466.92,
      "duration": 4.56
    },
    {
      "text": "we do dim equal to minus one is that",
      "start": 2469.119,
      "duration": 3.841
    },
    {
      "text": "because we have to sum over all the",
      "start": 2471.48,
      "duration": 4.28
    },
    {
      "text": "columns and all the if so that's why",
      "start": 2472.96,
      "duration": 4.28
    },
    {
      "text": "when you look at one row you'll see that",
      "start": 2475.76,
      "duration": 3.839
    },
    {
      "text": "it sums up to one so two things are",
      "start": 2477.24,
      "duration": 4.879
    },
    {
      "text": "important this D of K is the dimension",
      "start": 2479.599,
      "duration": 4.841
    },
    {
      "text": "it's the keys do shape minus one because",
      "start": 2482.119,
      "duration": 4.401
    },
    {
      "text": "we are looking at the column remember",
      "start": 2484.44,
      "duration": 5.639
    },
    {
      "text": "keys do shape is 3x 2 so when you do",
      "start": 2486.52,
      "duration": 5.72
    },
    {
      "text": "keys do shape and index it by minus one",
      "start": 2490.079,
      "duration": 4.24
    },
    {
      "text": "the result will be two so we are going",
      "start": 2492.24,
      "duration": 5.48
    },
    {
      "text": "to uh divide by square root of two uh",
      "start": 2494.319,
      "duration": 6.28
    },
    {
      "text": "and in Python remember here we are",
      "start": 2497.72,
      "duration": 6.8
    },
    {
      "text": "exponen by 05 so into into .5 means rais",
      "start": 2500.599,
      "duration": 7.081
    },
    {
      "text": "to.5 that is the same as dividing by the",
      "start": 2504.52,
      "duration": 4.4
    },
    {
      "text": "square root of",
      "start": 2507.68,
      "duration": 3.84
    },
    {
      "text": "two so every element will be first",
      "start": 2508.92,
      "duration": 4.56
    },
    {
      "text": "divided by the square root of two in the",
      "start": 2511.52,
      "duration": 4.559
    },
    {
      "text": "attention score and then we implement",
      "start": 2513.48,
      "duration": 5.359
    },
    {
      "text": "the soft Max so if you look at the",
      "start": 2516.079,
      "duration": 5.04
    },
    {
      "text": "attention weights for Journey you'll see",
      "start": 2518.839,
      "duration": 4.041
    },
    {
      "text": "that these are the attention weights",
      "start": 2521.119,
      "duration": 3.281
    },
    {
      "text": "let's actually check whether these are",
      "start": 2522.88,
      "duration": 3.92
    },
    {
      "text": "correct to what we saw yeah so let's",
      "start": 2524.4,
      "duration": 4.24
    },
    {
      "text": "look at the second row here the second",
      "start": 2526.8,
      "duration": 6.76
    },
    {
      "text": "row is 0.15 2264 etc for Journey and",
      "start": 2528.64,
      "duration": 6.8
    },
    {
      "text": "here you will see that the second the",
      "start": 2533.56,
      "duration": 4.759
    },
    {
      "text": "output is exactly the same that's a good",
      "start": 2535.44,
      "duration": 5.08
    },
    {
      "text": "sanity check and you'll see that all of",
      "start": 2538.319,
      "duration": 4.561
    },
    {
      "text": "this sum up to one so this is how we",
      "start": 2540.52,
      "duration": 5.599
    },
    {
      "text": "calculate the attention weights for one",
      "start": 2542.88,
      "duration": 6.08
    },
    {
      "text": "uh query and similarly we calculate the",
      "start": 2546.119,
      "duration": 4.801
    },
    {
      "text": "attention weights for all the queries so",
      "start": 2548.96,
      "duration": 3.76
    },
    {
      "text": "if I just replace this with attention",
      "start": 2550.92,
      "duration": 4.36
    },
    {
      "text": "scores which is the 6x6 we'll get the",
      "start": 2552.72,
      "duration": 5.879
    },
    {
      "text": "attention weight Matrix which is a 6x6",
      "start": 2555.28,
      "duration": 5.44
    },
    {
      "text": "Matrix okay now let's come to the",
      "start": 2558.599,
      "duration": 3.52
    },
    {
      "text": "question which all of you might be",
      "start": 2560.72,
      "duration": 3.2
    },
    {
      "text": "thinking and I don't think this is",
      "start": 2562.119,
      "duration": 3.641
    },
    {
      "text": "covered enough in other lectures and",
      "start": 2563.92,
      "duration": 4.159
    },
    {
      "text": "other videos but it's a very fascinating",
      "start": 2565.76,
      "duration": 4.319
    },
    {
      "text": "thing I took some time to understand",
      "start": 2568.079,
      "duration": 4.441
    },
    {
      "text": "this and I've come up with two reasons",
      "start": 2570.079,
      "duration": 4.081
    },
    {
      "text": "why we actually divide by the square",
      "start": 2572.52,
      "duration": 4.2
    },
    {
      "text": "root of Dimension the first reason is",
      "start": 2574.16,
      "duration": 5.48
    },
    {
      "text": "stabil in learning and let me illustrate",
      "start": 2576.72,
      "duration": 5.28
    },
    {
      "text": "this with an example so let's say if you",
      "start": 2579.64,
      "duration": 5.8
    },
    {
      "text": "have this tensor of values which is 0.1",
      "start": 2582.0,
      "duration": 9.52
    },
    {
      "text": "min-2 3 -2 and .5 okay if you take the",
      "start": 2585.44,
      "duration": 7.879
    },
    {
      "text": "soft Max of",
      "start": 2591.52,
      "duration": 4.319
    },
    {
      "text": "this uh versus now let's say if you",
      "start": 2593.319,
      "duration": 4.52
    },
    {
      "text": "multiply this with eight and then you",
      "start": 2595.839,
      "duration": 4.321
    },
    {
      "text": "take a soft Max so you'll see that the",
      "start": 2597.839,
      "duration": 5.321
    },
    {
      "text": "soft Max of the first is kind of it's",
      "start": 2600.16,
      "duration": 4.64
    },
    {
      "text": "good right it's diffused these values",
      "start": 2603.16,
      "duration": 3.959
    },
    {
      "text": "are diffused between 0o and one but if",
      "start": 2604.8,
      "duration": 3.759
    },
    {
      "text": "if you look at the soft Max of the",
      "start": 2607.119,
      "duration": 4.921
    },
    {
      "text": "second tensor you'll see that the values",
      "start": 2608.559,
      "duration": 6.321
    },
    {
      "text": "are disproportionately high which means",
      "start": 2612.04,
      "duration": 5.2
    },
    {
      "text": "that if there are some values in the",
      "start": 2614.88,
      "duration": 4.479
    },
    {
      "text": "original tensor if some values are very",
      "start": 2617.24,
      "duration": 4.319
    },
    {
      "text": "high and when you take the soft Max",
      "start": 2619.359,
      "duration": 4.361
    },
    {
      "text": "you'll get such kind of peaks in the",
      "start": 2621.559,
      "duration": 4.961
    },
    {
      "text": "softmax output I've actually explained",
      "start": 2623.72,
      "duration": 5.08
    },
    {
      "text": "this better here so the softmax function",
      "start": 2626.52,
      "duration": 4.039
    },
    {
      "text": "is sensitive to the magnitude of its",
      "start": 2628.8,
      "duration": 4.64
    },
    {
      "text": "inputs when the inputs are large the",
      "start": 2630.559,
      "duration": 4.841
    },
    {
      "text": "difference between the exponential value",
      "start": 2633.44,
      "duration": 4.08
    },
    {
      "text": "of each input becomes much more",
      "start": 2635.4,
      "duration": 4.919
    },
    {
      "text": "pronounced this causes the softmax out",
      "start": 2637.52,
      "duration": 5.319
    },
    {
      "text": "output to become pey where the highest",
      "start": 2640.319,
      "duration": 4.04
    },
    {
      "text": "value receives almost all the",
      "start": 2642.839,
      "duration": 3.561
    },
    {
      "text": "probability mass and we can check it",
      "start": 2644.359,
      "duration": 5.121
    },
    {
      "text": "here so when we multiply with uh8 you'll",
      "start": 2646.4,
      "duration": 5.0
    },
    {
      "text": "see that this has the highest value",
      "start": 2649.48,
      "duration": 4.639
    },
    {
      "text": "which is four and when we take the soft",
      "start": 2651.4,
      "duration": 6.64
    },
    {
      "text": "Max you'll see that the value is 08 here",
      "start": 2654.119,
      "duration": 5.44
    },
    {
      "text": "which is much higher than all the other",
      "start": 2658.04,
      "duration": 3.48
    },
    {
      "text": "values in fact it's around 10 to 15",
      "start": 2659.559,
      "duration": 4.081
    },
    {
      "text": "times higher that's what is meant by",
      "start": 2661.52,
      "duration": 5.079
    },
    {
      "text": "softmax becomes pey if the values inside",
      "start": 2663.64,
      "duration": 5.479
    },
    {
      "text": "the soft Max are very large so we don't",
      "start": 2666.599,
      "duration": 5.201
    },
    {
      "text": "want the values inside the soft Max to",
      "start": 2669.119,
      "duration": 4.881
    },
    {
      "text": "be very large and that's one reason why",
      "start": 2671.8,
      "duration": 5.559
    },
    {
      "text": "we scale or divide by the square root to",
      "start": 2674.0,
      "duration": 6.0
    },
    {
      "text": "reduce the values itself before taking",
      "start": 2677.359,
      "duration": 4.401
    },
    {
      "text": "soft Max we make sure that the values",
      "start": 2680.0,
      "duration": 3.72
    },
    {
      "text": "are not large and that's why we divide",
      "start": 2681.76,
      "duration": 3.48
    },
    {
      "text": "by the square root",
      "start": 2683.72,
      "duration": 4.0
    },
    {
      "text": "Factor so in attention mechanisms",
      "start": 2685.24,
      "duration": 4.76
    },
    {
      "text": "particularly in Transformers if the dot",
      "start": 2687.72,
      "duration": 3.879
    },
    {
      "text": "product between the query and the key",
      "start": 2690.0,
      "duration": 4.0
    },
    {
      "text": "Vector remember that we are ultimately",
      "start": 2691.599,
      "duration": 4.281
    },
    {
      "text": "applying soft Max on the dot product",
      "start": 2694.0,
      "duration": 3.599
    },
    {
      "text": "between query and key right because",
      "start": 2695.88,
      "duration": 3.6
    },
    {
      "text": "attention scores are just dot product",
      "start": 2697.599,
      "duration": 4.0
    },
    {
      "text": "between query and key and if the dot",
      "start": 2699.48,
      "duration": 3.639
    },
    {
      "text": "product becomes too",
      "start": 2701.599,
      "duration": 4.081
    },
    {
      "text": "large like multiplying by eight in the",
      "start": 2703.119,
      "duration": 4.44
    },
    {
      "text": "current example which we saw the",
      "start": 2705.68,
      "duration": 4.32
    },
    {
      "text": "attention scores can become very large",
      "start": 2707.559,
      "duration": 4.28
    },
    {
      "text": "and we don't want that this results in a",
      "start": 2710.0,
      "duration": 5.28
    },
    {
      "text": "very sharp softmax distribution and uh",
      "start": 2711.839,
      "duration": 6.321
    },
    {
      "text": "such sharp softmax distribution can",
      "start": 2715.28,
      "duration": 4.76
    },
    {
      "text": "become so the model can become overly",
      "start": 2718.16,
      "duration": 4.679
    },
    {
      "text": "confident in one particular key so in",
      "start": 2720.04,
      "duration": 4.24
    },
    {
      "text": "this case the model has become very",
      "start": 2722.839,
      "duration": 4.201
    },
    {
      "text": "confident in this fourth key or rather",
      "start": 2724.28,
      "duration": 6.36
    },
    {
      "text": "this uh fifth key we don't want that",
      "start": 2727.04,
      "duration": 6.4
    },
    {
      "text": "because that can make learning very",
      "start": 2730.64,
      "duration": 5.28
    },
    {
      "text": "unstable that's the first reason why we",
      "start": 2733.44,
      "duration": 4.6
    },
    {
      "text": "divide by square root to make sure that",
      "start": 2735.92,
      "duration": 3.84
    },
    {
      "text": "the values are not very large and to",
      "start": 2738.04,
      "duration": 4.68
    },
    {
      "text": "have stability in learning but still so",
      "start": 2739.76,
      "duration": 4.799
    },
    {
      "text": "I I knew this reason but then I was",
      "start": 2742.72,
      "duration": 4.56
    },
    {
      "text": "thinking but why square root why are we",
      "start": 2744.559,
      "duration": 5.121
    },
    {
      "text": "dividing by square root why why not just",
      "start": 2747.28,
      "duration": 4.839
    },
    {
      "text": "only the dimension what is the reason",
      "start": 2749.68,
      "duration": 4.48
    },
    {
      "text": "behind dividing by square root and then",
      "start": 2752.119,
      "duration": 3.921
    },
    {
      "text": "I came across a wonderful justification",
      "start": 2754.16,
      "duration": 4.56
    },
    {
      "text": "for this so so the reason for square",
      "start": 2756.04,
      "duration": 4.279
    },
    {
      "text": "root is that it's actually related to",
      "start": 2758.72,
      "duration": 2.52
    },
    {
      "text": "the",
      "start": 2760.319,
      "duration": 4.121
    },
    {
      "text": "variance uh so it turns out that the dot",
      "start": 2761.24,
      "duration": 5.4
    },
    {
      "text": "product of Q and K increases the",
      "start": 2764.44,
      "duration": 4.159
    },
    {
      "text": "variance because multiplying two random",
      "start": 2766.64,
      "duration": 4.0
    },
    {
      "text": "numbers increase the variance so",
      "start": 2768.599,
      "duration": 3.52
    },
    {
      "text": "remember that when we get to the",
      "start": 2770.64,
      "duration": 3.76
    },
    {
      "text": "attention scores we are multiplying q",
      "start": 2772.119,
      "duration": 5.681
    },
    {
      "text": "and K right the query and the key it",
      "start": 2774.4,
      "duration": 5.719
    },
    {
      "text": "turns out that if you don't divide by",
      "start": 2777.8,
      "duration": 5.08
    },
    {
      "text": "anything the higher the dimensions of",
      "start": 2780.119,
      "duration": 5.161
    },
    {
      "text": "these vectors whose dot product you are",
      "start": 2782.88,
      "duration": 4.36
    },
    {
      "text": "taking the variance goes on increasing",
      "start": 2785.28,
      "duration": 4.4
    },
    {
      "text": "that much and dividing by the square",
      "start": 2787.24,
      "duration": 4.44
    },
    {
      "text": "root of Dimension keeps the variance",
      "start": 2789.68,
      "duration": 4.56
    },
    {
      "text": "close to one let me explain this also",
      "start": 2791.68,
      "duration": 5.96
    },
    {
      "text": "with an example so let's say we have a",
      "start": 2794.24,
      "duration": 6.0
    },
    {
      "text": "query Vector which is generated randomly",
      "start": 2797.64,
      "duration": 5.919
    },
    {
      "text": "and a key Vector which is generated",
      "start": 2800.24,
      "duration": 6.76
    },
    {
      "text": "randomly uh okay and currently let's say",
      "start": 2803.559,
      "duration": 5.121
    },
    {
      "text": "I'm doing a five dimensional Vector so",
      "start": 2807.0,
      "duration": 3.28
    },
    {
      "text": "let's say I have a key Vector five",
      "start": 2808.68,
      "duration": 4.12
    },
    {
      "text": "dimensional key Vector which is sampled",
      "start": 2810.28,
      "duration": 4.44
    },
    {
      "text": "from a normal distribution and a five",
      "start": 2812.8,
      "duration": 3.72
    },
    {
      "text": "dimensional query Vector sampled from a",
      "start": 2814.72,
      "duration": 2.72
    },
    {
      "text": "normal",
      "start": 2816.52,
      "duration": 3.2
    },
    {
      "text": "distribution and then I'm taking a DOT",
      "start": 2817.44,
      "duration": 4.0
    },
    {
      "text": "product between the query and the key",
      "start": 2819.72,
      "duration": 3.639
    },
    {
      "text": "and then I'm also in the second case",
      "start": 2821.44,
      "duration": 4.24
    },
    {
      "text": "taking dividing by the square root of",
      "start": 2823.359,
      "duration": 5.041
    },
    {
      "text": "the dimension okay and I'm doing this",
      "start": 2825.68,
      "duration": 4.679
    },
    {
      "text": "thousand times so that I can get a",
      "start": 2828.4,
      "duration": 4.48
    },
    {
      "text": "distribution over the dot product so",
      "start": 2830.359,
      "duration": 4.24
    },
    {
      "text": "after I do this a thousand times what I",
      "start": 2832.88,
      "duration": 4.28
    },
    {
      "text": "do is I plot the variance before scaling",
      "start": 2834.599,
      "duration": 4.201
    },
    {
      "text": "and I plot the variance of the dot",
      "start": 2837.16,
      "duration": 4.72
    },
    {
      "text": "product after scaling so the results are",
      "start": 2838.8,
      "duration": 5.2
    },
    {
      "text": "surprising if the dimension is equal to",
      "start": 2841.88,
      "duration": 3.92
    },
    {
      "text": "five the variance of the dot product",
      "start": 2844.0,
      "duration": 3.72
    },
    {
      "text": "before scaling is actually very close to",
      "start": 2845.8,
      "duration": 4.44
    },
    {
      "text": "five if the dimension before scaling is",
      "start": 2847.72,
      "duration": 5.119
    },
    {
      "text": "20 the variance before scaling is very",
      "start": 2850.24,
      "duration": 5.599
    },
    {
      "text": "close to 20 this indicate that if the",
      "start": 2852.839,
      "duration": 5.201
    },
    {
      "text": "dimensions of the query and key vectors",
      "start": 2855.839,
      "duration": 4.881
    },
    {
      "text": "go on increasing and if you don't scale",
      "start": 2858.04,
      "duration": 5.559
    },
    {
      "text": "then the variance of the resulting dot",
      "start": 2860.72,
      "duration": 5.24
    },
    {
      "text": "product scales proportionately so if you",
      "start": 2863.599,
      "duration": 4.361
    },
    {
      "text": "have 100 dimensional key and query",
      "start": 2865.96,
      "duration": 4.119
    },
    {
      "text": "Vector the variance before scaling will",
      "start": 2867.96,
      "duration": 4.639
    },
    {
      "text": "be close to 100 and we can actually test",
      "start": 2870.079,
      "duration": 6.841
    },
    {
      "text": "this out so here let me do this 100 and",
      "start": 2872.599,
      "duration": 6.561
    },
    {
      "text": "compute variance",
      "start": 2876.92,
      "duration": 5.32
    },
    {
      "text": "100 so now I'm printing this for 100 and",
      "start": 2879.16,
      "duration": 5.88
    },
    {
      "text": "let me print this",
      "start": 2882.24,
      "duration": 2.8
    },
    {
      "text": "out okay I think I should replace this",
      "start": 2896.76,
      "duration": 7.28
    },
    {
      "text": "also with 100 uh and let me print this",
      "start": 2899.2,
      "duration": 7.2
    },
    {
      "text": "out okay so this is exactly what we are",
      "start": 2904.04,
      "duration": 4.12
    },
    {
      "text": "predicted right so the variance before",
      "start": 2906.4,
      "duration": 4.439
    },
    {
      "text": "scaling in this case is",
      "start": 2908.16,
      "duration": 6.919
    },
    {
      "text": "107 uh see so as the dimensions increase",
      "start": 2910.839,
      "duration": 6.361
    },
    {
      "text": "the variance increases now look at the",
      "start": 2915.079,
      "duration": 4.601
    },
    {
      "text": "power of scaling when you scale by the",
      "start": 2917.2,
      "duration": 5.2
    },
    {
      "text": "square root so see here we are scaling",
      "start": 2919.68,
      "duration": 5.0
    },
    {
      "text": "by the square root when you scale by the",
      "start": 2922.4,
      "duration": 5.0
    },
    {
      "text": "square root of Dimensions no matter how",
      "start": 2924.68,
      "duration": 4.2
    },
    {
      "text": "much you increase the dimension if you",
      "start": 2927.4,
      "duration": 3.04
    },
    {
      "text": "see the variance after scaling the",
      "start": 2928.88,
      "duration": 4.479
    },
    {
      "text": "variance is always close to one and",
      "start": 2930.44,
      "duration": 4.52
    },
    {
      "text": "that's the reason why square root is",
      "start": 2933.359,
      "duration": 3.561
    },
    {
      "text": "used if you don't use a square root the",
      "start": 2934.96,
      "duration": 4.08
    },
    {
      "text": "variance will not be close to one so let",
      "start": 2936.92,
      "duration": 5.24
    },
    {
      "text": "me actually not use the square root here",
      "start": 2939.04,
      "duration": 4.64
    },
    {
      "text": "and let me do it",
      "start": 2942.16,
      "duration": 4.199
    },
    {
      "text": "directly uh if you do it directly then",
      "start": 2943.68,
      "duration": 4.119
    },
    {
      "text": "you will see that the variance after",
      "start": 2946.359,
      "duration": 3.44
    },
    {
      "text": "scaling are some random values they are",
      "start": 2947.799,
      "duration": 4.441
    },
    {
      "text": "not close to one having the square root",
      "start": 2949.799,
      "duration": 3.32
    },
    {
      "text": "actually",
      "start": 2952.24,
      "duration": 3.48
    },
    {
      "text": "really uh having the square root make",
      "start": 2953.119,
      "duration": 5.2
    },
    {
      "text": "sure that even if the dimensions",
      "start": 2955.72,
      "duration": 4.76
    },
    {
      "text": "increase the variance after scaling",
      "start": 2958.319,
      "duration": 4.081
    },
    {
      "text": "remains close to one of the dot product",
      "start": 2960.48,
      "duration": 3.92
    },
    {
      "text": "between the query and the key and this",
      "start": 2962.4,
      "duration": 4.6
    },
    {
      "text": "is very important",
      "start": 2964.4,
      "duration": 4.36
    },
    {
      "text": "uh the reason why the variance should be",
      "start": 2967.0,
      "duration": 3.44
    },
    {
      "text": "close to one is that if the variance",
      "start": 2968.76,
      "duration": 3.4
    },
    {
      "text": "increases a lot it again makes the",
      "start": 2970.44,
      "duration": 4.28
    },
    {
      "text": "learning very unstable and we don't want",
      "start": 2972.16,
      "duration": 4.959
    },
    {
      "text": "that we want to keep the standard",
      "start": 2974.72,
      "duration": 4.879
    },
    {
      "text": "deviation of the variance closed so that",
      "start": 2977.119,
      "duration": 4.72
    },
    {
      "text": "the learning does not fly off in random",
      "start": 2979.599,
      "duration": 4.72
    },
    {
      "text": "directions and the values the variance",
      "start": 2981.839,
      "duration": 4.201
    },
    {
      "text": "generally should stay to one that that",
      "start": 2984.319,
      "duration": 5.24
    },
    {
      "text": "helps in the back propagation and that's",
      "start": 2986.04,
      "duration": 6.88
    },
    {
      "text": "also generally better for uh avoiding",
      "start": 2989.559,
      "duration": 5.841
    },
    {
      "text": "any computational issues so that's the",
      "start": 2992.92,
      "duration": 4.919
    },
    {
      "text": "reason why uh we want the variance to be",
      "start": 2995.4,
      "duration": 4.6
    },
    {
      "text": "close to one so this is the second",
      "start": 2997.839,
      "duration": 4.641
    },
    {
      "text": "reason why we especially use square",
      "start": 3000.0,
      "duration": 5.799
    },
    {
      "text": "root so uh there are two reasons the",
      "start": 3002.48,
      "duration": 4.839
    },
    {
      "text": "first reason is of course we want the",
      "start": 3005.799,
      "duration": 3.961
    },
    {
      "text": "values to be as small as possible this",
      "start": 3007.319,
      "duration": 5.441
    },
    {
      "text": "helps uh if the values are not small the",
      "start": 3009.76,
      "duration": 5.4
    },
    {
      "text": "soft Max becomes pey and then it starts",
      "start": 3012.76,
      "duration": 4.359
    },
    {
      "text": "giving preferential values to Keys which",
      "start": 3015.16,
      "duration": 3.56
    },
    {
      "text": "we don't want it can make the learning",
      "start": 3017.119,
      "duration": 4.2
    },
    {
      "text": "unstable but why square root the reason",
      "start": 3018.72,
      "duration": 4.359
    },
    {
      "text": "why square root is because specifically",
      "start": 3021.319,
      "duration": 3.24
    },
    {
      "text": "when you take the dot product between",
      "start": 3023.079,
      "duration": 4.081
    },
    {
      "text": "query and key to find the attention",
      "start": 3024.559,
      "duration": 5.121
    },
    {
      "text": "and if you don't scale as the dimensions",
      "start": 3027.16,
      "duration": 4.36
    },
    {
      "text": "of the query and key increase the dot",
      "start": 3029.68,
      "duration": 4.159
    },
    {
      "text": "product variance can become huge we",
      "start": 3031.52,
      "duration": 3.96
    },
    {
      "text": "don't want that because again that will",
      "start": 3033.839,
      "duration": 4.681
    },
    {
      "text": "make learning unstable so scaling by the",
      "start": 3035.48,
      "duration": 5.52
    },
    {
      "text": "square root makes the variance close to",
      "start": 3038.52,
      "duration": 4.88
    },
    {
      "text": "one so if you see after scaling it keeps",
      "start": 3041.0,
      "duration": 4.359
    },
    {
      "text": "the variance close to one and that's why",
      "start": 3043.4,
      "duration": 4.199
    },
    {
      "text": "we divide by the square root it's very",
      "start": 3045.359,
      "duration": 3.801
    },
    {
      "text": "important for you to have this",
      "start": 3047.599,
      "duration": 3.281
    },
    {
      "text": "understanding and not many people have",
      "start": 3049.16,
      "duration": 3.56
    },
    {
      "text": "this understanding but I hope I've",
      "start": 3050.88,
      "duration": 4.88
    },
    {
      "text": "clarified this um concept to you and you",
      "start": 3052.72,
      "duration": 5.0
    },
    {
      "text": "have appreciated why we are dividing by",
      "start": 3055.76,
      "duration": 4.76
    },
    {
      "text": "square root that's why this is also",
      "start": 3057.72,
      "duration": 4.879
    },
    {
      "text": "called as scaled dot product attention",
      "start": 3060.52,
      "duration": 4.799
    },
    {
      "text": "because we scale by the square",
      "start": 3062.599,
      "duration": 5.641
    },
    {
      "text": "root okay so now until now we have",
      "start": 3065.319,
      "duration": 4.121
    },
    {
      "text": "reached a stage where we have",
      "start": 3068.24,
      "duration": 2.8
    },
    {
      "text": "essentially computed the attention",
      "start": 3069.44,
      "duration": 4.359
    },
    {
      "text": "weights and now we essentially come to",
      "start": 3071.04,
      "duration": 5.519
    },
    {
      "text": "the last step which is now we are ready",
      "start": 3073.799,
      "duration": 5.081
    },
    {
      "text": "to compute the context",
      "start": 3076.559,
      "duration": 5.56
    },
    {
      "text": "Vector so let's go ahead and actually",
      "start": 3078.88,
      "duration": 5.32
    },
    {
      "text": "compute the context Vector but first",
      "start": 3082.119,
      "duration": 5.0
    },
    {
      "text": "what I want to do is I want to show you",
      "start": 3084.2,
      "duration": 4.72
    },
    {
      "text": "uh pictorially what all we have done",
      "start": 3087.119,
      "duration": 4.561
    },
    {
      "text": "until now so let's see what all we have",
      "start": 3088.92,
      "duration": 4.639
    },
    {
      "text": "done until now is that let's say if you",
      "start": 3091.68,
      "duration": 4.119
    },
    {
      "text": "focus on a particular query we have",
      "start": 3093.559,
      "duration": 4.76
    },
    {
      "text": "found the attention score between the",
      "start": 3095.799,
      "duration": 5.921
    },
    {
      "text": "query and all the input keys by taking a",
      "start": 3098.319,
      "duration": 5.081
    },
    {
      "text": "DOT product between the query and the",
      "start": 3101.72,
      "duration": 3.76
    },
    {
      "text": "keys so the attention scores are shown",
      "start": 3103.4,
      "duration": 4.32
    },
    {
      "text": "in the blue over here and then what we",
      "start": 3105.48,
      "duration": 4.56
    },
    {
      "text": "do is we divide by the square root of",
      "start": 3107.72,
      "duration": 4.8
    },
    {
      "text": "the key Dimension and then we normalize",
      "start": 3110.04,
      "duration": 4.84
    },
    {
      "text": "using soft Max and then we have found",
      "start": 3112.52,
      "duration": 4.72
    },
    {
      "text": "the attention weights",
      "start": 3114.88,
      "duration": 4.239
    },
    {
      "text": "uh and the attention weights sum up to",
      "start": 3117.24,
      "duration": 4.119
    },
    {
      "text": "one awesome so we have reached this",
      "start": 3119.119,
      "duration": 4.601
    },
    {
      "text": "stage and the final step essentially is",
      "start": 3121.359,
      "duration": 4.681
    },
    {
      "text": "to compute the context vectors so let's",
      "start": 3123.72,
      "duration": 5.56
    },
    {
      "text": "come to that right now so until now you",
      "start": 3126.04,
      "duration": 6.559
    },
    {
      "text": "might be thinking that I've used the key",
      "start": 3129.28,
      "duration": 5.319
    },
    {
      "text": "and the query but what about the value",
      "start": 3132.599,
      "duration": 4.641
    },
    {
      "text": "why did we even get the value Matrix the",
      "start": 3134.599,
      "duration": 5.2
    },
    {
      "text": "value Matrix will be useful in the final",
      "start": 3137.24,
      "duration": 4.72
    },
    {
      "text": "step so remember for every input",
      "start": 3139.799,
      "duration": 4.32
    },
    {
      "text": "embedding Vector we have also calculated",
      "start": 3141.96,
      "duration": 4.56
    },
    {
      "text": "the value Vector so the way the context",
      "start": 3144.119,
      "duration": 4.041
    },
    {
      "text": "text Vector is now found out is that we",
      "start": 3146.52,
      "duration": 3.76
    },
    {
      "text": "have the attention weights right so we",
      "start": 3148.16,
      "duration": 4.28
    },
    {
      "text": "just so for the first input embedding we",
      "start": 3150.28,
      "duration": 4.039
    },
    {
      "text": "multiply the value Vector with the first",
      "start": 3152.44,
      "duration": 4.159
    },
    {
      "text": "attention weight we multiply the second",
      "start": 3154.319,
      "duration": 3.961
    },
    {
      "text": "value Vector with the second attention",
      "start": 3156.599,
      "duration": 4.121
    },
    {
      "text": "weight similarly we multiply the last",
      "start": 3158.28,
      "duration": 4.039
    },
    {
      "text": "value Vector with the last attention",
      "start": 3160.72,
      "duration": 3.599
    },
    {
      "text": "weight and we are going to add all of",
      "start": 3162.319,
      "duration": 4.361
    },
    {
      "text": "these together and this is going to give",
      "start": 3164.319,
      "duration": 4.161
    },
    {
      "text": "us the final context",
      "start": 3166.68,
      "duration": 4.24
    },
    {
      "text": "Vector it's very similar to what we did",
      "start": 3168.48,
      "duration": 4.68
    },
    {
      "text": "earlier remember earlier we did not have",
      "start": 3170.92,
      "duration": 4.24
    },
    {
      "text": "this value Vector the value Vector was",
      "start": 3173.16,
      "duration": 4.0
    },
    {
      "text": "just the input embedding vector",
      "start": 3175.16,
      "duration": 4.0
    },
    {
      "text": "but the whole Essence here is that now",
      "start": 3177.16,
      "duration": 3.959
    },
    {
      "text": "we have calculated the attention weight",
      "start": 3179.16,
      "duration": 4.72
    },
    {
      "text": "so now it's time to assign the weightage",
      "start": 3181.119,
      "duration": 4.44
    },
    {
      "text": "assign the corresponding weightage to",
      "start": 3183.88,
      "duration": 4.4
    },
    {
      "text": "each input embedding vector or the value",
      "start": 3185.559,
      "duration": 4.401
    },
    {
      "text": "vector and sum them up to give the",
      "start": 3188.28,
      "duration": 4.92
    },
    {
      "text": "context Vector I'll show you intuitively",
      "start": 3189.96,
      "duration": 5.04
    },
    {
      "text": "what this means in a",
      "start": 3193.2,
      "duration": 5.0
    },
    {
      "text": "minute uh but let me take you to",
      "start": 3195.0,
      "duration": 5.72
    },
    {
      "text": "this whiteboard right now so that I can",
      "start": 3198.2,
      "duration": 4.76
    },
    {
      "text": "show you the next step okay so we have",
      "start": 3200.72,
      "duration": 4.16
    },
    {
      "text": "calculated the attention weights now and",
      "start": 3202.96,
      "duration": 3.52
    },
    {
      "text": "now we'll be calculating the cont",
      "start": 3204.88,
      "duration": 3.76
    },
    {
      "text": "context Vector so let me show you",
      "start": 3206.48,
      "duration": 3.96
    },
    {
      "text": "mathematically how we compute the",
      "start": 3208.64,
      "duration": 3.76
    },
    {
      "text": "context Vector so we have these",
      "start": 3210.44,
      "duration": 4.72
    },
    {
      "text": "attention weights which is a 6x6 Matrix",
      "start": 3212.4,
      "duration": 4.679
    },
    {
      "text": "and we have this values which you",
      "start": 3215.16,
      "duration": 3.639
    },
    {
      "text": "computed at the start of this lecture we",
      "start": 3217.079,
      "duration": 5.04
    },
    {
      "text": "have this value matrix it's a 6x2 matrix",
      "start": 3218.799,
      "duration": 5.921
    },
    {
      "text": "right so the first row are the values",
      "start": 3222.119,
      "duration": 4.561
    },
    {
      "text": "for your the second row are the values",
      "start": 3224.72,
      "duration": 4.399
    },
    {
      "text": "for Journey similarly the sixth row is",
      "start": 3226.68,
      "duration": 5.32
    },
    {
      "text": "the value for step now let's say we look",
      "start": 3229.119,
      "duration": 5.2
    },
    {
      "text": "at",
      "start": 3232.0,
      "duration": 2.319
    },
    {
      "text": "Journey uh and I want to find the",
      "start": 3234.4,
      "duration": 5.0
    },
    {
      "text": "context Vector for Journey and let's",
      "start": 3236.76,
      "duration": 3.92
    },
    {
      "text": "look at the attention weights for",
      "start": 3239.4,
      "duration": 4.199
    },
    {
      "text": "Journey it's the second row let me show",
      "start": 3240.68,
      "duration": 6.28
    },
    {
      "text": "you intuitively how you find the context",
      "start": 3243.599,
      "duration": 4.96
    },
    {
      "text": "Vector for Journey and you'll never",
      "start": 3246.96,
      "duration": 4.0
    },
    {
      "text": "forget this after I show the",
      "start": 3248.559,
      "duration": 4.601
    },
    {
      "text": "illustration okay so let's say these are",
      "start": 3250.96,
      "duration": 5.839
    },
    {
      "text": "the uh value vectors for the different",
      "start": 3253.16,
      "duration": 5.52
    },
    {
      "text": "tokens so this is the value Vector for",
      "start": 3256.799,
      "duration": 5.361
    },
    {
      "text": "Journey and let's say this is uh 3951",
      "start": 3258.68,
      "duration": 6.52
    },
    {
      "text": "and 1 the way we do",
      "start": 3262.16,
      "duration": 6.159
    },
    {
      "text": "the uh context Vector calculation is",
      "start": 3265.2,
      "duration": 5.919
    },
    {
      "text": "that let's look at the attention weights",
      "start": 3268.319,
      "duration": 4.841
    },
    {
      "text": "so the attention weights for the journey",
      "start": 3271.119,
      "duration": 5.041
    },
    {
      "text": "are this second row which means 0.15",
      "start": 3273.16,
      "duration": 6.04
    },
    {
      "text": "2264 Etc so this means that I'm paying",
      "start": 3276.16,
      "duration": 6.0
    },
    {
      "text": "15% attention to",
      "start": 3279.2,
      "duration": 7.119
    },
    {
      "text": "your I am paying 22% attention to",
      "start": 3282.16,
      "duration": 8.72
    },
    {
      "text": "journey I am paying 22% attention to",
      "start": 3286.319,
      "duration": 9.321
    },
    {
      "text": "begins I'm paying only 13% to width I'm",
      "start": 3290.88,
      "duration": 9.64
    },
    {
      "text": "paying only 9% to 1 and I'm only paying",
      "start": 3295.64,
      "duration": 9.0
    },
    {
      "text": "18% to step okay how do I encode all of",
      "start": 3300.52,
      "duration": 5.839
    },
    {
      "text": "this information to find the context",
      "start": 3304.64,
      "duration": 4.959
    },
    {
      "text": "Vector it's pretty simple you take the",
      "start": 3306.359,
      "duration": 6.76
    },
    {
      "text": "your vector and you multiply it by5",
      "start": 3309.599,
      "duration": 6.321
    },
    {
      "text": "because it only contributes 15% you take",
      "start": 3313.119,
      "duration": 5.921
    },
    {
      "text": "the journey Vector you multiply it by 22",
      "start": 3315.92,
      "duration": 6.08
    },
    {
      "text": "because it only contributes 22% you take",
      "start": 3319.04,
      "duration": 5.72
    },
    {
      "text": "the begins Vector you multiply it by",
      "start": 3322.0,
      "duration": 5.559
    },
    {
      "text": "2199 because that also contributes only",
      "start": 3324.76,
      "duration": 5.48
    },
    {
      "text": "22% similarly you take the one vector",
      "start": 3327.559,
      "duration": 5.081
    },
    {
      "text": "you multiply it with 0.09 because it",
      "start": 3330.24,
      "duration": 5.04
    },
    {
      "text": "contributes very less you take the step",
      "start": 3332.64,
      "duration": 5.88
    },
    {
      "text": "Vector you multiply it by8 because it",
      "start": 3335.28,
      "duration": 5.88
    },
    {
      "text": "only contributes 18% and then you add",
      "start": 3338.52,
      "duration": 4.72
    },
    {
      "text": "all of the small contributions together",
      "start": 3341.16,
      "duration": 3.959
    },
    {
      "text": "to give you the context Vector for",
      "start": 3343.24,
      "duration": 4.16
    },
    {
      "text": "Journey let me show you how this looks",
      "start": 3345.119,
      "duration": 5.881
    },
    {
      "text": "like so your the attention for your is",
      "start": 3347.4,
      "duration": 7.679
    },
    {
      "text": "how much 15 right so you will scale you",
      "start": 3351.0,
      "duration": 6.52
    },
    {
      "text": "will scale the your Vector let me show",
      "start": 3355.079,
      "duration": 4.881
    },
    {
      "text": "this with a different color you'll scale",
      "start": 3357.52,
      "duration": 4.079
    },
    {
      "text": "the your vector",
      "start": 3359.96,
      "duration": 4.92
    },
    {
      "text": "by5 the attention score for Journey was",
      "start": 3361.599,
      "duration": 6.601
    },
    {
      "text": "22 so you'll scale it by 22 for starts",
      "start": 3364.88,
      "duration": 7.159
    },
    {
      "text": "was also 22 and for width let's see how",
      "start": 3368.2,
      "duration": 6.44
    },
    {
      "text": "much it was for width for width it",
      "start": 3372.039,
      "duration": 5.32
    },
    {
      "text": "was3 so for width and one it was very",
      "start": 3374.64,
      "duration": 5.679
    },
    {
      "text": "low so for width and one they they make",
      "start": 3377.359,
      "duration": 5.76
    },
    {
      "text": "very less contributions for step it was",
      "start": 3380.319,
      "duration": 5.401
    },
    {
      "text": "around5 so now you have the six vectors",
      "start": 3383.119,
      "duration": 4.161
    },
    {
      "text": "and you will add add all of these six",
      "start": 3385.72,
      "duration": 3.879
    },
    {
      "text": "vectors together to give you the context",
      "start": 3387.28,
      "duration": 4.12
    },
    {
      "text": "Vector for",
      "start": 3389.599,
      "duration": 4.361
    },
    {
      "text": "Journey so when you add all of the six",
      "start": 3391.4,
      "duration": 4.28
    },
    {
      "text": "vectors together it will give you the",
      "start": 3393.96,
      "duration": 4.839
    },
    {
      "text": "context Vector for Journey if you have",
      "start": 3395.68,
      "duration": 4.96
    },
    {
      "text": "this kind of a visual representation in",
      "start": 3398.799,
      "duration": 3.641
    },
    {
      "text": "mind you will never forget what context",
      "start": 3400.64,
      "duration": 4.04
    },
    {
      "text": "Vector means now do you understand why",
      "start": 3402.44,
      "duration": 4.48
    },
    {
      "text": "the context Vector is richer than just",
      "start": 3404.68,
      "duration": 4.359
    },
    {
      "text": "the input embedding Vector if you just",
      "start": 3406.92,
      "duration": 3.72
    },
    {
      "text": "look at the input embedding Vector for",
      "start": 3409.039,
      "duration": 4.04
    },
    {
      "text": "Journey it has no information about how",
      "start": 3410.64,
      "duration": 4.439
    },
    {
      "text": "much attention should be paid to your",
      "start": 3413.079,
      "duration": 3.801
    },
    {
      "text": "step one",
      "start": 3415.079,
      "duration": 5.161
    },
    {
      "text": "withd and starts but now if you since",
      "start": 3416.88,
      "duration": 5.8
    },
    {
      "text": "you have this attention weight Matrix",
      "start": 3420.24,
      "duration": 3.76
    },
    {
      "text": "since you have this attention weight",
      "start": 3422.68,
      "duration": 3.52
    },
    {
      "text": "Matrix over here you exactly know how",
      "start": 3424.0,
      "duration": 3.92
    },
    {
      "text": "much relative importance should be paid",
      "start": 3426.2,
      "duration": 3.8
    },
    {
      "text": "to each of the other words so you scale",
      "start": 3427.92,
      "duration": 3.76
    },
    {
      "text": "the other vectors by that much amount",
      "start": 3430.0,
      "duration": 3.24
    },
    {
      "text": "and then you add all the vectors",
      "start": 3431.68,
      "duration": 3.679
    },
    {
      "text": "together to get the context Vector so",
      "start": 3433.24,
      "duration": 4.96
    },
    {
      "text": "the context Vector is an enriched Vector",
      "start": 3435.359,
      "duration": 5.2
    },
    {
      "text": "it contains the semantic meaning of",
      "start": 3438.2,
      "duration": 4.48
    },
    {
      "text": "Journey plus it also contains how all",
      "start": 3440.559,
      "duration": 4.441
    },
    {
      "text": "the other words attend to Journey",
      "start": 3442.68,
      "duration": 3.879
    },
    {
      "text": "remember none of these rates are",
      "start": 3445.0,
      "duration": 3.48
    },
    {
      "text": "optimized right now we are we have just",
      "start": 3446.559,
      "duration": 3.961
    },
    {
      "text": "initialized them randomly but when the",
      "start": 3448.48,
      "duration": 4.04
    },
    {
      "text": "llm is trained all of these context",
      "start": 3450.52,
      "duration": 4.16
    },
    {
      "text": "vectors will be perfectly optimized so",
      "start": 3452.52,
      "duration": 3.92
    },
    {
      "text": "you would know that in that particular",
      "start": 3454.68,
      "duration": 4.0
    },
    {
      "text": "sentence in that particular paragraph",
      "start": 3456.44,
      "duration": 4.8
    },
    {
      "text": "which word uh should Journey pay most",
      "start": 3458.68,
      "duration": 3.679
    },
    {
      "text": "attention",
      "start": 3461.24,
      "duration": 4.16
    },
    {
      "text": "to now this exact thing which I've shown",
      "start": 3462.359,
      "duration": 6.72
    },
    {
      "text": "you in uh in the graphical format can be",
      "start": 3465.4,
      "duration": 5.88
    },
    {
      "text": "computed in Matrix if you just multiply",
      "start": 3469.079,
      "duration": 4.441
    },
    {
      "text": "the attention weights with the values so",
      "start": 3471.28,
      "duration": 3.88
    },
    {
      "text": "if you multiply the attention weights",
      "start": 3473.52,
      "duration": 3.92
    },
    {
      "text": "with the values your multiplying a 6x6",
      "start": 3475.16,
      "duration": 5.12
    },
    {
      "text": "Matrix with a 6x2 so of course the",
      "start": 3477.44,
      "duration": 4.679
    },
    {
      "text": "matrix multiplication is possible and",
      "start": 3480.28,
      "duration": 4.12
    },
    {
      "text": "the resultant will be a 6x2 matrix like",
      "start": 3482.119,
      "duration": 4.841
    },
    {
      "text": "this so this is a 6x2 matrix which is a",
      "start": 3484.4,
      "duration": 5.719
    },
    {
      "text": "context Vector Matrix and each row",
      "start": 3486.96,
      "duration": 5.48
    },
    {
      "text": "corresponds to a context Vector for that",
      "start": 3490.119,
      "duration": 4.44
    },
    {
      "text": "token so if you look at the second row",
      "start": 3492.44,
      "duration": 4.28
    },
    {
      "text": "over here the second row corresponds to",
      "start": 3494.559,
      "duration": 3.961
    },
    {
      "text": "the context Vector for Journey which we",
      "start": 3496.72,
      "duration": 3.8
    },
    {
      "text": "have shown over here the first row",
      "start": 3498.52,
      "duration": 3.559
    },
    {
      "text": "corresponds to the context Vector for",
      "start": 3500.52,
      "duration": 3.92
    },
    {
      "text": "your similarly the last row corresponds",
      "start": 3502.079,
      "duration": 5.04
    },
    {
      "text": "to the context Vector for step",
      "start": 3504.44,
      "duration": 6.0
    },
    {
      "text": "one exercise I want to give you is that",
      "start": 3507.119,
      "duration": 5.96
    },
    {
      "text": "uh use this this visual representation",
      "start": 3510.44,
      "duration": 5.24
    },
    {
      "text": "of scaling so take the second row take",
      "start": 3513.079,
      "duration": 5.76
    },
    {
      "text": "journey and uh use the scaling approach",
      "start": 3515.68,
      "duration": 4.48
    },
    {
      "text": "which I showed you in the graphical",
      "start": 3518.839,
      "duration": 3.841
    },
    {
      "text": "representation so take the vector for",
      "start": 3520.16,
      "duration": 4.959
    },
    {
      "text": "your multiply it by 15 take the vector",
      "start": 3522.68,
      "duration": 5.399
    },
    {
      "text": "for Journey multiply it by 22 similarly",
      "start": 3525.119,
      "duration": 5.601
    },
    {
      "text": "take the vector for step multiply it by8",
      "start": 3528.079,
      "duration": 4.801
    },
    {
      "text": "add them all together and see whether",
      "start": 3530.72,
      "duration": 3.839
    },
    {
      "text": "the result matches with the second row",
      "start": 3532.88,
      "duration": 4.199
    },
    {
      "text": "over here that will give you an",
      "start": 3534.559,
      "duration": 3.921
    },
    {
      "text": "intuition of why this matrix",
      "start": 3537.079,
      "duration": 3.401
    },
    {
      "text": "multiplication actually gives us the",
      "start": 3538.48,
      "duration": 5.079
    },
    {
      "text": "exact same result as this graphical",
      "start": 3540.48,
      "duration": 4.839
    },
    {
      "text": "intuition based calculation which we did",
      "start": 3543.559,
      "duration": 4.441
    },
    {
      "text": "over here but if you forget this Matrix",
      "start": 3545.319,
      "duration": 4.841
    },
    {
      "text": "formula just remember the scaling based",
      "start": 3548.0,
      "duration": 3.92
    },
    {
      "text": "approach which we discussed in this",
      "start": 3550.16,
      "duration": 3.439
    },
    {
      "text": "graphical intuition and you will get the",
      "start": 3551.92,
      "duration": 4.32
    },
    {
      "text": "exact same value so remember that the",
      "start": 3553.599,
      "duration": 5.401
    },
    {
      "text": "context Vector Matrix is just a matrix",
      "start": 3556.24,
      "duration": 5.28
    },
    {
      "text": "product of attention weights and values",
      "start": 3559.0,
      "duration": 4.079
    },
    {
      "text": "attention weights multiplied by the",
      "start": 3561.52,
      "duration": 3.519
    },
    {
      "text": "values Matrix gives us the context",
      "start": 3563.079,
      "duration": 4.441
    },
    {
      "text": "Vector Matrix and this is exactly what",
      "start": 3565.039,
      "duration": 5.0
    },
    {
      "text": "we are going to implement in code right",
      "start": 3567.52,
      "duration": 6.279
    },
    {
      "text": "now uh so let us go to",
      "start": 3570.039,
      "duration": 5.56
    },
    {
      "text": "code",
      "start": 3573.799,
      "duration": 4.721
    },
    {
      "text": "yeah so we saw this we saw the square",
      "start": 3575.599,
      "duration": 5.081
    },
    {
      "text": "root and now we are going to uh",
      "start": 3578.52,
      "duration": 5.12
    },
    {
      "text": "implement the context Vector so remember",
      "start": 3580.68,
      "duration": 5.0
    },
    {
      "text": "that context Vector first we are going",
      "start": 3583.64,
      "duration": 3.64
    },
    {
      "text": "to only see the context Vector for",
      "start": 3585.68,
      "duration": 4.56
    },
    {
      "text": "Journey and it's the product between the",
      "start": 3587.28,
      "duration": 5.16
    },
    {
      "text": "attention Matrix attention weight for",
      "start": 3590.24,
      "duration": 4.72
    },
    {
      "text": "Journey multiplied by values let me",
      "start": 3592.44,
      "duration": 5.24
    },
    {
      "text": "explain this a bit so uh on the",
      "start": 3594.96,
      "duration": 4.28
    },
    {
      "text": "Whiteboard what we saw is we just",
      "start": 3597.68,
      "duration": 3.879
    },
    {
      "text": "multiplied the entire attention weights",
      "start": 3599.24,
      "duration": 4.559
    },
    {
      "text": "with the value right but if you want",
      "start": 3601.559,
      "duration": 5.721
    },
    {
      "text": "just the uh context Vector for Journey",
      "start": 3603.799,
      "duration": 5.201
    },
    {
      "text": "what you can do is just take the second",
      "start": 3607.28,
      "duration": 5.72
    },
    {
      "text": "row it will be uh 1X 6 and you multiply",
      "start": 3609.0,
      "duration": 6.68
    },
    {
      "text": "it with this values which is 6x2 and",
      "start": 3613.0,
      "duration": 5.24
    },
    {
      "text": "then you'll get a 1x two Vector which is",
      "start": 3615.68,
      "duration": 4.32
    },
    {
      "text": "the second row here and that will be the",
      "start": 3618.24,
      "duration": 3.839
    },
    {
      "text": "context Vector for Journey so this is",
      "start": 3620.0,
      "duration": 4.079
    },
    {
      "text": "what I have showed over here the context",
      "start": 3622.079,
      "duration": 3.921
    },
    {
      "text": "Vector 2 which is the context Vector for",
      "start": 3624.079,
      "duration": 3.561
    },
    {
      "text": "journey is just the product of the",
      "start": 3626.0,
      "duration": 3.52
    },
    {
      "text": "attention weights for Journey multiplied",
      "start": 3627.64,
      "duration": 4.679
    },
    {
      "text": "by the values Matrix and the result is",
      "start": 3629.52,
      "duration": 5.4
    },
    {
      "text": "3061 and",
      "start": 3632.319,
      "duration": 5.401
    },
    {
      "text": "8210 and let's actually see the result",
      "start": 3634.92,
      "duration": 4.8
    },
    {
      "text": "here and that exactly matches the second",
      "start": 3637.72,
      "duration": 6.359
    },
    {
      "text": "row which we have 3061 and 8210 awesome",
      "start": 3639.72,
      "duration": 6.52
    },
    {
      "text": "so our calculation seems to be correct",
      "start": 3644.079,
      "duration": 4.0
    },
    {
      "text": "so in the code right now we have only",
      "start": 3646.24,
      "duration": 4.72
    },
    {
      "text": "computed the single context Vector right",
      "start": 3648.079,
      "duration": 4.561
    },
    {
      "text": "now we are going to generalize the code",
      "start": 3650.96,
      "duration": 4.159
    },
    {
      "text": "a bit to compute all the context vectors",
      "start": 3652.64,
      "duration": 4.32
    },
    {
      "text": "it's going to be very simple because now",
      "start": 3655.119,
      "duration": 3.761
    },
    {
      "text": "we just multiply the attention weights",
      "start": 3656.96,
      "duration": 4.079
    },
    {
      "text": "with the values but we'll do this in a",
      "start": 3658.88,
      "duration": 4.919
    },
    {
      "text": "structured manner we'll Implement a self",
      "start": 3661.039,
      "duration": 5.441
    },
    {
      "text": "attention python class and what this",
      "start": 3663.799,
      "duration": 4.0
    },
    {
      "text": "class will do is that it will",
      "start": 3666.48,
      "duration": 3.319
    },
    {
      "text": "essentially have a forward method this",
      "start": 3667.799,
      "duration": 3.841
    },
    {
      "text": "forward method will compute the keys",
      "start": 3669.799,
      "duration": 3.921
    },
    {
      "text": "queries values it will compute the",
      "start": 3671.64,
      "duration": 3.88
    },
    {
      "text": "attention scores attention weights and",
      "start": 3673.72,
      "duration": 3.8
    },
    {
      "text": "the context vectors all in a very short",
      "start": 3675.52,
      "duration": 4.96
    },
    {
      "text": "piece of code so let's do that right now",
      "start": 3677.52,
      "duration": 4.88
    },
    {
      "text": "before that let us summarize what all we",
      "start": 3680.48,
      "duration": 4.2
    },
    {
      "text": "have seen so far so that you'll",
      "start": 3682.4,
      "duration": 3.959
    },
    {
      "text": "understand the python class much better",
      "start": 3684.68,
      "duration": 3.76
    },
    {
      "text": "so let me zoom out here a",
      "start": 3686.359,
      "duration": 4.76
    },
    {
      "text": "bit so remember how we started the",
      "start": 3688.44,
      "duration": 5.48
    },
    {
      "text": "lecture we started the lecture with uh",
      "start": 3691.119,
      "duration": 4.44
    },
    {
      "text": "we started the lecture with taking the",
      "start": 3693.92,
      "duration": 3.84
    },
    {
      "text": "inputs and then multiplying them with",
      "start": 3695.559,
      "duration": 4.121
    },
    {
      "text": "query key and the value to get the",
      "start": 3697.76,
      "duration": 3.799
    },
    {
      "text": "queries Matrix the key Matrix and the",
      "start": 3699.68,
      "duration": 4.6
    },
    {
      "text": "value Matrix okay then remember what we",
      "start": 3701.559,
      "duration": 4.441
    },
    {
      "text": "did next then we move to the attention",
      "start": 3704.28,
      "duration": 4.079
    },
    {
      "text": "scores we multiplied the queries with",
      "start": 3706.0,
      "duration": 4.24
    },
    {
      "text": "the transpose of the keys to get the",
      "start": 3708.359,
      "duration": 4.2
    },
    {
      "text": "attention scores so we had the attention",
      "start": 3710.24,
      "duration": 4.4
    },
    {
      "text": "scores Matrix then what we did is we",
      "start": 3712.559,
      "duration": 4.121
    },
    {
      "text": "scaled this by square root of the keys",
      "start": 3714.64,
      "duration": 4.6
    },
    {
      "text": "Dimension and then we took the soft Max",
      "start": 3716.68,
      "duration": 4.52
    },
    {
      "text": "this gave us the attention weights then",
      "start": 3719.24,
      "duration": 3.48
    },
    {
      "text": "we took the attention weights and we",
      "start": 3721.2,
      "duration": 3.52
    },
    {
      "text": "multiplied it by the values Matrix and",
      "start": 3722.72,
      "duration": 3.599
    },
    {
      "text": "that ultimately gave us the context",
      "start": 3724.72,
      "duration": 4.639
    },
    {
      "text": "Vector Matrix remember this flow so the",
      "start": 3726.319,
      "duration": 5.8
    },
    {
      "text": "flow is in four steps step number one is",
      "start": 3729.359,
      "duration": 4.76
    },
    {
      "text": "at the left side of the page which is",
      "start": 3732.119,
      "duration": 5.081
    },
    {
      "text": "converting the input embeddings into key",
      "start": 3734.119,
      "duration": 5.561
    },
    {
      "text": "query value Vector step number two is",
      "start": 3737.2,
      "duration": 4.8
    },
    {
      "text": "getting the attention scores step number",
      "start": 3739.68,
      "duration": 4.08
    },
    {
      "text": "three is getting the attention weights",
      "start": 3742.0,
      "duration": 3.48
    },
    {
      "text": "step number four is getting the context",
      "start": 3743.76,
      "duration": 4.68
    },
    {
      "text": "vector that's it and we are done that's",
      "start": 3745.48,
      "duration": 4.48
    },
    {
      "text": "exactly what we are going to implement",
      "start": 3748.44,
      "duration": 2.879
    },
    {
      "text": "in this python",
      "start": 3749.96,
      "duration": 4.72
    },
    {
      "text": "class so uh with the llm implementation",
      "start": 3751.319,
      "duration": 5.161
    },
    {
      "text": "which we are going to cover next in one",
      "start": 3754.68,
      "duration": 4.08
    },
    {
      "text": "of the subsequent lect lectures it's",
      "start": 3756.48,
      "duration": 4.48
    },
    {
      "text": "very useful to organize the code in a",
      "start": 3758.76,
      "duration": 4.48
    },
    {
      "text": "python class so we cannot keep on",
      "start": 3760.96,
      "duration": 4.079
    },
    {
      "text": "writing separate lines of codes like",
      "start": 3763.24,
      "duration": 3.799
    },
    {
      "text": "what we did over here right it's just",
      "start": 3765.039,
      "duration": 3.76
    },
    {
      "text": "better to have a class so that then we",
      "start": 3767.039,
      "duration": 4.121
    },
    {
      "text": "can create an instance of this class and",
      "start": 3768.799,
      "duration": 4.52
    },
    {
      "text": "then always return the context",
      "start": 3771.16,
      "duration": 4.8
    },
    {
      "text": "Vector okay so we are going to Define in",
      "start": 3773.319,
      "duration": 4.681
    },
    {
      "text": "this class called self attention version",
      "start": 3775.96,
      "duration": 4.48
    },
    {
      "text": "one and it will take two attributes the",
      "start": 3778.0,
      "duration": 4.88
    },
    {
      "text": "input Dimension and the output Dimension",
      "start": 3780.44,
      "duration": 4.96
    },
    {
      "text": "the input Dimension is the input Vector",
      "start": 3782.88,
      "duration": 4.88
    },
    {
      "text": "embedding Dimension the output Dimension",
      "start": 3785.4,
      "duration": 4.8
    },
    {
      "text": "is what we want the keys query and value",
      "start": 3787.76,
      "duration": 5.2
    },
    {
      "text": "dimension in GPT and other llms these",
      "start": 3790.2,
      "duration": 4.56
    },
    {
      "text": "these two are generally",
      "start": 3792.96,
      "duration": 4.159
    },
    {
      "text": "similar okay first thing what we do is",
      "start": 3794.76,
      "duration": 3.799
    },
    {
      "text": "when an instance of this class is",
      "start": 3797.119,
      "duration": 3.121
    },
    {
      "text": "created this init Constructor is",
      "start": 3798.559,
      "duration": 4.201
    },
    {
      "text": "automatically called and the query key",
      "start": 3800.24,
      "duration": 4.92
    },
    {
      "text": "and the value matrixes matrices are",
      "start": 3802.76,
      "duration": 5.16
    },
    {
      "text": "initialized randomly which means that",
      "start": 3805.16,
      "duration": 5.28
    },
    {
      "text": "they have a dimension of D in and D out",
      "start": 3807.92,
      "duration": 5.639
    },
    {
      "text": "so D in rows in our case three rows and",
      "start": 3810.44,
      "duration": 5.76
    },
    {
      "text": "D out columns two columns and then each",
      "start": 3813.559,
      "duration": 5.201
    },
    {
      "text": "element will be initialized in a random",
      "start": 3816.2,
      "duration": 4.879
    },
    {
      "text": "manner then what we do is we do the",
      "start": 3818.76,
      "duration": 4.68
    },
    {
      "text": "forward pass what happens in the forward",
      "start": 3821.079,
      "duration": 4.96
    },
    {
      "text": "pass is that it takes an input it takes",
      "start": 3823.44,
      "duration": 5.399
    },
    {
      "text": "X as the input which is the input uh",
      "start": 3826.039,
      "duration": 5.24
    },
    {
      "text": "input embedding Vector that needs to be",
      "start": 3828.839,
      "duration": 4.48
    },
    {
      "text": "given as an input to execute the forward",
      "start": 3831.279,
      "duration": 4.28
    },
    {
      "text": "method and then in the forward method",
      "start": 3833.319,
      "duration": 4.081
    },
    {
      "text": "what we do is we first compute the keys",
      "start": 3835.559,
      "duration": 5.841
    },
    {
      "text": "Matrix which is X multiplied by the uh",
      "start": 3837.4,
      "duration": 6.159
    },
    {
      "text": "weight trainable weight Matrix for key",
      "start": 3841.4,
      "duration": 3.879
    },
    {
      "text": "then we compute the query Matrix which",
      "start": 3843.559,
      "duration": 3.76
    },
    {
      "text": "is X multiplied by the trainable Matrix",
      "start": 3845.279,
      "duration": 4.201
    },
    {
      "text": "for query then we compute the value",
      "start": 3847.319,
      "duration": 3.921
    },
    {
      "text": "Matrix which is X multiplied by the",
      "start": 3849.48,
      "duration": 4.839
    },
    {
      "text": "trainable Matrix for value and this is",
      "start": 3851.24,
      "duration": 5.72
    },
    {
      "text": "uh exactly what we saw on the left side",
      "start": 3854.319,
      "duration": 4.201
    },
    {
      "text": "of the board over",
      "start": 3856.96,
      "duration": 4.399
    },
    {
      "text": "here here so until now we are at this",
      "start": 3858.52,
      "duration": 5.039
    },
    {
      "text": "stage where we are taking the inputs we",
      "start": 3861.359,
      "duration": 3.92
    },
    {
      "text": "are multiplying it with the m weight",
      "start": 3863.559,
      "duration": 4.201
    },
    {
      "text": "Matrix to get the queries keys and the",
      "start": 3865.279,
      "duration": 4.681
    },
    {
      "text": "values and now we'll go to the right",
      "start": 3867.76,
      "duration": 3.559
    },
    {
      "text": "side of the board to compute the",
      "start": 3869.96,
      "duration": 3.8
    },
    {
      "text": "attention scores so to get the attention",
      "start": 3871.319,
      "duration": 4.72
    },
    {
      "text": "scores we'll multiply queries with keys",
      "start": 3873.76,
      "duration": 4.64
    },
    {
      "text": "transpose so that's exactly what's done",
      "start": 3876.039,
      "duration": 3.921
    },
    {
      "text": "here to get the attention scores we",
      "start": 3878.4,
      "duration": 3.399
    },
    {
      "text": "multiply queries Matrix with keys",
      "start": 3879.96,
      "duration": 3.92
    },
    {
      "text": "transpose then we get the attention",
      "start": 3881.799,
      "duration": 4.081
    },
    {
      "text": "weights to get the attention weights",
      "start": 3883.88,
      "duration": 4.439
    },
    {
      "text": "we'll of course apply soft Max but",
      "start": 3885.88,
      "duration": 4.32
    },
    {
      "text": "before applying soft Max we'll divide",
      "start": 3888.319,
      "duration": 3.72
    },
    {
      "text": "the attention scores every element of",
      "start": 3890.2,
      "duration": 3.879
    },
    {
      "text": "the attention scores with the square",
      "start": 3892.039,
      "duration": 5.721
    },
    {
      "text": "root of the Keys embedding Dimension so",
      "start": 3894.079,
      "duration": 6.401
    },
    {
      "text": "keys do shape minus one Returns the",
      "start": 3897.76,
      "duration": 4.4
    },
    {
      "text": "columns which is the embedding",
      "start": 3900.48,
      "duration": 4.319
    },
    {
      "text": "dimensions in this case it's two columns",
      "start": 3902.16,
      "duration": 4.24
    },
    {
      "text": "of the keys Matrix so it will be square",
      "start": 3904.799,
      "duration": 3.681
    },
    {
      "text": "root of two the reason we do this",
      "start": 3906.4,
      "duration": 3.959
    },
    {
      "text": "division as we saw is first of all to",
      "start": 3908.48,
      "duration": 4.359
    },
    {
      "text": "make sure the values in The Matrix in",
      "start": 3910.359,
      "duration": 5.561
    },
    {
      "text": "the attention score Matrix are small",
      "start": 3912.839,
      "duration": 5.161
    },
    {
      "text": "second it also helps to make sure that",
      "start": 3915.92,
      "duration": 4.879
    },
    {
      "text": "the dot product between the quiz keys",
      "start": 3918.0,
      "duration": 6.72
    },
    {
      "text": "and the queries uh does its variance",
      "start": 3920.799,
      "duration": 5.841
    },
    {
      "text": "does not scale too much so we want its",
      "start": 3924.72,
      "duration": 4.04
    },
    {
      "text": "variance to be very close to one that's",
      "start": 3926.64,
      "duration": 4.24
    },
    {
      "text": "why we specifically divide by the square",
      "start": 3928.76,
      "duration": 3.279
    },
    {
      "text": "root of the",
      "start": 3930.88,
      "duration": 3.8
    },
    {
      "text": "dimension and here the DM equal to minus",
      "start": 3932.039,
      "duration": 4.361
    },
    {
      "text": "one just tells the soft Max that you",
      "start": 3934.68,
      "duration": 3.679
    },
    {
      "text": "have to sum across the columns and",
      "start": 3936.4,
      "duration": 4.24
    },
    {
      "text": "that's how we make sure that each row if",
      "start": 3938.359,
      "duration": 4.801
    },
    {
      "text": "you take each row it sums up to one so",
      "start": 3940.64,
      "duration": 4.199
    },
    {
      "text": "if you look at each row of the attention",
      "start": 3943.16,
      "duration": 3.879
    },
    {
      "text": "weight Matrix it will sum up to one and",
      "start": 3944.839,
      "duration": 4.24
    },
    {
      "text": "then the context Vector is just the",
      "start": 3947.039,
      "duration": 3.681
    },
    {
      "text": "product of the attention weights and the",
      "start": 3949.079,
      "duration": 3.401
    },
    {
      "text": "values this is the last step which we",
      "start": 3950.72,
      "duration": 6.28
    },
    {
      "text": "saw the context vector",
      "start": 3952.48,
      "duration": 4.52
    },
    {
      "text": "uh yeah so this was the last step which",
      "start": 3957.039,
      "duration": 3.441
    },
    {
      "text": "we saw the context Vector is just the",
      "start": 3958.72,
      "duration": 4.04
    },
    {
      "text": "product of the attention weights and the",
      "start": 3960.48,
      "duration": 4.72
    },
    {
      "text": "values so this is how we compute the",
      "start": 3962.76,
      "duration": 3.559
    },
    {
      "text": "context",
      "start": 3965.2,
      "duration": 3.599
    },
    {
      "text": "Vector so some key things to mention",
      "start": 3966.319,
      "duration": 4.8
    },
    {
      "text": "here in this pytorch code the self",
      "start": 3968.799,
      "duration": 4.201
    },
    {
      "text": "attention version one is a class derived",
      "start": 3971.119,
      "duration": 5.72
    },
    {
      "text": "from nn. module so nn. module uh which",
      "start": 3973.0,
      "duration": 5.76
    },
    {
      "text": "is a fundamental building block of P",
      "start": 3976.839,
      "duration": 4.44
    },
    {
      "text": "torch models and that provides necessary",
      "start": 3978.76,
      "duration": 4.72
    },
    {
      "text": "functionalities for model layer creation",
      "start": 3981.279,
      "duration": 4.52
    },
    {
      "text": "and management as I mentioned mentioned",
      "start": 3983.48,
      "duration": 3.76
    },
    {
      "text": "to you before the init method",
      "start": 3985.799,
      "duration": 3.721
    },
    {
      "text": "initializes trainable weight matrices",
      "start": 3987.24,
      "duration": 5.16
    },
    {
      "text": "query key and value for queries keys and",
      "start": 3989.52,
      "duration": 5.16
    },
    {
      "text": "values each transforming the input",
      "start": 3992.4,
      "duration": 4.36
    },
    {
      "text": "Dimension D in into an output Dimension",
      "start": 3994.68,
      "duration": 4.679
    },
    {
      "text": "D out and during the forward pass which",
      "start": 3996.76,
      "duration": 4.92
    },
    {
      "text": "is the forward method what we do is that",
      "start": 3999.359,
      "duration": 4.041
    },
    {
      "text": "we compute the attention scores by",
      "start": 4001.68,
      "duration": 4.04
    },
    {
      "text": "multiplying queries and keys normalize",
      "start": 4003.4,
      "duration": 4.76
    },
    {
      "text": "the scores using soft Max and finally we",
      "start": 4005.72,
      "duration": 4.559
    },
    {
      "text": "create a context Vector that's the last",
      "start": 4008.16,
      "duration": 4.439
    },
    {
      "text": "step so this is just an explanation of",
      "start": 4010.279,
      "duration": 4.08
    },
    {
      "text": "the code I'll share this entire code",
      "start": 4012.599,
      "duration": 3.121
    },
    {
      "text": "file with you so you'll have this",
      "start": 4014.359,
      "duration": 3.92
    },
    {
      "text": "explanation don't worry so let's try to",
      "start": 4015.72,
      "duration": 4.559
    },
    {
      "text": "create an instance of this",
      "start": 4018.279,
      "duration": 4.721
    },
    {
      "text": "class uh so I'm creating an instance of",
      "start": 4020.279,
      "duration": 6.921
    },
    {
      "text": "this class with uh two with three as the",
      "start": 4023.0,
      "duration": 6.799
    },
    {
      "text": "input embedding Dimension and D out is",
      "start": 4027.2,
      "duration": 3.68
    },
    {
      "text": "equal to",
      "start": 4029.799,
      "duration": 4.24
    },
    {
      "text": "two so here you see I have created this",
      "start": 4030.88,
      "duration": 5.76
    },
    {
      "text": "uh so print essay version one inputs so",
      "start": 4034.039,
      "duration": 4.721
    },
    {
      "text": "these are the six embedding vectors so",
      "start": 4036.64,
      "duration": 4.719
    },
    {
      "text": "here is the Matrix of the six context",
      "start": 4038.76,
      "duration": 4.88
    },
    {
      "text": "vectors so directly returned so what",
      "start": 4041.359,
      "duration": 4.24
    },
    {
      "text": "this print statement does is that it",
      "start": 4043.64,
      "duration": 4.679
    },
    {
      "text": "Returns the context Vector so actually",
      "start": 4045.599,
      "duration": 6.321
    },
    {
      "text": "when you do this uh self attention",
      "start": 4048.319,
      "duration": 5.52
    },
    {
      "text": "version one and you pass the input many",
      "start": 4051.92,
      "duration": 3.76
    },
    {
      "text": "things are happening when you pass the",
      "start": 4053.839,
      "duration": 3.921
    },
    {
      "text": "inputs these key query value Matrix",
      "start": 4055.68,
      "duration": 3.96
    },
    {
      "text": "matrices are created attention scores",
      "start": 4057.76,
      "duration": 4.039
    },
    {
      "text": "are calculated attention weights are",
      "start": 4059.64,
      "duration": 3.84
    },
    {
      "text": "calculated and the context Vector is",
      "start": 4061.799,
      "duration": 4.201
    },
    {
      "text": "calculated which is returned over here",
      "start": 4063.48,
      "duration": 4.76
    },
    {
      "text": "so it has six embedding vectors so each",
      "start": 4066.0,
      "duration": 4.68
    },
    {
      "text": "row corresponds to the context Vector so",
      "start": 4068.24,
      "duration": 4.24
    },
    {
      "text": "the first row corresponds to the context",
      "start": 4070.68,
      "duration": 4.2
    },
    {
      "text": "Vector for first token your second row",
      "start": 4072.48,
      "duration": 4.079
    },
    {
      "text": "corresponds to to the context Vector for",
      "start": 4074.88,
      "duration": 4.36
    },
    {
      "text": "second tokken Journey Etc similarly the",
      "start": 4076.559,
      "duration": 4.321
    },
    {
      "text": "last row corresponds to the context",
      "start": 4079.24,
      "duration": 4.44
    },
    {
      "text": "token for context Vector for step so",
      "start": 4080.88,
      "duration": 4.439
    },
    {
      "text": "that's why the dimensions here are six",
      "start": 4083.68,
      "duration": 3.399
    },
    {
      "text": "rows and two",
      "start": 4085.319,
      "duration": 4.48
    },
    {
      "text": "columns so since the inputs contain six",
      "start": 4087.079,
      "duration": 4.441
    },
    {
      "text": "embedding vectors we get a matrix",
      "start": 4089.799,
      "duration": 4.161
    },
    {
      "text": "storing the six context Vector remember",
      "start": 4091.52,
      "duration": 6.08
    },
    {
      "text": "we have we want six uh context we want a",
      "start": 4093.96,
      "duration": 5.56
    },
    {
      "text": "context Vector for each input embedding",
      "start": 4097.6,
      "duration": 3.84
    },
    {
      "text": "Vector that's the main goal which we",
      "start": 4099.52,
      "duration": 3.88
    },
    {
      "text": "started out in today's class and we have",
      "start": 4101.44,
      "duration": 4.08
    },
    {
      "text": "achieved that goal over here in a very",
      "start": 4103.4,
      "duration": 4.56
    },
    {
      "text": "compact manner in just maybe 10 to 15",
      "start": 4105.52,
      "duration": 4.839
    },
    {
      "text": "lines of code so if you have followed",
      "start": 4107.96,
      "duration": 4.44
    },
    {
      "text": "till here it's been a pretty long",
      "start": 4110.359,
      "duration": 3.721
    },
    {
      "text": "lecture you should be really proud of",
      "start": 4112.4,
      "duration": 3.319
    },
    {
      "text": "yourself because if you have understood",
      "start": 4114.08,
      "duration": 3.719
    },
    {
      "text": "until here I believe you have understood",
      "start": 4115.719,
      "duration": 4.321
    },
    {
      "text": "the core of the attention mechanism just",
      "start": 4117.799,
      "duration": 4.241
    },
    {
      "text": "write these Dimensions down once take",
      "start": 4120.04,
      "duration": 4.56
    },
    {
      "text": "the dot product yourself and see how the",
      "start": 4122.04,
      "duration": 4.56
    },
    {
      "text": "calculations play out on a book or on a",
      "start": 4124.6,
      "duration": 3.759
    },
    {
      "text": "piece of paper that's the best way to",
      "start": 4126.6,
      "duration": 4.28
    },
    {
      "text": "learn this concept I it all boils down",
      "start": 4128.359,
      "duration": 4.601
    },
    {
      "text": "to matrices and",
      "start": 4130.88,
      "duration": 4.56
    },
    {
      "text": "dimensions so as a quick check let's not",
      "start": 4132.96,
      "duration": 6.6
    },
    {
      "text": "notice the second row 3061 8210 and",
      "start": 4135.44,
      "duration": 6.359
    },
    {
      "text": "let's see whether it's the same as the",
      "start": 4139.56,
      "duration": 4.0
    },
    {
      "text": "context Vector for Journey which we have",
      "start": 4141.799,
      "duration": 4.161
    },
    {
      "text": "calculated earlier so that's the same so",
      "start": 4143.56,
      "duration": 4.0
    },
    {
      "text": "it's a good sanity check which means we",
      "start": 4145.96,
      "duration": 3.68
    },
    {
      "text": "are in the right direction now what we",
      "start": 4147.56,
      "duration": 3.96
    },
    {
      "text": "can do is that we can we can actually",
      "start": 4149.64,
      "duration": 4.599
    },
    {
      "text": "improve this self attention version one",
      "start": 4151.52,
      "duration": 5.319
    },
    {
      "text": "further by uh changing how these are",
      "start": 4154.239,
      "duration": 4.92
    },
    {
      "text": "defined So currently we are using nn.",
      "start": 4156.839,
      "duration": 4.761
    },
    {
      "text": "parameter right the main hypothesis is",
      "start": 4159.159,
      "duration": 4.801
    },
    {
      "text": "that why don't we use directly a NN do",
      "start": 4161.6,
      "duration": 5.48
    },
    {
      "text": "linear function because it automatically",
      "start": 4163.96,
      "duration": 6.44
    },
    {
      "text": "creates the uh initializes the weight",
      "start": 4167.08,
      "duration": 5.88
    },
    {
      "text": "Matrix in a manner which is good for",
      "start": 4170.4,
      "duration": 5.0
    },
    {
      "text": "computations so instead of just sampling",
      "start": 4172.96,
      "duration": 5.12
    },
    {
      "text": "from random values here why don't we use",
      "start": 4175.4,
      "duration": 5.08
    },
    {
      "text": "the linear function so that the",
      "start": 4178.08,
      "duration": 4.36
    },
    {
      "text": "initialization is done in a proper",
      "start": 4180.48,
      "duration": 4.44
    },
    {
      "text": "manner using p",
      "start": 4182.44,
      "duration": 5.44
    },
    {
      "text": "torch that's exactly what we do next so",
      "start": 4184.92,
      "duration": 4.52
    },
    {
      "text": "we can improve the self attention",
      "start": 4187.88,
      "duration": 3.56
    },
    {
      "text": "version one version one implementation",
      "start": 4189.44,
      "duration": 4.759
    },
    {
      "text": "further by utilizing the NN do linear",
      "start": 4191.44,
      "duration": 5.759
    },
    {
      "text": "layers of pytorch",
      "start": 4194.199,
      "duration": 4.601
    },
    {
      "text": "which effectively perform matrix",
      "start": 4197.199,
      "duration": 3.281
    },
    {
      "text": "multiplication when bias units are",
      "start": 4198.8,
      "duration": 4.399
    },
    {
      "text": "disabled so basically we can use nn.",
      "start": 4200.48,
      "duration": 4.84
    },
    {
      "text": "linear to also initialize random values",
      "start": 4203.199,
      "duration": 4.96
    },
    {
      "text": "of query key and the value value Matrix",
      "start": 4205.32,
      "duration": 5.12
    },
    {
      "text": "but the main advantage is that nn.",
      "start": 4208.159,
      "duration": 4.08
    },
    {
      "text": "linear has an optimized weight",
      "start": 4210.44,
      "duration": 4.32
    },
    {
      "text": "initialization scheme and that leads to",
      "start": 4212.239,
      "duration": 4.44
    },
    {
      "text": "more stable and effective model model",
      "start": 4214.76,
      "duration": 4.76
    },
    {
      "text": "learning you can of course use NN do",
      "start": 4216.679,
      "duration": 5.361
    },
    {
      "text": "parameter also but the main advantage of",
      "start": 4219.52,
      "duration": 5.88
    },
    {
      "text": "nn. linear is that it has a stable uh",
      "start": 4222.04,
      "duration": 5.199
    },
    {
      "text": "initialization scheme or rather I should",
      "start": 4225.4,
      "duration": 4.24
    },
    {
      "text": "say more optimized initialization scheme",
      "start": 4227.239,
      "duration": 4.281
    },
    {
      "text": "since we always use this for all types",
      "start": 4229.64,
      "duration": 4.36
    },
    {
      "text": "of neural network tasks so why not",
      "start": 4231.52,
      "duration": 5.8
    },
    {
      "text": "essentially uh use the linear layer we",
      "start": 4234.0,
      "duration": 5.8
    },
    {
      "text": "can just put the bias terms to false",
      "start": 4237.32,
      "duration": 4.0
    },
    {
      "text": "because we don't need this we just need",
      "start": 4239.8,
      "duration": 5.16
    },
    {
      "text": "to initialize a weight Matrix with d in",
      "start": 4241.32,
      "duration": 6.56
    },
    {
      "text": "and D out weight weight Matrix for query",
      "start": 4244.96,
      "duration": 5.8
    },
    {
      "text": "key and value with d in as the rows and",
      "start": 4247.88,
      "duration": 4.6
    },
    {
      "text": "D out as the columns but we don't need",
      "start": 4250.76,
      "duration": 3.919
    },
    {
      "text": "the bias Matrix so you can just use the",
      "start": 4252.48,
      "duration": 4.8
    },
    {
      "text": "linear lay and put the bias to",
      "start": 4254.679,
      "duration": 5.321
    },
    {
      "text": "false so it will initialize these weight",
      "start": 4257.28,
      "duration": 5.2
    },
    {
      "text": "Matrix weight matrices and that's",
      "start": 4260.0,
      "duration": 4.04
    },
    {
      "text": "usually more common practice for",
      "start": 4262.48,
      "duration": 3.8
    },
    {
      "text": "implementing the self attention class",
      "start": 4264.04,
      "duration": 6.119
    },
    {
      "text": "when we deal with llms so similar to uh",
      "start": 4266.28,
      "duration": 5.72
    },
    {
      "text": "here we created an instance of the self",
      "start": 4270.159,
      "duration": 4.121
    },
    {
      "text": "attention version one right similarly we",
      "start": 4272.0,
      "duration": 3.76
    },
    {
      "text": "can create an instance of the self",
      "start": 4274.28,
      "duration": 3.8
    },
    {
      "text": "attention version two and pass in the",
      "start": 4275.76,
      "duration": 4.24
    },
    {
      "text": "arguments as the input Dimension and the",
      "start": 4278.08,
      "duration": 4.32
    },
    {
      "text": "output so here again we get a six rows",
      "start": 4280.0,
      "duration": 4.88
    },
    {
      "text": "and two column tensor uh which is the",
      "start": 4282.4,
      "duration": 5.52
    },
    {
      "text": "context vectors for all the six uh input",
      "start": 4284.88,
      "duration": 5.4
    },
    {
      "text": "embedding vectors so you'll notice that",
      "start": 4287.92,
      "duration": 5.799
    },
    {
      "text": "these values are different than these",
      "start": 4290.28,
      "duration": 5.64
    },
    {
      "text": "values because the initialization",
      "start": 4293.719,
      "duration": 4.841
    },
    {
      "text": "schemes are different so they use",
      "start": 4295.92,
      "duration": 4.239
    },
    {
      "text": "different initial weights for the weight",
      "start": 4298.56,
      "duration": 4.159
    },
    {
      "text": "Matrix since nn. linear uses a more",
      "start": 4300.159,
      "duration": 4.801
    },
    {
      "text": "sophisticated weight initialization",
      "start": 4302.719,
      "duration": 5.92
    },
    {
      "text": "scheme so the linear uses usage of nn.",
      "start": 4304.96,
      "duration": 5.92
    },
    {
      "text": "linear leads to a more sophisticated",
      "start": 4308.639,
      "duration": 4.361
    },
    {
      "text": "weight initialization scheme than NN do",
      "start": 4310.88,
      "duration": 4.6
    },
    {
      "text": "parameter I won't go into the details of",
      "start": 4313.0,
      "duration": 4.32
    },
    {
      "text": "how the weights are initialized in nn.",
      "start": 4315.48,
      "duration": 4.719
    },
    {
      "text": "linear but you can explore this further",
      "start": 4317.32,
      "duration": 4.64
    },
    {
      "text": "that also is an interesting topic but",
      "start": 4320.199,
      "duration": 4.52
    },
    {
      "text": "the length of the lecture will increase",
      "start": 4321.96,
      "duration": 5.36
    },
    {
      "text": "further okay so that actually brings us",
      "start": 4324.719,
      "duration": 4.841
    },
    {
      "text": "to the end of today's lecture where we",
      "start": 4327.32,
      "duration": 5.08
    },
    {
      "text": "implemented query key value Matrix found",
      "start": 4329.56,
      "duration": 4.639
    },
    {
      "text": "the attention scores attention weights",
      "start": 4332.4,
      "duration": 3.56
    },
    {
      "text": "and the context vectors for all the",
      "start": 4334.199,
      "duration": 4.321
    },
    {
      "text": "input embeddings I just want to end",
      "start": 4335.96,
      "duration": 4.679
    },
    {
      "text": "today's lecture by uh showing you a",
      "start": 4338.52,
      "duration": 4.88
    },
    {
      "text": "schematic which illustrates what all we",
      "start": 4340.639,
      "duration": 6.52
    },
    {
      "text": "have implemented in today's class",
      "start": 4343.4,
      "duration": 3.759
    },
    {
      "text": "okay so at the end we implemented this",
      "start": 4348.56,
      "duration": 5.84
    },
    {
      "text": "self attention python class and uh this",
      "start": 4351.08,
      "duration": 5.639
    },
    {
      "text": "schematic actually explains everything",
      "start": 4354.4,
      "duration": 5.0
    },
    {
      "text": "so this is our input this is our input",
      "start": 4356.719,
      "duration": 4.48
    },
    {
      "text": "Matrix let me actually show it with a",
      "start": 4359.4,
      "duration": 6.08
    },
    {
      "text": "different color so it has six uh it has",
      "start": 4361.199,
      "duration": 6.48
    },
    {
      "text": "six rows and three columns so let's",
      "start": 4365.48,
      "duration": 3.96
    },
    {
      "text": "focus on the second row for now which is",
      "start": 4367.679,
      "duration": 4.241
    },
    {
      "text": "the input embedding for Journey so then",
      "start": 4369.44,
      "duration": 4.719
    },
    {
      "text": "what we do is that we first uh",
      "start": 4371.92,
      "duration": 4.6
    },
    {
      "text": "initialize a weight Matrix for query",
      "start": 4374.159,
      "duration": 4.881
    },
    {
      "text": "weight Matrix for key weight Matrix for",
      "start": 4376.52,
      "duration": 5.28
    },
    {
      "text": "value and uh we have to specify two",
      "start": 4379.04,
      "duration": 4.32
    },
    {
      "text": "things the input Dimension and the",
      "start": 4381.8,
      "duration": 3.879
    },
    {
      "text": "output Dimension the input Dimension",
      "start": 4383.36,
      "duration": 4.56
    },
    {
      "text": "here has to be the same as the vector",
      "start": 4385.679,
      "duration": 3.841
    },
    {
      "text": "embedding Dimension here because we are",
      "start": 4387.92,
      "duration": 4.68
    },
    {
      "text": "taking a we'll take uh product between",
      "start": 4389.52,
      "duration": 5.28
    },
    {
      "text": "the Matrix but the output Dimension can",
      "start": 4392.6,
      "duration": 5.0
    },
    {
      "text": "be anything generally in GPT like llms",
      "start": 4394.8,
      "duration": 4.48
    },
    {
      "text": "the output Dimension is the same as the",
      "start": 4397.6,
      "duration": 3.44
    },
    {
      "text": "input Vector Dimension but here we have",
      "start": 4399.28,
      "duration": 4.399
    },
    {
      "text": "chosen a different output Dimension so",
      "start": 4401.04,
      "duration": 4.44
    },
    {
      "text": "then what we do is we multiply all the",
      "start": 4403.679,
      "duration": 4.321
    },
    {
      "text": "input embedding vectors with the query",
      "start": 4405.48,
      "duration": 4.28
    },
    {
      "text": "weight Matrix the key weight Matrix and",
      "start": 4408.0,
      "duration": 3.8
    },
    {
      "text": "the value weight Matrix to get the",
      "start": 4409.76,
      "duration": 4.959
    },
    {
      "text": "queries Matrix the keys Matrix and the",
      "start": 4411.8,
      "duration": 6.399
    },
    {
      "text": "values Matrix so remember that these",
      "start": 4414.719,
      "duration": 8.041
    },
    {
      "text": "three the WQ w k and WV these three are",
      "start": 4418.199,
      "duration": 6.561
    },
    {
      "text": "the trainable weight matrics the",
      "start": 4422.76,
      "duration": 3.68
    },
    {
      "text": "parameters are initially initialized",
      "start": 4424.76,
      "duration": 5.959
    },
    {
      "text": "randomly but they are trained as the llm",
      "start": 4426.44,
      "duration": 7.88
    },
    {
      "text": "Lars uh okay so these are the queries",
      "start": 4430.719,
      "duration": 6.081
    },
    {
      "text": "keys and Valu Matrix then what we do is",
      "start": 4434.32,
      "duration": 5.359
    },
    {
      "text": "we take the queries uh we take a DOT",
      "start": 4436.8,
      "duration": 4.72
    },
    {
      "text": "product with the keys transposed and",
      "start": 4439.679,
      "duration": 4.441
    },
    {
      "text": "that gives us the attention scores which",
      "start": 4441.52,
      "duration": 4.32
    },
    {
      "text": "are normalized to give us the attention",
      "start": 4444.12,
      "duration": 4.48
    },
    {
      "text": "weight Matrix so if you look at Journey",
      "start": 4445.84,
      "duration": 5.6
    },
    {
      "text": "For example the first value here tells",
      "start": 4448.6,
      "duration": 5.599
    },
    {
      "text": "us the attention weight between journey",
      "start": 4451.44,
      "duration": 4.96
    },
    {
      "text": "and your the second value tells the",
      "start": 4454.199,
      "duration": 3.721
    },
    {
      "text": "attention weight between journey and",
      "start": 4456.4,
      "duration": 4.12
    },
    {
      "text": "journey similarly the last value here",
      "start": 4457.92,
      "duration": 3.92
    },
    {
      "text": "tells the attention weight between",
      "start": 4460.52,
      "duration": 3.36
    },
    {
      "text": "journey and step so this attention",
      "start": 4461.84,
      "duration": 3.72
    },
    {
      "text": "weight tells us how much you should",
      "start": 4463.88,
      "duration": 3.72
    },
    {
      "text": "attend to each word when the query is",
      "start": 4465.56,
      "duration": 4.84
    },
    {
      "text": "Journey similarly for all the other",
      "start": 4467.6,
      "duration": 5.28
    },
    {
      "text": "rows then what we do is that we take the",
      "start": 4470.4,
      "duration": 4.04
    },
    {
      "text": "attention weight Matrix and take a",
      "start": 4472.88,
      "duration": 4.0
    },
    {
      "text": "product with the values take a product",
      "start": 4474.44,
      "duration": 5.32
    },
    {
      "text": "with the values uh Matrix and then we",
      "start": 4476.88,
      "duration": 6.08
    },
    {
      "text": "finally get the context Vector there are",
      "start": 4479.76,
      "duration": 6.12
    },
    {
      "text": "uh there is one context Vector for each",
      "start": 4482.96,
      "duration": 4.679
    },
    {
      "text": "input embedding Vector so since there",
      "start": 4485.88,
      "duration": 3.88
    },
    {
      "text": "are six vectors your journey begins with",
      "start": 4487.639,
      "duration": 4.961
    },
    {
      "text": "one step the number of rows here is six",
      "start": 4489.76,
      "duration": 4.56
    },
    {
      "text": "the number of columns of the context",
      "start": 4492.6,
      "duration": 4.0
    },
    {
      "text": "Vector will will will always be equal to",
      "start": 4494.32,
      "duration": 3.96
    },
    {
      "text": "the D out Dimension which you have",
      "start": 4496.6,
      "duration": 4.32
    },
    {
      "text": "chosen here for the query key and the",
      "start": 4498.28,
      "duration": 3.64
    },
    {
      "text": "value",
      "start": 4500.92,
      "duration": 3.36
    },
    {
      "text": "Matrix so I believe this diagram",
      "start": 4501.92,
      "duration": 5.08
    },
    {
      "text": "illustrates what all we have learned so",
      "start": 4504.28,
      "duration": 7.2
    },
    {
      "text": "far uh okay so so self attention",
      "start": 4507.0,
      "duration": 6.32
    },
    {
      "text": "involves the trainable weight metrix",
      "start": 4511.48,
      "duration": 7.96
    },
    {
      "text": "metries WK WK WK WQ WK and WV these",
      "start": 4513.32,
      "duration": 8.399
    },
    {
      "text": "matrices essentially transform the input",
      "start": 4519.44,
      "duration": 4.88
    },
    {
      "text": "data into queries keys and values which",
      "start": 4521.719,
      "duration": 5.52
    },
    {
      "text": "are crucial components of the attention",
      "start": 4524.32,
      "duration": 6.319
    },
    {
      "text": "mechanism awesome now before we end this",
      "start": 4527.239,
      "duration": 7.161
    },
    {
      "text": "lecture I just want to tell you why uh",
      "start": 4530.639,
      "duration": 5.52
    },
    {
      "text": "like what is the meaning behind key",
      "start": 4534.4,
      "duration": 4.04
    },
    {
      "text": "query and value and why are we giving",
      "start": 4536.159,
      "duration": 4.201
    },
    {
      "text": "these fancy terms like key query and",
      "start": 4538.44,
      "duration": 5.44
    },
    {
      "text": "value to these so",
      "start": 4540.36,
      "duration": 6.56
    },
    {
      "text": "uh the simplest way to think of query is",
      "start": 4543.88,
      "duration": 5.12
    },
    {
      "text": "that it's analogous to search query in a",
      "start": 4546.92,
      "duration": 4.36
    },
    {
      "text": "database so it represents the current",
      "start": 4549.0,
      "duration": 5.08
    },
    {
      "text": "token the model is focusing on so if you",
      "start": 4551.28,
      "duration": 4.919
    },
    {
      "text": "ever find your s worried about what is",
      "start": 4554.08,
      "duration": 3.84
    },
    {
      "text": "the query just look at just think of it",
      "start": 4556.199,
      "duration": 3.241
    },
    {
      "text": "as the current token the model is",
      "start": 4557.92,
      "duration": 4.56
    },
    {
      "text": "focusing on so if I say the query is key",
      "start": 4559.44,
      "duration": 5.36
    },
    {
      "text": "if the query is Journey I simply mean",
      "start": 4562.48,
      "duration": 3.8
    },
    {
      "text": "that currently we are focusing on the",
      "start": 4564.8,
      "duration": 5.04
    },
    {
      "text": "word Journey that's it uh key in",
      "start": 4566.28,
      "duration": 5.52
    },
    {
      "text": "attention mechanism each item in the",
      "start": 4569.84,
      "duration": 4.04
    },
    {
      "text": "input sequence has a key so keys are",
      "start": 4571.8,
      "duration": 4.72
    },
    {
      "text": "used to match with the query so even",
      "start": 4573.88,
      "duration": 5.2
    },
    {
      "text": "Keys you can think of as items in the",
      "start": 4576.52,
      "duration": 4.199
    },
    {
      "text": "input sequence that's it that's the",
      "start": 4579.08,
      "duration": 4.36
    },
    {
      "text": "simplest way to think about",
      "start": 4580.719,
      "duration": 6.081
    },
    {
      "text": "key uh so so the key and the query are",
      "start": 4583.44,
      "duration": 5.68
    },
    {
      "text": "important to get the attention uh to get",
      "start": 4586.8,
      "duration": 3.879
    },
    {
      "text": "the attention weight or the attention",
      "start": 4589.12,
      "duration": 4.559
    },
    {
      "text": "score and then finally value so value",
      "start": 4590.679,
      "duration": 4.801
    },
    {
      "text": "represents the actual content or",
      "start": 4593.679,
      "duration": 3.721
    },
    {
      "text": "representation of the input items",
      "start": 4595.48,
      "duration": 4.44
    },
    {
      "text": "themselves so once the model determines",
      "start": 4597.4,
      "duration": 4.319
    },
    {
      "text": "which keys are most relevant to the",
      "start": 4599.92,
      "duration": 4.84
    },
    {
      "text": "query it retrieves the corresponding",
      "start": 4601.719,
      "duration": 5.44
    },
    {
      "text": "values so that's where the name comes",
      "start": 4604.76,
      "duration": 5.12
    },
    {
      "text": "from so once we find the query we have",
      "start": 4607.159,
      "duration": 5.881
    },
    {
      "text": "to find which key or which word relates",
      "start": 4609.88,
      "duration": 4.68
    },
    {
      "text": "more to the query or attends to the",
      "start": 4613.04,
      "duration": 2.4
    },
    {
      "text": "query",
      "start": 4614.56,
      "duration": 3.159
    },
    {
      "text": "that's why these are called keys like in",
      "start": 4615.44,
      "duration": 5.52
    },
    {
      "text": "a dictionary setting and value the",
      "start": 4617.719,
      "duration": 4.641
    },
    {
      "text": "reason these are called values is",
      "start": 4620.96,
      "duration": 3.12
    },
    {
      "text": "because when we find the ultimate",
      "start": 4622.36,
      "duration": 3.92
    },
    {
      "text": "context Vector we use the attention",
      "start": 4624.08,
      "duration": 4.52
    },
    {
      "text": "scores and then we use the original",
      "start": 4626.28,
      "duration": 4.359
    },
    {
      "text": "input embedding value so what is the",
      "start": 4628.6,
      "duration": 4.44
    },
    {
      "text": "representation of the input items that's",
      "start": 4630.639,
      "duration": 4.921
    },
    {
      "text": "why this value term comes into the",
      "start": 4633.04,
      "duration": 4.52
    },
    {
      "text": "picture so that's the underlying",
      "start": 4635.56,
      "duration": 5.44
    },
    {
      "text": "reasoning behind the query key and the",
      "start": 4637.56,
      "duration": 7.36
    },
    {
      "text": "value okay and uh in the next lecture",
      "start": 4641.0,
      "duration": 5.96
    },
    {
      "text": "what we'll be looking at is that we'll",
      "start": 4644.92,
      "duration": 6.279
    },
    {
      "text": "be looking at causal attention so until",
      "start": 4646.96,
      "duration": 6.16
    },
    {
      "text": "now we have looked at self attention",
      "start": 4651.199,
      "duration": 4.121
    },
    {
      "text": "right in the next lecture we'll modify",
      "start": 4653.12,
      "duration": 4.68
    },
    {
      "text": "the self attention mechanism so that we",
      "start": 4655.32,
      "duration": 4.48
    },
    {
      "text": "prevent the model from accessing future",
      "start": 4657.8,
      "duration": 3.52
    },
    {
      "text": "information in the",
      "start": 4659.8,
      "duration": 3.919
    },
    {
      "text": "sequence and then after that we'll be",
      "start": 4661.32,
      "duration": 4.839
    },
    {
      "text": "looking at multi-head attention which is",
      "start": 4663.719,
      "duration": 3.92
    },
    {
      "text": "essentially splitting the attention",
      "start": 4666.159,
      "duration": 3.921
    },
    {
      "text": "mechanism into multiple heads so the",
      "start": 4667.639,
      "duration": 3.681
    },
    {
      "text": "next lectures are going to be",
      "start": 4670.08,
      "duration": 3.24
    },
    {
      "text": "interesting I know these lectures are",
      "start": 4671.32,
      "duration": 4.76
    },
    {
      "text": "becoming a bit long but attention is the",
      "start": 4673.32,
      "duration": 5.56
    },
    {
      "text": "engine of Transformers so to truly",
      "start": 4676.08,
      "duration": 4.599
    },
    {
      "text": "understand Transformers and to truly",
      "start": 4678.88,
      "duration": 3.6
    },
    {
      "text": "understand large language models we have",
      "start": 4680.679,
      "duration": 4.401
    },
    {
      "text": "to have these lectures uh and you need",
      "start": 4682.48,
      "duration": 4.28
    },
    {
      "text": "to write these things down which I'm",
      "start": 4685.08,
      "duration": 3.44
    },
    {
      "text": "teaching you you you need to write the",
      "start": 4686.76,
      "duration": 3.12
    },
    {
      "text": "codes which I will share with you",
      "start": 4688.52,
      "duration": 3.6
    },
    {
      "text": "definitely so that you develop an",
      "start": 4689.88,
      "duration": 4.0
    },
    {
      "text": "understanding for it the lectures serve",
      "start": 4692.12,
      "duration": 3.96
    },
    {
      "text": "as a good starting point to cover all",
      "start": 4693.88,
      "duration": 4.52
    },
    {
      "text": "the concepts in a clear manner I take a",
      "start": 4696.08,
      "duration": 4.36
    },
    {
      "text": "whiteboard approach intuition Theory and",
      "start": 4698.4,
      "duration": 4.839
    },
    {
      "text": "coding in a lot of detail I don't think",
      "start": 4700.44,
      "duration": 5.36
    },
    {
      "text": "any other videos or content explain",
      "start": 4703.239,
      "duration": 4.281
    },
    {
      "text": "these Concepts in the level of detail",
      "start": 4705.8,
      "duration": 3.76
    },
    {
      "text": "which we are covering here but I believe",
      "start": 4707.52,
      "duration": 3.639
    },
    {
      "text": "that once you understand the detail and",
      "start": 4709.56,
      "duration": 3.56
    },
    {
      "text": "the nuts and bolts that's when you will",
      "start": 4711.159,
      "duration": 3.56
    },
    {
      "text": "be confident to work on Research",
      "start": 4713.12,
      "duration": 3.8
    },
    {
      "text": "problems that's when you'll be confident",
      "start": 4714.719,
      "duration": 5.321
    },
    {
      "text": "to make new discoveries in the field and",
      "start": 4716.92,
      "duration": 4.96
    },
    {
      "text": "I think ultimately it all boils down to",
      "start": 4720.04,
      "duration": 4.119
    },
    {
      "text": "matrices Dimensions dot product that's",
      "start": 4721.88,
      "duration": 4.839
    },
    {
      "text": "it and Vector Calculus if you understand",
      "start": 4724.159,
      "duration": 5.801
    },
    {
      "text": "these U you'll really Master everything",
      "start": 4726.719,
      "duration": 5.241
    },
    {
      "text": "that's that's what I believe so thank",
      "start": 4729.96,
      "duration": 3.44
    },
    {
      "text": "you so much everyone I hope you are",
      "start": 4731.96,
      "duration": 3.36
    },
    {
      "text": "liking these lectures please put your",
      "start": 4733.4,
      "duration": 5.16
    },
    {
      "text": "comments in the YouTube uh comment",
      "start": 4735.32,
      "duration": 5.28
    },
    {
      "text": "section and I'll reply to them thanks",
      "start": 4738.56,
      "duration": 3.4
    },
    {
      "text": "everyone I'll see you in the next",
      "start": 4740.6,
      "duration": 4.36
    },
    {
      "text": "lecture",
      "start": 4741.96,
      "duration": 3.0
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch Series today I'm very excited for this particular lecture because today we are going to look at implementing a self attention mechanism with trainable weights we are going to look at Key query and value and we'll also see why this self attention mechanism is also called as scaled do product attention so we are now moving very closer and closer to the actual attention mechanism which is implemented in LMS such as GPT today's lecture will be a great combination of mathematics uh Theory intuition and also coding I really enjoyed learning so much about this lecture and preparing the lecture material and I've condensed all the information in today's video so let's get started before we get started I want to quickly touch upon what we covered in the previous lecture in the previous lecture we implemented a self attention mechanism without trainable weights so this is the uh sentence which we looked at the sentence which we looked at was your journey starts with one step and we saw how to convert the embedding Vector the vector embedding for every single token into a context Vector for every single token and uh let me take you through the steps we implemented to do that and I'll go to the figure which really illustrates everything yeah so what we did essentially in the last lecture was that we broke down the initial sentence into the embedding vectors which were three-dimensional input embedding vectors and then we looked at queries so we took the example of let's say Journey which is the second query your journey begins with one step and then for each such query we found the attention scores with respect to the input embedding Vector so for the first word there is a attention score between the first word and the query for the second word there is a attention score similarly for the last word there is a attention score this attention score actually quantifies how much importance should be given to each word when we look at the query which is Journey and then based on these attention scores we found the attention weights the difference between the attention weights is that attention weights sum up to one so attention scores and attention weights intuitively mean the same thing the encode information about how much the query vector and the input embedding Vector are related to each other and attention weights are normalized which means they sum up to one the way we computed the attention scores is by implementing a DOT product operation so what we did is we implemented a DOT product between the query vector and the input Vector so let let me show this to you um in figure so that you have some reference to compare yeah so what we did essentially was we had this uh Journey which is the query Vector to find the attention score we found the dot product of this Vector with all the other vectors and that give the attention scores then we normalize the attention scores to give the to get the attention weights and then finally we use the attention weights to find the context Vector so here is the context Vector for Journey and similarly we found context Vector for all the other vectors for all the other input tokens so uh the steps which we implemented in the last class can be summed up in three uh categories first we computed the attention scores for that we computed the dot product between the inputs and the query then we computed attention weights which were normalized attention scores and then we computed context vectors so context vectors are essentially the weighted sum of the attention weights and the input vectors so here's a figure which explains how we found the context Vector so we found the attention weights for the given query and then let's say the first attention weight was multiplied by the first input Vector the second attention weight was multiplied by the second input Vector similarly the last attention weight was multiplied by the last input vector and we added all of these vectors to give us the context Vector for that given query in a similar manner we found the context Vector for all the other queries in the given sentence so this is what we implemented in the last lecture and we did not have any trainable weights we did not train anything in the last lecture everything was fixed the attention scores was calculated using the dot product um the attention weights was just normalization and the context Vector was just summation of the uh attention weights multiplied by the corresponding input Vector today we are going to look at a more real life situation which is actually implemented and we are going to consider trainable weights if you have not seen the previous lecture I highly recommend you to go through the previous lecture you'll appreciate the current lecture much much more okay so in this section we are going to learn about the self attention mechanism which is used in the original Transformer architecture the GPT models and most other popular large language models the self attention mechanism is also called as scaled do product attention and in this lecture we are going to see why this name comes into the picture and where this name is derived from I'm following this particular sequence in the attention Series in the last lecture we covered simplified self attention in today's lecture we are going to cover self attention with trainable weights in the next lecture we'll look at causal attention and in the final lecture we look at multi-head attention as I explained in the previous lecture also it's impossible to cover attention in one lecture that's why I have designed these extensive lectures to teach you the concept in a very proper manner it might get a bit complex at times but if you understand the concept you will Master Transformers because this is the heart this is the engine of Transformers and I'll show everything from scratch right up to the last dot product I'll multiply all matrices directly in front of you so that you understand matrix multiplication it's very important to do things on a whiteboard because then the understanding is improved much more okay so what we want to do in today's lecture is that we want to compute the context Vector for every given input token so the objective of today's lecture is the same as the objective was in the last lecture remember what we did previously in the previous lecture we found we took this sentence your journey starts with one step we we had the input embedding Vector for each of these tokens and then we found a context Vector for each of these tokens so this graph here shows the input embedding Vector for every token and the red the journey context it shows the context Vector for Journey similarly we found the context Vector for all the other input vectors to refresh your understanding the context Vector can be thought of as an enriched input embedding Vector so if you look at the word journe here the embedding Vector for Journey uh the embedding Vector for Journey Only in encapsulates or encodes the semantic meaning but it really contains no information about how that word Journey relates to the other words right the context Vector for Journey on the other hand has more information it not only contains the meaning of Journey but it also contains how Journey relates to step your with and one that's why the context Vector is thought of as an enriched embedding vector awesome so today what we are going to do is we are going to introduce weight matrices which are eventually optimized when the large language model is trained now these trainable weight matrices are very crucial because the model then learns to produce good context vectors in the last lecture we just uh looked at context vectors by essentially taking the dot product to get the attention weights that's it right we did not train anything but we'll see how these train weight matrices are constructed and once these weight matrices are trained the model can learn to produce good context Vector for every token so at the heart of this trainable weight matrices are three terminologies query key and value let me repeat that again we are going to implement the self attention mechanism step by step by introducing three trainable weight matrices the first is called the weight Matrix for query the second is called weight Matrix for key and the third is called the weight Matrix for Value so these three terminologies will show up again and again and again query key and value and let me show you a diagram which illustrates what these terminologies actually mean so uh here I have mentioned here step number one step number one which we are going to learn in today's lecture is how to convert in input embeddings which are the input vectors into key query and value vectors remember the goal here is the same as the last lecture we want to get from the input embeddings to context embeddings for every token but there are number of steps to be done and the first step here is to convert the input embeddings into key query and value vectors let's see what we mean by that and how to get these key query and value vectors okay so here are my inputs your journey start with one step right these are my uh six inputs and I've represented these six inputs as threedimensional vectors which you can also see in the graph below so if you look at the input Matrix the first row of this Matrix represents the three dimensional Vector for y the word Y the second row of this Matrix represents the three-dimensional Vector for Journey which can also be plotted here similarly the last row represents the three-dimensional Vector for the input word or the input token step right this is how the input embeddings are that's given to us now the next step to construct the query key and value Matrix is to look at three trainable weight Matrix the first trainable weight Matrix is called as the query weight Matrix the second trainable weight Matrix is called as the key weight Matrix and the third trainable weight Matrix is the value weight Matrix so what we going to do is that let's look at these weight matrices uh and let's also focus on the dimensions here so the input has Dimensions 6x3 because there are six rows one row for each input token and why three because the dimension Vector Dimension size is three now let's look at uh uh the query key and value trainable weight matrices so this W key W uh WQ W K and W V these three matrices which I've written over here I've initialized them with some random values but these are the ones which are actually trained we do not know these parameters so we initialize them randomly and then train them so to get context vectors later which we'll see in today's lecture these are the ones which are optimized now what these uh matrices do is actually they project the inputs into a different dimension space let me tell you what I mean by that so first let's focus on the qu query Matrix so if you look at the query Matrix and if you see the dimensions it's 3x2 uh so it has three rows and two columns so if you multiply the inputs if you multiply the input Matrix with the query weight Matrix what you will get is the resultant Matrix which is called as the queries so this is the queries Matrix and it's a 6x2 matrix so what has been done essentially is that each row here still corresponds to the individual words so the first row corresponds to your the second row corresponds to Journey the third row corresponds to begins with one and step but you can see here that the dimension has been changed usually when we train GPT the dimension is preserved but here I'm just illustrating that the dimensions can be changed when you multiply with the weight query Matrix so the simplest way to think of the query Matrix and all the other weight matrices is the transformation from let's say a threedimensional space into in this case a two- dimensional space uh so when we multiply the input Matrix with the query weight Matrix we get the queries Matrix which is a 6x2 matrix okay so now you can think of each row as corresponding to the corresponding input token your journey begins with one step the second weight Matrix is the key weight Matrix and that's all also 3x2 Matrix and very similar to what we did for the queries we'll multiply the inputs with the keys weight Matrix and then finally we'll have the keys Matrix which is again a 6x2 we'll see what these different uh matrices mean what's the meaning of query key and values but for now let's just look at the mathematical details of the implementation right and similarly to get the values to get the values Matrix we have to multiply the inputs with the weight Matrix for values and the weight Matrix for values is also 3x2 Matrix and when we multiply the inputs with values we again get a 6x2 so the way to interpret the uh query key and the value is that every row of the query key and value essentially represents one token and a representation for that token so henceforth after we get the queries key and values we are not going to look at the inut embeddings again the input embeddings have been transformed into three ve into three matrices the query the query Matrix the key Matrix and the value Matrix and remember this transformation is not fixed the key the key to these Transformations are these three weight matrices WQ w k and WV the parameters of these weight matrices are to be optimized later that's why these are called as the trainable weight matrices for now just think that what we have done in this first step is we have taken the input Matrix and we have converted the input Matrix into three other Matrix matrices queries keys and values and the way we have done that conversion is by multiplication of the input embedding with the trainable query Matrix with the trainable key Matrix and the trainable value Matrix and so we have these three queries keys and values Matrix which is constructed so you might be thinking why do we have these three and how do we get the attention scores how do we get the context vectors don't worry we'll come to all of that but first let's go to code and let's try to implement uh all of these okay so I'm going to take you through code right now so the first thing is to construct the inputs so we'll have as I mentioned the inputs is essentially a matrix which has six rows uh it has six rows and it has three columns so that's what I'm going to Define here the inputs is a tensor with six rows and three columns so each row corresponds to a particular word let's say journey is a three-dimensional Vector so I'm going to run this right now and you'll see that this block has been run and what we are going to do is that the next thing what we are going to do is initialize the query key and the value query key and the value Matrix query key and the value weight Matrix right and for that we need to define the dimensions right so uh here the dimensions are 3x2 the three has to be equal to the vector dimension of the input so that has to match because we are doing a matrix multiplication here so for all of these three weight query key and value weight Matrix the first value of the dimension three has to match the vector Dimension but the second dimension can be anything so uh if you when you initialize the query key and the value we are initializing random values and the shape of the Matrix is D in comma D out remember D in is just the shape of the input so we are just looking at one particular Vector of the input that's the vector Dimension three so that's also the first uh argument of the shape of the key query and value Matrix this exactly what we saw here this first argument has to be same as the uh Vector Dimension which is three and the out Dimension we are choosing to be two in this case so note that in GPT like models the input and out output dimensions are usually the same but for illustration purposes we are choosing the input Dimension is three and the output Dimension is two over here so these are the query key and the value weight matrices and each element in these weight matrices has been initialized in a random manner so you can actually see tor. nn. parameter and you can see the documentation for this uh yeah so what so you can see what it does it's uh so it's a kind of tensor that is to to be considered a module parameter and inside this you can pass some things so what we are doing is that we are passing the parameters to be random values with the shape of D in comma D out which means that the Matrix shape for query key and value will be 3 comma 2 so you can print out the trainable weight matrices so this is the trainable weight Matrix for query it's a three three row by two column tensor this is the trainable weight Matrix for key it's a three row and two column tensor and this is the trainable weight Matrix for Value this again is a three row and two column tensor awesome uh some minor things is that we are setting this requires grad right now to false but remember that uh later we are going to train the values in these matrices through back propagation so at that time we'll need to compute gradients and at that time we'll set the requires grad to be equal to True great so now we have computed the key query and value um key Quant value weight trainable weight Matrix right now we have to do the multiplication of the inputs with these Matrix to get the queries key and values uh final Matrix so to do that first what we can do is we can look at an individual element so let's say we look at this second element which is Journey and let's say we want to First convert this second element journey into its corresponding query uh let me show you how we can do that but first let me rub um some of the things here so that I can easily show you how we get the key query and value Matrix for Journey okay so what we are doing here is that we are focusing our attention um also let me change the color yeah so we are focusing our our attention on this second um input Vector which is Journey and then we want to get the query key and value Vector for Journey So currently the input Vector is a three-dimensional Vector now we need to get the two-dimensional Vector so the way to get that is look at these queries keys and values so the second row here will be the corresponding query Vector for Journey the second row here will be the corresponding key Vector for journey and the second row here will be the corresponding value Vector for journey so the way you get the this second element is just you take the uh you take the journey and uh then you are going to uh dot product it with this uh weight Matrix q and similarly for key and similarly for Value so this is what we are going to see in code right now um instead of directly showing you the matrix multiplication first I wanted to show it to you for each individual element so remember this if you look at each individual element this is one row and three columns so we can multiply it with the query three row two column and that will give us a one row two column which is the query Vector for Journey similarly for keys and values this is what I'm going to show you in code right now so uh let's look at the second element X2 and X2 has been defined earlier as inputs accessed and the index is one so since python has a zero indexing system inputs one will essentially be the input for Journey so that is defined by xor2 so we are going to uh find the query corresponding to this xor2 by multiplying it with the weight Matrix for query we are going to find the key for the journey vector by multiplying it with the key Matrix and we are going to find the value for the journey vector by multiplying it with the value weight Matrix and here you can see we get 4306 and 1.45 51 which is the query Vector for uh so I'm just printing the query here so this is the query Vector for Journey and let's see whether it matches so the query Vector for Journey was indeed 43 and 1.45 here if you can see uh what I'm showing in the color right now and that exactly matches what we have in code awesome so we are currently moving in the right direction as we can see based on the output the result is a two dimensional t two dimensional vector now what we can do is that we can actually obtain the keys and values uh and queries for all the different inputs this is exactly what we had written over here once we get the trainable weight Matrix we can just multiply the inputs with these weight Matrix and get the get the queries Matrix get the keys Matrix uh get the keys Matrix and get the values Matrix so this is what I'm going to show to you in code right now okay so to get the keys Matrix we just multiply the inputs with the weight Matrix for keys to get the values Matrix we just multiply the inputs with the weight Matrix for values and to get the queries Matrix we just multiply the inputs with the weight Matrix for query and uh let me just run this right now so if you run this you can just print out the shape of the keys values and queries and as expected it's 6 by two so we have six rows and two columns uh why six because there are six input tokens your journey begins with one step and for each input token we have a twood dimensional key Vector two dimensional value vector and two dimensional query Vector so as we can tell from the outputs we have successfully projected the six input tokens from a 3D input embedding space from a 2d embedding space for the keys values and queries okay so if you have understood up till now you have essentially understood the first part of today's lecture and and the first part was how to convert input embeddings how to convert input embeddings into key query and value vectors awesome now we are ready to move to step number two and uh before that let me just show you a schematic of what we have done until now so what we have done is that we have the inputs right and we have converted the inputs into their embedding the threedimensional embedding vectors then what we did was we had a key query and value and we multip IED every input with the key query and the value so here I'm showing keys and values so you can multiply every input with the keys queries and value to get the uh key query and value Matrix for every single input embedding that's what we have done until now essentially every input embedding Vector has been multiplied by with the key query and value trainable weight Matrix to get the final key query and value matrices for all the inputs so for the rest of the lecture imagine that we don't have the input embeddings at all we will only deal with the key query and value matrices so now let's move to the next step in the next step uh we have to compute the attention scores what is meant by attention scores we have to essentially compute that if you are given a query U let's say if you are given a particular query uh how does the other Keys attend to the that query let me explain this to you so let's say uh we have the query for the second word which is Journey and this is the query which is a two-dimensional Vector now we have to find out how does this query attend to the keys for the different input words so you can think of the keys right now as just individual tokens like what we did in the previous class remember in the previous class the query was just the just that particular token we did not have a separate Vector for query we just the journey itself was the query the journey token itself was the query so if ever you get confused in terms of intuition just think of the query as being the token itself although it's a bit different so what we are essentially doing is finding how that particular query so now we are looking at query number two which is related to journey and we want to see how the other words attend to Journey which means that when I'm predicting the next word how much in importance should I give to your how much importance should I give to Journey how much importance should I give to with one and step so my query is Journey and I'm going to look at how much importance I need to give to the other words so that's why we need to find the attention scores between the query and the key remember this intuition is very very important right now what we are essentially doing is that we are finding the attention score between the query and the key in the previous leure we just found the attention score between the input embedding vector and the other embedding vectors by taking a DOT product but now remember we don't have the input embeding space at all we are in an abstract space we are in the query key and value space so we are going to find how the query number two attends to the different key vectors and we are going to find it in the exact same manner as we did in the last class remember the mathematical operation which helps us to find whether two vectors are aligned or not that is the dot product operation so let's say if this is my query vector and if uh let me show the key Vector in some other color so let's say this is my query vector and let's say this is my key Vector these two vectors are very much aligned with each other right so the dot product will be maximum so it says that when I look at the query I should probably pay more attention to this key Vector whereas let's look at another key key Vector now which is like this so let's say there is another key Vector which is like this so now if you look at the query and this second key Vector they have a 90\u00b0 angle with each other they are not at all aligned which means that when you look at that query you should not pay attention to this green key over here and that is encapsulated by the dot product if you find the dot product between the yellow vector and the green Vector they have a 90\u00b0 angles the dot product will be zero that's what we are going to do now we are going to find the the attention scores between the particular query and all the other Keys remember every every query will have an attention score with all the other keys so for example if you look at query number two for Journey it will have an attention score with key number one it will have an attention score with the key number two and similarly it will have an attention score for the final key uh which is Step so this is what we are going to do next and let me show this to you in a picture tutorial representation right now okay so the way we are going to do this is by initially only focusing on the queries and keys okay so let's say we look at so this is our queries Matrix which is a 6x2 and I'm going to now focus on the second row of this because I'm going to look at Journey I'm going to look at the word Journey so the query for the word journey is the second row right now what I actually want to do is I want to find the uh dot product between this query and all the other key so I want to find the dot product between this query and the first row with the second row with the third row with the fourth row with the fifth row and with the sixth row so to essentially find the attention score for the second query all we need to do is we need to take the that particular row and we need to find the dot product with all the other rows of the keys so we'll have six attention scores and that those attention scores contain the information that when you look at the query for Journey how much importance should be given to other words like your journey begins with one step so this is what we are going to implement in code right now so I'm going to look at the keys of one um so I'm going to look at this keys so Keys 2 is keys of one which means the key for Journey uh which I have highlighted over here so this this is actually my key Keys underscore 2 so the keys uncr 2 in the code is the keys for Journey uh and then what I'm going to do is I'm going to uh take the dot product between actually before that let me right yeah so let me correct that a bit so this actually is queries so this is actually queries unroll 2 so query unroll 2 is is this query so let me reframe that again uh so here what I have is query query uncore 2 and to find the attention score for this query I'm going to take a DOT product between the second query and the keys Matrix so this is exactly what has been done in the in the code here so to find the attention score between the query 2 which is the query for Journey we are going to take a DOT product between the query and the keys transpose why are we taking a transpose here because look at the dimensions always look at the dimensions uh the dimensions for query is that it's one row and it's uh it's one row and it's two columns so we cannot directly multiply it with the keys because the keys has six rows and two columns whereas keys transpose we have two rows and six columns and that can be multiplied so then Keys transpose will be 2x 6 so let me write that Dimension down so Keys transpose will be 2x 6 and if you multiply 1X two Matrix with 2x 6 you'll get 1X 6 so you'll get six attention scores for all the different uh for the six input tokens your journey begins with one step so that sounds correct so this is what we are going to do over here so the attention scores two which is the attention score for Journey is just a matrix product between the query for Journey multiplied with the keys transpose so you get the six attention scores here so as you can see the first attention score is the encodes information about how much Journey attends to your the second attention score encodes information about how much Journey attends to Journey the third attention score encodes information about how much Journey attends with width similarly the last attention score encodes information about how Journey attends with step so the second score is the highest because of course journey and journey will be intuitively more aligned to each other but remember these scores don't mean anything right now because we have not trained any of the weight matrices these scores will only mean something when you train the weight matrices so ideally if you have a long paragraph and and if journey and step are more related to each other in that paragraph after the matrices are trained this last value which is the attention score between journey and step has to be the highest so up till now what I showed you is how to find the attention score the six values of the attention score for one query but now what if you want to find the attention score for all the other queries for the first query second third fourth fifth sixth the simplest way to do that is just for the second query you just multiplied it with the keys transpose right and similarly you'll do for all queries so why don't you just do a matrix multiplication so you multiply the queries Matrix with keys transpose and I've shown this over here for your reference so this is the query's Matrix which is 6x2 and the keys transpose is 2x6 so of course 6x2 can be multiplied with the 2x6 Matrix and ultimately you'll get a matrix like this which is a 6x6 Matrix and that is our attention score Matrix now what does this attention score Matrix symbolize so the first row is contains the attention scores between the first query and all all the other Keys the second row contains the attention scores between the second query and all the other Keys similarly the last row contains the attention score between the last query and all the other Keys that's the simple meaning of the attention scores and I'm again going to do this in code right now to get the attention scores I just take the matrix multiplication of queries with keys transpose and then you get the 6x6 uh attention scores Matrix that's it so we have calculated the attention score between every query with respect to all the other Keys awesome uh this is the second step and we have still not yet got to the context Vector so the first step was to convert the input embedding vectors to the key query and value the Second Step was to use the key and the query to get to the attention scores now the problem with these attention scores is that they are not interpretable right ideally I want to be able to let's say if I look at this second uh this second row which are the attention scores for the query journey I want to be able to make the statements like okay pay 10% attention to your pay 20% attention to Journey pay 30% attention to step pay 40% attention to width but I'm not able to make these interpretable statements because if you look at all these attention scores they do not sum up to one these look like random values that they are not summing up to one so we have to do the next step of normalization so normalization serves two purposes first it will help make things interpretable so I can make statements like okay when the query is Journey the you pay 20% attention to the first token 30% attention to the final token Etc and the second advantage of normalization is that it helps when we do back propagation generally in many machine learning Frameworks it's better to normalize things so that the scale stay consistent between zero and one so the third step what we are going to do is that we are going to compute uh the is terminology which is called as attention weights so up till now we have calculated attention scores that's fine now we are going to just normalize the attention weights uh so that the attention scores in each row sum up to one so there is a difference between attention scores and weights the meaning is the same but attention weights sum up to one they are normalized in the previous lecture what we did for normalization is that we looked at each row and we simply took the soft Max right uh and the soft Max function actually ensures that all the elements sum up to one I'm not going to cover the soft Max implementation in today's lecture because we have already seen it in the last lecture in a lot of detail but remember that softmax just make sure that all of the these quantities sum up to one and they lie between zero and one and they are positive but actually in today's lecture before we Implement soft Max there is one more very important step which is actually done and I'll come to why this step is done but remember that before implementing soft Max what is done is that all of these values are taken and they are scaled by something which is called as square root of the keys Dimension So currently the dimension of the keys is a it's two two Dimension right because remember the keys queries and the values Matrix we uh we took the three-dimensional input embedding and we transformed it into two dimensional so the dimension of the keys in this case is two uh so we are going to to scale everything by square root of two why two because if you look at the um let's look at the key Matrix again yeah this is my key Matrix right now and if you look at every uh if you look at every token here it has two Dimensions right so that's why we are going to uh we are going to scale by square root of two and you might be thinking okay this looks like magic who thought about the square root why do we do the scaling there is a very nice reason for that and I'm going to come to it for now just remember that after getting the attention score we are going to scale it by square root of D and that's why it's also called as scaled dot product attention remember we saw at the start the scaled we are going to scale by square root of D that that is one of the reasons why it's called scaled dot product attention so we are going to scale it and then we are going to apply soft Max so when we scale by square root of two it leads to this Matrix over here and then we have apply soft Max so you'll see that when soft Max activation is applied and if you look at each row right now you'll see that they sum up to one so if I look at the second row right now uh it corresponds to again um Journey so I can now confidently make statements like when the query is Journey pay 15% attention to your pay 22% attention to Journey itself pay 22% attention to with to begins pay 30 15% attention to width pay only 9% attention to one and pay around 18% attention to step remember these weights are not optimized but when they are optimized we can make interpretable statements like these all of the rows will sum up to one you can check these and this is called as the attention weight Matrix this is one of the most important step uh in getting to the context Vector please remember this step and uh we did two things here we scaled by the square root of the dimension and then we implement soft Max now let's go go to code and let's implement this in the process we'll also understand why we do the scaling by square root of D okay yeah so we compute the attention weights by scaling the attention scores and using the soft Max function the difference to earlier when I say earlier it's the previous lectures is now we scale the attention scores by dividing them by the square root of the embedding dimension of the keys and that embedding Dimensions is two so we are dividing by square root of two so let's do the same thing here so we have this attention scores 2 so it's a 6x6 Matrix which we have printed out here actually currently I'm just employing this on the attention scores for the second query okay no problem so attention scores 2 is the attention scores for the query 2 which is Journey so it's actually one row and six columns so what I'm going to do is that I'm going to take this attention scores for Journey I'm going to first divide it by the square root of the keys Dimension and then uh what I'm going to do is that I'm going to apply soft Max the reason we do dim equal to minus one is that because we have to sum over all the columns and all the if so that's why when you look at one row you'll see that it sums up to one so two things are important this D of K is the dimension it's the keys do shape minus one because we are looking at the column remember keys do shape is 3x 2 so when you do keys do shape and index it by minus one the result will be two so we are going to uh divide by square root of two uh and in Python remember here we are exponen by 05 so into into .5 means rais to.5 that is the same as dividing by the square root of two so every element will be first divided by the square root of two in the attention score and then we implement the soft Max so if you look at the attention weights for Journey you'll see that these are the attention weights let's actually check whether these are correct to what we saw yeah so let's look at the second row here the second row is 0.15 2264 etc for Journey and here you will see that the second the output is exactly the same that's a good sanity check and you'll see that all of this sum up to one so this is how we calculate the attention weights for one uh query and similarly we calculate the attention weights for all the queries so if I just replace this with attention scores which is the 6x6 we'll get the attention weight Matrix which is a 6x6 Matrix okay now let's come to the question which all of you might be thinking and I don't think this is covered enough in other lectures and other videos but it's a very fascinating thing I took some time to understand this and I've come up with two reasons why we actually divide by the square root of Dimension the first reason is stabil in learning and let me illustrate this with an example so let's say if you have this tensor of values which is 0.1 min-2 3 -2 and .5 okay if you take the soft Max of this uh versus now let's say if you multiply this with eight and then you take a soft Max so you'll see that the soft Max of the first is kind of it's good right it's diffused these values are diffused between 0o and one but if if you look at the soft Max of the second tensor you'll see that the values are disproportionately high which means that if there are some values in the original tensor if some values are very high and when you take the soft Max you'll get such kind of peaks in the softmax output I've actually explained this better here so the softmax function is sensitive to the magnitude of its inputs when the inputs are large the difference between the exponential value of each input becomes much more pronounced this causes the softmax out output to become pey where the highest value receives almost all the probability mass and we can check it here so when we multiply with uh8 you'll see that this has the highest value which is four and when we take the soft Max you'll see that the value is 08 here which is much higher than all the other values in fact it's around 10 to 15 times higher that's what is meant by softmax becomes pey if the values inside the soft Max are very large so we don't want the values inside the soft Max to be very large and that's one reason why we scale or divide by the square root to reduce the values itself before taking soft Max we make sure that the values are not large and that's why we divide by the square root Factor so in attention mechanisms particularly in Transformers if the dot product between the query and the key Vector remember that we are ultimately applying soft Max on the dot product between query and key right because attention scores are just dot product between query and key and if the dot product becomes too large like multiplying by eight in the current example which we saw the attention scores can become very large and we don't want that this results in a very sharp softmax distribution and uh such sharp softmax distribution can become so the model can become overly confident in one particular key so in this case the model has become very confident in this fourth key or rather this uh fifth key we don't want that because that can make learning very unstable that's the first reason why we divide by square root to make sure that the values are not very large and to have stability in learning but still so I I knew this reason but then I was thinking but why square root why are we dividing by square root why why not just only the dimension what is the reason behind dividing by square root and then I came across a wonderful justification for this so so the reason for square root is that it's actually related to the variance uh so it turns out that the dot product of Q and K increases the variance because multiplying two random numbers increase the variance so remember that when we get to the attention scores we are multiplying q and K right the query and the key it turns out that if you don't divide by anything the higher the dimensions of these vectors whose dot product you are taking the variance goes on increasing that much and dividing by the square root of Dimension keeps the variance close to one let me explain this also with an example so let's say we have a query Vector which is generated randomly and a key Vector which is generated randomly uh okay and currently let's say I'm doing a five dimensional Vector so let's say I have a key Vector five dimensional key Vector which is sampled from a normal distribution and a five dimensional query Vector sampled from a normal distribution and then I'm taking a DOT product between the query and the key and then I'm also in the second case taking dividing by the square root of the dimension okay and I'm doing this thousand times so that I can get a distribution over the dot product so after I do this a thousand times what I do is I plot the variance before scaling and I plot the variance of the dot product after scaling so the results are surprising if the dimension is equal to five the variance of the dot product before scaling is actually very close to five if the dimension before scaling is 20 the variance before scaling is very close to 20 this indicate that if the dimensions of the query and key vectors go on increasing and if you don't scale then the variance of the resulting dot product scales proportionately so if you have 100 dimensional key and query Vector the variance before scaling will be close to 100 and we can actually test this out so here let me do this 100 and compute variance 100 so now I'm printing this for 100 and let me print this out okay I think I should replace this also with 100 uh and let me print this out okay so this is exactly what we are predicted right so the variance before scaling in this case is 107 uh see so as the dimensions increase the variance increases now look at the power of scaling when you scale by the square root so see here we are scaling by the square root when you scale by the square root of Dimensions no matter how much you increase the dimension if you see the variance after scaling the variance is always close to one and that's the reason why square root is used if you don't use a square root the variance will not be close to one so let me actually not use the square root here and let me do it directly uh if you do it directly then you will see that the variance after scaling are some random values they are not close to one having the square root actually really uh having the square root make sure that even if the dimensions increase the variance after scaling remains close to one of the dot product between the query and the key and this is very important uh the reason why the variance should be close to one is that if the variance increases a lot it again makes the learning very unstable and we don't want that we want to keep the standard deviation of the variance closed so that the learning does not fly off in random directions and the values the variance generally should stay to one that that helps in the back propagation and that's also generally better for uh avoiding any computational issues so that's the reason why uh we want the variance to be close to one so this is the second reason why we especially use square root so uh there are two reasons the first reason is of course we want the values to be as small as possible this helps uh if the values are not small the soft Max becomes pey and then it starts giving preferential values to Keys which we don't want it can make the learning unstable but why square root the reason why square root is because specifically when you take the dot product between query and key to find the attention and if you don't scale as the dimensions of the query and key increase the dot product variance can become huge we don't want that because again that will make learning unstable so scaling by the square root makes the variance close to one so if you see after scaling it keeps the variance close to one and that's why we divide by the square root it's very important for you to have this understanding and not many people have this understanding but I hope I've clarified this um concept to you and you have appreciated why we are dividing by square root that's why this is also called as scaled dot product attention because we scale by the square root okay so now until now we have reached a stage where we have essentially computed the attention weights and now we essentially come to the last step which is now we are ready to compute the context Vector so let's go ahead and actually compute the context Vector but first what I want to do is I want to show you uh pictorially what all we have done until now so let's see what all we have done until now is that let's say if you focus on a particular query we have found the attention score between the query and all the input keys by taking a DOT product between the query and the keys so the attention scores are shown in the blue over here and then what we do is we divide by the square root of the key Dimension and then we normalize using soft Max and then we have found the attention weights uh and the attention weights sum up to one awesome so we have reached this stage and the final step essentially is to compute the context vectors so let's come to that right now so until now you might be thinking that I've used the key and the query but what about the value why did we even get the value Matrix the value Matrix will be useful in the final step so remember for every input embedding Vector we have also calculated the value Vector so the way the context text Vector is now found out is that we have the attention weights right so we just so for the first input embedding we multiply the value Vector with the first attention weight we multiply the second value Vector with the second attention weight similarly we multiply the last value Vector with the last attention weight and we are going to add all of these together and this is going to give us the final context Vector it's very similar to what we did earlier remember earlier we did not have this value Vector the value Vector was just the input embedding vector but the whole Essence here is that now we have calculated the attention weight so now it's time to assign the weightage assign the corresponding weightage to each input embedding vector or the value vector and sum them up to give the context Vector I'll show you intuitively what this means in a minute uh but let me take you to this whiteboard right now so that I can show you the next step okay so we have calculated the attention weights now and now we'll be calculating the cont context Vector so let me show you mathematically how we compute the context Vector so we have these attention weights which is a 6x6 Matrix and we have this values which you computed at the start of this lecture we have this value matrix it's a 6x2 matrix right so the first row are the values for your the second row are the values for Journey similarly the sixth row is the value for step now let's say we look at Journey uh and I want to find the context Vector for Journey and let's look at the attention weights for Journey it's the second row let me show you intuitively how you find the context Vector for Journey and you'll never forget this after I show the illustration okay so let's say these are the uh value vectors for the different tokens so this is the value Vector for Journey and let's say this is uh 3951 and 1 the way we do the uh context Vector calculation is that let's look at the attention weights so the attention weights for the journey are this second row which means 0.15 2264 Etc so this means that I'm paying 15% attention to your I am paying 22% attention to journey I am paying 22% attention to begins I'm paying only 13% to width I'm paying only 9% to 1 and I'm only paying 18% to step okay how do I encode all of this information to find the context Vector it's pretty simple you take the your vector and you multiply it by5 because it only contributes 15% you take the journey Vector you multiply it by 22 because it only contributes 22% you take the begins Vector you multiply it by 2199 because that also contributes only 22% similarly you take the one vector you multiply it with 0.09 because it contributes very less you take the step Vector you multiply it by8 because it only contributes 18% and then you add all of the small contributions together to give you the context Vector for Journey let me show you how this looks like so your the attention for your is how much 15 right so you will scale you will scale the your Vector let me show this with a different color you'll scale the your vector by5 the attention score for Journey was 22 so you'll scale it by 22 for starts was also 22 and for width let's see how much it was for width for width it was3 so for width and one it was very low so for width and one they they make very less contributions for step it was around5 so now you have the six vectors and you will add add all of these six vectors together to give you the context Vector for Journey so when you add all of the six vectors together it will give you the context Vector for Journey if you have this kind of a visual representation in mind you will never forget what context Vector means now do you understand why the context Vector is richer than just the input embedding Vector if you just look at the input embedding Vector for Journey it has no information about how much attention should be paid to your step one withd and starts but now if you since you have this attention weight Matrix since you have this attention weight Matrix over here you exactly know how much relative importance should be paid to each of the other words so you scale the other vectors by that much amount and then you add all the vectors together to get the context Vector so the context Vector is an enriched Vector it contains the semantic meaning of Journey plus it also contains how all the other words attend to Journey remember none of these rates are optimized right now we are we have just initialized them randomly but when the llm is trained all of these context vectors will be perfectly optimized so you would know that in that particular sentence in that particular paragraph which word uh should Journey pay most attention to now this exact thing which I've shown you in uh in the graphical format can be computed in Matrix if you just multiply the attention weights with the values so if you multiply the attention weights with the values your multiplying a 6x6 Matrix with a 6x2 so of course the matrix multiplication is possible and the resultant will be a 6x2 matrix like this so this is a 6x2 matrix which is a context Vector Matrix and each row corresponds to a context Vector for that token so if you look at the second row over here the second row corresponds to the context Vector for Journey which we have shown over here the first row corresponds to the context Vector for your similarly the last row corresponds to the context Vector for step one exercise I want to give you is that uh use this this visual representation of scaling so take the second row take journey and uh use the scaling approach which I showed you in the graphical representation so take the vector for your multiply it by 15 take the vector for Journey multiply it by 22 similarly take the vector for step multiply it by8 add them all together and see whether the result matches with the second row over here that will give you an intuition of why this matrix multiplication actually gives us the exact same result as this graphical intuition based calculation which we did over here but if you forget this Matrix formula just remember the scaling based approach which we discussed in this graphical intuition and you will get the exact same value so remember that the context Vector Matrix is just a matrix product of attention weights and values attention weights multiplied by the values Matrix gives us the context Vector Matrix and this is exactly what we are going to implement in code right now uh so let us go to code yeah so we saw this we saw the square root and now we are going to uh implement the context Vector so remember that context Vector first we are going to only see the context Vector for Journey and it's the product between the attention Matrix attention weight for Journey multiplied by values let me explain this a bit so uh on the Whiteboard what we saw is we just multiplied the entire attention weights with the value right but if you want just the uh context Vector for Journey what you can do is just take the second row it will be uh 1X 6 and you multiply it with this values which is 6x2 and then you'll get a 1x two Vector which is the second row here and that will be the context Vector for Journey so this is what I have showed over here the context Vector 2 which is the context Vector for journey is just the product of the attention weights for Journey multiplied by the values Matrix and the result is 3061 and 8210 and let's actually see the result here and that exactly matches the second row which we have 3061 and 8210 awesome so our calculation seems to be correct so in the code right now we have only computed the single context Vector right now we are going to generalize the code a bit to compute all the context vectors it's going to be very simple because now we just multiply the attention weights with the values but we'll do this in a structured manner we'll Implement a self attention python class and what this class will do is that it will essentially have a forward method this forward method will compute the keys queries values it will compute the attention scores attention weights and the context vectors all in a very short piece of code so let's do that right now before that let us summarize what all we have seen so far so that you'll understand the python class much better so let me zoom out here a bit so remember how we started the lecture we started the lecture with uh we started the lecture with taking the inputs and then multiplying them with query key and the value to get the queries Matrix the key Matrix and the value Matrix okay then remember what we did next then we move to the attention scores we multiplied the queries with the transpose of the keys to get the attention scores so we had the attention scores Matrix then what we did is we scaled this by square root of the keys Dimension and then we took the soft Max this gave us the attention weights then we took the attention weights and we multiplied it by the values Matrix and that ultimately gave us the context Vector Matrix remember this flow so the flow is in four steps step number one is at the left side of the page which is converting the input embeddings into key query value Vector step number two is getting the attention scores step number three is getting the attention weights step number four is getting the context vector that's it and we are done that's exactly what we are going to implement in this python class so uh with the llm implementation which we are going to cover next in one of the subsequent lect lectures it's very useful to organize the code in a python class so we cannot keep on writing separate lines of codes like what we did over here right it's just better to have a class so that then we can create an instance of this class and then always return the context Vector okay so we are going to Define in this class called self attention version one and it will take two attributes the input Dimension and the output Dimension the input Dimension is the input Vector embedding Dimension the output Dimension is what we want the keys query and value dimension in GPT and other llms these these two are generally similar okay first thing what we do is when an instance of this class is created this init Constructor is automatically called and the query key and the value matrixes matrices are initialized randomly which means that they have a dimension of D in and D out so D in rows in our case three rows and D out columns two columns and then each element will be initialized in a random manner then what we do is we do the forward pass what happens in the forward pass is that it takes an input it takes X as the input which is the input uh input embedding Vector that needs to be given as an input to execute the forward method and then in the forward method what we do is we first compute the keys Matrix which is X multiplied by the uh weight trainable weight Matrix for key then we compute the query Matrix which is X multiplied by the trainable Matrix for query then we compute the value Matrix which is X multiplied by the trainable Matrix for value and this is uh exactly what we saw on the left side of the board over here here so until now we are at this stage where we are taking the inputs we are multiplying it with the m weight Matrix to get the queries keys and the values and now we'll go to the right side of the board to compute the attention scores so to get the attention scores we'll multiply queries with keys transpose so that's exactly what's done here to get the attention scores we multiply queries Matrix with keys transpose then we get the attention weights to get the attention weights we'll of course apply soft Max but before applying soft Max we'll divide the attention scores every element of the attention scores with the square root of the Keys embedding Dimension so keys do shape minus one Returns the columns which is the embedding dimensions in this case it's two columns of the keys Matrix so it will be square root of two the reason we do this division as we saw is first of all to make sure the values in The Matrix in the attention score Matrix are small second it also helps to make sure that the dot product between the quiz keys and the queries uh does its variance does not scale too much so we want its variance to be very close to one that's why we specifically divide by the square root of the dimension and here the DM equal to minus one just tells the soft Max that you have to sum across the columns and that's how we make sure that each row if you take each row it sums up to one so if you look at each row of the attention weight Matrix it will sum up to one and then the context Vector is just the product of the attention weights and the values this is the last step which we saw the context vector uh yeah so this was the last step which we saw the context Vector is just the product of the attention weights and the values so this is how we compute the context Vector so some key things to mention here in this pytorch code the self attention version one is a class derived from nn. module so nn. module uh which is a fundamental building block of P torch models and that provides necessary functionalities for model layer creation and management as I mentioned mentioned to you before the init method initializes trainable weight matrices query key and value for queries keys and values each transforming the input Dimension D in into an output Dimension D out and during the forward pass which is the forward method what we do is that we compute the attention scores by multiplying queries and keys normalize the scores using soft Max and finally we create a context Vector that's the last step so this is just an explanation of the code I'll share this entire code file with you so you'll have this explanation don't worry so let's try to create an instance of this class uh so I'm creating an instance of this class with uh two with three as the input embedding Dimension and D out is equal to two so here you see I have created this uh so print essay version one inputs so these are the six embedding vectors so here is the Matrix of the six context vectors so directly returned so what this print statement does is that it Returns the context Vector so actually when you do this uh self attention version one and you pass the input many things are happening when you pass the inputs these key query value Matrix matrices are created attention scores are calculated attention weights are calculated and the context Vector is calculated which is returned over here so it has six embedding vectors so each row corresponds to the context Vector so the first row corresponds to the context Vector for first token your second row corresponds to to the context Vector for second tokken Journey Etc similarly the last row corresponds to the context token for context Vector for step so that's why the dimensions here are six rows and two columns so since the inputs contain six embedding vectors we get a matrix storing the six context Vector remember we have we want six uh context we want a context Vector for each input embedding Vector that's the main goal which we started out in today's class and we have achieved that goal over here in a very compact manner in just maybe 10 to 15 lines of code so if you have followed till here it's been a pretty long lecture you should be really proud of yourself because if you have understood until here I believe you have understood the core of the attention mechanism just write these Dimensions down once take the dot product yourself and see how the calculations play out on a book or on a piece of paper that's the best way to learn this concept I it all boils down to matrices and dimensions so as a quick check let's not notice the second row 3061 8210 and let's see whether it's the same as the context Vector for Journey which we have calculated earlier so that's the same so it's a good sanity check which means we are in the right direction now what we can do is that we can we can actually improve this self attention version one further by uh changing how these are defined So currently we are using nn. parameter right the main hypothesis is that why don't we use directly a NN do linear function because it automatically creates the uh initializes the weight Matrix in a manner which is good for computations so instead of just sampling from random values here why don't we use the linear function so that the initialization is done in a proper manner using p torch that's exactly what we do next so we can improve the self attention version one version one implementation further by utilizing the NN do linear layers of pytorch which effectively perform matrix multiplication when bias units are disabled so basically we can use nn. linear to also initialize random values of query key and the value value Matrix but the main advantage is that nn. linear has an optimized weight initialization scheme and that leads to more stable and effective model model learning you can of course use NN do parameter also but the main advantage of nn. linear is that it has a stable uh initialization scheme or rather I should say more optimized initialization scheme since we always use this for all types of neural network tasks so why not essentially uh use the linear layer we can just put the bias terms to false because we don't need this we just need to initialize a weight Matrix with d in and D out weight weight Matrix for query key and value with d in as the rows and D out as the columns but we don't need the bias Matrix so you can just use the linear lay and put the bias to false so it will initialize these weight Matrix weight matrices and that's usually more common practice for implementing the self attention class when we deal with llms so similar to uh here we created an instance of the self attention version one right similarly we can create an instance of the self attention version two and pass in the arguments as the input Dimension and the output so here again we get a six rows and two column tensor uh which is the context vectors for all the six uh input embedding vectors so you'll notice that these values are different than these values because the initialization schemes are different so they use different initial weights for the weight Matrix since nn. linear uses a more sophisticated weight initialization scheme so the linear uses usage of nn. linear leads to a more sophisticated weight initialization scheme than NN do parameter I won't go into the details of how the weights are initialized in nn. linear but you can explore this further that also is an interesting topic but the length of the lecture will increase further okay so that actually brings us to the end of today's lecture where we implemented query key value Matrix found the attention scores attention weights and the context vectors for all the input embeddings I just want to end today's lecture by uh showing you a schematic which illustrates what all we have implemented in today's class okay so at the end we implemented this self attention python class and uh this schematic actually explains everything so this is our input this is our input Matrix let me actually show it with a different color so it has six uh it has six rows and three columns so let's focus on the second row for now which is the input embedding for Journey so then what we do is that we first uh initialize a weight Matrix for query weight Matrix for key weight Matrix for value and uh we have to specify two things the input Dimension and the output Dimension the input Dimension here has to be the same as the vector embedding Dimension here because we are taking a we'll take uh product between the Matrix but the output Dimension can be anything generally in GPT like llms the output Dimension is the same as the input Vector Dimension but here we have chosen a different output Dimension so then what we do is we multiply all the input embedding vectors with the query weight Matrix the key weight Matrix and the value weight Matrix to get the queries Matrix the keys Matrix and the values Matrix so remember that these three the WQ w k and WV these three are the trainable weight matrics the parameters are initially initialized randomly but they are trained as the llm Lars uh okay so these are the queries keys and Valu Matrix then what we do is we take the queries uh we take a DOT product with the keys transposed and that gives us the attention scores which are normalized to give us the attention weight Matrix so if you look at Journey For example the first value here tells us the attention weight between journey and your the second value tells the attention weight between journey and journey similarly the last value here tells the attention weight between journey and step so this attention weight tells us how much you should attend to each word when the query is Journey similarly for all the other rows then what we do is that we take the attention weight Matrix and take a product with the values take a product with the values uh Matrix and then we finally get the context Vector there are uh there is one context Vector for each input embedding Vector so since there are six vectors your journey begins with one step the number of rows here is six the number of columns of the context Vector will will will always be equal to the D out Dimension which you have chosen here for the query key and the value Matrix so I believe this diagram illustrates what all we have learned so far uh okay so so self attention involves the trainable weight metrix metries WK WK WK WQ WK and WV these matrices essentially transform the input data into queries keys and values which are crucial components of the attention mechanism awesome now before we end this lecture I just want to tell you why uh like what is the meaning behind key query and value and why are we giving these fancy terms like key query and value to these so uh the simplest way to think of query is that it's analogous to search query in a database so it represents the current token the model is focusing on so if you ever find your s worried about what is the query just look at just think of it as the current token the model is focusing on so if I say the query is key if the query is Journey I simply mean that currently we are focusing on the word Journey that's it uh key in attention mechanism each item in the input sequence has a key so keys are used to match with the query so even Keys you can think of as items in the input sequence that's it that's the simplest way to think about key uh so so the key and the query are important to get the attention uh to get the attention weight or the attention score and then finally value so value represents the actual content or representation of the input items themselves so once the model determines which keys are most relevant to the query it retrieves the corresponding values so that's where the name comes from so once we find the query we have to find which key or which word relates more to the query or attends to the query that's why these are called keys like in a dictionary setting and value the reason these are called values is because when we find the ultimate context Vector we use the attention scores and then we use the original input embedding value so what is the representation of the input items that's why this value term comes into the picture so that's the underlying reasoning behind the query key and the value okay and uh in the next lecture what we'll be looking at is that we'll be looking at causal attention so until now we have looked at self attention right in the next lecture we'll modify the self attention mechanism so that we prevent the model from accessing future information in the sequence and then after that we'll be looking at multi-head attention which is essentially splitting the attention mechanism into multiple heads so the next lectures are going to be interesting I know these lectures are becoming a bit long but attention is the engine of Transformers so to truly understand Transformers and to truly understand large language models we have to have these lectures uh and you need to write these things down which I'm teaching you you you need to write the codes which I will share with you definitely so that you develop an understanding for it the lectures serve as a good starting point to cover all the concepts in a clear manner I take a whiteboard approach intuition Theory and coding in a lot of detail I don't think any other videos or content explain these Concepts in the level of detail which we are covering here but I believe that once you understand the detail and the nuts and bolts that's when you will be confident to work on Research problems that's when you'll be confident to make new discoveries in the field and I think ultimately it all boils down to matrices Dimensions dot product that's it and Vector Calculus if you understand these U you'll really Master everything that's that's what I believe so thank you so much everyone I hope you are liking these lectures please put your comments in the YouTube uh comment section and I'll reply to them thanks everyone I'll see you in the next lecture"
}