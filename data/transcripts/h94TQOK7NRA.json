{
  "video": {
    "video_id": "h94TQOK7NRA",
    "title": "Lecture 16: Causal Self Attention Mechanism  | Coded from scratch in Python",
    "duration": 3355.0,
    "index": 15
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.24
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.319,
      "duration": 5.161
    },
    {
      "text": "in the build large language models from",
      "start": 8.24,
      "duration": 3.399
    },
    {
      "text": "scratch",
      "start": 10.48,
      "duration": 4.119
    },
    {
      "text": "series we have been covering attention",
      "start": 11.639,
      "duration": 5.161
    },
    {
      "text": "and the attention mechanism in a lot of",
      "start": 14.599,
      "duration": 3.961
    },
    {
      "text": "detail for the past two or three",
      "start": 16.8,
      "duration": 4.2
    },
    {
      "text": "lectures the lectures were long they",
      "start": 18.56,
      "duration": 5.6
    },
    {
      "text": "were more than 1 hour each but I think",
      "start": 21.0,
      "duration": 5.32
    },
    {
      "text": "it's extremely important for you to",
      "start": 24.16,
      "duration": 4.519
    },
    {
      "text": "understand the attention mechanism and",
      "start": 26.32,
      "duration": 4.879
    },
    {
      "text": "uh that's why I'm devoting so much time",
      "start": 28.679,
      "duration": 5.281
    },
    {
      "text": "to these set of lectures just to give",
      "start": 31.199,
      "duration": 4.641
    },
    {
      "text": "you a quick recap of what all we have",
      "start": 33.96,
      "duration": 4.32
    },
    {
      "text": "covered so far in this in the first",
      "start": 35.84,
      "duration": 4.12
    },
    {
      "text": "lecture on attention we started with a",
      "start": 38.28,
      "duration": 3.92
    },
    {
      "text": "simplified self attention mechanism",
      "start": 39.96,
      "duration": 4.2
    },
    {
      "text": "without trainable weights in the",
      "start": 42.2,
      "duration": 4.199
    },
    {
      "text": "previous lecture we added trainable",
      "start": 44.16,
      "duration": 4.28
    },
    {
      "text": "weights and we discussed the self",
      "start": 46.399,
      "duration": 4.0
    },
    {
      "text": "attention mechanism with trainable",
      "start": 48.44,
      "duration": 4.08
    },
    {
      "text": "weights here we also looked at the",
      "start": 50.399,
      "duration": 4.8
    },
    {
      "text": "concept of key query and values which",
      "start": 52.52,
      "duration": 5.8
    },
    {
      "text": "we'll revise just in a moment and then",
      "start": 55.199,
      "duration": 5.561
    },
    {
      "text": "today our main aim is to learn about",
      "start": 58.32,
      "duration": 4.68
    },
    {
      "text": "something which is called as causal",
      "start": 60.76,
      "duration": 4.6
    },
    {
      "text": "attention after learning about causal",
      "start": 63.0,
      "duration": 4.32
    },
    {
      "text": "attention in the next lecture we'll move",
      "start": 65.36,
      "duration": 4.6
    },
    {
      "text": "to multi-head attention which is the",
      "start": 67.32,
      "duration": 4.76
    },
    {
      "text": "actual attention mechanism which is used",
      "start": 69.96,
      "duration": 6.0
    },
    {
      "text": "in GPT and other modern large language",
      "start": 72.08,
      "duration": 6.679
    },
    {
      "text": "models I believe this sequential flow is",
      "start": 75.96,
      "duration": 5.159
    },
    {
      "text": "extremely important because if you start",
      "start": 78.759,
      "duration": 3.801
    },
    {
      "text": "understanding multi-head attention",
      "start": 81.119,
      "duration": 3.64
    },
    {
      "text": "directly you will not understand the",
      "start": 82.56,
      "duration": 4.04
    },
    {
      "text": "basics you will not understand the nuts",
      "start": 84.759,
      "duration": 4.68
    },
    {
      "text": "and bolts of what exactly is attention",
      "start": 86.6,
      "duration": 5.44
    },
    {
      "text": "how did we we get to multihead attention",
      "start": 89.439,
      "duration": 4.121
    },
    {
      "text": "so let's get started with today's",
      "start": 92.04,
      "duration": 3.68
    },
    {
      "text": "lecture on causal",
      "start": 93.56,
      "duration": 4.72
    },
    {
      "text": "attention Okay first let us revise",
      "start": 95.72,
      "duration": 4.6
    },
    {
      "text": "everything we know about self attention",
      "start": 98.28,
      "duration": 3.76
    },
    {
      "text": "what we have learned in the previous",
      "start": 100.32,
      "duration": 4.28
    },
    {
      "text": "lectures so the example which we have",
      "start": 102.04,
      "duration": 5.439
    },
    {
      "text": "started looking is this one sentence",
      "start": 104.6,
      "duration": 5.479
    },
    {
      "text": "which is your journey starts with one",
      "start": 107.479,
      "duration": 5.32
    },
    {
      "text": "step so you'll see that there are six",
      "start": 110.079,
      "duration": 7.0
    },
    {
      "text": "words in this sentence the first step of",
      "start": 112.799,
      "duration": 7.401
    },
    {
      "text": "any um data processing pipeline for a",
      "start": 117.079,
      "duration": 5.161
    },
    {
      "text": "large language model is to convert these",
      "start": 120.2,
      "duration": 4.959
    },
    {
      "text": "words into tokens to convert these",
      "start": 122.24,
      "duration": 5.999
    },
    {
      "text": "tokens into token IDs and to convert the",
      "start": 125.159,
      "duration": 6.321
    },
    {
      "text": "token IDs into Vector embeddings so here",
      "start": 128.239,
      "duration": 5.441
    },
    {
      "text": "you can see that for each token we have",
      "start": 131.48,
      "duration": 4.399
    },
    {
      "text": "threedimensional Vector embeddings so",
      "start": 133.68,
      "duration": 3.96
    },
    {
      "text": "for the token y we have a",
      "start": 135.879,
      "duration": 4.0
    },
    {
      "text": "three-dimensional Vector embedding for",
      "start": 137.64,
      "duration": 4.0
    },
    {
      "text": "the token Journey we have a",
      "start": 139.879,
      "duration": 3.72
    },
    {
      "text": "three-dimensional Vector embedding and",
      "start": 141.64,
      "duration": 3.679
    },
    {
      "text": "for the token step we have a",
      "start": 143.599,
      "duration": 3.881
    },
    {
      "text": "three-dimensional Vector embedding",
      "start": 145.319,
      "duration": 4.401
    },
    {
      "text": "remember generally one word is not equal",
      "start": 147.48,
      "duration": 5.759
    },
    {
      "text": "to to one token because uh GPT and other",
      "start": 149.72,
      "duration": 5.799
    },
    {
      "text": "modern llms use a bite pair encoder",
      "start": 153.239,
      "duration": 4.801
    },
    {
      "text": "which are sub tokenizers but for the",
      "start": 155.519,
      "duration": 4.36
    },
    {
      "text": "purposes of this lecture I'm going to",
      "start": 158.04,
      "duration": 4.36
    },
    {
      "text": "use word and token",
      "start": 159.879,
      "duration": 4.64
    },
    {
      "text": "interchangeably okay so the first step",
      "start": 162.4,
      "duration": 4.4
    },
    {
      "text": "is to convert all of the tokens into",
      "start": 164.519,
      "duration": 4.401
    },
    {
      "text": "Vector embeddings I'm choosing a",
      "start": 166.8,
      "duration": 4.0
    },
    {
      "text": "three-dimensional Vector embedding here",
      "start": 168.92,
      "duration": 4.36
    },
    {
      "text": "just for the sake of demonstration",
      "start": 170.8,
      "duration": 5.159
    },
    {
      "text": "remember that in models like GPT it's",
      "start": 173.28,
      "duration": 4.44
    },
    {
      "text": "common to have Vector dimensions of",
      "start": 175.959,
      "duration": 5.081
    },
    {
      "text": "500,000 or even more Ive just plotted",
      "start": 177.72,
      "duration": 4.879
    },
    {
      "text": "these vectors and how they look like",
      "start": 181.04,
      "duration": 3.64
    },
    {
      "text": "over here in the three-dimensional space",
      "start": 182.599,
      "duration": 4.56
    },
    {
      "text": "these are the input embeddings the goal",
      "start": 184.68,
      "duration": 4.76
    },
    {
      "text": "of any attention mechanism is to take",
      "start": 187.159,
      "duration": 4.761
    },
    {
      "text": "these input embeddings for every vector",
      "start": 189.44,
      "duration": 5.12
    },
    {
      "text": "and convert them into context embeddings",
      "start": 191.92,
      "duration": 5.76
    },
    {
      "text": "or context vectors for every token now",
      "start": 194.56,
      "duration": 5.16
    },
    {
      "text": "what's the difference between uh input",
      "start": 197.68,
      "duration": 3.919
    },
    {
      "text": "embedding and context embedding or a",
      "start": 199.72,
      "duration": 4.2
    },
    {
      "text": "context Vector so let's say if you look",
      "start": 201.599,
      "duration": 3.761
    },
    {
      "text": "at the",
      "start": 203.92,
      "duration": 3.76
    },
    {
      "text": "journey this is an input embedding for",
      "start": 205.36,
      "duration": 4.439
    },
    {
      "text": "Journey this green Vector which you see",
      "start": 207.68,
      "duration": 4.36
    },
    {
      "text": "over here which contains which encodes",
      "start": 209.799,
      "duration": 5.241
    },
    {
      "text": "some semantic meaning about Journey but",
      "start": 212.04,
      "duration": 6.08
    },
    {
      "text": "it does not carry any information about",
      "start": 215.04,
      "duration": 5.04
    },
    {
      "text": "how the other words in the sentence such",
      "start": 218.12,
      "duration": 6.479
    },
    {
      "text": "as your step with one how all these",
      "start": 220.08,
      "duration": 6.0
    },
    {
      "text": "words relate to",
      "start": 224.599,
      "duration": 3.92
    },
    {
      "text": "Journey how much attention should you",
      "start": 226.08,
      "duration": 4.079
    },
    {
      "text": "pay to each of these words when you are",
      "start": 228.519,
      "duration": 5.321
    },
    {
      "text": "looking at Journey the embedding vector",
      "start": 230.159,
      "duration": 5.681
    },
    {
      "text": "or the input Vector for Journey contains",
      "start": 233.84,
      "duration": 4.479
    },
    {
      "text": "no such information whereas the context",
      "start": 235.84,
      "duration": 4.679
    },
    {
      "text": "Vector contains this information",
      "start": 238.319,
      "duration": 4.161
    },
    {
      "text": "the context Vector not only contains",
      "start": 240.519,
      "duration": 4.321
    },
    {
      "text": "semantic meaning about Journey but it",
      "start": 242.48,
      "duration": 4.72
    },
    {
      "text": "also encodes meaning about how Journey",
      "start": 244.84,
      "duration": 5.36
    },
    {
      "text": "relates with all these other words so",
      "start": 247.2,
      "duration": 4.56
    },
    {
      "text": "that's the main aim of the attention",
      "start": 250.2,
      "duration": 5.16
    },
    {
      "text": "mechanism to get context vectors for",
      "start": 251.76,
      "duration": 6.08
    },
    {
      "text": "each of our input embedding vectors why",
      "start": 255.36,
      "duration": 4.559
    },
    {
      "text": "do we need context vectors because it",
      "start": 257.84,
      "duration": 3.799
    },
    {
      "text": "makes the task of the next word",
      "start": 259.919,
      "duration": 4.641
    },
    {
      "text": "predictions much more better much more",
      "start": 261.639,
      "duration": 5.761
    },
    {
      "text": "reliable because now we are encoding",
      "start": 264.56,
      "duration": 4.68
    },
    {
      "text": "information of how much attention needs",
      "start": 267.4,
      "duration": 4.16
    },
    {
      "text": "to be paid at different words in a",
      "start": 269.24,
      "duration": 3.92
    },
    {
      "text": "sequence of",
      "start": 271.56,
      "duration": 4.24
    },
    {
      "text": "sentences so that's the whole aim of",
      "start": 273.16,
      "duration": 4.879
    },
    {
      "text": "attention mechanisms as we saw in the",
      "start": 275.8,
      "duration": 4.679
    },
    {
      "text": "previous lecture the first step is to",
      "start": 278.039,
      "duration": 4.761
    },
    {
      "text": "multiply the inputs with the query",
      "start": 280.479,
      "duration": 5.881
    },
    {
      "text": "Matrix the key Matrix and the value",
      "start": 282.8,
      "duration": 5.839
    },
    {
      "text": "Matrix when you multi when you do this",
      "start": 286.36,
      "duration": 4.72
    },
    {
      "text": "multiplication you get the query query",
      "start": 288.639,
      "duration": 4.681
    },
    {
      "text": "Matrix so there's a difference so this",
      "start": 291.08,
      "duration": 6.08
    },
    {
      "text": "WQ w k and WV are weight matrices so",
      "start": 293.32,
      "duration": 5.8
    },
    {
      "text": "these are the trainable weight Matrix",
      "start": 297.16,
      "duration": 4.24
    },
    {
      "text": "this is the trainable query Matrix this",
      "start": 299.12,
      "duration": 4.28
    },
    {
      "text": "is the trainable key Matrix and this is",
      "start": 301.4,
      "duration": 4.76
    },
    {
      "text": "the trainable value Matrix you multiply",
      "start": 303.4,
      "duration": 5.0
    },
    {
      "text": "the inputs Matrix with these trainable",
      "start": 306.16,
      "duration": 4.44
    },
    {
      "text": "weight matrices and then you get the",
      "start": 308.4,
      "duration": 5.88
    },
    {
      "text": "final queries Matrix the keys Matrix and",
      "start": 310.6,
      "duration": 5.52
    },
    {
      "text": "the values",
      "start": 314.28,
      "duration": 4.639
    },
    {
      "text": "Matrix that's the first step now",
      "start": 316.12,
      "duration": 8.359
    },
    {
      "text": "remember that this WQ w k and WV these",
      "start": 318.919,
      "duration": 8.921
    },
    {
      "text": "training weight matrices are not fixed",
      "start": 324.479,
      "duration": 5.28
    },
    {
      "text": "their parameters need to be trained",
      "start": 327.84,
      "duration": 4.24
    },
    {
      "text": "based on input data and this training is",
      "start": 329.759,
      "duration": 4.761
    },
    {
      "text": "a part of the llm currently when we are",
      "start": 332.08,
      "duration": 4.28
    },
    {
      "text": "learning attention mechanisms we are not",
      "start": 334.52,
      "duration": 3.799
    },
    {
      "text": "learning about this training procedure",
      "start": 336.36,
      "duration": 4.839
    },
    {
      "text": "we are only learning about the uh",
      "start": 338.319,
      "duration": 5.481
    },
    {
      "text": "forward pass procedure how to take input",
      "start": 341.199,
      "duration": 4.28
    },
    {
      "text": "embeddings and how to convert them into",
      "start": 343.8,
      "duration": 3.8
    },
    {
      "text": "context vectors we'll come to training",
      "start": 345.479,
      "duration": 4.84
    },
    {
      "text": "and back propagation later in this",
      "start": 347.6,
      "duration": 5.4
    },
    {
      "text": "course okay so once you obtain the",
      "start": 350.319,
      "duration": 5.361
    },
    {
      "text": "queries keys and the values Matrix the",
      "start": 353.0,
      "duration": 5.039
    },
    {
      "text": "next step is to compute the attention",
      "start": 355.68,
      "duration": 5.0
    },
    {
      "text": "scores to compute the attention scores",
      "start": 358.039,
      "duration": 4.521
    },
    {
      "text": "what we do is we multiply the queries",
      "start": 360.68,
      "duration": 4.12
    },
    {
      "text": "Matrix with the transpose of the keys",
      "start": 362.56,
      "duration": 5.199
    },
    {
      "text": "Matrix so these are the attention scores",
      "start": 364.8,
      "duration": 6.88
    },
    {
      "text": "and each row represents the attention",
      "start": 367.759,
      "duration": 5.761
    },
    {
      "text": "corresponding to to that particular",
      "start": 371.68,
      "duration": 4.44
    },
    {
      "text": "query with all the other keys so let's",
      "start": 373.52,
      "duration": 4.72
    },
    {
      "text": "say if you look at the second row it",
      "start": 376.12,
      "duration": 4.199
    },
    {
      "text": "corresponds to let's say the query for",
      "start": 378.24,
      "duration": 4.0
    },
    {
      "text": "Journey because the first row",
      "start": 380.319,
      "duration": 3.6
    },
    {
      "text": "corresponds to your the second row",
      "start": 382.24,
      "duration": 3.28
    },
    {
      "text": "corresponds to Journey the third row",
      "start": 383.919,
      "duration": 4.361
    },
    {
      "text": "corresponds to begins Etc so if you look",
      "start": 385.52,
      "duration": 4.88
    },
    {
      "text": "at the second row",
      "start": 388.28,
      "duration": 3.639
    },
    {
      "text": "the",
      "start": 390.4,
      "duration": 4.16
    },
    {
      "text": "first the first value in the second row",
      "start": 391.919,
      "duration": 4.481
    },
    {
      "text": "is basically when you are looking at the",
      "start": 394.56,
      "duration": 4.0
    },
    {
      "text": "query Journey how much attention should",
      "start": 396.4,
      "duration": 4.68
    },
    {
      "text": "you pay to the first input embedding",
      "start": 398.56,
      "duration": 4.52
    },
    {
      "text": "Vector which is",
      "start": 401.08,
      "duration": 4.399
    },
    {
      "text": "your when you're looking at Journey how",
      "start": 403.08,
      "duration": 3.88
    },
    {
      "text": "much attention should you pay to the",
      "start": 405.479,
      "duration": 4.16
    },
    {
      "text": "second input embedding Vector similarly",
      "start": 406.96,
      "duration": 4.799
    },
    {
      "text": "when you look at the last entry of the",
      "start": 409.639,
      "duration": 4.441
    },
    {
      "text": "second row this encodes information of",
      "start": 411.759,
      "duration": 4.481
    },
    {
      "text": "how much attention should you be paying",
      "start": 414.08,
      "duration": 5.76
    },
    {
      "text": "to the sixth input embedding which is",
      "start": 416.24,
      "duration": 5.399
    },
    {
      "text": "Step so your journey begins with one",
      "start": 419.84,
      "duration": 5.039
    },
    {
      "text": "step so basically every row contains",
      "start": 421.639,
      "duration": 4.801
    },
    {
      "text": "information that when you're looking at",
      "start": 424.879,
      "duration": 5.04
    },
    {
      "text": "a particular query how much uh attention",
      "start": 426.44,
      "duration": 5.68
    },
    {
      "text": "should be given to all the other keys in",
      "start": 429.919,
      "duration": 4.601
    },
    {
      "text": "the sentence if you are unclear about",
      "start": 432.12,
      "duration": 3.84
    },
    {
      "text": "this please go through the previous",
      "start": 434.52,
      "duration": 3.2
    },
    {
      "text": "lecture where I've covered this in an",
      "start": 435.96,
      "duration": 4.88
    },
    {
      "text": "extensive amount of detail here I'm just",
      "start": 437.72,
      "duration": 5.8
    },
    {
      "text": "providing a recap so that you you",
      "start": 440.84,
      "duration": 4.16
    },
    {
      "text": "understand or you revise what we have",
      "start": 443.52,
      "duration": 3.56
    },
    {
      "text": "learned so far these are the attention",
      "start": 445.0,
      "duration": 4.56
    },
    {
      "text": "scores the next step is to convert these",
      "start": 447.08,
      "duration": 4.959
    },
    {
      "text": "attention scores into attention weights",
      "start": 449.56,
      "duration": 4.16
    },
    {
      "text": "and the way we do that is first we",
      "start": 452.039,
      "duration": 3.401
    },
    {
      "text": "divide by square root of the key",
      "start": 453.72,
      "duration": 4.12
    },
    {
      "text": "Dimension and then we apply the soft Max",
      "start": 455.44,
      "duration": 4.479
    },
    {
      "text": "activation function again in the",
      "start": 457.84,
      "duration": 3.88
    },
    {
      "text": "previous lecture I have explained why we",
      "start": 459.919,
      "duration": 4.041
    },
    {
      "text": "divide by the square root of the keys",
      "start": 461.72,
      "duration": 4.24
    },
    {
      "text": "Dimension and how do we apply the soft",
      "start": 463.96,
      "duration": 4.079
    },
    {
      "text": "Max but basically these are the",
      "start": 465.96,
      "duration": 4.0
    },
    {
      "text": "attention these are the attention",
      "start": 468.039,
      "duration": 3.6
    },
    {
      "text": "weights which we compute from the",
      "start": 469.96,
      "duration": 4.04
    },
    {
      "text": "attention scores the difference between",
      "start": 471.639,
      "duration": 4.041
    },
    {
      "text": "attention scores and weights is that",
      "start": 474.0,
      "duration": 3.879
    },
    {
      "text": "they intuitively mean the same thing but",
      "start": 475.68,
      "duration": 4.12
    },
    {
      "text": "if you look at each row of the tension",
      "start": 477.879,
      "duration": 3.76
    },
    {
      "text": "weight Matrix you'll see that each row",
      "start": 479.8,
      "duration": 4.2
    },
    {
      "text": "essentially sums up to one so there is a",
      "start": 481.639,
      "duration": 4.52
    },
    {
      "text": "probabilistic meaning Associated now",
      "start": 484.0,
      "duration": 3.84
    },
    {
      "text": "when you look at Journey you have to pay",
      "start": 486.159,
      "duration": 4.76
    },
    {
      "text": "15% attention to the first input",
      "start": 487.84,
      "duration": 5.52
    },
    {
      "text": "embedding 22% attention to the second",
      "start": 490.919,
      "duration": 4.24
    },
    {
      "text": "input embedding and finally we can say",
      "start": 493.36,
      "duration": 4.399
    },
    {
      "text": "18% attention to the last input",
      "start": 495.159,
      "duration": 4.841
    },
    {
      "text": "embeding these are the attention weights",
      "start": 497.759,
      "duration": 4.88
    },
    {
      "text": "and the last step is that you take the",
      "start": 500.0,
      "duration": 4.72
    },
    {
      "text": "attention weight Matrix and you multiply",
      "start": 502.639,
      "duration": 3.96
    },
    {
      "text": "it by the values Matrix remember the",
      "start": 504.72,
      "duration": 3.64
    },
    {
      "text": "keys query and the value Matrix have",
      "start": 506.599,
      "duration": 4.04
    },
    {
      "text": "been computed over here",
      "start": 508.36,
      "duration": 4.039
    },
    {
      "text": "so you take the attention weight Matrix",
      "start": 510.639,
      "duration": 3.721
    },
    {
      "text": "you multiply it by the values Matrix and",
      "start": 512.399,
      "duration": 3.721
    },
    {
      "text": "ultimately you get this context Vector",
      "start": 514.36,
      "duration": 4.359
    },
    {
      "text": "Matrix this is the final Vector which we",
      "start": 516.12,
      "duration": 4.799
    },
    {
      "text": "are looking for so here you can see",
      "start": 518.719,
      "duration": 4.641
    },
    {
      "text": "there are six six rows right each row",
      "start": 520.919,
      "duration": 4.241
    },
    {
      "text": "corresponds to the context Vector for",
      "start": 523.36,
      "duration": 4.0
    },
    {
      "text": "that particular token so the first row",
      "start": 525.16,
      "duration": 3.84
    },
    {
      "text": "is the context Vector for your the",
      "start": 527.36,
      "duration": 3.159
    },
    {
      "text": "second row is the context Vector for",
      "start": 529.0,
      "duration": 3.92
    },
    {
      "text": "Journey similarly the last row is the",
      "start": 530.519,
      "duration": 3.921
    },
    {
      "text": "context Vector for",
      "start": 532.92,
      "duration": 4.88
    },
    {
      "text": "step okay and in this plot I have shown",
      "start": 534.44,
      "duration": 5.92
    },
    {
      "text": "the so if you look at the journey Vector",
      "start": 537.8,
      "duration": 3.88
    },
    {
      "text": "you'll see that this is just the",
      "start": 540.36,
      "duration": 3.159
    },
    {
      "text": "embedding Vector the green but if you",
      "start": 541.68,
      "duration": 3.68
    },
    {
      "text": "look at the journey context Vector now",
      "start": 543.519,
      "duration": 3.281
    },
    {
      "text": "you'll see that it's different than the",
      "start": 545.36,
      "duration": 4.32
    },
    {
      "text": "journey because it also contains how",
      "start": 546.8,
      "duration": 4.64
    },
    {
      "text": "much attention should be paid to all the",
      "start": 549.68,
      "duration": 4.32
    },
    {
      "text": "other words so the journey context",
      "start": 551.44,
      "duration": 5.2
    },
    {
      "text": "Vector is much more richer than journey",
      "start": 554.0,
      "duration": 4.44
    },
    {
      "text": "and the context vectors are what we'll",
      "start": 556.64,
      "duration": 4.84
    },
    {
      "text": "be using as inputs for the llm training",
      "start": 558.44,
      "duration": 5.0
    },
    {
      "text": "so we'll get such context vectors for",
      "start": 561.48,
      "duration": 4.4
    },
    {
      "text": "all the input embeddings which we have",
      "start": 563.44,
      "duration": 4.12
    },
    {
      "text": "this is the recap of what all we have",
      "start": 565.88,
      "duration": 4.6
    },
    {
      "text": "covered so far I hope you are with me",
      "start": 567.56,
      "duration": 6.44
    },
    {
      "text": "until this stage now what is causal",
      "start": 570.48,
      "duration": 6.12
    },
    {
      "text": "attention and why do we need it so first",
      "start": 574.0,
      "duration": 4.839
    },
    {
      "text": "of all causal attention is also called",
      "start": 576.6,
      "duration": 4.919
    },
    {
      "text": "as mask attention so when you read some",
      "start": 578.839,
      "duration": 4.401
    },
    {
      "text": "research papers and when you see some",
      "start": 581.519,
      "duration": 3.841
    },
    {
      "text": "tutorials you'll see that this term is",
      "start": 583.24,
      "duration": 4.52
    },
    {
      "text": "also called as masked attention it is a",
      "start": 585.36,
      "duration": 5.24
    },
    {
      "text": "special form of self attention so what",
      "start": 587.76,
      "duration": 4.96
    },
    {
      "text": "this causal attention does is that it",
      "start": 590.6,
      "duration": 6.2
    },
    {
      "text": "restricts the model to only consider the",
      "start": 592.72,
      "duration": 6.08
    },
    {
      "text": "previous and the current inputs in a",
      "start": 596.8,
      "duration": 4.44
    },
    {
      "text": "sequence when processing any given",
      "start": 598.8,
      "duration": 5.36
    },
    {
      "text": "token so let me explain to you further",
      "start": 601.24,
      "duration": 5.599
    },
    {
      "text": "what this means this is in contrast to",
      "start": 604.16,
      "duration": 4.239
    },
    {
      "text": "the self attention mechanism which",
      "start": 606.839,
      "duration": 3.401
    },
    {
      "text": "allows access to the entire input",
      "start": 608.399,
      "duration": 4.241
    },
    {
      "text": "sequence at once so remember what we did",
      "start": 610.24,
      "duration": 4.0
    },
    {
      "text": "here when I explained this attention",
      "start": 612.64,
      "duration": 3.8
    },
    {
      "text": "metrix to you this attention score",
      "start": 614.24,
      "duration": 4.32
    },
    {
      "text": "Matrix when we look at a particular",
      "start": 616.44,
      "duration": 4.16
    },
    {
      "text": "query such as Journey we look at its",
      "start": 618.56,
      "duration": 5.04
    },
    {
      "text": "attention with all the other uh tokens",
      "start": 620.6,
      "duration": 5.4
    },
    {
      "text": "right your begins one step we do not",
      "start": 623.6,
      "duration": 4.2
    },
    {
      "text": "look at whether these tokens come before",
      "start": 626.0,
      "duration": 3.6
    },
    {
      "text": "Journey or whether they come after after",
      "start": 627.8,
      "duration": 3.84
    },
    {
      "text": "Journey that's what changed in the",
      "start": 629.6,
      "duration": 4.239
    },
    {
      "text": "causal attention in causal attention",
      "start": 631.64,
      "duration": 4.68
    },
    {
      "text": "when we look at any particular query we",
      "start": 633.839,
      "duration": 4.641
    },
    {
      "text": "only consider the attention of that",
      "start": 636.32,
      "duration": 4.639
    },
    {
      "text": "query with respect to tokens which come",
      "start": 638.48,
      "duration": 3.64
    },
    {
      "text": "before that",
      "start": 640.959,
      "duration": 3.68
    },
    {
      "text": "query let me show you how what that",
      "start": 642.12,
      "duration": 5.48
    },
    {
      "text": "means in a moment so when Computing",
      "start": 644.639,
      "duration": 5.241
    },
    {
      "text": "attention scores the causal attention",
      "start": 647.6,
      "duration": 5.28
    },
    {
      "text": "mechanism ensures that the model only",
      "start": 649.88,
      "duration": 6.28
    },
    {
      "text": "factors in tokens that occur at or",
      "start": 652.88,
      "duration": 6.68
    },
    {
      "text": "before the current token in the sequence",
      "start": 656.16,
      "duration": 5.56
    },
    {
      "text": "to achieve this in GPT like large",
      "start": 659.56,
      "duration": 4.56
    },
    {
      "text": "language models for each token processed",
      "start": 661.72,
      "duration": 5.239
    },
    {
      "text": "we mask out the future tokens which come",
      "start": 664.12,
      "duration": 5.519
    },
    {
      "text": "after the current token let me show you",
      "start": 666.959,
      "duration": 5.041
    },
    {
      "text": "visually what all of this really means",
      "start": 669.639,
      "duration": 4.76
    },
    {
      "text": "so until now the attention weight Matrix",
      "start": 672.0,
      "duration": 3.839
    },
    {
      "text": "which we have seen looks something like",
      "start": 674.399,
      "duration": 3.201
    },
    {
      "text": "this so if you look at the row for",
      "start": 675.839,
      "duration": 4.56
    },
    {
      "text": "Journey you can get the attention weight",
      "start": 677.6,
      "duration": 5.039
    },
    {
      "text": "of Journey with all the other tokens",
      "start": 680.399,
      "duration": 5.521
    },
    {
      "text": "right of your journey",
      "start": 682.639,
      "duration": 7.121
    },
    {
      "text": "starts with one step",
      "start": 685.92,
      "duration": 7.64
    },
    {
      "text": "so this row con consist of six values",
      "start": 689.76,
      "duration": 8.6
    },
    {
      "text": "but the main main U goal of the causal",
      "start": 693.56,
      "duration": 6.88
    },
    {
      "text": "attention mechanism is that when you",
      "start": 698.36,
      "duration": 3.88
    },
    {
      "text": "look at a particular token such as",
      "start": 700.44,
      "duration": 3.92
    },
    {
      "text": "Journey you should only consider the",
      "start": 702.24,
      "duration": 4.039
    },
    {
      "text": "attention scores of Journey with the",
      "start": 704.36,
      "duration": 3.919
    },
    {
      "text": "words which come before Journey such as",
      "start": 706.279,
      "duration": 4.921
    },
    {
      "text": "your and journey so there are only two",
      "start": 708.279,
      "duration": 5.281
    },
    {
      "text": "attention scores which are relevant here",
      "start": 711.2,
      "duration": 4.079
    },
    {
      "text": "all the attention scores which come",
      "start": 713.56,
      "duration": 3.959
    },
    {
      "text": "after this point are masked",
      "start": 715.279,
      "duration": 4.601
    },
    {
      "text": "out which means that they no longer",
      "start": 717.519,
      "duration": 5.0
    },
    {
      "text": "exist they are set to zero similarly",
      "start": 719.88,
      "duration": 4.16
    },
    {
      "text": "when you look at width let's say you",
      "start": 722.519,
      "duration": 4.161
    },
    {
      "text": "look at width so before width we have",
      "start": 724.04,
      "duration": 4.76
    },
    {
      "text": "your journey starts with there are four",
      "start": 726.68,
      "duration": 4.56
    },
    {
      "text": "tokens so we have the attention scores",
      "start": 728.8,
      "duration": 5.12
    },
    {
      "text": "for those four tokens but the attention",
      "start": 731.24,
      "duration": 4.959
    },
    {
      "text": "scores for all of the future tokens will",
      "start": 733.92,
      "duration": 5.12
    },
    {
      "text": "be masked out or will be set to zero",
      "start": 736.199,
      "duration": 6.041
    },
    {
      "text": "will be converted to zero whereas if you",
      "start": 739.04,
      "duration": 5.88
    },
    {
      "text": "look at the last word step your journey",
      "start": 742.24,
      "duration": 4.64
    },
    {
      "text": "starts with one step all of these come",
      "start": 744.92,
      "duration": 3.96
    },
    {
      "text": "before step right so we have all the",
      "start": 746.88,
      "duration": 3.68
    },
    {
      "text": "atten scores which are considered",
      "start": 748.88,
      "duration": 3.88
    },
    {
      "text": "nothing will be masked out in the first",
      "start": 750.56,
      "duration": 4.16
    },
    {
      "text": "token your there is nothing which comes",
      "start": 752.76,
      "duration": 4.199
    },
    {
      "text": "before your so then every single thing",
      "start": 754.72,
      "duration": 6.2
    },
    {
      "text": "after your will essentially be masked",
      "start": 756.959,
      "duration": 3.961
    },
    {
      "text": "out when we look at masking the context",
      "start": 761.0,
      "duration": 5.48
    },
    {
      "text": "size is also important because remember",
      "start": 764.16,
      "duration": 4.44
    },
    {
      "text": "the context size specifies how many",
      "start": 766.48,
      "duration": 4.039
    },
    {
      "text": "words the llm can look at before",
      "start": 768.6,
      "duration": 4.12
    },
    {
      "text": "predicting the next word so when I show",
      "start": 770.519,
      "duration": 4.041
    },
    {
      "text": "this Matrix I'm assuming that this much",
      "start": 772.72,
      "duration": 4.64
    },
    {
      "text": "is the context size so context size is",
      "start": 774.56,
      "duration": 5.48
    },
    {
      "text": "six uh so this is the the main purpose",
      "start": 777.36,
      "duration": 5.24
    },
    {
      "text": "of the causal attention mechanism so we",
      "start": 780.04,
      "duration": 5.08
    },
    {
      "text": "mask out out the attention weights above",
      "start": 782.6,
      "duration": 4.76
    },
    {
      "text": "the diagonal this is very important if",
      "start": 785.12,
      "duration": 4.44
    },
    {
      "text": "you look out if you look at this second",
      "start": 787.36,
      "duration": 3.88
    },
    {
      "text": "Matrix there is a key pattern which You",
      "start": 789.56,
      "duration": 4.2
    },
    {
      "text": "observe over here and that is what we'll",
      "start": 791.24,
      "duration": 5.399
    },
    {
      "text": "exploit when we code if you take this",
      "start": 793.76,
      "duration": 5.24
    },
    {
      "text": "diagonal and if you look at everything",
      "start": 796.639,
      "duration": 4.921
    },
    {
      "text": "which occurs Above This diagonal it is",
      "start": 799.0,
      "duration": 4.8
    },
    {
      "text": "essentially zero right which means that",
      "start": 801.56,
      "duration": 4.76
    },
    {
      "text": "it is essentially mased out students who",
      "start": 803.8,
      "duration": 5.039
    },
    {
      "text": "know about the Triangular Matrix the the",
      "start": 806.32,
      "duration": 4.28
    },
    {
      "text": "lower triangular Matrix and the upper",
      "start": 808.839,
      "duration": 3.68
    },
    {
      "text": "triangular Matrix will really relate to",
      "start": 810.6,
      "duration": 4.4
    },
    {
      "text": "this and understand this much better uh",
      "start": 812.519,
      "duration": 4.841
    },
    {
      "text": "we'll come to that in a moment in code",
      "start": 815.0,
      "duration": 4.56
    },
    {
      "text": "but for now just remember that we mask",
      "start": 817.36,
      "duration": 4.08
    },
    {
      "text": "out the attention weights above the",
      "start": 819.56,
      "duration": 4.079
    },
    {
      "text": "diagonal like this we set those",
      "start": 821.44,
      "duration": 4.32
    },
    {
      "text": "attention weights to be equal to",
      "start": 823.639,
      "duration": 5.241
    },
    {
      "text": "zero and then we normalize the nonmass",
      "start": 825.76,
      "duration": 4.8
    },
    {
      "text": "attention weights such that the",
      "start": 828.88,
      "duration": 3.639
    },
    {
      "text": "attention weights sum up to one in each",
      "start": 830.56,
      "duration": 4.92
    },
    {
      "text": "row so your question would be that okay",
      "start": 832.519,
      "duration": 5.76
    },
    {
      "text": "if everything else is set to zero then",
      "start": 835.48,
      "duration": 4.76
    },
    {
      "text": "these weight will no longer sum up to",
      "start": 838.279,
      "duration": 3.401
    },
    {
      "text": "one right we'll ensure that the",
      "start": 840.24,
      "duration": 3.92
    },
    {
      "text": "normalization is done once more so that",
      "start": 841.68,
      "duration": 4.32
    },
    {
      "text": "whatever attention weights are remaining",
      "start": 844.16,
      "duration": 4.76
    },
    {
      "text": "they indeed sum up to one so this is the",
      "start": 846.0,
      "duration": 4.36
    },
    {
      "text": "main idea",
      "start": 848.92,
      "duration": 6.24
    },
    {
      "text": "behind uh the causal attention mechanism",
      "start": 850.36,
      "duration": 6.719
    },
    {
      "text": "so this this thing here what I'm",
      "start": 855.16,
      "duration": 4.119
    },
    {
      "text": "coloring in red right now which is above",
      "start": 857.079,
      "duration": 4.0
    },
    {
      "text": "the diagonal it's called as the causal",
      "start": 859.279,
      "duration": 4.24
    },
    {
      "text": "attention mask because we are masking",
      "start": 861.079,
      "duration": 5.0
    },
    {
      "text": "out all of those attention weights so",
      "start": 863.519,
      "duration": 5.12
    },
    {
      "text": "now let us see how to apply a causal",
      "start": 866.079,
      "duration": 4.161
    },
    {
      "text": "attention",
      "start": 868.639,
      "duration": 4.161
    },
    {
      "text": "mask so the strategy which we'll follow",
      "start": 870.24,
      "duration": 5.12
    },
    {
      "text": "is that exactly what we have done over",
      "start": 872.8,
      "duration": 4.479
    },
    {
      "text": "here in the flow map which earlier",
      "start": 875.36,
      "duration": 4.159
    },
    {
      "text": "showed you here what we'll do is that we",
      "start": 877.279,
      "duration": 5.041
    },
    {
      "text": "get the attention weights in a similar",
      "start": 879.519,
      "duration": 4.641
    },
    {
      "text": "manner to what we have obtained and then",
      "start": 882.32,
      "duration": 3.879
    },
    {
      "text": "we just set the elements above the",
      "start": 884.16,
      "duration": 3.76
    },
    {
      "text": "diagonal we set the elements above the",
      "start": 886.199,
      "duration": 3.961
    },
    {
      "text": "diagonal to be zero and then we",
      "start": 887.92,
      "duration": 4.359
    },
    {
      "text": "renormalize the attention weights that",
      "start": 890.16,
      "duration": 3.599
    },
    {
      "text": "is the strategy which we are going to",
      "start": 892.279,
      "duration": 4.0
    },
    {
      "text": "follow that is also mentioned over here",
      "start": 893.759,
      "duration": 4.961
    },
    {
      "text": "so we'll first get the attention scores",
      "start": 896.279,
      "duration": 4.401
    },
    {
      "text": "like what we had previously then we get",
      "start": 898.72,
      "duration": 3.4
    },
    {
      "text": "the attention weights this is what we",
      "start": 900.68,
      "duration": 4.24
    },
    {
      "text": "did previously then we will add this one",
      "start": 902.12,
      "duration": 5.6
    },
    {
      "text": "step we'll mask the elements above the",
      "start": 904.92,
      "duration": 5.359
    },
    {
      "text": "diagonal to be zero then we'll get M",
      "start": 907.72,
      "duration": 4.559
    },
    {
      "text": "attention scores and then we'll again",
      "start": 910.279,
      "duration": 4.12
    },
    {
      "text": "normalize them to get M attention",
      "start": 912.279,
      "duration": 4.401
    },
    {
      "text": "weights so that we ensure that each row",
      "start": 914.399,
      "duration": 5.44
    },
    {
      "text": "again sums up to one so now let us",
      "start": 916.68,
      "duration": 6.76
    },
    {
      "text": "encode this logic in code but just",
      "start": 919.839,
      "duration": 5.8
    },
    {
      "text": "remember that all we are doing is we are",
      "start": 923.44,
      "duration": 3.839
    },
    {
      "text": "getting the attention weights and we are",
      "start": 925.639,
      "duration": 3.361
    },
    {
      "text": "zeroing out the elements above the",
      "start": 927.279,
      "duration": 3.161
    },
    {
      "text": "diagonal that's",
      "start": 929.0,
      "duration": 4.639
    },
    {
      "text": "it okay so let us go to code right now",
      "start": 930.44,
      "duration": 5.079
    },
    {
      "text": "and the goal which we have is hiding",
      "start": 933.639,
      "duration": 3.88
    },
    {
      "text": "future words with causal",
      "start": 935.519,
      "duration": 4.68
    },
    {
      "text": "attention now for this remember that we",
      "start": 937.519,
      "duration": 4.88
    },
    {
      "text": "have worked previously in the previous",
      "start": 940.199,
      "duration": 3.64
    },
    {
      "text": "lecture we have written this self",
      "start": 942.399,
      "duration": 3.88
    },
    {
      "text": "attention version two what the self",
      "start": 943.839,
      "duration": 5.881
    },
    {
      "text": "attention class does is that it uh it",
      "start": 946.279,
      "duration": 5.601
    },
    {
      "text": "basically takes us through this entire",
      "start": 949.72,
      "duration": 4.08
    },
    {
      "text": "flowchart pipeline which I've mentioned",
      "start": 951.88,
      "duration": 3.16
    },
    {
      "text": "over here let",
      "start": 953.8,
      "duration": 4.68
    },
    {
      "text": "me show that yeah this pipeline so what",
      "start": 955.04,
      "duration": 5.239
    },
    {
      "text": "that self attention class in Python",
      "start": 958.48,
      "duration": 3.599
    },
    {
      "text": "which I showed you does is that first it",
      "start": 960.279,
      "duration": 4.961
    },
    {
      "text": "initializes these query key and value M",
      "start": 962.079,
      "duration": 5.56
    },
    {
      "text": "weight matrices to random values then it",
      "start": 965.24,
      "duration": 5.0
    },
    {
      "text": "multiplies the inputs with these to get",
      "start": 967.639,
      "duration": 5.0
    },
    {
      "text": "the queries keys and the value Matrix",
      "start": 970.24,
      "duration": 4.48
    },
    {
      "text": "then it multiplies queries with key keys",
      "start": 972.639,
      "duration": 4.281
    },
    {
      "text": "transpose to get the attention scores",
      "start": 974.72,
      "duration": 3.84
    },
    {
      "text": "then it scales by square root of",
      "start": 976.92,
      "duration": 3.44
    },
    {
      "text": "Dimension does soft Max to get the",
      "start": 978.56,
      "duration": 3.92
    },
    {
      "text": "attention weights and then it multiplies",
      "start": 980.36,
      "duration": 3.8
    },
    {
      "text": "attention weights with values to get the",
      "start": 982.48,
      "duration": 4.96
    },
    {
      "text": "context Vector Matrix so if you see uh",
      "start": 984.16,
      "duration": 5.0
    },
    {
      "text": "if you take the forward method me thir",
      "start": 987.44,
      "duration": 3.56
    },
    {
      "text": "in this self attention class we",
      "start": 989.16,
      "duration": 4.039
    },
    {
      "text": "basically get the keys queries and the",
      "start": 991.0,
      "duration": 5.68
    },
    {
      "text": "values uh so the these are the W key W",
      "start": 993.199,
      "duration": 5.961
    },
    {
      "text": "query and the W value are the trainable",
      "start": 996.68,
      "duration": 4.24
    },
    {
      "text": "key query and value weight matrices",
      "start": 999.16,
      "duration": 3.96
    },
    {
      "text": "which are initialized randomly and then",
      "start": 1000.92,
      "duration": 4.44
    },
    {
      "text": "we multiply them with",
      "start": 1003.12,
      "duration": 5.519
    },
    {
      "text": "the uh with the inputs basically to get",
      "start": 1005.36,
      "duration": 6.919
    },
    {
      "text": "the keys queries and the values Matrix",
      "start": 1008.639,
      "duration": 6.76
    },
    {
      "text": "so remember here the way we actually get",
      "start": 1012.279,
      "duration": 5.56
    },
    {
      "text": "these Keys query and value Matrix is",
      "start": 1015.399,
      "duration": 5.281
    },
    {
      "text": "that we pass in the input X here and",
      "start": 1017.839,
      "duration": 4.721
    },
    {
      "text": "then we multiply that input to the",
      "start": 1020.68,
      "duration": 3.68
    },
    {
      "text": "trainable key query and value weight",
      "start": 1022.56,
      "duration": 4.04
    },
    {
      "text": "Matrix to get the keys the queries and",
      "start": 1024.36,
      "duration": 4.719
    },
    {
      "text": "the values so these Keys queries and the",
      "start": 1026.6,
      "duration": 5.199
    },
    {
      "text": "values which are highlighted in the code",
      "start": 1029.079,
      "duration": 5.801
    },
    {
      "text": "are these yeah these queries keys and",
      "start": 1031.799,
      "duration": 5.04
    },
    {
      "text": "values Matrix here this is the queries",
      "start": 1034.88,
      "duration": 4.079
    },
    {
      "text": "this is the keys and this is the values",
      "start": 1036.839,
      "duration": 3.801
    },
    {
      "text": "which have been obtained after",
      "start": 1038.959,
      "duration": 3.401
    },
    {
      "text": "multiplication of the inputs with the",
      "start": 1040.64,
      "duration": 2.6
    },
    {
      "text": "weight",
      "start": 1042.36,
      "duration": 3.319
    },
    {
      "text": "matrices okay then what we do is we",
      "start": 1043.24,
      "duration": 4.079
    },
    {
      "text": "multiply the queries with the keys",
      "start": 1045.679,
      "duration": 4.12
    },
    {
      "text": "transpose to get the attention scores we",
      "start": 1047.319,
      "duration": 4.841
    },
    {
      "text": "do a soft then we divide the attention",
      "start": 1049.799,
      "duration": 4.12
    },
    {
      "text": "scores with square root of the keys",
      "start": 1052.16,
      "duration": 4.12
    },
    {
      "text": "Dimension we apply soft Max to get the",
      "start": 1053.919,
      "duration": 4.64
    },
    {
      "text": "attention weights and then we multiply",
      "start": 1056.28,
      "duration": 4.04
    },
    {
      "text": "the attention weights with the values to",
      "start": 1058.559,
      "duration": 3.761
    },
    {
      "text": "get the context Vector this is what is",
      "start": 1060.32,
      "duration": 3.68
    },
    {
      "text": "happening in the self attention class",
      "start": 1062.32,
      "duration": 4.0
    },
    {
      "text": "self attention version two so we'll",
      "start": 1064.0,
      "duration": 3.84
    },
    {
      "text": "start out with the self attention",
      "start": 1066.32,
      "duration": 4.08
    },
    {
      "text": "version two we'll uh first get the",
      "start": 1067.84,
      "duration": 4.719
    },
    {
      "text": "queries and the keys Matrix we'll get",
      "start": 1070.4,
      "duration": 4.04
    },
    {
      "text": "the attention scores by multiplication",
      "start": 1072.559,
      "duration": 4.48
    },
    {
      "text": "of the queries with the keys transpose",
      "start": 1074.44,
      "duration": 5.92
    },
    {
      "text": "and then uh the attention weight will be",
      "start": 1077.039,
      "duration": 4.12
    },
    {
      "text": "uh",
      "start": 1080.36,
      "duration": 3.28
    },
    {
      "text": "we'll divide the attention scores with",
      "start": 1081.159,
      "duration": 4.361
    },
    {
      "text": "square root of keys we'll take the soft",
      "start": 1083.64,
      "duration": 3.96
    },
    {
      "text": "Max so these are the attention weights",
      "start": 1085.52,
      "duration": 4.279
    },
    {
      "text": "we which we have obtained until now we",
      "start": 1087.6,
      "duration": 3.76
    },
    {
      "text": "have not implemented the causal",
      "start": 1089.799,
      "duration": 3.88
    },
    {
      "text": "attention the inputs over here so let me",
      "start": 1091.36,
      "duration": 4.08
    },
    {
      "text": "copy paste the inputs which we had",
      "start": 1093.679,
      "duration": 4.401
    },
    {
      "text": "defined those are the six words your",
      "start": 1095.44,
      "duration": 5.28
    },
    {
      "text": "journey begins with one step these are",
      "start": 1098.08,
      "duration": 4.44
    },
    {
      "text": "the inputs so let me copy paste the",
      "start": 1100.72,
      "duration": 4.199
    },
    {
      "text": "inputs here so that you can look at the",
      "start": 1102.52,
      "duration": 4.0
    },
    {
      "text": "entire code at one",
      "start": 1104.919,
      "duration": 4.801
    },
    {
      "text": "glance okay so before this I'm copy",
      "start": 1106.52,
      "duration": 5.56
    },
    {
      "text": "pasting the inputs right",
      "start": 1109.72,
      "duration": 5.0
    },
    {
      "text": "now great so these are my inputs these",
      "start": 1112.08,
      "duration": 4.839
    },
    {
      "text": "are the six words your journey begins",
      "start": 1114.72,
      "duration": 4.439
    },
    {
      "text": "with one step and from these input",
      "start": 1116.919,
      "duration": 4.721
    },
    {
      "text": "embedding vectors we have uh got the",
      "start": 1119.159,
      "duration": 4.961
    },
    {
      "text": "attention weights so I printed them out",
      "start": 1121.64,
      "duration": 5.0
    },
    {
      "text": "right now when we get the attention",
      "start": 1124.12,
      "duration": 3.919
    },
    {
      "text": "weights this is where the real",
      "start": 1126.64,
      "duration": 3.2
    },
    {
      "text": "implementation of the causal attention",
      "start": 1128.039,
      "duration": 3.841
    },
    {
      "text": "mechanism starts out so what we are",
      "start": 1129.84,
      "duration": 3.719
    },
    {
      "text": "going to do now is that first we are",
      "start": 1131.88,
      "duration": 4.36
    },
    {
      "text": "going to generate a mask we are going to",
      "start": 1133.559,
      "duration": 4.321
    },
    {
      "text": "generate a mask which looks something",
      "start": 1136.24,
      "duration": 3.96
    },
    {
      "text": "like this now this is a mask where you",
      "start": 1137.88,
      "duration": 4.279
    },
    {
      "text": "will see that all the elements above the",
      "start": 1140.2,
      "duration": 4.839
    },
    {
      "text": "diagonal are equal to zero so ideally",
      "start": 1142.159,
      "duration": 4.321
    },
    {
      "text": "that is what we want to do with this",
      "start": 1145.039,
      "duration": 3.321
    },
    {
      "text": "attention weight Matrix right remember",
      "start": 1146.48,
      "duration": 4.72
    },
    {
      "text": "what we saw over",
      "start": 1148.36,
      "duration": 2.84
    },
    {
      "text": "here let me take you to that that the",
      "start": 1152.12,
      "duration": 4.799
    },
    {
      "text": "visual representation yeah remember what",
      "start": 1155.2,
      "duration": 3.839
    },
    {
      "text": "we saw over here we take the attention",
      "start": 1156.919,
      "duration": 4.041
    },
    {
      "text": "weight Matrix and all the elements above",
      "start": 1159.039,
      "duration": 4.321
    },
    {
      "text": "the diagonal will be set to zero so",
      "start": 1160.96,
      "duration": 4.48
    },
    {
      "text": "essentially if we have a mask like this",
      "start": 1163.36,
      "duration": 4.04
    },
    {
      "text": "and if we multiply the attention weights",
      "start": 1165.44,
      "duration": 5.0
    },
    {
      "text": "with this mask ideally all the elements",
      "start": 1167.4,
      "duration": 5.519
    },
    {
      "text": "above the diagonal will be set to zero",
      "start": 1170.44,
      "duration": 4.32
    },
    {
      "text": "so now we are going to construct this",
      "start": 1172.919,
      "duration": 4.681
    },
    {
      "text": "mask using the Python's Trill function",
      "start": 1174.76,
      "duration": 4.919
    },
    {
      "text": "so what is Trill so there are two types",
      "start": 1177.6,
      "duration": 6.36
    },
    {
      "text": "of uh matrices so upper",
      "start": 1179.679,
      "duration": 7.801
    },
    {
      "text": "triangular so let us see so there is an",
      "start": 1183.96,
      "duration": 5.28
    },
    {
      "text": "upper triangular Matrix and a lower",
      "start": 1187.48,
      "duration": 3.96
    },
    {
      "text": "triangular Matrix which I'll just show",
      "start": 1189.24,
      "duration": 4.439
    },
    {
      "text": "over here the upper triangular Matrix",
      "start": 1191.44,
      "duration": 3.88
    },
    {
      "text": "essentially looks something like this",
      "start": 1193.679,
      "duration": 3.0
    },
    {
      "text": "where all the elements below the",
      "start": 1195.32,
      "duration": 5.32
    },
    {
      "text": "diagonal are zero so this is tryu in",
      "start": 1196.679,
      "duration": 5.081
    },
    {
      "text": "Python",
      "start": 1200.64,
      "duration": 5.44
    },
    {
      "text": "tryu in Python yeah this is the upper",
      "start": 1201.76,
      "duration": 7.32
    },
    {
      "text": "triangular Matrix in Python and this is",
      "start": 1206.08,
      "duration": 5.4
    },
    {
      "text": "the lower triangular Matrix which is try",
      "start": 1209.08,
      "duration": 5.12
    },
    {
      "text": "and lower so Tri L what this lower",
      "start": 1211.48,
      "duration": 4.679
    },
    {
      "text": "triangular Matrix does is that all the",
      "start": 1214.2,
      "duration": 3.599
    },
    {
      "text": "elements above the diagonal will be",
      "start": 1216.159,
      "duration": 4.721
    },
    {
      "text": "equal to zero so if you search but I",
      "start": 1217.799,
      "duration": 4.561
    },
    {
      "text": "should not search numai so we are",
      "start": 1220.88,
      "duration": 4.24
    },
    {
      "text": "looking at torch. Trill so first let us",
      "start": 1222.36,
      "duration": 7.48
    },
    {
      "text": "look at torch do Tru so this is torch.",
      "start": 1225.12,
      "duration": 8.039
    },
    {
      "text": "Tru so if we use Tru it results in an",
      "start": 1229.84,
      "duration": 4.959
    },
    {
      "text": "upper triangular Matrix what shown on",
      "start": 1233.159,
      "duration": 5.801
    },
    {
      "text": "the left but if we use torch.",
      "start": 1234.799,
      "duration": 7.161
    },
    {
      "text": "Trill if you use tor. Trill what it will",
      "start": 1238.96,
      "duration": 4.839
    },
    {
      "text": "result is it will result in a lower",
      "start": 1241.96,
      "duration": 4.76
    },
    {
      "text": "triangular Matrix which means that all",
      "start": 1243.799,
      "duration": 5.401
    },
    {
      "text": "the elements above the diagonal will be",
      "start": 1246.72,
      "duration": 5.4
    },
    {
      "text": "set to zero so to construct a mask which",
      "start": 1249.2,
      "duration": 4.8
    },
    {
      "text": "looks something like this can you think",
      "start": 1252.12,
      "duration": 3.28
    },
    {
      "text": "about whether we'll need an upper",
      "start": 1254.0,
      "duration": 3.28
    },
    {
      "text": "triangular Matrix or a lower triangular",
      "start": 1255.4,
      "duration": 4.12
    },
    {
      "text": "Matrix",
      "start": 1257.28,
      "duration": 4.0
    },
    {
      "text": "okay so since all the elements above the",
      "start": 1259.52,
      "duration": 3.76
    },
    {
      "text": "diagonal are set to zero we'll need a",
      "start": 1261.28,
      "duration": 4.279
    },
    {
      "text": "lower triangular Matrix so that's why we",
      "start": 1263.28,
      "duration": 6.96
    },
    {
      "text": "use the torch. trill and uh the reason",
      "start": 1265.559,
      "duration": 6.801
    },
    {
      "text": "so in torch. Trill what we have to do we",
      "start": 1270.24,
      "duration": 5.2
    },
    {
      "text": "have to pass in um what that Matrix is",
      "start": 1272.36,
      "duration": 4.72
    },
    {
      "text": "going to look like So currently I'm just",
      "start": 1275.44,
      "duration": 3.28
    },
    {
      "text": "going to create a matrix of ones and",
      "start": 1277.08,
      "duration": 4.0
    },
    {
      "text": "zeros right so what I'll do is that the",
      "start": 1278.72,
      "duration": 3.959
    },
    {
      "text": "Matrix which I'm going to pass in this",
      "start": 1281.08,
      "duration": 3.64
    },
    {
      "text": "torch. one's context length comma",
      "start": 1282.679,
      "duration": 4.201
    },
    {
      "text": "context length so if you print out this",
      "start": 1284.72,
      "duration": 3.88
    },
    {
      "text": "let me show you what this Matrix",
      "start": 1286.88,
      "duration": 5.399
    },
    {
      "text": "actually looks like if you print",
      "start": 1288.6,
      "duration": 6.72
    },
    {
      "text": "out this Matrix it looks like this and",
      "start": 1292.279,
      "duration": 4.52
    },
    {
      "text": "then what I'm going to do I'm going to",
      "start": 1295.32,
      "duration": 4.359
    },
    {
      "text": "apply the the lower triangular Matrix",
      "start": 1296.799,
      "duration": 5.561
    },
    {
      "text": "function on this Matrix so what will it",
      "start": 1299.679,
      "duration": 4.12
    },
    {
      "text": "will do is that it will set all the",
      "start": 1302.36,
      "duration": 3.199
    },
    {
      "text": "elements above the diagonal to be equal",
      "start": 1303.799,
      "duration": 3.921
    },
    {
      "text": "to zero so that's exactly what's",
      "start": 1305.559,
      "duration": 5.641
    },
    {
      "text": "happened here so mask simple will be",
      "start": 1307.72,
      "duration": 5.72
    },
    {
      "text": "applying the torch. trill function to",
      "start": 1311.2,
      "duration": 4.8
    },
    {
      "text": "this tor. one's Matrix and so when I",
      "start": 1313.44,
      "duration": 5.2
    },
    {
      "text": "print out mask simple I'll get this mask",
      "start": 1316.0,
      "duration": 4.12
    },
    {
      "text": "where all the elements above the",
      "start": 1318.64,
      "duration": 3.919
    },
    {
      "text": "diagonal are equal to zero and remember",
      "start": 1320.12,
      "duration": 4.4
    },
    {
      "text": "the length of this mass is specified by",
      "start": 1322.559,
      "duration": 4.761
    },
    {
      "text": "the context length why because the",
      "start": 1324.52,
      "duration": 5.6
    },
    {
      "text": "context length is how many words the llm",
      "start": 1327.32,
      "duration": 4.52
    },
    {
      "text": "can look at before predicting the next",
      "start": 1330.12,
      "duration": 3.919
    },
    {
      "text": "word so if you look at this visual",
      "start": 1331.84,
      "duration": 3.959
    },
    {
      "text": "representation here the context length",
      "start": 1334.039,
      "duration": 4.161
    },
    {
      "text": "is equal to six because the llm can look",
      "start": 1335.799,
      "duration": 4.801
    },
    {
      "text": "at six words before predicting the next",
      "start": 1338.2,
      "duration": 4.0
    },
    {
      "text": "so in the example which I have shown the",
      "start": 1340.6,
      "duration": 3.8
    },
    {
      "text": "context length is just uh you can just",
      "start": 1342.2,
      "duration": 3.92
    },
    {
      "text": "look at the number of rows of the",
      "start": 1344.4,
      "duration": 3.639
    },
    {
      "text": "attention scores Matrix or the attention",
      "start": 1346.12,
      "duration": 4.08
    },
    {
      "text": "weight Matrix matx so here there are six",
      "start": 1348.039,
      "duration": 4.041
    },
    {
      "text": "rows right because we have six tokens",
      "start": 1350.2,
      "duration": 3.52
    },
    {
      "text": "and the context length which I'm using",
      "start": 1352.08,
      "duration": 4.56
    },
    {
      "text": "in this case is six so that that is how",
      "start": 1353.72,
      "duration": 4.839
    },
    {
      "text": "we create the mask simple and we print",
      "start": 1356.64,
      "duration": 4.72
    },
    {
      "text": "it out over here great now if you",
      "start": 1358.559,
      "duration": 6.081
    },
    {
      "text": "multiply the attention weights with this",
      "start": 1361.36,
      "duration": 5.36
    },
    {
      "text": "mask what you should if you multiply",
      "start": 1364.64,
      "duration": 4.84
    },
    {
      "text": "this attention weight Matrix with this",
      "start": 1366.72,
      "duration": 5.079
    },
    {
      "text": "uh mask simple this mask what you should",
      "start": 1369.48,
      "duration": 3.92
    },
    {
      "text": "get is that all the elements above the",
      "start": 1371.799,
      "duration": 3.801
    },
    {
      "text": "diagonal will be set to zero that's",
      "start": 1373.4,
      "duration": 4.759
    },
    {
      "text": "exactly what we are doing so now what",
      "start": 1375.6,
      "duration": 4.76
    },
    {
      "text": "we'll do is that we'll Define another",
      "start": 1378.159,
      "duration": 4.241
    },
    {
      "text": "variable which is called Mass underscore",
      "start": 1380.36,
      "duration": 5.28
    },
    {
      "text": "simple which is the final attention",
      "start": 1382.4,
      "duration": 5.519
    },
    {
      "text": "weight Matrix after multiplication of",
      "start": 1385.64,
      "duration": 4.399
    },
    {
      "text": "the attention weights with the mask",
      "start": 1387.919,
      "duration": 4.601
    },
    {
      "text": "which we have obtained earlier and when",
      "start": 1390.039,
      "duration": 5.0
    },
    {
      "text": "we print this out we'll get this type of",
      "start": 1392.52,
      "duration": 4.2
    },
    {
      "text": "attention weight Matrix where you will",
      "start": 1395.039,
      "duration": 4.161
    },
    {
      "text": "see that all the elements above the",
      "start": 1396.72,
      "duration": 5.0
    },
    {
      "text": "diagonal are equal to zero so that's",
      "start": 1399.2,
      "duration": 4.839
    },
    {
      "text": "awesome right this is exactly what we",
      "start": 1401.72,
      "duration": 5.24
    },
    {
      "text": "wanted but the next step is that you",
      "start": 1404.039,
      "duration": 4.64
    },
    {
      "text": "will see that these cannot be are",
      "start": 1406.96,
      "duration": 3.959
    },
    {
      "text": "attention weights because each row does",
      "start": 1408.679,
      "duration": 5.041
    },
    {
      "text": "not sum up to one so then the next step",
      "start": 1410.919,
      "duration": 4.76
    },
    {
      "text": "is to normalize the attention weight so",
      "start": 1413.72,
      "duration": 4.079
    },
    {
      "text": "that each row sums up to one so what",
      "start": 1415.679,
      "duration": 4.6
    },
    {
      "text": "we'll be doing is that we'll be taking",
      "start": 1417.799,
      "duration": 4.961
    },
    {
      "text": "the sum of each row and then dividing",
      "start": 1420.279,
      "duration": 4.361
    },
    {
      "text": "all the elements in that row with the",
      "start": 1422.76,
      "duration": 3.96
    },
    {
      "text": "sum so for example if you look at the",
      "start": 1424.64,
      "duration": 3.68
    },
    {
      "text": "second row we'll take the sum of the",
      "start": 1426.72,
      "duration": 3.48
    },
    {
      "text": "second row and we'll divide all the",
      "start": 1428.32,
      "duration": 4.0
    },
    {
      "text": "elements of the second row with that sum",
      "start": 1430.2,
      "duration": 4.359
    },
    {
      "text": "that way we'll ensure that all the",
      "start": 1432.32,
      "duration": 4.719
    },
    {
      "text": "elements in a row sum up to",
      "start": 1434.559,
      "duration": 4.761
    },
    {
      "text": "one so this this is what we are going to",
      "start": 1437.039,
      "duration": 4.281
    },
    {
      "text": "do next so we'll take we'll calculate",
      "start": 1439.32,
      "duration": 4.0
    },
    {
      "text": "the sum of each row and then we'll",
      "start": 1441.32,
      "duration": 4.479
    },
    {
      "text": "divide each row with the sum so then we",
      "start": 1443.32,
      "duration": 5.2
    },
    {
      "text": "get the mass simple normalized so here",
      "start": 1445.799,
      "duration": 4.24
    },
    {
      "text": "you'll see that we get an attention",
      "start": 1448.52,
      "duration": 4.6
    },
    {
      "text": "weight Matrix where each uh each row",
      "start": 1450.039,
      "duration": 5.201
    },
    {
      "text": "effectively sums up to one this is",
      "start": 1453.12,
      "duration": 4.36
    },
    {
      "text": "amazing this is exactly what we need",
      "start": 1455.24,
      "duration": 4.96
    },
    {
      "text": "this is the main U modification",
      "start": 1457.48,
      "duration": 4.799
    },
    {
      "text": "introduced by the caal tension mechanism",
      "start": 1460.2,
      "duration": 4.32
    },
    {
      "text": "it's as simple as this and now we'll",
      "start": 1462.279,
      "duration": 4.481
    },
    {
      "text": "multiply this with the values Matrix to",
      "start": 1464.52,
      "duration": 5.24
    },
    {
      "text": "get the context Vector Matrix that's it",
      "start": 1466.76,
      "duration": 5.36
    },
    {
      "text": "this is the if you understand this much",
      "start": 1469.76,
      "duration": 4.039
    },
    {
      "text": "from this lecture you would have",
      "start": 1472.12,
      "duration": 4.679
    },
    {
      "text": "understood 80% what of what I wanted to",
      "start": 1473.799,
      "duration": 6.321
    },
    {
      "text": "convey now let's go next so you might be",
      "start": 1476.799,
      "duration": 4.961
    },
    {
      "text": "thinking okay we have already done out",
      "start": 1480.12,
      "duration": 3.64
    },
    {
      "text": "done most of the things right so what do",
      "start": 1481.76,
      "duration": 4.84
    },
    {
      "text": "we need to do after this well there are",
      "start": 1483.76,
      "duration": 5.639
    },
    {
      "text": "some issues so if you look at the causal",
      "start": 1486.6,
      "duration": 4.64
    },
    {
      "text": "attention the main purpose of causal",
      "start": 1489.399,
      "duration": 5.0
    },
    {
      "text": "attention essentially is to not have any",
      "start": 1491.24,
      "duration": 5.72
    },
    {
      "text": "influence of the future tokens right but",
      "start": 1494.399,
      "duration": 4.361
    },
    {
      "text": "if you carefully see what we have done",
      "start": 1496.96,
      "duration": 5.599
    },
    {
      "text": "here we have essentially uh applied soft",
      "start": 1498.76,
      "duration": 5.639
    },
    {
      "text": "Max to the attention scores which we had",
      "start": 1502.559,
      "duration": 5.12
    },
    {
      "text": "obtained earlier right so this this",
      "start": 1504.399,
      "duration": 5.801
    },
    {
      "text": "attention weight Matrix even if you look",
      "start": 1507.679,
      "duration": 4.761
    },
    {
      "text": "let's say if you look at the second row",
      "start": 1510.2,
      "duration": 3.92
    },
    {
      "text": "and if you look at the first two entries",
      "start": 1512.44,
      "duration": 3.8
    },
    {
      "text": "of the second row these two entries are",
      "start": 1514.12,
      "duration": 3.919
    },
    {
      "text": "already influenced by all the other",
      "start": 1516.24,
      "duration": 4.2
    },
    {
      "text": "entries why because when you take the",
      "start": 1518.039,
      "duration": 4.481
    },
    {
      "text": "soft Max in the denominator you have the",
      "start": 1520.44,
      "duration": 4.04
    },
    {
      "text": "exponential sum of all the",
      "start": 1522.52,
      "duration": 4.68
    },
    {
      "text": "weights so even if you zero out all the",
      "start": 1524.48,
      "duration": 6.079
    },
    {
      "text": "future tokens it's not essentially",
      "start": 1527.2,
      "duration": 5.24
    },
    {
      "text": "cancelling the influence of the future",
      "start": 1530.559,
      "duration": 4.24
    },
    {
      "text": "tokens because the future tokens have",
      "start": 1532.44,
      "duration": 3.959
    },
    {
      "text": "already influenced the initial two",
      "start": 1534.799,
      "duration": 3.6
    },
    {
      "text": "values when we take the soft",
      "start": 1536.399,
      "duration": 4.481
    },
    {
      "text": "Max that is what disadvantage of this",
      "start": 1538.399,
      "duration": 4.481
    },
    {
      "text": "approach we are we are employing soft",
      "start": 1540.88,
      "duration": 4.76
    },
    {
      "text": "Max here and then again what we are",
      "start": 1542.88,
      "duration": 5.36
    },
    {
      "text": "doing is we are doing this kind of",
      "start": 1545.64,
      "duration": 6.44
    },
    {
      "text": "renormalization by U dividing with the",
      "start": 1548.24,
      "duration": 7.319
    },
    {
      "text": "sum so this leads to a data leakage",
      "start": 1552.08,
      "duration": 6.52
    },
    {
      "text": "problem why data leakage because the",
      "start": 1555.559,
      "duration": 6.041
    },
    {
      "text": "although we zero out the elements above",
      "start": 1558.6,
      "duration": 5.199
    },
    {
      "text": "the diagonal since we are taking soft",
      "start": 1561.6,
      "duration": 4.559
    },
    {
      "text": "Max before the elements which come in",
      "start": 1563.799,
      "duration": 4.161
    },
    {
      "text": "the future do affect the previous",
      "start": 1566.159,
      "duration": 4.481
    },
    {
      "text": "elements also so we need a way to avoid",
      "start": 1567.96,
      "duration": 5.52
    },
    {
      "text": "this so there is a smarter way to do",
      "start": 1570.64,
      "duration": 5.44
    },
    {
      "text": "this renormalization and let me tell you",
      "start": 1573.48,
      "duration": 4.84
    },
    {
      "text": "what that smarter way",
      "start": 1576.08,
      "duration": 5.28
    },
    {
      "text": "is so if you look at what all we have",
      "start": 1578.32,
      "duration": 5.0
    },
    {
      "text": "done until now so what we did is",
      "start": 1581.36,
      "duration": 3.799
    },
    {
      "text": "essentially this we took the attention",
      "start": 1583.32,
      "duration": 5.04
    },
    {
      "text": "scores we applied soft Max so this",
      "start": 1585.159,
      "duration": 4.88
    },
    {
      "text": "already brought in the influence of",
      "start": 1588.36,
      "duration": 5.08
    },
    {
      "text": "future tokens then we mask with zero",
      "start": 1590.039,
      "duration": 5.601
    },
    {
      "text": "then we again normalize the rows and",
      "start": 1593.44,
      "duration": 4.0
    },
    {
      "text": "then we got the attention weight Matrix",
      "start": 1595.64,
      "duration": 4.039
    },
    {
      "text": "right this is what we did right now what",
      "start": 1597.44,
      "duration": 4.239
    },
    {
      "text": "if there is a more efficient way so the",
      "start": 1599.679,
      "duration": 3.841
    },
    {
      "text": "efficient way is that what if we have",
      "start": 1601.679,
      "duration": 4.72
    },
    {
      "text": "the attention scores then we apply",
      "start": 1603.52,
      "duration": 4.639
    },
    {
      "text": "something called as an upper triangular",
      "start": 1606.399,
      "duration": 4.321
    },
    {
      "text": "Infinity mask and then we just apply",
      "start": 1608.159,
      "duration": 5.361
    },
    {
      "text": "softmax once this will ensure that there",
      "start": 1610.72,
      "duration": 5.48
    },
    {
      "text": "is no leakage problem let me explain",
      "start": 1613.52,
      "duration": 4.36
    },
    {
      "text": "what I mean by the upper triangular",
      "start": 1616.2,
      "duration": 4.52
    },
    {
      "text": "Infinity mask so let's",
      "start": 1617.88,
      "duration": 5.679
    },
    {
      "text": "say we have the let me first show you",
      "start": 1620.72,
      "duration": 5.12
    },
    {
      "text": "the attention scores so let's say we",
      "start": 1623.559,
      "duration": 5.561
    },
    {
      "text": "have the attention score Matrix right uh",
      "start": 1625.84,
      "duration": 5.48
    },
    {
      "text": "instead of applying soft Max earlier and",
      "start": 1629.12,
      "duration": 4.039
    },
    {
      "text": "getting the attention weight Matrix what",
      "start": 1631.32,
      "duration": 4.56
    },
    {
      "text": "if we replace so let's say for the first",
      "start": 1633.159,
      "duration": 4.88
    },
    {
      "text": "row what if we replace these values with",
      "start": 1635.88,
      "duration": 4.48
    },
    {
      "text": "negative Infinity for the second row",
      "start": 1638.039,
      "duration": 4.201
    },
    {
      "text": "we'll replace these values with negative",
      "start": 1640.36,
      "duration": 4.0
    },
    {
      "text": "Infinity basically what if we replace",
      "start": 1642.24,
      "duration": 4.24
    },
    {
      "text": "all the entries above the diagonal with",
      "start": 1644.36,
      "duration": 4.64
    },
    {
      "text": "negative Infinity like this and then we",
      "start": 1646.48,
      "duration": 5.24
    },
    {
      "text": "take the soft Max what that will ensure",
      "start": 1649.0,
      "duration": 5.559
    },
    {
      "text": "is that anyway when we take the soft Max",
      "start": 1651.72,
      "duration": 4.48
    },
    {
      "text": "when you do the exponent of negative",
      "start": 1654.559,
      "duration": 3.641
    },
    {
      "text": "Infinity it's going to be zero so when",
      "start": 1656.2,
      "duration": 4.04
    },
    {
      "text": "you take the soft Max of let's say this",
      "start": 1658.2,
      "duration": 4.64
    },
    {
      "text": "row all of these entries will anyway be",
      "start": 1660.24,
      "duration": 4.159
    },
    {
      "text": "zero and then they will automatically",
      "start": 1662.84,
      "duration": 3.4
    },
    {
      "text": "sum up to one because we are taking the",
      "start": 1664.399,
      "duration": 4.841
    },
    {
      "text": "soft Max so this kind of a trick will",
      "start": 1666.24,
      "duration": 5.64
    },
    {
      "text": "ensure that we are not having the data",
      "start": 1669.24,
      "duration": 4.919
    },
    {
      "text": "leakage problem because the attention",
      "start": 1671.88,
      "duration": 4.519
    },
    {
      "text": "scores are calculated so now when you",
      "start": 1674.159,
      "duration": 3.841
    },
    {
      "text": "look at each row there is no influence",
      "start": 1676.399,
      "duration": 3.601
    },
    {
      "text": "of future tokens yet because we have not",
      "start": 1678.0,
      "duration": 4.48
    },
    {
      "text": "done the soft Max then we just replace",
      "start": 1680.0,
      "duration": 4.039
    },
    {
      "text": "the elements above the diagonal with",
      "start": 1682.48,
      "duration": 3.439
    },
    {
      "text": "negative Infinity there is no influence",
      "start": 1684.039,
      "duration": 4.161
    },
    {
      "text": "of future tokens now we have cancelled",
      "start": 1685.919,
      "duration": 3.88
    },
    {
      "text": "the influence of future",
      "start": 1688.2,
      "duration": 4.04
    },
    {
      "text": "tokens by replacing them with negative",
      "start": 1689.799,
      "duration": 4.48
    },
    {
      "text": "infinity and we have not even done soft",
      "start": 1692.24,
      "duration": 4.799
    },
    {
      "text": "Max now and then we will do soft Max to",
      "start": 1694.279,
      "duration": 5.161
    },
    {
      "text": "this Matrix what the soft Max will do is",
      "start": 1697.039,
      "duration": 3.841
    },
    {
      "text": "that it will kill two birds with the",
      "start": 1699.44,
      "duration": 3.839
    },
    {
      "text": "same Stone it will replace all of these",
      "start": 1700.88,
      "duration": 4.639
    },
    {
      "text": "entries with zero because exponent of",
      "start": 1703.279,
      "duration": 4.52
    },
    {
      "text": "negative Infinity is anyway zero and",
      "start": 1705.519,
      "duration": 3.961
    },
    {
      "text": "since we are applying soft Max it will",
      "start": 1707.799,
      "duration": 3.641
    },
    {
      "text": "anyway ensure that the sum of every row",
      "start": 1709.48,
      "duration": 4.16
    },
    {
      "text": "is equal to one so it will ensure that",
      "start": 1711.44,
      "duration": 4.2
    },
    {
      "text": "the attention weight Matrix the rows all",
      "start": 1713.64,
      "duration": 4.6
    },
    {
      "text": "sum up to one and that is exactly what",
      "start": 1715.64,
      "duration": 6.8
    },
    {
      "text": "we are going to do next so now if if I",
      "start": 1718.24,
      "duration": 6.4
    },
    {
      "text": "give you this Matrix and if I tell you",
      "start": 1722.44,
      "duration": 3.64
    },
    {
      "text": "that you want to replace all the",
      "start": 1724.64,
      "duration": 5.36
    },
    {
      "text": "elements above the diagonal with zero uh",
      "start": 1726.08,
      "duration": 7.719
    },
    {
      "text": "or negative Infinity which whether you",
      "start": 1730.0,
      "duration": 6.559
    },
    {
      "text": "will use the upper triangular Matrix or",
      "start": 1733.799,
      "duration": 3.841
    },
    {
      "text": "whether you will use the lower",
      "start": 1736.559,
      "duration": 3.48
    },
    {
      "text": "triangular",
      "start": 1737.64,
      "duration": 2.399
    },
    {
      "text": "Matrix okay so let me tell you how this",
      "start": 1747.08,
      "duration": 4.599
    },
    {
      "text": "is actually done the way this works is",
      "start": 1749.44,
      "duration": 4.44
    },
    {
      "text": "that we first make a upper triangular",
      "start": 1751.679,
      "duration": 4.641
    },
    {
      "text": "Matrix so let me print this out to show",
      "start": 1753.88,
      "duration": 4.519
    },
    {
      "text": "you what we are doing",
      "start": 1756.32,
      "duration": 6.359
    },
    {
      "text": "here so we print this out right",
      "start": 1758.399,
      "duration": 6.961
    },
    {
      "text": "now incomplete input maybe I need one",
      "start": 1762.679,
      "duration": 4.201
    },
    {
      "text": "more bracket over",
      "start": 1765.36,
      "duration": 4.52
    },
    {
      "text": "here yeah so what we are going to do is",
      "start": 1766.88,
      "duration": 6.639
    },
    {
      "text": "that we are going to take uh again 6x6",
      "start": 1769.88,
      "duration": 5.399
    },
    {
      "text": "Matrix of ones and we are going to take",
      "start": 1773.519,
      "duration": 3.52
    },
    {
      "text": "an upper triangular Matrix this time",
      "start": 1775.279,
      "duration": 3.24
    },
    {
      "text": "remember earlier we took a lower",
      "start": 1777.039,
      "duration": 4.201
    },
    {
      "text": "triangular Matrix let me tell you why we",
      "start": 1778.519,
      "duration": 4.76
    },
    {
      "text": "take an upper triangular Matrix so we",
      "start": 1781.24,
      "duration": 3.64
    },
    {
      "text": "take an upper triangular Matrix where",
      "start": 1783.279,
      "duration": 3.841
    },
    {
      "text": "all of these are ones so what we are",
      "start": 1784.88,
      "duration": 4.32
    },
    {
      "text": "going to code later is that we are going",
      "start": 1787.12,
      "duration": 4.12
    },
    {
      "text": "to say that look at all of the places",
      "start": 1789.2,
      "duration": 3.68
    },
    {
      "text": "where there are ones and replace those",
      "start": 1791.24,
      "duration": 4.6
    },
    {
      "text": "ones with negative Infinity that is the",
      "start": 1792.88,
      "duration": 5.159
    },
    {
      "text": "mask which we are going to construct so",
      "start": 1795.84,
      "duration": 6.0
    },
    {
      "text": "so we have this mask tensor over here",
      "start": 1798.039,
      "duration": 7.76
    },
    {
      "text": "which is a vector of essentially zeros",
      "start": 1801.84,
      "duration": 6.559
    },
    {
      "text": "but all the elements above the TR above",
      "start": 1805.799,
      "duration": 5.24
    },
    {
      "text": "the diagonal are one then what we do is",
      "start": 1808.399,
      "duration": 4.921
    },
    {
      "text": "we use this attention scores do mask",
      "start": 1811.039,
      "duration": 4.88
    },
    {
      "text": "fill function so what this mask fill",
      "start": 1813.32,
      "duration": 6.04
    },
    {
      "text": "function does in tensor flow or torch.",
      "start": 1815.919,
      "duration": 5.48
    },
    {
      "text": "tensor I'll share this link with you",
      "start": 1819.36,
      "duration": 5.48
    },
    {
      "text": "what this uh function does is that uh it",
      "start": 1821.399,
      "duration": 5.201
    },
    {
      "text": "looks at the argument first so what's",
      "start": 1824.84,
      "duration": 4.0
    },
    {
      "text": "there inside is that we take this mask",
      "start": 1826.6,
      "duration": 4.6
    },
    {
      "text": "we take this mask Matrix and we find out",
      "start": 1828.84,
      "duration": 5.319
    },
    {
      "text": "all of the places where the uh Matrix",
      "start": 1831.2,
      "duration": 5.28
    },
    {
      "text": "returns a positive or True Value and",
      "start": 1834.159,
      "duration": 3.921
    },
    {
      "text": "those are all the places which are above",
      "start": 1836.48,
      "duration": 4.72
    },
    {
      "text": "the diagonal right and we'll replace",
      "start": 1838.08,
      "duration": 6.28
    },
    {
      "text": "this so what this Mas field does is that",
      "start": 1841.2,
      "duration": 5.44
    },
    {
      "text": "it looks for all the places",
      "start": 1844.36,
      "duration": 5.0
    },
    {
      "text": "where uh this mask Matrix has positive",
      "start": 1846.64,
      "duration": 5.24
    },
    {
      "text": "values and then in the attention score",
      "start": 1849.36,
      "duration": 4.64
    },
    {
      "text": "Matrix will replace all of those with",
      "start": 1851.88,
      "duration": 5.08
    },
    {
      "text": "negative Infinity so effectively what",
      "start": 1854.0,
      "duration": 5.639
    },
    {
      "text": "this uh ATT attention scores. mask fill",
      "start": 1856.96,
      "duration": 4.079
    },
    {
      "text": "function does is that it takes the",
      "start": 1859.639,
      "duration": 3.92
    },
    {
      "text": "attention scores Matrix and it replaces",
      "start": 1861.039,
      "duration": 4.401
    },
    {
      "text": "all of the elements above the diagonal",
      "start": 1863.559,
      "duration": 4.08
    },
    {
      "text": "with a negative Infinity this is exactly",
      "start": 1865.44,
      "duration": 4.959
    },
    {
      "text": "what we wanted and now what we do is we",
      "start": 1867.639,
      "duration": 4.92
    },
    {
      "text": "take this Matrix and we apply torge do",
      "start": 1870.399,
      "duration": 4.961
    },
    {
      "text": "soft Max so again as we did previously",
      "start": 1872.559,
      "duration": 4.681
    },
    {
      "text": "First We Take The Mask Matrix and divide",
      "start": 1875.36,
      "duration": 3.48
    },
    {
      "text": "it by the square root of the keys",
      "start": 1877.24,
      "duration": 4.319
    },
    {
      "text": "Dimension and then we apply the soft Max",
      "start": 1878.84,
      "duration": 4.36
    },
    {
      "text": "so then that will ensure that all the",
      "start": 1881.559,
      "duration": 4.08
    },
    {
      "text": "infinity values will anyway become zero",
      "start": 1883.2,
      "duration": 5.64
    },
    {
      "text": "and each row will sum up to one so now",
      "start": 1885.639,
      "duration": 5.04
    },
    {
      "text": "my final attention weight Matrix looks",
      "start": 1888.84,
      "duration": 4.04
    },
    {
      "text": "something like this where you will see",
      "start": 1890.679,
      "duration": 3.96
    },
    {
      "text": "that the data leakage problem is not",
      "start": 1892.88,
      "duration": 4.6
    },
    {
      "text": "there because I apply soft Max after all",
      "start": 1894.639,
      "duration": 4.361
    },
    {
      "text": "of the future elements are set to",
      "start": 1897.48,
      "duration": 3.88
    },
    {
      "text": "negative infinity and second all of the",
      "start": 1899.0,
      "duration": 4.799
    },
    {
      "text": "attention weight Matrix rows sum up to",
      "start": 1901.36,
      "duration": 5.76
    },
    {
      "text": "one so the causal attention mechanism is",
      "start": 1903.799,
      "duration": 6.0
    },
    {
      "text": "satisfied and also the soft Max is",
      "start": 1907.12,
      "duration": 4.799
    },
    {
      "text": "satisfied the data leakage problem is",
      "start": 1909.799,
      "duration": 4.641
    },
    {
      "text": "not there and each row sums up to one so",
      "start": 1911.919,
      "duration": 4.24
    },
    {
      "text": "I have essentially obtained everything",
      "start": 1914.44,
      "duration": 3.92
    },
    {
      "text": "which I wanted in calcul of these",
      "start": 1916.159,
      "duration": 3.36
    },
    {
      "text": "attention",
      "start": 1918.36,
      "duration": 3.72
    },
    {
      "text": "weights just to I've written some of",
      "start": 1919.519,
      "duration": 4.241
    },
    {
      "text": "these explanations over here so that you",
      "start": 1922.08,
      "duration": 4.24
    },
    {
      "text": "can understand it better so masking in",
      "start": 1923.76,
      "duration": 5.0
    },
    {
      "text": "Transformers set scores for future",
      "start": 1926.32,
      "duration": 4.56
    },
    {
      "text": "tokens to a very large large negative",
      "start": 1928.76,
      "duration": 4.919
    },
    {
      "text": "value such as these uh making their",
      "start": 1930.88,
      "duration": 4.679
    },
    {
      "text": "influence in the softmax calculation",
      "start": 1933.679,
      "duration": 4.521
    },
    {
      "text": "effectively zero the softmax function",
      "start": 1935.559,
      "duration": 4.401
    },
    {
      "text": "then recalculates attention weights",
      "start": 1938.2,
      "duration": 4.28
    },
    {
      "text": "among the unmask tokens this process",
      "start": 1939.96,
      "duration": 4.719
    },
    {
      "text": "ensures no information leakage from the",
      "start": 1942.48,
      "duration": 4.679
    },
    {
      "text": "mass tokens focusing the model solely on",
      "start": 1944.679,
      "duration": 4.801
    },
    {
      "text": "intended data",
      "start": 1947.159,
      "duration": 4.36
    },
    {
      "text": "now uh since we have got the attention",
      "start": 1949.48,
      "duration": 4.679
    },
    {
      "text": "weight Matrix we can just simply",
      "start": 1951.519,
      "duration": 4.681
    },
    {
      "text": "multiply them with the values Matrix to",
      "start": 1954.159,
      "duration": 5.281
    },
    {
      "text": "get the context Vector that's it this is",
      "start": 1956.2,
      "duration": 5.16
    },
    {
      "text": "the implementation of the causal",
      "start": 1959.44,
      "duration": 4.8
    },
    {
      "text": "attention mechanism in Python but there",
      "start": 1961.36,
      "duration": 5.159
    },
    {
      "text": "is one more additional step which is",
      "start": 1964.24,
      "duration": 4.12
    },
    {
      "text": "typically implemented along with the",
      "start": 1966.519,
      "duration": 4.481
    },
    {
      "text": "causal attention mechanism and that is",
      "start": 1968.36,
      "duration": 4.72
    },
    {
      "text": "implementing the causal attention",
      "start": 1971.0,
      "duration": 3.519
    },
    {
      "text": "mechanism with",
      "start": 1973.08,
      "duration": 3.8
    },
    {
      "text": "Dropout so if you're not familiar with",
      "start": 1974.519,
      "duration": 4.28
    },
    {
      "text": "Dropout it's actually a deep learning",
      "start": 1976.88,
      "duration": 3.519
    },
    {
      "text": "technique where you take a neural",
      "start": 1978.799,
      "duration": 3.161
    },
    {
      "text": "network and you randomly switch on",
      "start": 1980.399,
      "duration": 4.28
    },
    {
      "text": "neurons in different layers to zero what",
      "start": 1981.96,
      "duration": 4.48
    },
    {
      "text": "this does is that usually when you are",
      "start": 1984.679,
      "duration": 5.161
    },
    {
      "text": "training some neurons become lazy and",
      "start": 1986.44,
      "duration": 5.0
    },
    {
      "text": "they do not do any work because they",
      "start": 1989.84,
      "duration": 3.28
    },
    {
      "text": "realize that other neurons are anyway",
      "start": 1991.44,
      "duration": 4.199
    },
    {
      "text": "doing most of the work and the result is",
      "start": 1993.12,
      "duration": 4.72
    },
    {
      "text": "pretty well so I'll just switch off so",
      "start": 1995.639,
      "duration": 3.76
    },
    {
      "text": "that's a lazy neuron problem or",
      "start": 1997.84,
      "duration": 4.04
    },
    {
      "text": "codependency problem what Dropout",
      "start": 1999.399,
      "duration": 5.361
    },
    {
      "text": "ensures is that when a lazy neuron sees",
      "start": 2001.88,
      "duration": 5.0
    },
    {
      "text": "that the other neuron is Switched Off it",
      "start": 2004.76,
      "duration": 4.6
    },
    {
      "text": "it's forced to do the work uh that's the",
      "start": 2006.88,
      "duration": 4.44
    },
    {
      "text": "simplest way of thinking about it so",
      "start": 2009.36,
      "duration": 4.319
    },
    {
      "text": "Dropout randomly turns neurons off so it",
      "start": 2011.32,
      "duration": 4.359
    },
    {
      "text": "ensures that all the neurons essentially",
      "start": 2013.679,
      "duration": 5.36
    },
    {
      "text": "participates and this leads to better",
      "start": 2015.679,
      "duration": 5.641
    },
    {
      "text": "generalization it prevents overfitting",
      "start": 2019.039,
      "duration": 4.801
    },
    {
      "text": "and it does better on the test data we",
      "start": 2021.32,
      "duration": 6.28
    },
    {
      "text": "will so the main advantage of Dropout is",
      "start": 2023.84,
      "duration": 5.199
    },
    {
      "text": "that it prevents overfitting and",
      "start": 2027.6,
      "duration": 3.16
    },
    {
      "text": "improves generalization",
      "start": 2029.039,
      "duration": 4.281
    },
    {
      "text": "performance in Transformer architecture",
      "start": 2030.76,
      "duration": 5.08
    },
    {
      "text": "including models such as GPT Dropout in",
      "start": 2033.32,
      "duration": 4.479
    },
    {
      "text": "the attention mechanism is implemented",
      "start": 2035.84,
      "duration": 3.92
    },
    {
      "text": "and it's applied usually in two specific",
      "start": 2037.799,
      "duration": 4.48
    },
    {
      "text": "areas first it's applied after the",
      "start": 2039.76,
      "duration": 5.039
    },
    {
      "text": "calculation of the attention scores and",
      "start": 2042.279,
      "duration": 5.0
    },
    {
      "text": "second it's after applying attention",
      "start": 2044.799,
      "duration": 5.56
    },
    {
      "text": "weights to the value vectors so there",
      "start": 2047.279,
      "duration": 6.12
    },
    {
      "text": "are two specific uh ways in",
      "start": 2050.359,
      "duration": 6.76
    },
    {
      "text": "which um Dropout can generally be",
      "start": 2053.399,
      "duration": 6.081
    },
    {
      "text": "implemented first is after you get the",
      "start": 2057.119,
      "duration": 4.24
    },
    {
      "text": "context Vector itself after applying",
      "start": 2059.48,
      "duration": 3.439
    },
    {
      "text": "attention weights to the value vectors",
      "start": 2061.359,
      "duration": 4.48
    },
    {
      "text": "you can Implement Dropout but the more",
      "start": 2062.919,
      "duration": 6.2
    },
    {
      "text": "common way is to Implement Dropout after",
      "start": 2065.839,
      "duration": 5.361
    },
    {
      "text": "calculation of the attention weights or",
      "start": 2069.119,
      "duration": 4.201
    },
    {
      "text": "the attention scores and hence we are",
      "start": 2071.2,
      "duration": 4.52
    },
    {
      "text": "going to consider that so essentially",
      "start": 2073.32,
      "duration": 3.96
    },
    {
      "text": "what is done in the dropouts is that",
      "start": 2075.72,
      "duration": 2.72
    },
    {
      "text": "let's say if you have an attention",
      "start": 2077.28,
      "duration": 2.72
    },
    {
      "text": "weight Matrix which with causal",
      "start": 2078.44,
      "duration": 3.6
    },
    {
      "text": "attention implemented so all future",
      "start": 2080.0,
      "duration": 4.32
    },
    {
      "text": "tokens have been masked what we will do",
      "start": 2082.04,
      "duration": 4.119
    },
    {
      "text": "is that we will first create a Dropout",
      "start": 2084.32,
      "duration": 5.16
    },
    {
      "text": "mask what this Dropout mask specifies is",
      "start": 2086.159,
      "duration": 5.041
    },
    {
      "text": "what all neurons need to be randomly",
      "start": 2089.48,
      "duration": 4.199
    },
    {
      "text": "turned off so let's say if we Implement",
      "start": 2091.2,
      "duration": 5.56
    },
    {
      "text": "a Dropout with a probability of 0.5 this",
      "start": 2093.679,
      "duration": 6.001
    },
    {
      "text": "means that on average 50% of the",
      "start": 2096.76,
      "duration": 4.4
    },
    {
      "text": "attention weights in each row will be",
      "start": 2099.68,
      "duration": 3.24
    },
    {
      "text": "turned off so let's say if you look at",
      "start": 2101.16,
      "duration": 3.679
    },
    {
      "text": "the second row let's say this will be",
      "start": 2102.92,
      "duration": 4.04
    },
    {
      "text": "turned off if you look at the third row",
      "start": 2104.839,
      "duration": 5.0
    },
    {
      "text": "50% right so three entries so randomly",
      "start": 2106.96,
      "duration": 4.28
    },
    {
      "text": "this will be turned off this will be",
      "start": 2109.839,
      "duration": 4.561
    },
    {
      "text": "turned off if you look at the uh fifth",
      "start": 2111.24,
      "duration": 6.16
    },
    {
      "text": "row 50% so you'll you'll randomly zero",
      "start": 2114.4,
      "duration": 4.84
    },
    {
      "text": "out certain",
      "start": 2117.4,
      "duration": 4.32
    },
    {
      "text": "elements so this this is how Dropout is",
      "start": 2119.24,
      "duration": 4.32
    },
    {
      "text": "implemented so this is the Dropout mask",
      "start": 2121.72,
      "duration": 3.6
    },
    {
      "text": "which you can see over here wherever the",
      "start": 2123.56,
      "duration": 4.32
    },
    {
      "text": "mask appears those particular element M",
      "start": 2125.32,
      "duration": 4.16
    },
    {
      "text": "will need to be zeroed out so if you",
      "start": 2127.88,
      "duration": 4.36
    },
    {
      "text": "look at the fourth row over here uh let",
      "start": 2129.48,
      "duration": 5.879
    },
    {
      "text": "me rub some of the things over here yeah",
      "start": 2132.24,
      "duration": 5.24
    },
    {
      "text": "so if you look at the fourth row in this",
      "start": 2135.359,
      "duration": 5.281
    },
    {
      "text": "Dropout mask we have a mask position",
      "start": 2137.48,
      "duration": 6.119
    },
    {
      "text": "here here and here so we have a position",
      "start": 2140.64,
      "duration": 5.8
    },
    {
      "text": "at 1 four and five so the first entry",
      "start": 2143.599,
      "duration": 4.881
    },
    {
      "text": "will be masked it will be removed the",
      "start": 2146.44,
      "duration": 4.52
    },
    {
      "text": "fourth entry will be masked so only two",
      "start": 2148.48,
      "duration": 5.359
    },
    {
      "text": "entries are going to survive here 24 and",
      "start": 2150.96,
      "duration": 7.0
    },
    {
      "text": "24 so here you can see over here 24 and",
      "start": 2153.839,
      "duration": 6.0
    },
    {
      "text": "point 24 are the only two entries",
      "start": 2157.96,
      "duration": 4.2
    },
    {
      "text": "surviving in this row so essentially",
      "start": 2159.839,
      "duration": 4.721
    },
    {
      "text": "what the Dropout uh does in very simple",
      "start": 2162.16,
      "duration": 4.64
    },
    {
      "text": "terms is that it looks at rows and then",
      "start": 2164.56,
      "duration": 4.559
    },
    {
      "text": "it randomly switches switches off",
      "start": 2166.8,
      "duration": 4.84
    },
    {
      "text": "attention weights with a particular",
      "start": 2169.119,
      "duration": 3.681
    },
    {
      "text": "given",
      "start": 2171.64,
      "duration": 3.959
    },
    {
      "text": "probability uh so now let me Implement",
      "start": 2172.8,
      "duration": 6.2
    },
    {
      "text": "first the Dropout in um in Python so in",
      "start": 2175.599,
      "duration": 5.201
    },
    {
      "text": "the following code example what we are",
      "start": 2179.0,
      "duration": 3.28
    },
    {
      "text": "going to do is we are going to use a",
      "start": 2180.8,
      "duration": 4.24
    },
    {
      "text": "dropout rate of 50% which means that we",
      "start": 2182.28,
      "duration": 4.36
    },
    {
      "text": "are going to mask out half of the",
      "start": 2185.04,
      "duration": 3.76
    },
    {
      "text": "attention weights",
      "start": 2186.64,
      "duration": 4.4
    },
    {
      "text": "later when we train the GPT model we are",
      "start": 2188.8,
      "duration": 3.92
    },
    {
      "text": "going to use a lower dropout rate of",
      "start": 2191.04,
      "duration": 3.6
    },
    {
      "text": "around 0.1",
      "start": 2192.72,
      "duration": 5.08
    },
    {
      "text": "or02 so uh in the following code we",
      "start": 2194.64,
      "duration": 5.36
    },
    {
      "text": "apply pytorch Dropout implementation to",
      "start": 2197.8,
      "duration": 5.12
    },
    {
      "text": "a 6x6 tensor consisting of just ones for",
      "start": 2200.0,
      "duration": 4.68
    },
    {
      "text": "illustration purposes and then we'll",
      "start": 2202.92,
      "duration": 3.28
    },
    {
      "text": "actually apply it on the attention",
      "start": 2204.68,
      "duration": 3.76
    },
    {
      "text": "weight Matrix which we have so let's say",
      "start": 2206.2,
      "duration": 5.2
    },
    {
      "text": "we have a 6x6 uh we have an example",
      "start": 2208.44,
      "duration": 5.56
    },
    {
      "text": "which is a 6x6 Matrix of on let me print",
      "start": 2211.4,
      "duration": 5.0
    },
    {
      "text": "it out over",
      "start": 2214.0,
      "duration": 5.359
    },
    {
      "text": "here uh uh",
      "start": 2216.4,
      "duration": 8.919
    },
    {
      "text": "yeah so let me print print example so",
      "start": 2219.359,
      "duration": 8.561
    },
    {
      "text": "let's say we have a matrix 6x6 so these",
      "start": 2225.319,
      "duration": 5.0
    },
    {
      "text": "are all ones then we'll Implement tor.",
      "start": 2227.92,
      "duration": 5.32
    },
    {
      "text": "nn. Dropout point5 what this is going to",
      "start": 2230.319,
      "duration": 5.401
    },
    {
      "text": "do is that it will look at each row and",
      "start": 2233.24,
      "duration": 5.16
    },
    {
      "text": "then on average it will switch off 50%",
      "start": 2235.72,
      "duration": 5.359
    },
    {
      "text": "of the weights and what this will do is",
      "start": 2238.4,
      "duration": 5.48
    },
    {
      "text": "that since the 50% of Weights are",
      "start": 2241.079,
      "duration": 4.721
    },
    {
      "text": "Switched Off which means 0.5 all the",
      "start": 2243.88,
      "duration": 3.92
    },
    {
      "text": "other weights are rescaled by that that",
      "start": 2245.8,
      "duration": 4.64
    },
    {
      "text": "much amount so all the other weights",
      "start": 2247.8,
      "duration": 3.799
    },
    {
      "text": "which are not Switched Off will be",
      "start": 2250.44,
      "duration": 3.48
    },
    {
      "text": "rescaled by two it will be divided by",
      "start": 2251.599,
      "duration": 5.081
    },
    {
      "text": "0.5 or they'll be multiplied by two so",
      "start": 2253.92,
      "duration": 4.399
    },
    {
      "text": "if you look at the first row over here",
      "start": 2256.68,
      "duration": 4.0
    },
    {
      "text": "you'll see that two weights are switched",
      "start": 2258.319,
      "duration": 3.921
    },
    {
      "text": "off if you look at the second row you'll",
      "start": 2260.68,
      "duration": 3.2
    },
    {
      "text": "switch you'll see that four weights have",
      "start": 2262.24,
      "duration": 2.96
    },
    {
      "text": "been switched off you look at the third",
      "start": 2263.88,
      "duration": 2.64
    },
    {
      "text": "row you'll see that one weight has been",
      "start": 2265.2,
      "duration": 3.8
    },
    {
      "text": "switched off so remember this is",
      "start": 2266.52,
      "duration": 7.24
    },
    {
      "text": "probabilistic so if you take 10,000 rows",
      "start": 2269.0,
      "duration": 6.64
    },
    {
      "text": "you'll see that on an average",
      "start": 2273.76,
      "duration": 4.96
    },
    {
      "text": "50% of every row will be switched off so",
      "start": 2275.64,
      "duration": 5.479
    },
    {
      "text": "that does not guarantee that three exact",
      "start": 2278.72,
      "duration": 4.119
    },
    {
      "text": "neurons or three exact weights will be",
      "start": 2281.119,
      "duration": 4.041
    },
    {
      "text": "switched off in every row two three or",
      "start": 2282.839,
      "duration": 4.961
    },
    {
      "text": "four neurons might be switched off but",
      "start": 2285.16,
      "duration": 4.32
    },
    {
      "text": "on an average three neurons will be",
      "start": 2287.8,
      "duration": 4.0
    },
    {
      "text": "switched off in every row so when",
      "start": 2289.48,
      "duration": 4.04
    },
    {
      "text": "applying Dropout to an attention weight",
      "start": 2291.8,
      "duration": 4.039
    },
    {
      "text": "Matrix with the rate of 50% half of the",
      "start": 2293.52,
      "duration": 3.96
    },
    {
      "text": "elements with the of the Matrix are",
      "start": 2295.839,
      "duration": 3.881
    },
    {
      "text": "randomly set to zero remember this is",
      "start": 2297.48,
      "duration": 4.359
    },
    {
      "text": "probabilistic to compensate for the",
      "start": 2299.72,
      "duration": 3.92
    },
    {
      "text": "reduction in active elements the values",
      "start": 2301.839,
      "duration": 3.401
    },
    {
      "text": "of the remaining elements in The Matrix",
      "start": 2303.64,
      "duration": 4.0
    },
    {
      "text": "are scaled up by a factor of two this is",
      "start": 2305.24,
      "duration": 5.56
    },
    {
      "text": "how tor. nn. Dropout is implemented and",
      "start": 2307.64,
      "duration": 5.0
    },
    {
      "text": "you can even check this so if I click on",
      "start": 2310.8,
      "duration": 5.48
    },
    {
      "text": "tor. NN drop. Dropout you can see the",
      "start": 2312.64,
      "duration": 5.28
    },
    {
      "text": "documentation for the",
      "start": 2316.28,
      "duration": 5.559
    },
    {
      "text": "dropout dropout class in tensor flow or",
      "start": 2317.92,
      "duration": 5.88
    },
    {
      "text": "py torch rather so this is a pytorch",
      "start": 2321.839,
      "duration": 4.721
    },
    {
      "text": "Dropout class so you'll see that during",
      "start": 2323.8,
      "duration": 4.6
    },
    {
      "text": "training randomly zeros out some of the",
      "start": 2326.56,
      "duration": 4.24
    },
    {
      "text": "elements with probability P outputs are",
      "start": 2328.4,
      "duration": 5.28
    },
    {
      "text": "scaled by a factor of 1 upon 1 minus P",
      "start": 2330.8,
      "duration": 4.48
    },
    {
      "text": "that is exactly the kind of scaling",
      "start": 2333.68,
      "duration": 3.84
    },
    {
      "text": "which we are seeing over here so the the",
      "start": 2335.28,
      "duration": 3.92
    },
    {
      "text": "scaling is crucial to maintain the",
      "start": 2337.52,
      "duration": 4.4
    },
    {
      "text": "overall overall balance of the attention",
      "start": 2339.2,
      "duration": 5.399
    },
    {
      "text": "weights uh ensuring that the average",
      "start": 2341.92,
      "duration": 4.48
    },
    {
      "text": "influence of the attention mechanism",
      "start": 2344.599,
      "duration": 3.52
    },
    {
      "text": "remains consistent during training and",
      "start": 2346.4,
      "duration": 4.679
    },
    {
      "text": "inference phases now let us actually",
      "start": 2348.119,
      "duration": 4.641
    },
    {
      "text": "take the attention weights which we had",
      "start": 2351.079,
      "duration": 3.721
    },
    {
      "text": "over here these were the final attention",
      "start": 2352.76,
      "duration": 3.319
    },
    {
      "text": "weights and we are going to apply",
      "start": 2354.8,
      "duration": 4.88
    },
    {
      "text": "Dropout layer so I take the attention",
      "start": 2356.079,
      "duration": 6.121
    },
    {
      "text": "weight Matrix and I apply Dropout to it",
      "start": 2359.68,
      "duration": 5.12
    },
    {
      "text": "and here Dropout is being defined as a",
      "start": 2362.2,
      "duration": 4.56
    },
    {
      "text": "class and the class takes here an",
      "start": 2364.8,
      "duration": 3.88
    },
    {
      "text": "instance of the Dropout class is created",
      "start": 2366.76,
      "duration": 4.559
    },
    {
      "text": "that the input argument is 05 which",
      "start": 2368.68,
      "duration": 5.12
    },
    {
      "text": "means the dropout rate is 0.5 so here",
      "start": 2371.319,
      "duration": 5.76
    },
    {
      "text": "you can see that compared to this um",
      "start": 2373.8,
      "duration": 4.76
    },
    {
      "text": "versus let's say if you see the",
      "start": 2377.079,
      "duration": 3.0
    },
    {
      "text": "attention weight Matrix you drop out",
      "start": 2378.56,
      "duration": 3.44
    },
    {
      "text": "you'll see that some attention weights",
      "start": 2380.079,
      "duration": 4.24
    },
    {
      "text": "will be randomly set to zero and the",
      "start": 2382.0,
      "duration": 3.88
    },
    {
      "text": "weights which are not set to zero will",
      "start": 2384.319,
      "duration": 3.921
    },
    {
      "text": "be scaled by two so in the first row",
      "start": 2385.88,
      "duration": 3.92
    },
    {
      "text": "you'll see that this first weight is not",
      "start": 2388.24,
      "duration": 3.4
    },
    {
      "text": "set to zero so it will be multiplied by",
      "start": 2389.8,
      "duration": 4.0
    },
    {
      "text": "two let's look at the second row so we",
      "start": 2391.64,
      "duration": 6.199
    },
    {
      "text": "have 3986 and 60 and4 after implementing",
      "start": 2393.8,
      "duration": 7.0
    },
    {
      "text": "Dropout both of them are set to",
      "start": 2397.839,
      "duration": 5.961
    },
    {
      "text": "zero uh then let's look at the third row",
      "start": 2400.8,
      "duration": 5.84
    },
    {
      "text": "2526 3791",
      "start": 2403.8,
      "duration": 5.64
    },
    {
      "text": "3683 after implementing Dropout none of",
      "start": 2406.64,
      "duration": 5.56
    },
    {
      "text": "them are set to zero so since it's",
      "start": 2409.44,
      "duration": 4.84
    },
    {
      "text": "probabilistic in nature some weights",
      "start": 2412.2,
      "duration": 3.84
    },
    {
      "text": "will be set to zero some will not but",
      "start": 2414.28,
      "duration": 3.76
    },
    {
      "text": "overall 50% of the attention weights",
      "start": 2416.04,
      "duration": 3.4
    },
    {
      "text": "will be set to",
      "start": 2418.04,
      "duration": 4.68
    },
    {
      "text": "zero so as you can see the resulting ATT",
      "start": 2419.44,
      "duration": 4.84
    },
    {
      "text": "attention weight Matrix now has",
      "start": 2422.72,
      "duration": 3.48
    },
    {
      "text": "additional elements zeroed out and the",
      "start": 2424.28,
      "duration": 4.079
    },
    {
      "text": "remaining ones are rescaped SC this is",
      "start": 2426.2,
      "duration": 4.399
    },
    {
      "text": "exactly what we",
      "start": 2428.359,
      "duration": 5.161
    },
    {
      "text": "wanted now we have gained an actual",
      "start": 2430.599,
      "duration": 4.681
    },
    {
      "text": "understanding of causal attention and",
      "start": 2433.52,
      "duration": 3.799
    },
    {
      "text": "Dropout masking we will Implement a",
      "start": 2435.28,
      "duration": 5.44
    },
    {
      "text": "causal attention class in Python so this",
      "start": 2437.319,
      "duration": 5.161
    },
    {
      "text": "is also what we are going to see next on",
      "start": 2440.72,
      "duration": 3.68
    },
    {
      "text": "the",
      "start": 2442.48,
      "duration": 4.08
    },
    {
      "text": "Whiteboard so the next section which we",
      "start": 2444.4,
      "duration": 4.4
    },
    {
      "text": "are going to see is uh implementing a",
      "start": 2446.56,
      "duration": 4.279
    },
    {
      "text": "causal attention class which",
      "start": 2448.8,
      "duration": 3.96
    },
    {
      "text": "incorporates causal attention and",
      "start": 2450.839,
      "duration": 4.321
    },
    {
      "text": "Dropout into the self attention class",
      "start": 2452.76,
      "duration": 4.92
    },
    {
      "text": "which we have implemented earlier so to",
      "start": 2455.16,
      "duration": 4.56
    },
    {
      "text": "do this first I want you to have a",
      "start": 2457.68,
      "duration": 3.72
    },
    {
      "text": "visual understanding of what we are",
      "start": 2459.72,
      "duration": 4.24
    },
    {
      "text": "going to implement that will make",
      "start": 2461.4,
      "duration": 4.56
    },
    {
      "text": "understanding the code so much easier so",
      "start": 2463.96,
      "duration": 3.159
    },
    {
      "text": "if you have understood the self",
      "start": 2465.96,
      "duration": 3.04
    },
    {
      "text": "attention class when we implement this",
      "start": 2467.119,
      "duration": 3.761
    },
    {
      "text": "causal attention it's exactly going to",
      "start": 2469.0,
      "duration": 3.68
    },
    {
      "text": "be the same except for a few small",
      "start": 2470.88,
      "duration": 4.479
    },
    {
      "text": "changes so we will have the inputs we",
      "start": 2472.68,
      "duration": 5.2
    },
    {
      "text": "will multiply it with the weight query",
      "start": 2475.359,
      "duration": 4.161
    },
    {
      "text": "weight key and the weight value",
      "start": 2477.88,
      "duration": 4.239
    },
    {
      "text": "trainable matrices then we'll obtain the",
      "start": 2479.52,
      "duration": 5.64
    },
    {
      "text": "queries keys and the value Matrix then",
      "start": 2482.119,
      "duration": 4.601
    },
    {
      "text": "we'll get the attention scores by",
      "start": 2485.16,
      "duration": 4.32
    },
    {
      "text": "multiplying queries with keys transfer",
      "start": 2486.72,
      "duration": 4.2
    },
    {
      "text": "then what we'll do is that we'll",
      "start": 2489.48,
      "duration": 4.68
    },
    {
      "text": "Implement uh we'll mask out so all of",
      "start": 2490.92,
      "duration": 5.08
    },
    {
      "text": "these diagonals will be replaced with",
      "start": 2494.16,
      "duration": 3.72
    },
    {
      "text": "minus all the elements above the",
      "start": 2496.0,
      "duration": 3.64
    },
    {
      "text": "diagonal will be replaced with minus",
      "start": 2497.88,
      "duration": 4.28
    },
    {
      "text": "infinity then we will do scaling by",
      "start": 2499.64,
      "duration": 5.0
    },
    {
      "text": "square root of the dimension and we'll",
      "start": 2502.16,
      "duration": 4.679
    },
    {
      "text": "do Dropout and we'll do soft",
      "start": 2504.64,
      "duration": 4.6
    },
    {
      "text": "Max so that will give us the attention",
      "start": 2506.839,
      "duration": 4.361
    },
    {
      "text": "weights so remember now the attention",
      "start": 2509.24,
      "duration": 4.119
    },
    {
      "text": "weights all the elements above the",
      "start": 2511.2,
      "duration": 4.119
    },
    {
      "text": "diagonal will be equal to zero and some",
      "start": 2513.359,
      "duration": 3.881
    },
    {
      "text": "of the elements will be randomly switch",
      "start": 2515.319,
      "duration": 3.76
    },
    {
      "text": "switched off because we are implementing",
      "start": 2517.24,
      "duration": 4.52
    },
    {
      "text": "Dropout and then we'll get the we'll",
      "start": 2519.079,
      "duration": 4.201
    },
    {
      "text": "multiply the attention weights with the",
      "start": 2521.76,
      "duration": 3.12
    },
    {
      "text": "values and we'll get the context Vector",
      "start": 2523.28,
      "duration": 3.44
    },
    {
      "text": "Matrix this is all which we are going to",
      "start": 2524.88,
      "duration": 6.52
    },
    {
      "text": "do in the uh causal attention class one",
      "start": 2526.72,
      "duration": 6.879
    },
    {
      "text": "more additional step which we are going",
      "start": 2531.4,
      "duration": 3.64
    },
    {
      "text": "to do is that we are going to look at",
      "start": 2533.599,
      "duration": 4.201
    },
    {
      "text": "batches so this is the first batch right",
      "start": 2535.04,
      "duration": 4.24
    },
    {
      "text": "so this is the first sentence your",
      "start": 2537.8,
      "duration": 4.24
    },
    {
      "text": "journey begins with one step what we",
      "start": 2539.28,
      "duration": 5.52
    },
    {
      "text": "ideally want to do is we want to develop",
      "start": 2542.04,
      "duration": 4.4
    },
    {
      "text": "the attention class which can handle",
      "start": 2544.8,
      "duration": 3.88
    },
    {
      "text": "multiple Cent sentences at once so what",
      "start": 2546.44,
      "duration": 4.2
    },
    {
      "text": "if there is a second sentence also that",
      "start": 2548.68,
      "duration": 4.36
    },
    {
      "text": "second sentence can be my name is",
      "start": 2550.64,
      "duration": 4.32
    },
    {
      "text": "something something let's say so that",
      "start": 2553.04,
      "duration": 3.68
    },
    {
      "text": "second sentence will also be handled in",
      "start": 2554.96,
      "duration": 4.2
    },
    {
      "text": "a very similar Manner and then I'll get",
      "start": 2556.72,
      "duration": 4.04
    },
    {
      "text": "another context weight Matrix in a",
      "start": 2559.16,
      "duration": 5.64
    },
    {
      "text": "similar manner so my the class which I",
      "start": 2560.76,
      "duration": 6.2
    },
    {
      "text": "Define should be able to handle both of",
      "start": 2564.8,
      "duration": 4.64
    },
    {
      "text": "these batches together so let's see how",
      "start": 2566.96,
      "duration": 4.8
    },
    {
      "text": "we can Implement",
      "start": 2569.44,
      "duration": 5.28
    },
    {
      "text": "that uh one more thing yeah so as I",
      "start": 2571.76,
      "duration": 4.559
    },
    {
      "text": "mentioned one more thing is to ensure",
      "start": 2574.72,
      "duration": 3.2
    },
    {
      "text": "that the code can handle batches",
      "start": 2576.319,
      "duration": 3.561
    },
    {
      "text": "consisting of more than one input as I",
      "start": 2577.92,
      "duration": 3.159
    },
    {
      "text": "showed you",
      "start": 2579.88,
      "duration": 3.64
    },
    {
      "text": "earlier so what we are going to do is",
      "start": 2581.079,
      "duration": 3.961
    },
    {
      "text": "that we are going to create a simple",
      "start": 2583.52,
      "duration": 3.88
    },
    {
      "text": "batch which has two inputs so as we",
      "start": 2585.04,
      "duration": 4.88
    },
    {
      "text": "already saw the first input is a six row",
      "start": 2587.4,
      "duration": 4.719
    },
    {
      "text": "and a three column Matrix now we are",
      "start": 2589.92,
      "duration": 4.48
    },
    {
      "text": "just going to add one more input so then",
      "start": 2592.119,
      "duration": 5.72
    },
    {
      "text": "the batch will be two a tensor which has",
      "start": 2594.4,
      "duration": 6.679
    },
    {
      "text": "two so we have two batches and each has",
      "start": 2597.839,
      "duration": 6.881
    },
    {
      "text": "6x3 so this is the incoming tensor which",
      "start": 2601.079,
      "duration": 5.841
    },
    {
      "text": "our class should be equipped to handle",
      "start": 2604.72,
      "duration": 4.119
    },
    {
      "text": "so so this results in a 3D tensor",
      "start": 2606.92,
      "duration": 3.96
    },
    {
      "text": "consisting of two input text with six",
      "start": 2608.839,
      "duration": 4.48
    },
    {
      "text": "tokens the first text can be your",
      "start": 2610.88,
      "duration": 4.76
    },
    {
      "text": "journey begins with one step the second",
      "start": 2613.319,
      "duration": 6.681
    },
    {
      "text": "sentence can let's say be my name is uh",
      "start": 2615.64,
      "duration": 6.24
    },
    {
      "text": "my name is so and so let's say that's",
      "start": 2620.0,
      "duration": 5.4
    },
    {
      "text": "the second sentence now uh why is this",
      "start": 2621.88,
      "duration": 6.92
    },
    {
      "text": "6x3 because each sentence has six tokens",
      "start": 2625.4,
      "duration": 5.8
    },
    {
      "text": "and each token has three dimensional",
      "start": 2628.8,
      "duration": 4.12
    },
    {
      "text": "Vector",
      "start": 2631.2,
      "duration": 4.44
    },
    {
      "text": "embedding so the following causal",
      "start": 2632.92,
      "duration": 4.52
    },
    {
      "text": "attention class is very similar to the",
      "start": 2635.64,
      "duration": 4.0
    },
    {
      "text": "self attention class except that we are",
      "start": 2637.44,
      "duration": 3.679
    },
    {
      "text": "going to add two things we are going to",
      "start": 2639.64,
      "duration": 3.24
    },
    {
      "text": "add the Dropout and we are going to add",
      "start": 2641.119,
      "duration": 4.521
    },
    {
      "text": "the causal mask okay so let's go through",
      "start": 2642.88,
      "duration": 5.56
    },
    {
      "text": "this class now uh first what we are",
      "start": 2645.64,
      "duration": 4.56
    },
    {
      "text": "going to do is that the shape of the",
      "start": 2648.44,
      "duration": 3.96
    },
    {
      "text": "input is now different because now the",
      "start": 2650.2,
      "duration": 4.32
    },
    {
      "text": "input shape has the First Dimension as",
      "start": 2652.4,
      "duration": 5.64
    },
    {
      "text": "the batch size uh whereas in the self",
      "start": 2654.52,
      "duration": 6.2
    },
    {
      "text": "attention class if you scroll up earlier",
      "start": 2658.04,
      "duration": 4.6
    },
    {
      "text": "the shape of the input which was there",
      "start": 2660.72,
      "duration": 4.08
    },
    {
      "text": "here the shape of the input was just the",
      "start": 2662.64,
      "duration": 3.719
    },
    {
      "text": "number of rows were six and the number",
      "start": 2664.8,
      "duration": 3.559
    },
    {
      "text": "of columns were three because it did not",
      "start": 2666.359,
      "duration": 4.161
    },
    {
      "text": "have batches but now the shape of the",
      "start": 2668.359,
      "duration": 3.561
    },
    {
      "text": "input is different the shape of the",
      "start": 2670.52,
      "duration": 4.12
    },
    {
      "text": "input let's say is 2x 6x3 so first is",
      "start": 2671.92,
      "duration": 5.32
    },
    {
      "text": "the batch number or the batch size the",
      "start": 2674.64,
      "duration": 4.28
    },
    {
      "text": "second is the number of tokens and the",
      "start": 2677.24,
      "duration": 4.319
    },
    {
      "text": "third is the vector embedding Dimension",
      "start": 2678.92,
      "duration": 6.639
    },
    {
      "text": "so B comma number of tokens comma the",
      "start": 2681.559,
      "duration": 6.0
    },
    {
      "text": "embedding Dimension is X do",
      "start": 2685.559,
      "duration": 4.601
    },
    {
      "text": "shape then what we do is that we",
      "start": 2687.559,
      "duration": 5.641
    },
    {
      "text": "multiply the input with the key weight",
      "start": 2690.16,
      "duration": 4.919
    },
    {
      "text": "Matrix we multiply the input with the",
      "start": 2693.2,
      "duration": 3.52
    },
    {
      "text": "query weight Matrix we multiply the",
      "start": 2695.079,
      "duration": 3.601
    },
    {
      "text": "input with the value weight Matrix to",
      "start": 2696.72,
      "duration": 4.839
    },
    {
      "text": "get the keys queries and the values then",
      "start": 2698.68,
      "duration": 4.679
    },
    {
      "text": "what we are going to do next is that we",
      "start": 2701.559,
      "duration": 3.681
    },
    {
      "text": "are going to multiply the queries with",
      "start": 2703.359,
      "duration": 4.321
    },
    {
      "text": "the keys transpose and why we are doing",
      "start": 2705.24,
      "duration": 4.319
    },
    {
      "text": "1 comma two over here because we are",
      "start": 2707.68,
      "duration": 3.919
    },
    {
      "text": "only interested in the number of tokens",
      "start": 2709.559,
      "duration": 4.201
    },
    {
      "text": "Dimension and the inputs Dimensions",
      "start": 2711.599,
      "duration": 4.641
    },
    {
      "text": "remember that we are looking at it in",
      "start": 2713.76,
      "duration": 4.839
    },
    {
      "text": "batches right so when you look at the",
      "start": 2716.24,
      "duration": 5.2
    },
    {
      "text": "first batch uh you when you look at the",
      "start": 2718.599,
      "duration": 5.201
    },
    {
      "text": "first batch you only care about the",
      "start": 2721.44,
      "duration": 3.879
    },
    {
      "text": "number of tokens and the input",
      "start": 2723.8,
      "duration": 2.96
    },
    {
      "text": "Dimensions when you look at the second",
      "start": 2725.319,
      "duration": 3.0
    },
    {
      "text": "batch you only care about the number of",
      "start": 2726.76,
      "duration": 4.04
    },
    {
      "text": "tokens and the input Dimensions so when",
      "start": 2728.319,
      "duration": 3.961
    },
    {
      "text": "you get the attention scores you",
      "start": 2730.8,
      "duration": 3.48
    },
    {
      "text": "multiply you take the queries and you",
      "start": 2732.28,
      "duration": 4.839
    },
    {
      "text": "multiply keys. transpose 1 comma",
      "start": 2734.28,
      "duration": 5.52
    },
    {
      "text": "2 uh let me explain this to you right",
      "start": 2737.119,
      "duration": 5.921
    },
    {
      "text": "now yeah so let me explain how this uh",
      "start": 2739.8,
      "duration": 6.799
    },
    {
      "text": "queries and key transpose actually works",
      "start": 2743.04,
      "duration": 5.72
    },
    {
      "text": "so now the queries which I have will be",
      "start": 2746.599,
      "duration": 4.52
    },
    {
      "text": "in batches right so if you look at the",
      "start": 2748.76,
      "duration": 5.48
    },
    {
      "text": "first batch uh let's say these are the",
      "start": 2751.119,
      "duration": 5.96
    },
    {
      "text": "so this is the first token and this this",
      "start": 2754.24,
      "duration": 4.4
    },
    {
      "text": "is the second token I'm just showing two",
      "start": 2757.079,
      "duration": 3.681
    },
    {
      "text": "tokens now and if this is the second",
      "start": 2758.64,
      "duration": 3.8
    },
    {
      "text": "batch then this is the first token and",
      "start": 2760.76,
      "duration": 3.76
    },
    {
      "text": "this is the second token both of these",
      "start": 2762.44,
      "duration": 3.679
    },
    {
      "text": "batches are now coming together in the",
      "start": 2764.52,
      "duration": 3.839
    },
    {
      "text": "queries whereas if you look at the keys",
      "start": 2766.119,
      "duration": 5.161
    },
    {
      "text": "let's say these are the keys the reason",
      "start": 2768.359,
      "duration": 5.681
    },
    {
      "text": "we are taking the keys transpose is that",
      "start": 2771.28,
      "duration": 4.559
    },
    {
      "text": "for the queries to be multiplied with",
      "start": 2774.04,
      "duration": 3.72
    },
    {
      "text": "the keys the keys need to look like this",
      "start": 2775.839,
      "duration": 3.52
    },
    {
      "text": "otherwise the matrix multiplication",
      "start": 2777.76,
      "duration": 3.68
    },
    {
      "text": "cannot happen so if you look at the",
      "start": 2779.359,
      "duration": 4.441
    },
    {
      "text": "queries now the queries are this and",
      "start": 2781.44,
      "duration": 5.04
    },
    {
      "text": "when you do keys. transpose 1 comma 2",
      "start": 2783.8,
      "duration": 4.12
    },
    {
      "text": "they will look look something like this",
      "start": 2786.48,
      "duration": 3.0
    },
    {
      "text": "which means that we'll still have two",
      "start": 2787.92,
      "duration": 5.52
    },
    {
      "text": "rows but inside the so it was 2A 6 comma",
      "start": 2789.48,
      "duration": 6.28
    },
    {
      "text": "2 right so we'll still have two rows but",
      "start": 2793.44,
      "duration": 4.8
    },
    {
      "text": "inside each row the that particular",
      "start": 2795.76,
      "duration": 4.76
    },
    {
      "text": "Matrix will be transposed so without",
      "start": 2798.24,
      "duration": 4.119
    },
    {
      "text": "transpose the keys look like this and",
      "start": 2800.52,
      "duration": 4.16
    },
    {
      "text": "when you do uh when you do this so when",
      "start": 2802.359,
      "duration": 4.841
    },
    {
      "text": "you do keys. transpose 1 comma 2 what",
      "start": 2804.68,
      "duration": 5.84
    },
    {
      "text": "will be preserved is that inside the uh",
      "start": 2807.2,
      "duration": 5.68
    },
    {
      "text": "so the rows will be converted into",
      "start": 2810.52,
      "duration": 4.48
    },
    {
      "text": "columns so the keys will Keys transpose",
      "start": 2812.88,
      "duration": 4.16
    },
    {
      "text": "will start looking like this and when",
      "start": 2815.0,
      "duration": 3.839
    },
    {
      "text": "you multiply the queries with the keys",
      "start": 2817.04,
      "duration": 3.96
    },
    {
      "text": "transpose what will happen is that the",
      "start": 2818.839,
      "duration": 4.201
    },
    {
      "text": "batches will be processed sequentially",
      "start": 2821.0,
      "duration": 4.72
    },
    {
      "text": "so in the first batch this queries will",
      "start": 2823.04,
      "duration": 5.079
    },
    {
      "text": "be multiplied with this Keys transpose",
      "start": 2825.72,
      "duration": 4.8
    },
    {
      "text": "and you'll get a result then this then",
      "start": 2828.119,
      "duration": 5.2
    },
    {
      "text": "this queries will be multiplied with",
      "start": 2830.52,
      "duration": 4.599
    },
    {
      "text": "this Keys transpose and you'll get a",
      "start": 2833.319,
      "duration": 3.721
    },
    {
      "text": "result and both those results will be",
      "start": 2835.119,
      "duration": 4.361
    },
    {
      "text": "stacked together so here just the",
      "start": 2837.04,
      "duration": 4.079
    },
    {
      "text": "multiplication has been shown and",
      "start": 2839.48,
      "duration": 4.119
    },
    {
      "text": "finally you'll get the attention scores",
      "start": 2841.119,
      "duration": 5.081
    },
    {
      "text": "where both the results have been stacked",
      "start": 2843.599,
      "duration": 3.801
    },
    {
      "text": "together",
      "start": 2846.2,
      "duration": 6.0
    },
    {
      "text": "this is how uh it actually works in",
      "start": 2847.4,
      "duration": 7.24
    },
    {
      "text": "batches and then what we do is that once",
      "start": 2852.2,
      "duration": 5.44
    },
    {
      "text": "we get the attention scores uh as I told",
      "start": 2854.64,
      "duration": 6.919
    },
    {
      "text": "you earlier first we are going to uh we",
      "start": 2857.64,
      "duration": 5.88
    },
    {
      "text": "are essentially going",
      "start": 2861.559,
      "duration": 4.841
    },
    {
      "text": "to create yeah so here we are creating",
      "start": 2863.52,
      "duration": 6.24
    },
    {
      "text": "an upper triangular mask which is of all",
      "start": 2866.4,
      "duration": 7.56
    },
    {
      "text": "ones uh except so which is once so let's",
      "start": 2869.76,
      "duration": 7.16
    },
    {
      "text": "see the upper triangular so upper",
      "start": 2873.96,
      "duration": 4.879
    },
    {
      "text": "triangular Matrix is ones above the",
      "start": 2876.92,
      "duration": 3.679
    },
    {
      "text": "diagonal right so there are ones above",
      "start": 2878.839,
      "duration": 3.28
    },
    {
      "text": "the diagonals and everything below the",
      "start": 2880.599,
      "duration": 3.96
    },
    {
      "text": "diagonal is zero and then wherever there",
      "start": 2882.119,
      "duration": 4.921
    },
    {
      "text": "is one those will be replaced with minus",
      "start": 2884.559,
      "duration": 6.121
    },
    {
      "text": "infinity in the attention scores Matrix",
      "start": 2887.04,
      "duration": 5.12
    },
    {
      "text": "then what we are going to do is we are",
      "start": 2890.68,
      "duration": 3.96
    },
    {
      "text": "going to divide by square root of the",
      "start": 2892.16,
      "duration": 4.0
    },
    {
      "text": "key Dimension and we are going to take",
      "start": 2894.64,
      "duration": 4.4
    },
    {
      "text": "the soft Max this is exactly what we saw",
      "start": 2896.16,
      "duration": 4.84
    },
    {
      "text": "this is the attention weight Matrix",
      "start": 2899.04,
      "duration": 4.48
    },
    {
      "text": "where all the rows sum up to one and the",
      "start": 2901.0,
      "duration": 4.64
    },
    {
      "text": "causal attention mask is applied and",
      "start": 2903.52,
      "duration": 4.839
    },
    {
      "text": "then finally what we do is that we uh",
      "start": 2905.64,
      "duration": 4.56
    },
    {
      "text": "apply the Dropout to these attention",
      "start": 2908.359,
      "duration": 4.041
    },
    {
      "text": "weights and the Dropout has been defined",
      "start": 2910.2,
      "duration": 4.24
    },
    {
      "text": "over here where the dropout rate is",
      "start": 2912.4,
      "duration": 4.32
    },
    {
      "text": "taken as an attribute when we create an",
      "start": 2914.44,
      "duration": 4.44
    },
    {
      "text": "instance of the causal attention",
      "start": 2916.72,
      "duration": 4.72
    },
    {
      "text": "class and then the context Vector is a",
      "start": 2918.88,
      "duration": 4.0
    },
    {
      "text": "product of the attention weights",
      "start": 2921.44,
      "duration": 3.119
    },
    {
      "text": "multiplied by the values and then we",
      "start": 2922.88,
      "duration": 3.84
    },
    {
      "text": "ultimately return the context",
      "start": 2924.559,
      "duration": 4.881
    },
    {
      "text": "Vector so let's see now how this",
      "start": 2926.72,
      "duration": 6.119
    },
    {
      "text": "actually works out in practice so uh",
      "start": 2929.44,
      "duration": 5.639
    },
    {
      "text": "first the context length is batch. shape",
      "start": 2932.839,
      "duration": 4.641
    },
    {
      "text": "of one because why one because we have a",
      "start": 2935.079,
      "duration": 4.961
    },
    {
      "text": "batch size of six so batch do shape of",
      "start": 2937.48,
      "duration": 5.24
    },
    {
      "text": "one will be six and then what we'll be",
      "start": 2940.04,
      "duration": 4.64
    },
    {
      "text": "doing is that we'll be defining a caal",
      "start": 2942.72,
      "duration": 4.16
    },
    {
      "text": "attention class with the input dimension",
      "start": 2944.68,
      "duration": 5.24
    },
    {
      "text": "of D in now let's see what D in actually",
      "start": 2946.88,
      "duration": 6.64
    },
    {
      "text": "is so D in will be",
      "start": 2949.92,
      "duration": 3.6
    },
    {
      "text": "uh let's print this out actually let's",
      "start": 2953.799,
      "duration": 6.361
    },
    {
      "text": "see what DN is so I think DN is three",
      "start": 2956.64,
      "duration": 6.12
    },
    {
      "text": "because the vector size is three and D",
      "start": 2960.16,
      "duration": 7.439
    },
    {
      "text": "out so if I print D in uh that will be",
      "start": 2962.76,
      "duration": 7.12
    },
    {
      "text": "equal to three correct and if I print D",
      "start": 2967.599,
      "duration": 4.2
    },
    {
      "text": "out I think that will be equal to two",
      "start": 2969.88,
      "duration": 3.4
    },
    {
      "text": "because those are the dimensions which",
      "start": 2971.799,
      "duration": 4.241
    },
    {
      "text": "we have used yeah D out will be equal to",
      "start": 2973.28,
      "duration": 4.839
    },
    {
      "text": "two the context length will be equal to",
      "start": 2976.04,
      "duration": 5.279
    },
    {
      "text": "6 and then 0 comma 0 is 0. 0 is",
      "start": 2978.119,
      "duration": 4.801
    },
    {
      "text": "essentially the Dropout so here we are",
      "start": 2981.319,
      "duration": 4.04
    },
    {
      "text": "saying that don't do so put the dropout",
      "start": 2982.92,
      "duration": 4.919
    },
    {
      "text": "rate to be zero so then what we do is",
      "start": 2985.359,
      "duration": 4.161
    },
    {
      "text": "that we create an instance of this",
      "start": 2987.839,
      "duration": 4.441
    },
    {
      "text": "causal attention class and then we pass",
      "start": 2989.52,
      "duration": 4.16
    },
    {
      "text": "in the batch which we have defined",
      "start": 2992.28,
      "duration": 3.44
    },
    {
      "text": "earlier so now you can see that here we",
      "start": 2993.68,
      "duration": 3.48
    },
    {
      "text": "defined a batch where we stack two",
      "start": 2995.72,
      "duration": 3.68
    },
    {
      "text": "inputs on top of each other when we",
      "start": 2997.16,
      "duration": 4.36
    },
    {
      "text": "process the first input when we process",
      "start": 2999.4,
      "duration": 3.919
    },
    {
      "text": "the first input we should get a context",
      "start": 3001.52,
      "duration": 3.52
    },
    {
      "text": "Vector Matrix of",
      "start": 3003.319,
      "duration": 4.24
    },
    {
      "text": "size U so here there are six rows and",
      "start": 3005.04,
      "duration": 4.16
    },
    {
      "text": "two columns right so when we process the",
      "start": 3007.559,
      "duration": 3.24
    },
    {
      "text": "first input of this batch we'll get a",
      "start": 3009.2,
      "duration": 3.96
    },
    {
      "text": "context Vector of the size",
      "start": 3010.799,
      "duration": 4.121
    },
    {
      "text": "6x2",
      "start": 3013.16,
      "duration": 6.6
    },
    {
      "text": "um 6x2 and when we process the second",
      "start": 3014.92,
      "duration": 6.72
    },
    {
      "text": "input in the batch we'll get a context",
      "start": 3019.76,
      "duration": 4.079
    },
    {
      "text": "Vector Matrix of size again we'll get a",
      "start": 3021.64,
      "duration": 5.4
    },
    {
      "text": "context Vector Matrix of size 6x2 so",
      "start": 3023.839,
      "duration": 4.841
    },
    {
      "text": "there will be two context vectors of",
      "start": 3027.04,
      "duration": 3.84
    },
    {
      "text": "size 6x2 so the resultant answer should",
      "start": 3028.68,
      "duration": 5.0
    },
    {
      "text": "be 2x 6x2 it will be a 3D",
      "start": 3030.88,
      "duration": 4.64
    },
    {
      "text": "threedimensional tensor let's see if",
      "start": 3033.68,
      "duration": 5.24
    },
    {
      "text": "that's indeed the case um so now here",
      "start": 3035.52,
      "duration": 5.88
    },
    {
      "text": "you can see that I've have passed in my",
      "start": 3038.92,
      "duration": 5.08
    },
    {
      "text": "batch of inputs and here are the context",
      "start": 3041.4,
      "duration": 4.88
    },
    {
      "text": "vectors here's my resultant answer and",
      "start": 3044.0,
      "duration": 3.92
    },
    {
      "text": "if you print the shape of the context",
      "start": 3046.28,
      "duration": 3.839
    },
    {
      "text": "Vector which is the resultant answer",
      "start": 3047.92,
      "duration": 4.56
    },
    {
      "text": "it's 2x 6x2 why because we have two",
      "start": 3050.119,
      "duration": 4.641
    },
    {
      "text": "matrices of 6x2 which we are stack which",
      "start": 3052.48,
      "duration": 4.24
    },
    {
      "text": "are stacked on top of each other so so",
      "start": 3054.76,
      "duration": 3.76
    },
    {
      "text": "you can even print out",
      "start": 3056.72,
      "duration": 5.2
    },
    {
      "text": "the so you can even print out the",
      "start": 3058.52,
      "duration": 6.2
    },
    {
      "text": "context vectors now so if I do",
      "start": 3061.92,
      "duration": 5.12
    },
    {
      "text": "print context",
      "start": 3064.72,
      "duration": 4.76
    },
    {
      "text": "Vex this will print out the context",
      "start": 3067.04,
      "duration": 4.88
    },
    {
      "text": "vectors and you'll see that here we get",
      "start": 3069.48,
      "duration": 5.24
    },
    {
      "text": "so the first is uh this is the context",
      "start": 3071.92,
      "duration": 4.6
    },
    {
      "text": "Vector Matrix of the first input this is",
      "start": 3074.72,
      "duration": 3.52
    },
    {
      "text": "the context Vector Matrix of the second",
      "start": 3076.52,
      "duration": 3.24
    },
    {
      "text": "input and they are stacked on top of",
      "start": 3078.24,
      "duration": 4.079
    },
    {
      "text": "each other awesome so which means the",
      "start": 3079.76,
      "duration": 4.68
    },
    {
      "text": "causal attention class which we have",
      "start": 3082.319,
      "duration": 3.721
    },
    {
      "text": "which we have written is capable of",
      "start": 3084.44,
      "duration": 3.8
    },
    {
      "text": "handling B",
      "start": 3086.04,
      "duration": 3.92
    },
    {
      "text": "I did not explain this thing here which",
      "start": 3088.24,
      "duration": 3.76
    },
    {
      "text": "is register buffer so why do we need a",
      "start": 3089.96,
      "duration": 4.359
    },
    {
      "text": "buffer when we create this mask so the",
      "start": 3092.0,
      "duration": 4.04
    },
    {
      "text": "main thing is that it's not really",
      "start": 3094.319,
      "duration": 3.52
    },
    {
      "text": "necessary for all use cases but it",
      "start": 3096.04,
      "duration": 4.079
    },
    {
      "text": "offers some Advantage here so when we",
      "start": 3097.839,
      "duration": 3.96
    },
    {
      "text": "use the causal attention class in a",
      "start": 3100.119,
      "duration": 3.68
    },
    {
      "text": "large language model buffers are",
      "start": 3101.799,
      "duration": 3.76
    },
    {
      "text": "automatically moved to the appropriate",
      "start": 3103.799,
      "duration": 4.76
    },
    {
      "text": "device CPU or GPU along with our model",
      "start": 3105.559,
      "duration": 4.481
    },
    {
      "text": "which will be relevant when training the",
      "start": 3108.559,
      "duration": 4.321
    },
    {
      "text": "llm in future chapters usually matrices",
      "start": 3110.04,
      "duration": 4.799
    },
    {
      "text": "are this matrices like this which are",
      "start": 3112.88,
      "duration": 4.28
    },
    {
      "text": "fixed which which need not need to be",
      "start": 3114.839,
      "duration": 4.201
    },
    {
      "text": "trained so this is an upper triangular",
      "start": 3117.16,
      "duration": 3.8
    },
    {
      "text": "Matrix right all the elements above the",
      "start": 3119.04,
      "duration": 4.4
    },
    {
      "text": "diagonal will be one will be one it's a",
      "start": 3120.96,
      "duration": 4.8
    },
    {
      "text": "fixed Matrix we will not train this",
      "start": 3123.44,
      "duration": 4.32
    },
    {
      "text": "usually it's better to Define all of",
      "start": 3125.76,
      "duration": 5.839
    },
    {
      "text": "these as the using the register buffer",
      "start": 3127.76,
      "duration": 5.359
    },
    {
      "text": "because then these are automatically",
      "start": 3131.599,
      "duration": 3.281
    },
    {
      "text": "move to the appropriate device along",
      "start": 3133.119,
      "duration": 3.96
    },
    {
      "text": "with our model and since we are anywh",
      "start": 3134.88,
      "duration": 5.36
    },
    {
      "text": "not training them um it's much more",
      "start": 3137.079,
      "duration": 5.441
    },
    {
      "text": "convenient so we don't need to manually",
      "start": 3140.24,
      "duration": 3.96
    },
    {
      "text": "ensure that these tensors are on the",
      "start": 3142.52,
      "duration": 3.559
    },
    {
      "text": "same device as the model parameters",
      "start": 3144.2,
      "duration": 4.919
    },
    {
      "text": "avoiding device M mismatch errors later",
      "start": 3146.079,
      "duration": 5.841
    },
    {
      "text": "when we move to GPU calculations this",
      "start": 3149.119,
      "duration": 5.44
    },
    {
      "text": "will be very important so just remember",
      "start": 3151.92,
      "duration": 5.96
    },
    {
      "text": "that U these masks or these matrices",
      "start": 3154.559,
      "duration": 5.361
    },
    {
      "text": "which are not trained it's it's good to",
      "start": 3157.88,
      "duration": 4.32
    },
    {
      "text": "Define them using register buffer so",
      "start": 3159.92,
      "duration": 3.879
    },
    {
      "text": "that they can be automatically moved to",
      "start": 3162.2,
      "duration": 4.08
    },
    {
      "text": "the appropriate devices we don't need to",
      "start": 3163.799,
      "duration": 4.081
    },
    {
      "text": "ensure that they are on the same device",
      "start": 3166.28,
      "duration": 3.0
    },
    {
      "text": "as our model",
      "start": 3167.88,
      "duration": 3.8
    },
    {
      "text": "parameters so that is an important thing",
      "start": 3169.28,
      "duration": 4.2
    },
    {
      "text": "to be aware of the second thing is that",
      "start": 3171.68,
      "duration": 5.36
    },
    {
      "text": "here we have used the uh colon number of",
      "start": 3173.48,
      "duration": 5.76
    },
    {
      "text": "tokens so this is to ensure for cases",
      "start": 3177.04,
      "duration": 4.279
    },
    {
      "text": "where the number of tokens in the batch",
      "start": 3179.24,
      "duration": 4.24
    },
    {
      "text": "is smaller than the supported context",
      "start": 3181.319,
      "duration": 4.52
    },
    {
      "text": "size if this were not written that's",
      "start": 3183.48,
      "duration": 4.72
    },
    {
      "text": "also fine then the mask will be of the",
      "start": 3185.839,
      "duration": 4.601
    },
    {
      "text": "context size but if the number of tokens",
      "start": 3188.2,
      "duration": 4.44
    },
    {
      "text": "are smaller than the context size the",
      "start": 3190.44,
      "duration": 4.2
    },
    {
      "text": "mask is created only up till the number",
      "start": 3192.64,
      "duration": 4.04
    },
    {
      "text": "of tokens this might happen if one of",
      "start": 3194.64,
      "duration": 3.84
    },
    {
      "text": "the batch has smaller number of tokens",
      "start": 3196.68,
      "duration": 3.48
    },
    {
      "text": "than the contact size especially one of",
      "start": 3198.48,
      "duration": 4.28
    },
    {
      "text": "the ending batches Etc but these are",
      "start": 3200.16,
      "duration": 4.36
    },
    {
      "text": "edge cases which you now don't need to",
      "start": 3202.76,
      "duration": 3.44
    },
    {
      "text": "worry about all you need to understand",
      "start": 3204.52,
      "duration": 3.52
    },
    {
      "text": "is that what this class has effectively",
      "start": 3206.2,
      "duration": 5.52
    },
    {
      "text": "done is that we have uh implemented the",
      "start": 3208.04,
      "duration": 5.799
    },
    {
      "text": "causal attention mask which means that",
      "start": 3211.72,
      "duration": 3.96
    },
    {
      "text": "all the elements above the diagonal are",
      "start": 3213.839,
      "duration": 4.201
    },
    {
      "text": "set to zero and we have ensured that all",
      "start": 3215.68,
      "duration": 4.28
    },
    {
      "text": "the rows of the attention weight sum up",
      "start": 3218.04,
      "duration": 4.519
    },
    {
      "text": "to one using the soft Max and we have",
      "start": 3219.96,
      "duration": 4.599
    },
    {
      "text": "also implemented the Dropout layer to",
      "start": 3222.559,
      "duration": 5.121
    },
    {
      "text": "ensure generalizability and to prevent",
      "start": 3224.559,
      "duration": 5.52
    },
    {
      "text": "overfitting um so I think this actually",
      "start": 3227.68,
      "duration": 4.04
    },
    {
      "text": "brings us to the end of this lecture on",
      "start": 3230.079,
      "duration": 4.24
    },
    {
      "text": "the causal attention class uh in the",
      "start": 3231.72,
      "duration": 4.72
    },
    {
      "text": "next section what we'll be doing is that",
      "start": 3234.319,
      "duration": 3.681
    },
    {
      "text": "we will expand on this concept and",
      "start": 3236.44,
      "duration": 4.04
    },
    {
      "text": "Implement a multi-head attention module",
      "start": 3238.0,
      "duration": 4.359
    },
    {
      "text": "that implements several of these causal",
      "start": 3240.48,
      "duration": 4.56
    },
    {
      "text": "attention mechanisms in parallel so what",
      "start": 3242.359,
      "duration": 4.76
    },
    {
      "text": "the attention mechanism",
      "start": 3245.04,
      "duration": 5.12
    },
    {
      "text": "in GPT and in other llms what they are",
      "start": 3247.119,
      "duration": 5.601
    },
    {
      "text": "doing is that they take these causal",
      "start": 3250.16,
      "duration": 5.0
    },
    {
      "text": "attention mechanisms and they stack them",
      "start": 3252.72,
      "duration": 5.56
    },
    {
      "text": "together so let me show you this graph",
      "start": 3255.16,
      "duration": 4.959
    },
    {
      "text": "this plot of what all we have learned so",
      "start": 3258.28,
      "duration": 5.0
    },
    {
      "text": "far so until now what we have learned is",
      "start": 3260.119,
      "duration": 5.0
    },
    {
      "text": "that we have learned about causal",
      "start": 3263.28,
      "duration": 4.0
    },
    {
      "text": "attention in this section when multiple",
      "start": 3265.119,
      "duration": 3.921
    },
    {
      "text": "causal attention heads are stacked",
      "start": 3267.28,
      "duration": 3.36
    },
    {
      "text": "together it leads to multi-head",
      "start": 3269.04,
      "duration": 3.759
    },
    {
      "text": "attention and that's what's actually",
      "start": 3270.64,
      "duration": 4.679
    },
    {
      "text": "implemented in GPT but now try to think",
      "start": 3272.799,
      "duration": 4.681
    },
    {
      "text": "about it without covering these lectures",
      "start": 3275.319,
      "duration": 3.641
    },
    {
      "text": "how can you understand multi-head",
      "start": 3277.48,
      "duration": 4.04
    },
    {
      "text": "attention directly without understanding",
      "start": 3278.96,
      "duration": 4.76
    },
    {
      "text": "causal attention you cannot understand",
      "start": 3281.52,
      "duration": 4.44
    },
    {
      "text": "multi-ad attention to understand causal",
      "start": 3283.72,
      "duration": 3.76
    },
    {
      "text": "attention you need to understand key",
      "start": 3285.96,
      "duration": 3.52
    },
    {
      "text": "query value that's why I have developed",
      "start": 3287.48,
      "duration": 3.72
    },
    {
      "text": "these lecture Series so that we cover",
      "start": 3289.48,
      "duration": 3.56
    },
    {
      "text": "each module in a sequential Manner and",
      "start": 3291.2,
      "duration": 4.119
    },
    {
      "text": "in a lot of detail I know the lectures",
      "start": 3293.04,
      "duration": 3.92
    },
    {
      "text": "are becoming tough the lectures are",
      "start": 3295.319,
      "duration": 4.201
    },
    {
      "text": "becoming long but if you follow what I'm",
      "start": 3296.96,
      "duration": 5.04
    },
    {
      "text": "doing and if you implement this code",
      "start": 3299.52,
      "duration": 4.16
    },
    {
      "text": "you'll start to understand things in a",
      "start": 3302.0,
      "duration": 3.799
    },
    {
      "text": "much better manner I believe that all",
      "start": 3303.68,
      "duration": 3.919
    },
    {
      "text": "the other tutorials all the other videos",
      "start": 3305.799,
      "duration": 3.481
    },
    {
      "text": "on YouTube out there currently they are",
      "start": 3307.599,
      "duration": 3.921
    },
    {
      "text": "very short they do not explain all of",
      "start": 3309.28,
      "duration": 4.64
    },
    {
      "text": "the details I think the devil lies in",
      "start": 3311.52,
      "duration": 4.12
    },
    {
      "text": "the details we need to understand the",
      "start": 3313.92,
      "duration": 4.0
    },
    {
      "text": "details we need to deal with Dimensions",
      "start": 3315.64,
      "duration": 4.28
    },
    {
      "text": "you need to understand how batches work",
      "start": 3317.92,
      "duration": 4.08
    },
    {
      "text": "why we have a three-dimensional tensor",
      "start": 3319.92,
      "duration": 3.919
    },
    {
      "text": "don't be scared of dimensions and",
      "start": 3322.0,
      "duration": 3.96
    },
    {
      "text": "matrices the student who Masters",
      "start": 3323.839,
      "duration": 3.321
    },
    {
      "text": "dimension",
      "start": 3325.96,
      "duration": 4.079
    },
    {
      "text": "matrices linear algebra fundamentals",
      "start": 3327.16,
      "duration": 4.399
    },
    {
      "text": "they will really understand what is",
      "start": 3330.039,
      "duration": 3.76
    },
    {
      "text": "going on here I'm deliberately trying to",
      "start": 3331.559,
      "duration": 4.28
    },
    {
      "text": "have a mix of the Whiteboard notes and",
      "start": 3333.799,
      "duration": 4.721
    },
    {
      "text": "the coding in Jupiter notebook so that",
      "start": 3335.839,
      "duration": 4.881
    },
    {
      "text": "you understand the basics the theory as",
      "start": 3338.52,
      "duration": 4.559
    },
    {
      "text": "well as you implement the code thank you",
      "start": 3340.72,
      "duration": 3.96
    },
    {
      "text": "so much everyone I'll see you in the",
      "start": 3343.079,
      "duration": 2.96
    },
    {
      "text": "next lecture where we'll cover",
      "start": 3344.68,
      "duration": 3.8
    },
    {
      "text": "multi-head attention in a lot of detail",
      "start": 3346.039,
      "duration": 5.721
    },
    {
      "text": "thanks everyone",
      "start": 3348.48,
      "duration": 3.28
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch series we have been covering attention and the attention mechanism in a lot of detail for the past two or three lectures the lectures were long they were more than 1 hour each but I think it's extremely important for you to understand the attention mechanism and uh that's why I'm devoting so much time to these set of lectures just to give you a quick recap of what all we have covered so far in this in the first lecture on attention we started with a simplified self attention mechanism without trainable weights in the previous lecture we added trainable weights and we discussed the self attention mechanism with trainable weights here we also looked at the concept of key query and values which we'll revise just in a moment and then today our main aim is to learn about something which is called as causal attention after learning about causal attention in the next lecture we'll move to multi-head attention which is the actual attention mechanism which is used in GPT and other modern large language models I believe this sequential flow is extremely important because if you start understanding multi-head attention directly you will not understand the basics you will not understand the nuts and bolts of what exactly is attention how did we we get to multihead attention so let's get started with today's lecture on causal attention Okay first let us revise everything we know about self attention what we have learned in the previous lectures so the example which we have started looking is this one sentence which is your journey starts with one step so you'll see that there are six words in this sentence the first step of any um data processing pipeline for a large language model is to convert these words into tokens to convert these tokens into token IDs and to convert the token IDs into Vector embeddings so here you can see that for each token we have threedimensional Vector embeddings so for the token y we have a three-dimensional Vector embedding for the token Journey we have a three-dimensional Vector embedding and for the token step we have a three-dimensional Vector embedding remember generally one word is not equal to to one token because uh GPT and other modern llms use a bite pair encoder which are sub tokenizers but for the purposes of this lecture I'm going to use word and token interchangeably okay so the first step is to convert all of the tokens into Vector embeddings I'm choosing a three-dimensional Vector embedding here just for the sake of demonstration remember that in models like GPT it's common to have Vector dimensions of 500,000 or even more Ive just plotted these vectors and how they look like over here in the three-dimensional space these are the input embeddings the goal of any attention mechanism is to take these input embeddings for every vector and convert them into context embeddings or context vectors for every token now what's the difference between uh input embedding and context embedding or a context Vector so let's say if you look at the journey this is an input embedding for Journey this green Vector which you see over here which contains which encodes some semantic meaning about Journey but it does not carry any information about how the other words in the sentence such as your step with one how all these words relate to Journey how much attention should you pay to each of these words when you are looking at Journey the embedding vector or the input Vector for Journey contains no such information whereas the context Vector contains this information the context Vector not only contains semantic meaning about Journey but it also encodes meaning about how Journey relates with all these other words so that's the main aim of the attention mechanism to get context vectors for each of our input embedding vectors why do we need context vectors because it makes the task of the next word predictions much more better much more reliable because now we are encoding information of how much attention needs to be paid at different words in a sequence of sentences so that's the whole aim of attention mechanisms as we saw in the previous lecture the first step is to multiply the inputs with the query Matrix the key Matrix and the value Matrix when you multi when you do this multiplication you get the query query Matrix so there's a difference so this WQ w k and WV are weight matrices so these are the trainable weight Matrix this is the trainable query Matrix this is the trainable key Matrix and this is the trainable value Matrix you multiply the inputs Matrix with these trainable weight matrices and then you get the final queries Matrix the keys Matrix and the values Matrix that's the first step now remember that this WQ w k and WV these training weight matrices are not fixed their parameters need to be trained based on input data and this training is a part of the llm currently when we are learning attention mechanisms we are not learning about this training procedure we are only learning about the uh forward pass procedure how to take input embeddings and how to convert them into context vectors we'll come to training and back propagation later in this course okay so once you obtain the queries keys and the values Matrix the next step is to compute the attention scores to compute the attention scores what we do is we multiply the queries Matrix with the transpose of the keys Matrix so these are the attention scores and each row represents the attention corresponding to to that particular query with all the other keys so let's say if you look at the second row it corresponds to let's say the query for Journey because the first row corresponds to your the second row corresponds to Journey the third row corresponds to begins Etc so if you look at the second row the first the first value in the second row is basically when you are looking at the query Journey how much attention should you pay to the first input embedding Vector which is your when you're looking at Journey how much attention should you pay to the second input embedding Vector similarly when you look at the last entry of the second row this encodes information of how much attention should you be paying to the sixth input embedding which is Step so your journey begins with one step so basically every row contains information that when you're looking at a particular query how much uh attention should be given to all the other keys in the sentence if you are unclear about this please go through the previous lecture where I've covered this in an extensive amount of detail here I'm just providing a recap so that you you understand or you revise what we have learned so far these are the attention scores the next step is to convert these attention scores into attention weights and the way we do that is first we divide by square root of the key Dimension and then we apply the soft Max activation function again in the previous lecture I have explained why we divide by the square root of the keys Dimension and how do we apply the soft Max but basically these are the attention these are the attention weights which we compute from the attention scores the difference between attention scores and weights is that they intuitively mean the same thing but if you look at each row of the tension weight Matrix you'll see that each row essentially sums up to one so there is a probabilistic meaning Associated now when you look at Journey you have to pay 15% attention to the first input embedding 22% attention to the second input embedding and finally we can say 18% attention to the last input embeding these are the attention weights and the last step is that you take the attention weight Matrix and you multiply it by the values Matrix remember the keys query and the value Matrix have been computed over here so you take the attention weight Matrix you multiply it by the values Matrix and ultimately you get this context Vector Matrix this is the final Vector which we are looking for so here you can see there are six six rows right each row corresponds to the context Vector for that particular token so the first row is the context Vector for your the second row is the context Vector for Journey similarly the last row is the context Vector for step okay and in this plot I have shown the so if you look at the journey Vector you'll see that this is just the embedding Vector the green but if you look at the journey context Vector now you'll see that it's different than the journey because it also contains how much attention should be paid to all the other words so the journey context Vector is much more richer than journey and the context vectors are what we'll be using as inputs for the llm training so we'll get such context vectors for all the input embeddings which we have this is the recap of what all we have covered so far I hope you are with me until this stage now what is causal attention and why do we need it so first of all causal attention is also called as mask attention so when you read some research papers and when you see some tutorials you'll see that this term is also called as masked attention it is a special form of self attention so what this causal attention does is that it restricts the model to only consider the previous and the current inputs in a sequence when processing any given token so let me explain to you further what this means this is in contrast to the self attention mechanism which allows access to the entire input sequence at once so remember what we did here when I explained this attention metrix to you this attention score Matrix when we look at a particular query such as Journey we look at its attention with all the other uh tokens right your begins one step we do not look at whether these tokens come before Journey or whether they come after after Journey that's what changed in the causal attention in causal attention when we look at any particular query we only consider the attention of that query with respect to tokens which come before that query let me show you how what that means in a moment so when Computing attention scores the causal attention mechanism ensures that the model only factors in tokens that occur at or before the current token in the sequence to achieve this in GPT like large language models for each token processed we mask out the future tokens which come after the current token let me show you visually what all of this really means so until now the attention weight Matrix which we have seen looks something like this so if you look at the row for Journey you can get the attention weight of Journey with all the other tokens right of your journey starts with one step so this row con consist of six values but the main main U goal of the causal attention mechanism is that when you look at a particular token such as Journey you should only consider the attention scores of Journey with the words which come before Journey such as your and journey so there are only two attention scores which are relevant here all the attention scores which come after this point are masked out which means that they no longer exist they are set to zero similarly when you look at width let's say you look at width so before width we have your journey starts with there are four tokens so we have the attention scores for those four tokens but the attention scores for all of the future tokens will be masked out or will be set to zero will be converted to zero whereas if you look at the last word step your journey starts with one step all of these come before step right so we have all the atten scores which are considered nothing will be masked out in the first token your there is nothing which comes before your so then every single thing after your will essentially be masked out when we look at masking the context size is also important because remember the context size specifies how many words the llm can look at before predicting the next word so when I show this Matrix I'm assuming that this much is the context size so context size is six uh so this is the the main purpose of the causal attention mechanism so we mask out out the attention weights above the diagonal this is very important if you look out if you look at this second Matrix there is a key pattern which You observe over here and that is what we'll exploit when we code if you take this diagonal and if you look at everything which occurs Above This diagonal it is essentially zero right which means that it is essentially mased out students who know about the Triangular Matrix the the lower triangular Matrix and the upper triangular Matrix will really relate to this and understand this much better uh we'll come to that in a moment in code but for now just remember that we mask out the attention weights above the diagonal like this we set those attention weights to be equal to zero and then we normalize the nonmass attention weights such that the attention weights sum up to one in each row so your question would be that okay if everything else is set to zero then these weight will no longer sum up to one right we'll ensure that the normalization is done once more so that whatever attention weights are remaining they indeed sum up to one so this is the main idea behind uh the causal attention mechanism so this this thing here what I'm coloring in red right now which is above the diagonal it's called as the causal attention mask because we are masking out all of those attention weights so now let us see how to apply a causal attention mask so the strategy which we'll follow is that exactly what we have done over here in the flow map which earlier showed you here what we'll do is that we get the attention weights in a similar manner to what we have obtained and then we just set the elements above the diagonal we set the elements above the diagonal to be zero and then we renormalize the attention weights that is the strategy which we are going to follow that is also mentioned over here so we'll first get the attention scores like what we had previously then we get the attention weights this is what we did previously then we will add this one step we'll mask the elements above the diagonal to be zero then we'll get M attention scores and then we'll again normalize them to get M attention weights so that we ensure that each row again sums up to one so now let us encode this logic in code but just remember that all we are doing is we are getting the attention weights and we are zeroing out the elements above the diagonal that's it okay so let us go to code right now and the goal which we have is hiding future words with causal attention now for this remember that we have worked previously in the previous lecture we have written this self attention version two what the self attention class does is that it uh it basically takes us through this entire flowchart pipeline which I've mentioned over here let me show that yeah this pipeline so what that self attention class in Python which I showed you does is that first it initializes these query key and value M weight matrices to random values then it multiplies the inputs with these to get the queries keys and the value Matrix then it multiplies queries with key keys transpose to get the attention scores then it scales by square root of Dimension does soft Max to get the attention weights and then it multiplies attention weights with values to get the context Vector Matrix so if you see uh if you take the forward method me thir in this self attention class we basically get the keys queries and the values uh so the these are the W key W query and the W value are the trainable key query and value weight matrices which are initialized randomly and then we multiply them with the uh with the inputs basically to get the keys queries and the values Matrix so remember here the way we actually get these Keys query and value Matrix is that we pass in the input X here and then we multiply that input to the trainable key query and value weight Matrix to get the keys the queries and the values so these Keys queries and the values which are highlighted in the code are these yeah these queries keys and values Matrix here this is the queries this is the keys and this is the values which have been obtained after multiplication of the inputs with the weight matrices okay then what we do is we multiply the queries with the keys transpose to get the attention scores we do a soft then we divide the attention scores with square root of the keys Dimension we apply soft Max to get the attention weights and then we multiply the attention weights with the values to get the context Vector this is what is happening in the self attention class self attention version two so we'll start out with the self attention version two we'll uh first get the queries and the keys Matrix we'll get the attention scores by multiplication of the queries with the keys transpose and then uh the attention weight will be uh we'll divide the attention scores with square root of keys we'll take the soft Max so these are the attention weights we which we have obtained until now we have not implemented the causal attention the inputs over here so let me copy paste the inputs which we had defined those are the six words your journey begins with one step these are the inputs so let me copy paste the inputs here so that you can look at the entire code at one glance okay so before this I'm copy pasting the inputs right now great so these are my inputs these are the six words your journey begins with one step and from these input embedding vectors we have uh got the attention weights so I printed them out right now when we get the attention weights this is where the real implementation of the causal attention mechanism starts out so what we are going to do now is that first we are going to generate a mask we are going to generate a mask which looks something like this now this is a mask where you will see that all the elements above the diagonal are equal to zero so ideally that is what we want to do with this attention weight Matrix right remember what we saw over here let me take you to that that the visual representation yeah remember what we saw over here we take the attention weight Matrix and all the elements above the diagonal will be set to zero so essentially if we have a mask like this and if we multiply the attention weights with this mask ideally all the elements above the diagonal will be set to zero so now we are going to construct this mask using the Python's Trill function so what is Trill so there are two types of uh matrices so upper triangular so let us see so there is an upper triangular Matrix and a lower triangular Matrix which I'll just show over here the upper triangular Matrix essentially looks something like this where all the elements below the diagonal are zero so this is tryu in Python tryu in Python yeah this is the upper triangular Matrix in Python and this is the lower triangular Matrix which is try and lower so Tri L what this lower triangular Matrix does is that all the elements above the diagonal will be equal to zero so if you search but I should not search numai so we are looking at torch. Trill so first let us look at torch do Tru so this is torch. Tru so if we use Tru it results in an upper triangular Matrix what shown on the left but if we use torch. Trill if you use tor. Trill what it will result is it will result in a lower triangular Matrix which means that all the elements above the diagonal will be set to zero so to construct a mask which looks something like this can you think about whether we'll need an upper triangular Matrix or a lower triangular Matrix okay so since all the elements above the diagonal are set to zero we'll need a lower triangular Matrix so that's why we use the torch. trill and uh the reason so in torch. Trill what we have to do we have to pass in um what that Matrix is going to look like So currently I'm just going to create a matrix of ones and zeros right so what I'll do is that the Matrix which I'm going to pass in this torch. one's context length comma context length so if you print out this let me show you what this Matrix actually looks like if you print out this Matrix it looks like this and then what I'm going to do I'm going to apply the the lower triangular Matrix function on this Matrix so what will it will do is that it will set all the elements above the diagonal to be equal to zero so that's exactly what's happened here so mask simple will be applying the torch. trill function to this tor. one's Matrix and so when I print out mask simple I'll get this mask where all the elements above the diagonal are equal to zero and remember the length of this mass is specified by the context length why because the context length is how many words the llm can look at before predicting the next word so if you look at this visual representation here the context length is equal to six because the llm can look at six words before predicting the next so in the example which I have shown the context length is just uh you can just look at the number of rows of the attention scores Matrix or the attention weight Matrix matx so here there are six rows right because we have six tokens and the context length which I'm using in this case is six so that that is how we create the mask simple and we print it out over here great now if you multiply the attention weights with this mask what you should if you multiply this attention weight Matrix with this uh mask simple this mask what you should get is that all the elements above the diagonal will be set to zero that's exactly what we are doing so now what we'll do is that we'll Define another variable which is called Mass underscore simple which is the final attention weight Matrix after multiplication of the attention weights with the mask which we have obtained earlier and when we print this out we'll get this type of attention weight Matrix where you will see that all the elements above the diagonal are equal to zero so that's awesome right this is exactly what we wanted but the next step is that you will see that these cannot be are attention weights because each row does not sum up to one so then the next step is to normalize the attention weight so that each row sums up to one so what we'll be doing is that we'll be taking the sum of each row and then dividing all the elements in that row with the sum so for example if you look at the second row we'll take the sum of the second row and we'll divide all the elements of the second row with that sum that way we'll ensure that all the elements in a row sum up to one so this this is what we are going to do next so we'll take we'll calculate the sum of each row and then we'll divide each row with the sum so then we get the mass simple normalized so here you'll see that we get an attention weight Matrix where each uh each row effectively sums up to one this is amazing this is exactly what we need this is the main U modification introduced by the caal tension mechanism it's as simple as this and now we'll multiply this with the values Matrix to get the context Vector Matrix that's it this is the if you understand this much from this lecture you would have understood 80% what of what I wanted to convey now let's go next so you might be thinking okay we have already done out done most of the things right so what do we need to do after this well there are some issues so if you look at the causal attention the main purpose of causal attention essentially is to not have any influence of the future tokens right but if you carefully see what we have done here we have essentially uh applied soft Max to the attention scores which we had obtained earlier right so this this attention weight Matrix even if you look let's say if you look at the second row and if you look at the first two entries of the second row these two entries are already influenced by all the other entries why because when you take the soft Max in the denominator you have the exponential sum of all the weights so even if you zero out all the future tokens it's not essentially cancelling the influence of the future tokens because the future tokens have already influenced the initial two values when we take the soft Max that is what disadvantage of this approach we are we are employing soft Max here and then again what we are doing is we are doing this kind of renormalization by U dividing with the sum so this leads to a data leakage problem why data leakage because the although we zero out the elements above the diagonal since we are taking soft Max before the elements which come in the future do affect the previous elements also so we need a way to avoid this so there is a smarter way to do this renormalization and let me tell you what that smarter way is so if you look at what all we have done until now so what we did is essentially this we took the attention scores we applied soft Max so this already brought in the influence of future tokens then we mask with zero then we again normalize the rows and then we got the attention weight Matrix right this is what we did right now what if there is a more efficient way so the efficient way is that what if we have the attention scores then we apply something called as an upper triangular Infinity mask and then we just apply softmax once this will ensure that there is no leakage problem let me explain what I mean by the upper triangular Infinity mask so let's say we have the let me first show you the attention scores so let's say we have the attention score Matrix right uh instead of applying soft Max earlier and getting the attention weight Matrix what if we replace so let's say for the first row what if we replace these values with negative Infinity for the second row we'll replace these values with negative Infinity basically what if we replace all the entries above the diagonal with negative Infinity like this and then we take the soft Max what that will ensure is that anyway when we take the soft Max when you do the exponent of negative Infinity it's going to be zero so when you take the soft Max of let's say this row all of these entries will anyway be zero and then they will automatically sum up to one because we are taking the soft Max so this kind of a trick will ensure that we are not having the data leakage problem because the attention scores are calculated so now when you look at each row there is no influence of future tokens yet because we have not done the soft Max then we just replace the elements above the diagonal with negative Infinity there is no influence of future tokens now we have cancelled the influence of future tokens by replacing them with negative infinity and we have not even done soft Max now and then we will do soft Max to this Matrix what the soft Max will do is that it will kill two birds with the same Stone it will replace all of these entries with zero because exponent of negative Infinity is anyway zero and since we are applying soft Max it will anyway ensure that the sum of every row is equal to one so it will ensure that the attention weight Matrix the rows all sum up to one and that is exactly what we are going to do next so now if if I give you this Matrix and if I tell you that you want to replace all the elements above the diagonal with zero uh or negative Infinity which whether you will use the upper triangular Matrix or whether you will use the lower triangular Matrix okay so let me tell you how this is actually done the way this works is that we first make a upper triangular Matrix so let me print this out to show you what we are doing here so we print this out right now incomplete input maybe I need one more bracket over here yeah so what we are going to do is that we are going to take uh again 6x6 Matrix of ones and we are going to take an upper triangular Matrix this time remember earlier we took a lower triangular Matrix let me tell you why we take an upper triangular Matrix so we take an upper triangular Matrix where all of these are ones so what we are going to code later is that we are going to say that look at all of the places where there are ones and replace those ones with negative Infinity that is the mask which we are going to construct so so we have this mask tensor over here which is a vector of essentially zeros but all the elements above the TR above the diagonal are one then what we do is we use this attention scores do mask fill function so what this mask fill function does in tensor flow or torch. tensor I'll share this link with you what this uh function does is that uh it looks at the argument first so what's there inside is that we take this mask we take this mask Matrix and we find out all of the places where the uh Matrix returns a positive or True Value and those are all the places which are above the diagonal right and we'll replace this so what this Mas field does is that it looks for all the places where uh this mask Matrix has positive values and then in the attention score Matrix will replace all of those with negative Infinity so effectively what this uh ATT attention scores. mask fill function does is that it takes the attention scores Matrix and it replaces all of the elements above the diagonal with a negative Infinity this is exactly what we wanted and now what we do is we take this Matrix and we apply torge do soft Max so again as we did previously First We Take The Mask Matrix and divide it by the square root of the keys Dimension and then we apply the soft Max so then that will ensure that all the infinity values will anyway become zero and each row will sum up to one so now my final attention weight Matrix looks something like this where you will see that the data leakage problem is not there because I apply soft Max after all of the future elements are set to negative infinity and second all of the attention weight Matrix rows sum up to one so the causal attention mechanism is satisfied and also the soft Max is satisfied the data leakage problem is not there and each row sums up to one so I have essentially obtained everything which I wanted in calcul of these attention weights just to I've written some of these explanations over here so that you can understand it better so masking in Transformers set scores for future tokens to a very large large negative value such as these uh making their influence in the softmax calculation effectively zero the softmax function then recalculates attention weights among the unmask tokens this process ensures no information leakage from the mass tokens focusing the model solely on intended data now uh since we have got the attention weight Matrix we can just simply multiply them with the values Matrix to get the context Vector that's it this is the implementation of the causal attention mechanism in Python but there is one more additional step which is typically implemented along with the causal attention mechanism and that is implementing the causal attention mechanism with Dropout so if you're not familiar with Dropout it's actually a deep learning technique where you take a neural network and you randomly switch on neurons in different layers to zero what this does is that usually when you are training some neurons become lazy and they do not do any work because they realize that other neurons are anyway doing most of the work and the result is pretty well so I'll just switch off so that's a lazy neuron problem or codependency problem what Dropout ensures is that when a lazy neuron sees that the other neuron is Switched Off it it's forced to do the work uh that's the simplest way of thinking about it so Dropout randomly turns neurons off so it ensures that all the neurons essentially participates and this leads to better generalization it prevents overfitting and it does better on the test data we will so the main advantage of Dropout is that it prevents overfitting and improves generalization performance in Transformer architecture including models such as GPT Dropout in the attention mechanism is implemented and it's applied usually in two specific areas first it's applied after the calculation of the attention scores and second it's after applying attention weights to the value vectors so there are two specific uh ways in which um Dropout can generally be implemented first is after you get the context Vector itself after applying attention weights to the value vectors you can Implement Dropout but the more common way is to Implement Dropout after calculation of the attention weights or the attention scores and hence we are going to consider that so essentially what is done in the dropouts is that let's say if you have an attention weight Matrix which with causal attention implemented so all future tokens have been masked what we will do is that we will first create a Dropout mask what this Dropout mask specifies is what all neurons need to be randomly turned off so let's say if we Implement a Dropout with a probability of 0.5 this means that on average 50% of the attention weights in each row will be turned off so let's say if you look at the second row let's say this will be turned off if you look at the third row 50% right so three entries so randomly this will be turned off this will be turned off if you look at the uh fifth row 50% so you'll you'll randomly zero out certain elements so this this is how Dropout is implemented so this is the Dropout mask which you can see over here wherever the mask appears those particular element M will need to be zeroed out so if you look at the fourth row over here uh let me rub some of the things over here yeah so if you look at the fourth row in this Dropout mask we have a mask position here here and here so we have a position at 1 four and five so the first entry will be masked it will be removed the fourth entry will be masked so only two entries are going to survive here 24 and 24 so here you can see over here 24 and point 24 are the only two entries surviving in this row so essentially what the Dropout uh does in very simple terms is that it looks at rows and then it randomly switches switches off attention weights with a particular given probability uh so now let me Implement first the Dropout in um in Python so in the following code example what we are going to do is we are going to use a dropout rate of 50% which means that we are going to mask out half of the attention weights later when we train the GPT model we are going to use a lower dropout rate of around 0.1 or02 so uh in the following code we apply pytorch Dropout implementation to a 6x6 tensor consisting of just ones for illustration purposes and then we'll actually apply it on the attention weight Matrix which we have so let's say we have a 6x6 uh we have an example which is a 6x6 Matrix of on let me print it out over here uh uh yeah so let me print print example so let's say we have a matrix 6x6 so these are all ones then we'll Implement tor. nn. Dropout point5 what this is going to do is that it will look at each row and then on average it will switch off 50% of the weights and what this will do is that since the 50% of Weights are Switched Off which means 0.5 all the other weights are rescaled by that that much amount so all the other weights which are not Switched Off will be rescaled by two it will be divided by 0.5 or they'll be multiplied by two so if you look at the first row over here you'll see that two weights are switched off if you look at the second row you'll switch you'll see that four weights have been switched off you look at the third row you'll see that one weight has been switched off so remember this is probabilistic so if you take 10,000 rows you'll see that on an average 50% of every row will be switched off so that does not guarantee that three exact neurons or three exact weights will be switched off in every row two three or four neurons might be switched off but on an average three neurons will be switched off in every row so when applying Dropout to an attention weight Matrix with the rate of 50% half of the elements with the of the Matrix are randomly set to zero remember this is probabilistic to compensate for the reduction in active elements the values of the remaining elements in The Matrix are scaled up by a factor of two this is how tor. nn. Dropout is implemented and you can even check this so if I click on tor. NN drop. Dropout you can see the documentation for the dropout dropout class in tensor flow or py torch rather so this is a pytorch Dropout class so you'll see that during training randomly zeros out some of the elements with probability P outputs are scaled by a factor of 1 upon 1 minus P that is exactly the kind of scaling which we are seeing over here so the the scaling is crucial to maintain the overall overall balance of the attention weights uh ensuring that the average influence of the attention mechanism remains consistent during training and inference phases now let us actually take the attention weights which we had over here these were the final attention weights and we are going to apply Dropout layer so I take the attention weight Matrix and I apply Dropout to it and here Dropout is being defined as a class and the class takes here an instance of the Dropout class is created that the input argument is 05 which means the dropout rate is 0.5 so here you can see that compared to this um versus let's say if you see the attention weight Matrix you drop out you'll see that some attention weights will be randomly set to zero and the weights which are not set to zero will be scaled by two so in the first row you'll see that this first weight is not set to zero so it will be multiplied by two let's look at the second row so we have 3986 and 60 and4 after implementing Dropout both of them are set to zero uh then let's look at the third row 2526 3791 3683 after implementing Dropout none of them are set to zero so since it's probabilistic in nature some weights will be set to zero some will not but overall 50% of the attention weights will be set to zero so as you can see the resulting ATT attention weight Matrix now has additional elements zeroed out and the remaining ones are rescaped SC this is exactly what we wanted now we have gained an actual understanding of causal attention and Dropout masking we will Implement a causal attention class in Python so this is also what we are going to see next on the Whiteboard so the next section which we are going to see is uh implementing a causal attention class which incorporates causal attention and Dropout into the self attention class which we have implemented earlier so to do this first I want you to have a visual understanding of what we are going to implement that will make understanding the code so much easier so if you have understood the self attention class when we implement this causal attention it's exactly going to be the same except for a few small changes so we will have the inputs we will multiply it with the weight query weight key and the weight value trainable matrices then we'll obtain the queries keys and the value Matrix then we'll get the attention scores by multiplying queries with keys transfer then what we'll do is that we'll Implement uh we'll mask out so all of these diagonals will be replaced with minus all the elements above the diagonal will be replaced with minus infinity then we will do scaling by square root of the dimension and we'll do Dropout and we'll do soft Max so that will give us the attention weights so remember now the attention weights all the elements above the diagonal will be equal to zero and some of the elements will be randomly switch switched off because we are implementing Dropout and then we'll get the we'll multiply the attention weights with the values and we'll get the context Vector Matrix this is all which we are going to do in the uh causal attention class one more additional step which we are going to do is that we are going to look at batches so this is the first batch right so this is the first sentence your journey begins with one step what we ideally want to do is we want to develop the attention class which can handle multiple Cent sentences at once so what if there is a second sentence also that second sentence can be my name is something something let's say so that second sentence will also be handled in a very similar Manner and then I'll get another context weight Matrix in a similar manner so my the class which I Define should be able to handle both of these batches together so let's see how we can Implement that uh one more thing yeah so as I mentioned one more thing is to ensure that the code can handle batches consisting of more than one input as I showed you earlier so what we are going to do is that we are going to create a simple batch which has two inputs so as we already saw the first input is a six row and a three column Matrix now we are just going to add one more input so then the batch will be two a tensor which has two so we have two batches and each has 6x3 so this is the incoming tensor which our class should be equipped to handle so so this results in a 3D tensor consisting of two input text with six tokens the first text can be your journey begins with one step the second sentence can let's say be my name is uh my name is so and so let's say that's the second sentence now uh why is this 6x3 because each sentence has six tokens and each token has three dimensional Vector embedding so the following causal attention class is very similar to the self attention class except that we are going to add two things we are going to add the Dropout and we are going to add the causal mask okay so let's go through this class now uh first what we are going to do is that the shape of the input is now different because now the input shape has the First Dimension as the batch size uh whereas in the self attention class if you scroll up earlier the shape of the input which was there here the shape of the input was just the number of rows were six and the number of columns were three because it did not have batches but now the shape of the input is different the shape of the input let's say is 2x 6x3 so first is the batch number or the batch size the second is the number of tokens and the third is the vector embedding Dimension so B comma number of tokens comma the embedding Dimension is X do shape then what we do is that we multiply the input with the key weight Matrix we multiply the input with the query weight Matrix we multiply the input with the value weight Matrix to get the keys queries and the values then what we are going to do next is that we are going to multiply the queries with the keys transpose and why we are doing 1 comma two over here because we are only interested in the number of tokens Dimension and the inputs Dimensions remember that we are looking at it in batches right so when you look at the first batch uh you when you look at the first batch you only care about the number of tokens and the input Dimensions when you look at the second batch you only care about the number of tokens and the input Dimensions so when you get the attention scores you multiply you take the queries and you multiply keys. transpose 1 comma 2 uh let me explain this to you right now yeah so let me explain how this uh queries and key transpose actually works so now the queries which I have will be in batches right so if you look at the first batch uh let's say these are the so this is the first token and this this is the second token I'm just showing two tokens now and if this is the second batch then this is the first token and this is the second token both of these batches are now coming together in the queries whereas if you look at the keys let's say these are the keys the reason we are taking the keys transpose is that for the queries to be multiplied with the keys the keys need to look like this otherwise the matrix multiplication cannot happen so if you look at the queries now the queries are this and when you do keys. transpose 1 comma 2 they will look look something like this which means that we'll still have two rows but inside the so it was 2A 6 comma 2 right so we'll still have two rows but inside each row the that particular Matrix will be transposed so without transpose the keys look like this and when you do uh when you do this so when you do keys. transpose 1 comma 2 what will be preserved is that inside the uh so the rows will be converted into columns so the keys will Keys transpose will start looking like this and when you multiply the queries with the keys transpose what will happen is that the batches will be processed sequentially so in the first batch this queries will be multiplied with this Keys transpose and you'll get a result then this then this queries will be multiplied with this Keys transpose and you'll get a result and both those results will be stacked together so here just the multiplication has been shown and finally you'll get the attention scores where both the results have been stacked together this is how uh it actually works in batches and then what we do is that once we get the attention scores uh as I told you earlier first we are going to uh we are essentially going to create yeah so here we are creating an upper triangular mask which is of all ones uh except so which is once so let's see the upper triangular so upper triangular Matrix is ones above the diagonal right so there are ones above the diagonals and everything below the diagonal is zero and then wherever there is one those will be replaced with minus infinity in the attention scores Matrix then what we are going to do is we are going to divide by square root of the key Dimension and we are going to take the soft Max this is exactly what we saw this is the attention weight Matrix where all the rows sum up to one and the causal attention mask is applied and then finally what we do is that we uh apply the Dropout to these attention weights and the Dropout has been defined over here where the dropout rate is taken as an attribute when we create an instance of the causal attention class and then the context Vector is a product of the attention weights multiplied by the values and then we ultimately return the context Vector so let's see now how this actually works out in practice so uh first the context length is batch. shape of one because why one because we have a batch size of six so batch do shape of one will be six and then what we'll be doing is that we'll be defining a caal attention class with the input dimension of D in now let's see what D in actually is so D in will be uh let's print this out actually let's see what DN is so I think DN is three because the vector size is three and D out so if I print D in uh that will be equal to three correct and if I print D out I think that will be equal to two because those are the dimensions which we have used yeah D out will be equal to two the context length will be equal to 6 and then 0 comma 0 is 0. 0 is essentially the Dropout so here we are saying that don't do so put the dropout rate to be zero so then what we do is that we create an instance of this causal attention class and then we pass in the batch which we have defined earlier so now you can see that here we defined a batch where we stack two inputs on top of each other when we process the first input when we process the first input we should get a context Vector Matrix of size U so here there are six rows and two columns right so when we process the first input of this batch we'll get a context Vector of the size 6x2 um 6x2 and when we process the second input in the batch we'll get a context Vector Matrix of size again we'll get a context Vector Matrix of size 6x2 so there will be two context vectors of size 6x2 so the resultant answer should be 2x 6x2 it will be a 3D threedimensional tensor let's see if that's indeed the case um so now here you can see that I've have passed in my batch of inputs and here are the context vectors here's my resultant answer and if you print the shape of the context Vector which is the resultant answer it's 2x 6x2 why because we have two matrices of 6x2 which we are stack which are stacked on top of each other so so you can even print out the so you can even print out the context vectors now so if I do print context Vex this will print out the context vectors and you'll see that here we get so the first is uh this is the context Vector Matrix of the first input this is the context Vector Matrix of the second input and they are stacked on top of each other awesome so which means the causal attention class which we have which we have written is capable of handling B I did not explain this thing here which is register buffer so why do we need a buffer when we create this mask so the main thing is that it's not really necessary for all use cases but it offers some Advantage here so when we use the causal attention class in a large language model buffers are automatically moved to the appropriate device CPU or GPU along with our model which will be relevant when training the llm in future chapters usually matrices are this matrices like this which are fixed which which need not need to be trained so this is an upper triangular Matrix right all the elements above the diagonal will be one will be one it's a fixed Matrix we will not train this usually it's better to Define all of these as the using the register buffer because then these are automatically move to the appropriate device along with our model and since we are anywh not training them um it's much more convenient so we don't need to manually ensure that these tensors are on the same device as the model parameters avoiding device M mismatch errors later when we move to GPU calculations this will be very important so just remember that U these masks or these matrices which are not trained it's it's good to Define them using register buffer so that they can be automatically moved to the appropriate devices we don't need to ensure that they are on the same device as our model parameters so that is an important thing to be aware of the second thing is that here we have used the uh colon number of tokens so this is to ensure for cases where the number of tokens in the batch is smaller than the supported context size if this were not written that's also fine then the mask will be of the context size but if the number of tokens are smaller than the context size the mask is created only up till the number of tokens this might happen if one of the batch has smaller number of tokens than the contact size especially one of the ending batches Etc but these are edge cases which you now don't need to worry about all you need to understand is that what this class has effectively done is that we have uh implemented the causal attention mask which means that all the elements above the diagonal are set to zero and we have ensured that all the rows of the attention weight sum up to one using the soft Max and we have also implemented the Dropout layer to ensure generalizability and to prevent overfitting um so I think this actually brings us to the end of this lecture on the causal attention class uh in the next section what we'll be doing is that we will expand on this concept and Implement a multi-head attention module that implements several of these causal attention mechanisms in parallel so what the attention mechanism in GPT and in other llms what they are doing is that they take these causal attention mechanisms and they stack them together so let me show you this graph this plot of what all we have learned so far so until now what we have learned is that we have learned about causal attention in this section when multiple causal attention heads are stacked together it leads to multi-head attention and that's what's actually implemented in GPT but now try to think about it without covering these lectures how can you understand multi-head attention directly without understanding causal attention you cannot understand multi-ad attention to understand causal attention you need to understand key query value that's why I have developed these lecture Series so that we cover each module in a sequential Manner and in a lot of detail I know the lectures are becoming tough the lectures are becoming long but if you follow what I'm doing and if you implement this code you'll start to understand things in a much better manner I believe that all the other tutorials all the other videos on YouTube out there currently they are very short they do not explain all of the details I think the devil lies in the details we need to understand the details we need to deal with Dimensions you need to understand how batches work why we have a three-dimensional tensor don't be scared of dimensions and matrices the student who Masters dimension matrices linear algebra fundamentals they will really understand what is going on here I'm deliberately trying to have a mix of the Whiteboard notes and the coding in Jupiter notebook so that you understand the basics the theory as well as you implement the code thank you so much everyone I'll see you in the next lecture where we'll cover multi-head attention in a lot of detail thanks everyone"
}