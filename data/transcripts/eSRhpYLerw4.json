{
  "video": {
    "video_id": "eSRhpYLerw4",
    "title": "Lecture 14: Simplified Attention Mechanism  - Coded from scratch in Python | No trainable weights",
    "duration": 4762.0,
    "index": 13
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 8.36
    },
    {
      "text": "hello everyone welcome to this lecture",
      "start": 5.24,
      "duration": 5.359
    },
    {
      "text": "in the build large language models from",
      "start": 8.36,
      "duration": 5.479
    },
    {
      "text": "scratch Series in the last lecture we",
      "start": 10.599,
      "duration": 5.6
    },
    {
      "text": "looked at the intuition behind the",
      "start": 13.839,
      "duration": 3.401
    },
    {
      "text": "attention",
      "start": 16.199,
      "duration": 4.0
    },
    {
      "text": "mechanism and we saw the limitations in",
      "start": 17.24,
      "duration": 4.879
    },
    {
      "text": "the recurrent neural networks which",
      "start": 20.199,
      "duration": 4.681
    },
    {
      "text": "prompted The Invention which is at the",
      "start": 22.119,
      "duration": 5.4
    },
    {
      "text": "heart of the attention mechanism in",
      "start": 24.88,
      "duration": 4.96
    },
    {
      "text": "today's lecture we are going to look at",
      "start": 27.519,
      "duration": 4.761
    },
    {
      "text": "the the mathematical foundations behind",
      "start": 29.84,
      "duration": 5.08
    },
    {
      "text": "attention and we are also going to code",
      "start": 32.28,
      "duration": 4.599
    },
    {
      "text": "out a very simplified version of the",
      "start": 34.92,
      "duration": 4.2
    },
    {
      "text": "attention mechanism from scratch in",
      "start": 36.879,
      "duration": 4.921
    },
    {
      "text": "Python so I'm calling this",
      "start": 39.12,
      "duration": 6.04
    },
    {
      "text": "simplified U simplified self attention",
      "start": 41.8,
      "duration": 5.919
    },
    {
      "text": "mechanism without trainable",
      "start": 45.16,
      "duration": 5.76
    },
    {
      "text": "weights before we proceed to the content",
      "start": 47.719,
      "duration": 5.32
    },
    {
      "text": "of the today's lecture here are the",
      "start": 50.92,
      "duration": 3.56
    },
    {
      "text": "lecture notes which we covered",
      "start": 53.039,
      "duration": 3.84
    },
    {
      "text": "previously in our introductory lecture",
      "start": 54.48,
      "duration": 4.399
    },
    {
      "text": "on attention so if you have not seen",
      "start": 56.879,
      "duration": 4.121
    },
    {
      "text": "this lecture I highly encourage you to",
      "start": 58.879,
      "duration": 5.24
    },
    {
      "text": "go through the lecture to understand the",
      "start": 61.0,
      "duration": 5.24
    },
    {
      "text": "intuition that way you will appreciate",
      "start": 64.119,
      "duration": 4.881
    },
    {
      "text": "today's lecture much more in the last",
      "start": 66.24,
      "duration": 4.879
    },
    {
      "text": "lecture I also showed I also told you",
      "start": 69.0,
      "duration": 4.04
    },
    {
      "text": "the plan which I have to cover attention",
      "start": 71.119,
      "duration": 4.481
    },
    {
      "text": "in this series of lectures I strongly",
      "start": 73.04,
      "duration": 4.719
    },
    {
      "text": "believe that it is impossible to cover",
      "start": 75.6,
      "duration": 3.8
    },
    {
      "text": "everything related to the attention",
      "start": 77.759,
      "duration": 4.0
    },
    {
      "text": "mechanism in one video or even two",
      "start": 79.4,
      "duration": 5.52
    },
    {
      "text": "videos so I've planned a set of uh three",
      "start": 81.759,
      "duration": 5.601
    },
    {
      "text": "to four different videos to cover this",
      "start": 84.92,
      "duration": 3.8
    },
    {
      "text": "entire",
      "start": 87.36,
      "duration": 3.64
    },
    {
      "text": "concept the the way I'll be doing this",
      "start": 88.72,
      "duration": 4.439
    },
    {
      "text": "is that first in today's lecture I'll be",
      "start": 91.0,
      "duration": 3.68
    },
    {
      "text": "going through the simplified self",
      "start": 93.159,
      "duration": 3.521
    },
    {
      "text": "attention mechanism without any",
      "start": 94.68,
      "duration": 4.16
    },
    {
      "text": "trainable weights just to introduce the",
      "start": 96.68,
      "duration": 4.56
    },
    {
      "text": "broad idea then in the next lecture",
      "start": 98.84,
      "duration": 4.08
    },
    {
      "text": "we'll move to self attention with",
      "start": 101.24,
      "duration": 3.96
    },
    {
      "text": "trainable weights the way it's done in",
      "start": 102.92,
      "duration": 5.32
    },
    {
      "text": "modern llms including GPT then we'll",
      "start": 105.2,
      "duration": 5.48
    },
    {
      "text": "move to causal attention and then",
      "start": 108.24,
      "duration": 4.0
    },
    {
      "text": "finally we'll move to multi-head",
      "start": 110.68,
      "duration": 3.719
    },
    {
      "text": "attention all of these will be separate",
      "start": 112.24,
      "duration": 4.32
    },
    {
      "text": "series of lectures and in each lecture",
      "start": 114.399,
      "duration": 4.201
    },
    {
      "text": "we'll I'll show you the mathematical",
      "start": 116.56,
      "duration": 4.0
    },
    {
      "text": "foundations on this white board and then",
      "start": 118.6,
      "duration": 3.24
    },
    {
      "text": "we'll code",
      "start": 120.56,
      "duration": 4.199
    },
    {
      "text": "out the attention mechanism showed in",
      "start": 121.84,
      "duration": 5.399
    },
    {
      "text": "that lecture from scratch in",
      "start": 124.759,
      "duration": 5.12
    },
    {
      "text": "Python so if you have got this overall",
      "start": 127.239,
      "duration": 5.681
    },
    {
      "text": "workflow let me go to the lecture notes",
      "start": 129.879,
      "duration": 6.281
    },
    {
      "text": "of today's lecture and we'll get",
      "start": 132.92,
      "duration": 5.92
    },
    {
      "text": "started awesome so the main goal which",
      "start": 136.16,
      "duration": 5.719
    },
    {
      "text": "we have today is to implement a simple",
      "start": 138.84,
      "duration": 5.56
    },
    {
      "text": "variant of the self attention mechanism",
      "start": 141.879,
      "duration": 4.121
    },
    {
      "text": "which does not have any trainable",
      "start": 144.4,
      "duration": 3.919
    },
    {
      "text": "weights which is free from any trainable",
      "start": 146.0,
      "duration": 4.879
    },
    {
      "text": "weights so to motivate today's lecture",
      "start": 148.319,
      "duration": 5.0
    },
    {
      "text": "let's start with a simple sentence in",
      "start": 150.879,
      "duration": 4.0
    },
    {
      "text": "the whole of today's lecture we'll be",
      "start": 153.319,
      "duration": 3.601
    },
    {
      "text": "dealing with this sentence which is your",
      "start": 154.879,
      "duration": 6.36
    },
    {
      "text": "journey starts with one step now if this",
      "start": 156.92,
      "duration": 6.52
    },
    {
      "text": "sentence is given to a large language",
      "start": 161.239,
      "duration": 4.401
    },
    {
      "text": "model there are number of things which",
      "start": 163.44,
      "duration": 5.24
    },
    {
      "text": "are done first we will be pre-processing",
      "start": 165.64,
      "duration": 5.239
    },
    {
      "text": "this sentence we'll take this sentence",
      "start": 168.68,
      "duration": 4.96
    },
    {
      "text": "we'll convert it into individual tokens",
      "start": 170.879,
      "duration": 5.201
    },
    {
      "text": "GPT uses the bite pair encoder which is",
      "start": 173.64,
      "duration": 5.2
    },
    {
      "text": "a subw tokenizer so it will convert",
      "start": 176.08,
      "duration": 4.56
    },
    {
      "text": "these tokens and then then we'll have",
      "start": 178.84,
      "duration": 4.28
    },
    {
      "text": "token IDs for each of these",
      "start": 180.64,
      "duration": 5.08
    },
    {
      "text": "tokens then that token ID will be",
      "start": 183.12,
      "duration": 5.36
    },
    {
      "text": "converted into a vector representation",
      "start": 185.72,
      "duration": 4.12
    },
    {
      "text": "which is also called as a vector",
      "start": 188.48,
      "duration": 3.759
    },
    {
      "text": "embedding so each of these word will",
      "start": 189.84,
      "duration": 3.92
    },
    {
      "text": "have a vector embedding in a higher",
      "start": 192.239,
      "duration": 4.36
    },
    {
      "text": "dimensional space when we deal with llms",
      "start": 193.76,
      "duration": 5.64
    },
    {
      "text": "like GPT the dimensional space can be as",
      "start": 196.599,
      "duration": 5.321
    },
    {
      "text": "high as 500 700 or even more than",
      "start": 199.4,
      "duration": 4.919
    },
    {
      "text": "thousand dimensional Vector space but",
      "start": 201.92,
      "duration": 5.08
    },
    {
      "text": "for the sake of today's demonstration",
      "start": 204.319,
      "duration": 4.441
    },
    {
      "text": "we'll be considering a three-dimensional",
      "start": 207.0,
      "duration": 4.12
    },
    {
      "text": "Vector space so here's how these vectors",
      "start": 208.76,
      "duration": 4.6
    },
    {
      "text": "can look like for each word in the input",
      "start": 211.12,
      "duration": 4.0
    },
    {
      "text": "sentence we have a vector in the",
      "start": 213.36,
      "duration": 3.92
    },
    {
      "text": "three-dimensional space these vectors",
      "start": 215.12,
      "duration": 4.8
    },
    {
      "text": "are not just normal vectors they capture",
      "start": 217.28,
      "duration": 5.159
    },
    {
      "text": "the semantic meaning of each word so",
      "start": 219.92,
      "duration": 4.519
    },
    {
      "text": "Words which are semantically related to",
      "start": 222.439,
      "duration": 4.08
    },
    {
      "text": "each other like let's say cat and puppy",
      "start": 224.439,
      "duration": 4.681
    },
    {
      "text": "cat and kitten dog and puppy will be",
      "start": 226.519,
      "duration": 4.521
    },
    {
      "text": "closer to each",
      "start": 229.12,
      "duration": 4.36
    },
    {
      "text": "other um so you might be thinking that",
      "start": 231.04,
      "duration": 4.479
    },
    {
      "text": "okay this is pretty cool and this can be",
      "start": 233.48,
      "duration": 3.839
    },
    {
      "text": "the input to the large language model",
      "start": 235.519,
      "duration": 4.321
    },
    {
      "text": "then why do we need attention the main",
      "start": 237.319,
      "duration": 4.84
    },
    {
      "text": "problem here is that let's say if we",
      "start": 239.84,
      "duration": 4.88
    },
    {
      "text": "look at Journey what the vector",
      "start": 242.159,
      "duration": 4.681
    },
    {
      "text": "embedding does is that the high",
      "start": 244.72,
      "duration": 6.2
    },
    {
      "text": "dimensional projection of this word does",
      "start": 246.84,
      "duration": 6.399
    },
    {
      "text": "capture the meaning of this word itself",
      "start": 250.92,
      "duration": 5.039
    },
    {
      "text": "but it does not contain any information",
      "start": 253.239,
      "duration": 5.24
    },
    {
      "text": "about how this word Journey relates to",
      "start": 255.959,
      "duration": 5.641
    },
    {
      "text": "other words in the sentence how much is",
      "start": 258.479,
      "duration": 5.801
    },
    {
      "text": "the relative importance between your and",
      "start": 261.6,
      "duration": 5.68
    },
    {
      "text": "journey starts and journey with and",
      "start": 264.28,
      "duration": 6.359
    },
    {
      "text": "journey one and journey and step",
      "start": 267.28,
      "duration": 4.919
    },
    {
      "text": "with respect to",
      "start": 270.639,
      "duration": 3.921
    },
    {
      "text": "Journey the embedding Vector does not",
      "start": 272.199,
      "duration": 5.361
    },
    {
      "text": "contain any of this information and this",
      "start": 274.56,
      "duration": 5.0
    },
    {
      "text": "information is very crucial for us to",
      "start": 277.56,
      "duration": 4.68
    },
    {
      "text": "know if this sentence appears in a large",
      "start": 279.56,
      "duration": 4.639
    },
    {
      "text": "body of text and we want to predict the",
      "start": 282.24,
      "duration": 4.32
    },
    {
      "text": "next word we really need to know the",
      "start": 284.199,
      "duration": 4.761
    },
    {
      "text": "context we need to know that which word",
      "start": 286.56,
      "duration": 4.28
    },
    {
      "text": "in this sentence is closely related to",
      "start": 288.96,
      "duration": 4.2
    },
    {
      "text": "Journey let's say that will really help",
      "start": 290.84,
      "duration": 5.04
    },
    {
      "text": "us predict the next word in other words",
      "start": 293.16,
      "duration": 5.2
    },
    {
      "text": "we need to know how much attention",
      "start": 295.88,
      "duration": 4.4
    },
    {
      "text": "should we pay to each each of these",
      "start": 298.36,
      "duration": 4.36
    },
    {
      "text": "words when we look at Journey that's",
      "start": 300.28,
      "duration": 5.08
    },
    {
      "text": "where attention mechanism comes into the",
      "start": 302.72,
      "duration": 4.759
    },
    {
      "text": "picture so the whole goal of the",
      "start": 305.36,
      "duration": 3.88
    },
    {
      "text": "attention mechanism is to take the",
      "start": 307.479,
      "duration": 5.361
    },
    {
      "text": "embedding Vector for uh take the vector",
      "start": 309.24,
      "duration": 6.04
    },
    {
      "text": "embedding for Journey let's say and then",
      "start": 312.84,
      "duration": 4.52
    },
    {
      "text": "transform it into another Vector which",
      "start": 315.28,
      "duration": 3.88
    },
    {
      "text": "is called as the context",
      "start": 317.36,
      "duration": 4.72
    },
    {
      "text": "Vector the context Vector can be thought",
      "start": 319.16,
      "duration": 5.08
    },
    {
      "text": "of as an enriched embedding Vector",
      "start": 322.08,
      "duration": 3.839
    },
    {
      "text": "because it contains much more",
      "start": 324.24,
      "duration": 3.88
    },
    {
      "text": "information it not only contains the",
      "start": 325.919,
      "duration": 3.881
    },
    {
      "text": "semantic meaning which is the embedding",
      "start": 328.12,
      "duration": 3.76
    },
    {
      "text": "Vector also contains but it also",
      "start": 329.8,
      "duration": 5.0
    },
    {
      "text": "contains information about how that",
      "start": 331.88,
      "duration": 5.52
    },
    {
      "text": "given word relates to other words in the",
      "start": 334.8,
      "duration": 5.119
    },
    {
      "text": "sentence and uh this contextual",
      "start": 337.4,
      "duration": 4.72
    },
    {
      "text": "information really helps a lot in",
      "start": 339.919,
      "duration": 4.361
    },
    {
      "text": "predicting the next word in large",
      "start": 342.12,
      "duration": 3.919
    },
    {
      "text": "language model",
      "start": 344.28,
      "duration": 4.359
    },
    {
      "text": "tasks one such context Vector will be",
      "start": 346.039,
      "duration": 4.961
    },
    {
      "text": "generated from each of the embedding",
      "start": 348.639,
      "duration": 5.12
    },
    {
      "text": "vectors so if you look at all of the",
      "start": 351.0,
      "duration": 4.199
    },
    {
      "text": "embedding vectors which I have shown",
      "start": 353.759,
      "duration": 3.761
    },
    {
      "text": "here we'll have a corresponding context",
      "start": 355.199,
      "duration": 4.0
    },
    {
      "text": "Vector for each of these embedding",
      "start": 357.52,
      "duration": 2.679
    },
    {
      "text": "vector",
      "start": 359.199,
      "duration": 3.321
    },
    {
      "text": "and that is the main purpose or that is",
      "start": 360.199,
      "duration": 4.72
    },
    {
      "text": "the main goal of attention mechanism",
      "start": 362.52,
      "duration": 4.36
    },
    {
      "text": "even the simplified attention mechanism",
      "start": 364.919,
      "duration": 4.12
    },
    {
      "text": "which we are going to consider today and",
      "start": 366.88,
      "duration": 4.439
    },
    {
      "text": "even the complex multi-head attention",
      "start": 369.039,
      "duration": 4.16
    },
    {
      "text": "which is used in modern large language",
      "start": 371.319,
      "duration": 4.361
    },
    {
      "text": "models the goal of all of these",
      "start": 373.199,
      "duration": 4.801
    },
    {
      "text": "mechanisms is the same we need to",
      "start": 375.68,
      "duration": 4.32
    },
    {
      "text": "convert the embedding Vector into",
      "start": 378.0,
      "duration": 4.28
    },
    {
      "text": "context vectors which are then inputs to",
      "start": 380.0,
      "duration": 3.4
    },
    {
      "text": "the",
      "start": 382.28,
      "duration": 3.44
    },
    {
      "text": "llms so let's get started with",
      "start": 383.4,
      "duration": 4.16
    },
    {
      "text": "accomplishing this aim of converting the",
      "start": 385.72,
      "duration": 4.56
    },
    {
      "text": "embedding vectors into context vectors",
      "start": 387.56,
      "duration": 4.84
    },
    {
      "text": "so here's the input Vector which we have",
      "start": 390.28,
      "duration": 5.0
    },
    {
      "text": "your journey starts with one step each",
      "start": 392.4,
      "duration": 5.519
    },
    {
      "text": "of this each of the tokens is converted",
      "start": 395.28,
      "duration": 4.479
    },
    {
      "text": "into a threedimensional vector it's",
      "start": 397.919,
      "duration": 3.881
    },
    {
      "text": "called as a vector embedding and the",
      "start": 399.759,
      "duration": 3.761
    },
    {
      "text": "notation which we are going to use is we",
      "start": 401.8,
      "duration": 4.0
    },
    {
      "text": "are going to denote X for the inputs so",
      "start": 403.52,
      "duration": 4.799
    },
    {
      "text": "X1 will be the vector representation of",
      "start": 405.8,
      "duration": 5.399
    },
    {
      "text": "the first token X2 will be the vector",
      "start": 408.319,
      "duration": 5.72
    },
    {
      "text": "representation of the second token X3",
      "start": 411.199,
      "duration": 4.641
    },
    {
      "text": "will be the vector representation of the",
      "start": 414.039,
      "duration": 3.44
    },
    {
      "text": "third token and these will be",
      "start": 415.84,
      "duration": 4.44
    },
    {
      "text": "threedimensional tokens for the sake of",
      "start": 417.479,
      "duration": 4.921
    },
    {
      "text": "Simplicity throughout this lecture when",
      "start": 420.28,
      "duration": 5.0
    },
    {
      "text": "I use the word token and word I'll use",
      "start": 422.4,
      "duration": 5.44
    },
    {
      "text": "tokens and words interchangeably",
      "start": 425.28,
      "duration": 5.24
    },
    {
      "text": "normally one token is not equal to one",
      "start": 427.84,
      "duration": 5.759
    },
    {
      "text": "word because GPT uses a subword",
      "start": 430.52,
      "duration": 5.6
    },
    {
      "text": "tokenizer but for the sake of Simplicity",
      "start": 433.599,
      "duration": 4.44
    },
    {
      "text": "I'm going to use tokens and words",
      "start": 436.12,
      "duration": 4.079
    },
    {
      "text": "interchangeably in today's",
      "start": 438.039,
      "duration": 4.321
    },
    {
      "text": "lecture so let's say if we look at",
      "start": 440.199,
      "duration": 5.321
    },
    {
      "text": "Journey the input embedding Vector is X2",
      "start": 442.36,
      "duration": 6.08
    },
    {
      "text": "two because it is the second word our",
      "start": 445.52,
      "duration": 5.959
    },
    {
      "text": "main aim uh in this lecture is to",
      "start": 448.44,
      "duration": 6.12
    },
    {
      "text": "essentially create a context Vector so",
      "start": 451.479,
      "duration": 4.961
    },
    {
      "text": "the context Vector for X2 will be",
      "start": 454.56,
      "duration": 3.28
    },
    {
      "text": "denoted by",
      "start": 456.44,
      "duration": 4.199
    },
    {
      "text": "zed2 similarly we need one such context",
      "start": 457.84,
      "duration": 5.0
    },
    {
      "text": "Vector for all the input",
      "start": 460.639,
      "duration": 4.641
    },
    {
      "text": "vectors and to compute the context",
      "start": 462.84,
      "duration": 4.199
    },
    {
      "text": "Vector from the embedding Vector we'll",
      "start": 465.28,
      "duration": 4.039
    },
    {
      "text": "need to have information about how much",
      "start": 467.039,
      "duration": 5.521
    },
    {
      "text": "in importance needs to be given to X1",
      "start": 469.319,
      "duration": 4.88
    },
    {
      "text": "which is the word your how much",
      "start": 472.56,
      "duration": 4.12
    },
    {
      "text": "information needs to be given to X3 and",
      "start": 474.199,
      "duration": 4.241
    },
    {
      "text": "how much information needs to be given",
      "start": 476.68,
      "duration": 3.28
    },
    {
      "text": "to all the other word when we are",
      "start": 478.44,
      "duration": 4.199
    },
    {
      "text": "looking at Journey For example and this",
      "start": 479.96,
      "duration": 4.84
    },
    {
      "text": "is captured by this metric which is",
      "start": 482.639,
      "duration": 3.921
    },
    {
      "text": "called as attention",
      "start": 484.8,
      "duration": 4.2
    },
    {
      "text": "weights so we need to decide how much",
      "start": 486.56,
      "duration": 4.44
    },
    {
      "text": "attention we we need to give to each",
      "start": 489.0,
      "duration": 4.08
    },
    {
      "text": "input token when we are Computing the",
      "start": 491.0,
      "duration": 5.159
    },
    {
      "text": "context Vector for any uh embedding",
      "start": 493.08,
      "duration": 5.32
    },
    {
      "text": "vector and based on these attention",
      "start": 496.159,
      "duration": 4.6
    },
    {
      "text": "weights we will finally derive the",
      "start": 498.4,
      "duration": 4.68
    },
    {
      "text": "context Vector which is denoted by",
      "start": 500.759,
      "duration": 6.28
    },
    {
      "text": "Zed so we'll see mathematically how to",
      "start": 503.08,
      "duration": 5.839
    },
    {
      "text": "uh manipulate the attention weights the",
      "start": 507.039,
      "duration": 3.801
    },
    {
      "text": "input vector VOR we'll also see what are",
      "start": 508.919,
      "duration": 3.721
    },
    {
      "text": "attention scores and how to get the",
      "start": 510.84,
      "duration": 3.92
    },
    {
      "text": "context Vector for each of the given",
      "start": 512.64,
      "duration": 3.24
    },
    {
      "text": "embedding",
      "start": 514.76,
      "duration": 3.519
    },
    {
      "text": "vectors but I hope until now you have",
      "start": 515.88,
      "duration": 4.48
    },
    {
      "text": "understood the task or the aim which we",
      "start": 518.279,
      "duration": 3.521
    },
    {
      "text": "have in today's",
      "start": 520.36,
      "duration": 4.12
    },
    {
      "text": "lecture and as I mentioned the",
      "start": 521.8,
      "duration": 4.36
    },
    {
      "text": "representation or the notation which",
      "start": 524.48,
      "duration": 3.2
    },
    {
      "text": "will be which we will be following is",
      "start": 526.16,
      "duration": 4.239
    },
    {
      "text": "that for the inputs we'll be using X so",
      "start": 527.68,
      "duration": 5.279
    },
    {
      "text": "X of one will be the token embedding one",
      "start": 530.399,
      "duration": 4.041
    },
    {
      "text": "which is the vector representation of",
      "start": 532.959,
      "duration": 4.0
    },
    {
      "text": "the first token X of two will be the",
      "start": 534.44,
      "duration": 4.36
    },
    {
      "text": "token embedding two which is the vector",
      "start": 536.959,
      "duration": 3.361
    },
    {
      "text": "representation of of the second token",
      "start": 538.8,
      "duration": 2.92
    },
    {
      "text": "Journey",
      "start": 540.32,
      "duration": 3.72
    },
    {
      "text": "Etc so these are three dimensional",
      "start": 541.72,
      "duration": 4.4
    },
    {
      "text": "embeddings in the example which we are",
      "start": 544.04,
      "duration": 5.32
    },
    {
      "text": "considering in today's lecture now uh I",
      "start": 546.12,
      "duration": 4.88
    },
    {
      "text": "have just written down the goal which I",
      "start": 549.36,
      "duration": 4.0
    },
    {
      "text": "was talking about the goal of today's",
      "start": 551.0,
      "duration": 5.72
    },
    {
      "text": "lecture or the goal of any attention",
      "start": 553.36,
      "duration": 5.8
    },
    {
      "text": "mechanism is to basically calculate a",
      "start": 556.72,
      "duration": 5.44
    },
    {
      "text": "context Vector for each element in the",
      "start": 559.16,
      "duration": 5.44
    },
    {
      "text": "input and as I mentioned before the",
      "start": 562.16,
      "duration": 4.359
    },
    {
      "text": "context Vector can be thought of as an",
      "start": 564.6,
      "duration": 3.919
    },
    {
      "text": "enriched embedding Vector which also",
      "start": 566.519,
      "duration": 3.961
    },
    {
      "text": "contains information about how that",
      "start": 568.519,
      "duration": 4.601
    },
    {
      "text": "particular word relates to other words",
      "start": 570.48,
      "duration": 4.64
    },
    {
      "text": "in the",
      "start": 573.12,
      "duration": 5.56
    },
    {
      "text": "sentence okay so let us get started and",
      "start": 575.12,
      "duration": 6.12
    },
    {
      "text": "we'll dive into code right now so here's",
      "start": 578.68,
      "duration": 4.32
    },
    {
      "text": "the Jupiter notebook file and I'll be",
      "start": 581.24,
      "duration": 4.48
    },
    {
      "text": "sharing this file with you and uh we'll",
      "start": 583.0,
      "duration": 4.76
    },
    {
      "text": "get started implementing a simplified",
      "start": 585.72,
      "duration": 4.52
    },
    {
      "text": "attention mechanism in",
      "start": 587.76,
      "duration": 5.079
    },
    {
      "text": "Python so let's start taking a deep dive",
      "start": 590.24,
      "duration": 6.279
    },
    {
      "text": "into this code so uh we are going to",
      "start": 592.839,
      "duration": 5.361
    },
    {
      "text": "consider threedimensional embedding",
      "start": 596.519,
      "duration": 3.401
    },
    {
      "text": "vectors and the the reason we are",
      "start": 598.2,
      "duration": 3.4
    },
    {
      "text": "choosing this small Dimension is just",
      "start": 599.92,
      "duration": 4.72
    },
    {
      "text": "for illustration purposes to ensure that",
      "start": 601.6,
      "duration": 6.28
    },
    {
      "text": "uh we do not consider an very complex",
      "start": 604.64,
      "duration": 5.72
    },
    {
      "text": "example and confuse",
      "start": 607.88,
      "duration": 4.639
    },
    {
      "text": "everyone but the learnings which we are",
      "start": 610.36,
      "duration": 4.2
    },
    {
      "text": "going to have today can easily be",
      "start": 612.519,
      "duration": 3.961
    },
    {
      "text": "extended to a higher dimensional Vector",
      "start": 614.56,
      "duration": 3.0
    },
    {
      "text": "space",
      "start": 616.48,
      "duration": 4.12
    },
    {
      "text": "also so we have the inputs tensor which",
      "start": 617.56,
      "duration": 6.04
    },
    {
      "text": "is inputs equal to torch do tensor and",
      "start": 620.6,
      "duration": 6.679
    },
    {
      "text": "for every input token we have a",
      "start": 623.6,
      "duration": 4.919
    },
    {
      "text": "embedding Vector which is a",
      "start": 627.279,
      "duration": 3.281
    },
    {
      "text": "three-dimensional vector Vector so for",
      "start": 628.519,
      "duration": 4.161
    },
    {
      "text": "the token or the word y we have this",
      "start": 630.56,
      "duration": 4.279
    },
    {
      "text": "three-dimensional Vector for the word",
      "start": 632.68,
      "duration": 4.2
    },
    {
      "text": "Journey we have this three-dimensional",
      "start": 634.839,
      "duration": 4.56
    },
    {
      "text": "Vector for the word starts we have this",
      "start": 636.88,
      "duration": 5.199
    },
    {
      "text": "threedimensional Vector right up to step",
      "start": 639.399,
      "duration": 5.0
    },
    {
      "text": "we have this three-dimensional Vector so",
      "start": 642.079,
      "duration": 4.401
    },
    {
      "text": "there are six threedimensional vectors",
      "start": 644.399,
      "duration": 4.68
    },
    {
      "text": "which we are considering over here and",
      "start": 646.48,
      "duration": 4.88
    },
    {
      "text": "uh I have just plotted these vectors",
      "start": 649.079,
      "duration": 4.32
    },
    {
      "text": "here in in the threedimensional vector",
      "start": 651.36,
      "duration": 4.64
    },
    {
      "text": "space for you to have a look these",
      "start": 653.399,
      "duration": 4.721
    },
    {
      "text": "Vector embeddings capture the semantic",
      "start": 656.0,
      "duration": 4.639
    },
    {
      "text": "meaning so journey and starts will be",
      "start": 658.12,
      "duration": 4.56
    },
    {
      "text": "bit more closer to each other because",
      "start": 660.639,
      "duration": 4.44
    },
    {
      "text": "they are semantically more related than",
      "start": 662.68,
      "duration": 5.2
    },
    {
      "text": "let's say journey and other words so",
      "start": 665.079,
      "duration": 5.2
    },
    {
      "text": "this is how the vectors look like in 3D",
      "start": 667.88,
      "duration": 5.12
    },
    {
      "text": "space uh there is no reason to plot this",
      "start": 670.279,
      "duration": 4.68
    },
    {
      "text": "graph you can do this entire exercise",
      "start": 673.0,
      "duration": 4.32
    },
    {
      "text": "without these visuals but the reason I",
      "start": 674.959,
      "duration": 4.361
    },
    {
      "text": "wanted to show you this is because when",
      "start": 677.32,
      "duration": 3.759
    },
    {
      "text": "I learned about Vector embeddings for",
      "start": 679.32,
      "duration": 4.12
    },
    {
      "text": "the first time I was really fascinated",
      "start": 681.079,
      "duration": 4.041
    },
    {
      "text": "by the concept that words can be",
      "start": 683.44,
      "duration": 5.0
    },
    {
      "text": "represented as vectors uh it seemed very",
      "start": 685.12,
      "duration": 5.279
    },
    {
      "text": "surprising to me that how can words be",
      "start": 688.44,
      "duration": 4.44
    },
    {
      "text": "represented mathematically as vectors",
      "start": 690.399,
      "duration": 4.601
    },
    {
      "text": "but when I saw this I really was I",
      "start": 692.88,
      "duration": 4.84
    },
    {
      "text": "really liked the concept so visual",
      "start": 695.0,
      "duration": 4.639
    },
    {
      "text": "understanding helps and that's why I'm",
      "start": 697.72,
      "duration": 4.16
    },
    {
      "text": "showing you this plot so if someone is",
      "start": 699.639,
      "duration": 4.0
    },
    {
      "text": "not familiar with natural language",
      "start": 701.88,
      "duration": 4.0
    },
    {
      "text": "processing and your intuition is not",
      "start": 703.639,
      "duration": 5.0
    },
    {
      "text": "developed in this space such plots can",
      "start": 705.88,
      "duration": 4.48
    },
    {
      "text": "really help you visualize things which",
      "start": 708.639,
      "duration": 3.88
    },
    {
      "text": "you will never forget so Vector",
      "start": 710.36,
      "duration": 4.0
    },
    {
      "text": "embedding becomes much more easier if",
      "start": 712.519,
      "duration": 4.76
    },
    {
      "text": "you have visuals like these okay now",
      "start": 714.36,
      "duration": 5.76
    },
    {
      "text": "let's move to the next step uh first",
      "start": 717.279,
      "duration": 4.601
    },
    {
      "text": "let's look at this tensor and let's try",
      "start": 720.12,
      "duration": 3.68
    },
    {
      "text": "to understand what each row and each",
      "start": 721.88,
      "duration": 3.36
    },
    {
      "text": "column of this tensor actually",
      "start": 723.8,
      "duration": 4.0
    },
    {
      "text": "represents so each row of this tensor",
      "start": 725.24,
      "duration": 5.08
    },
    {
      "text": "represents each token so the first row",
      "start": 727.8,
      "duration": 5.12
    },
    {
      "text": "is the first token or the first word Etc",
      "start": 730.32,
      "duration": 4.319
    },
    {
      "text": "and each column represents that",
      "start": 732.92,
      "duration": 3.52
    },
    {
      "text": "particular Vector Dimension there are",
      "start": 734.639,
      "duration": 3.961
    },
    {
      "text": "three dimensions and hence there are",
      "start": 736.44,
      "duration": 4.24
    },
    {
      "text": "three",
      "start": 738.6,
      "duration": 5.56
    },
    {
      "text": "columns okay now uh we'll be moving to",
      "start": 740.68,
      "duration": 6.08
    },
    {
      "text": "the next aspect which is discussing a",
      "start": 744.16,
      "duration": 4.76
    },
    {
      "text": "bit about query and what exactly are",
      "start": 746.76,
      "duration": 4.4
    },
    {
      "text": "query",
      "start": 748.92,
      "duration": 2.24
    },
    {
      "text": "um so now what we'll be doing is that",
      "start": 752.24,
      "duration": 5.599
    },
    {
      "text": "we'll be finding a context Vector for",
      "start": 754.639,
      "duration": 7.121
    },
    {
      "text": "each element so if you look at um this",
      "start": 757.839,
      "duration": 6.281
    },
    {
      "text": "sentence right here we'll be finding a",
      "start": 761.76,
      "duration": 5.04
    },
    {
      "text": "context Vector for each of the embedding",
      "start": 764.12,
      "duration": 5.079
    },
    {
      "text": "vectors but for the sake of",
      "start": 766.8,
      "duration": 4.36
    },
    {
      "text": "demonstration we are going to start with",
      "start": 769.199,
      "duration": 4.561
    },
    {
      "text": "journey so we are going to exclusively",
      "start": 771.16,
      "duration": 4.679
    },
    {
      "text": "look at Journey right now and we are",
      "start": 773.76,
      "duration": 3.84
    },
    {
      "text": "going to find the context Vector for",
      "start": 775.839,
      "duration": 3.481
    },
    {
      "text": "Journey and then we are going to to",
      "start": 777.6,
      "duration": 3.76
    },
    {
      "text": "apply the same analysis to all the other",
      "start": 779.32,
      "duration": 4.639
    },
    {
      "text": "words in this",
      "start": 781.36,
      "duration": 5.24
    },
    {
      "text": "sentence uh so now as I mentioned we'll",
      "start": 783.959,
      "duration": 4.481
    },
    {
      "text": "be focusing on the second element which",
      "start": 786.6,
      "duration": 5.4
    },
    {
      "text": "is X of two why two because it's the",
      "start": 788.44,
      "duration": 6.32
    },
    {
      "text": "second element of X so X1 is the first",
      "start": 792.0,
      "duration": 5.12
    },
    {
      "text": "element X2 is the second element so we",
      "start": 794.76,
      "duration": 4.72
    },
    {
      "text": "are looking at X2 because we are looking",
      "start": 797.12,
      "duration": 5.44
    },
    {
      "text": "at the word Journey the element or the",
      "start": 799.48,
      "duration": 5.08
    },
    {
      "text": "token which we are looking at right now",
      "start": 802.56,
      "duration": 4.719
    },
    {
      "text": "it's also called as the",
      "start": 804.56,
      "duration": 4.76
    },
    {
      "text": "query and since we are looking looking",
      "start": 807.279,
      "duration": 4.56
    },
    {
      "text": "at the token Journey that becomes the",
      "start": 809.32,
      "duration": 5.759
    },
    {
      "text": "query this uh terminology will also show",
      "start": 811.839,
      "duration": 5.24
    },
    {
      "text": "up later when you look when we look at",
      "start": 815.079,
      "duration": 4.44
    },
    {
      "text": "multi-head attention mechanism but for",
      "start": 817.079,
      "duration": 4.601
    },
    {
      "text": "now uh just keep in mind that we are",
      "start": 819.519,
      "duration": 4.721
    },
    {
      "text": "looking at this uh",
      "start": 821.68,
      "duration": 6.12
    },
    {
      "text": "token journey and that becomes our",
      "start": 824.24,
      "duration": 6.08
    },
    {
      "text": "query and the corresponding context",
      "start": 827.8,
      "duration": 4.839
    },
    {
      "text": "Vector for this query which is X2 will",
      "start": 830.32,
      "duration": 6.0
    },
    {
      "text": "be Z of two and that is essentially an",
      "start": 832.639,
      "duration": 6.721
    },
    {
      "text": "embedding which contains information",
      "start": 836.32,
      "duration": 6.72
    },
    {
      "text": "about X2 and all the other input",
      "start": 839.36,
      "duration": 6.399
    },
    {
      "text": "elements so this context Vector not only",
      "start": 843.04,
      "duration": 4.88
    },
    {
      "text": "contains information about that",
      "start": 845.759,
      "duration": 4.241
    },
    {
      "text": "particular word which is Journey but it",
      "start": 847.92,
      "duration": 4.24
    },
    {
      "text": "also contains information about all the",
      "start": 850.0,
      "duration": 4.72
    },
    {
      "text": "other input",
      "start": 852.16,
      "duration": 2.56
    },
    {
      "text": "elements again I would like to emphasize",
      "start": 854.759,
      "duration": 4.08
    },
    {
      "text": "here that in this lecture we are not",
      "start": 857.16,
      "duration": 3.96
    },
    {
      "text": "looking at trainable weights later we",
      "start": 858.839,
      "duration": 4.201
    },
    {
      "text": "are going to add trainable weights which",
      "start": 861.12,
      "duration": 4.279
    },
    {
      "text": "is how it's actually done in llms",
      "start": 863.04,
      "duration": 5.32
    },
    {
      "text": "because this helps the llms to",
      "start": 865.399,
      "duration": 4.56
    },
    {
      "text": "understand the context text much better",
      "start": 868.36,
      "duration": 4.24
    },
    {
      "text": "and learn in a better",
      "start": 869.959,
      "duration": 6.481
    },
    {
      "text": "way uh okay now we have this task that",
      "start": 872.6,
      "duration": 5.88
    },
    {
      "text": "we we have a query which is the word",
      "start": 876.44,
      "duration": 5.399
    },
    {
      "text": "journey and the task is to convert this",
      "start": 878.48,
      "duration": 6.0
    },
    {
      "text": "the embedding Vector for journey into a",
      "start": 881.839,
      "duration": 6.641
    },
    {
      "text": "context Vector the first task the first",
      "start": 884.48,
      "duration": 6.159
    },
    {
      "text": "step of implementing this task is to",
      "start": 888.48,
      "duration": 5.08
    },
    {
      "text": "compute the intermediate values W which",
      "start": 890.639,
      "duration": 5.241
    },
    {
      "text": "are also referred to as the attention",
      "start": 893.56,
      "duration": 5.76
    },
    {
      "text": "scores so the first task is to basically",
      "start": 895.88,
      "duration": 6.04
    },
    {
      "text": "do the following we have a query which",
      "start": 899.32,
      "duration": 4.759
    },
    {
      "text": "is the word journey and we have all",
      "start": 901.92,
      "duration": 4.08
    },
    {
      "text": "these other input words",
      "start": 904.079,
      "duration": 4.801
    },
    {
      "text": "right now what we have to quantify",
      "start": 906.0,
      "duration": 5.519
    },
    {
      "text": "basically is that how much importance",
      "start": 908.88,
      "duration": 4.92
    },
    {
      "text": "should be paid to each of these other",
      "start": 911.519,
      "duration": 4.68
    },
    {
      "text": "words or how much attention should be",
      "start": 913.8,
      "duration": 5.279
    },
    {
      "text": "paid to each of the input word for the",
      "start": 916.199,
      "duration": 4.801
    },
    {
      "text": "query",
      "start": 919.079,
      "duration": 4.801
    },
    {
      "text": "journey and this is Quantified by a",
      "start": 921.0,
      "duration": 4.759
    },
    {
      "text": "mathematical metric which is called as",
      "start": 923.88,
      "duration": 4.36
    },
    {
      "text": "the attention score so the attention",
      "start": 925.759,
      "duration": 5.08
    },
    {
      "text": "score exist exists between the query and",
      "start": 928.24,
      "duration": 5.159
    },
    {
      "text": "every input Vector so there will be an",
      "start": 930.839,
      "duration": 4.401
    },
    {
      "text": "attention score between the query and",
      "start": 933.399,
      "duration": 4.081
    },
    {
      "text": "the first input Vector there will be an",
      "start": 935.24,
      "duration": 4.2
    },
    {
      "text": "attention score between the query and",
      "start": 937.48,
      "duration": 4.56
    },
    {
      "text": "the second input Vector there will be an",
      "start": 939.44,
      "duration": 4.8
    },
    {
      "text": "attention score between the query and",
      "start": 942.04,
      "duration": 4.359
    },
    {
      "text": "the third input vector and finally there",
      "start": 944.24,
      "duration": 3.68
    },
    {
      "text": "will be an attention score between the",
      "start": 946.399,
      "duration": 3.8
    },
    {
      "text": "query and the final input",
      "start": 947.92,
      "duration": 4.8
    },
    {
      "text": "Vector the first step of getting to the",
      "start": 950.199,
      "duration": 4.401
    },
    {
      "text": "context Vector is to find these",
      "start": 952.72,
      "duration": 4.44
    },
    {
      "text": "attention scores which will help us",
      "start": 954.6,
      "duration": 4.08
    },
    {
      "text": "understand how much importance should be",
      "start": 957.16,
      "duration": 4.679
    },
    {
      "text": "given to each of the tokens in the",
      "start": 958.68,
      "duration": 6.8
    },
    {
      "text": "input okay for a moment assume that you",
      "start": 961.839,
      "duration": 5.24
    },
    {
      "text": "do not know anything about large",
      "start": 965.48,
      "duration": 3.359
    },
    {
      "text": "language models you do not know anything",
      "start": 967.079,
      "duration": 3.361
    },
    {
      "text": "about machine learning or natural",
      "start": 968.839,
      "duration": 4.721
    },
    {
      "text": "language processing just assume that you",
      "start": 970.44,
      "duration": 5.399
    },
    {
      "text": "are a student who is Guided by intuition",
      "start": 973.56,
      "duration": 3.36
    },
    {
      "text": "and",
      "start": 975.839,
      "duration": 3.48
    },
    {
      "text": "Mathematics and try to think of this",
      "start": 976.92,
      "duration": 6.8
    },
    {
      "text": "question you have this input query X2",
      "start": 979.319,
      "duration": 6.721
    },
    {
      "text": "and you have these other embedding input",
      "start": 983.72,
      "duration": 5.88
    },
    {
      "text": "embedding vectors how would you find",
      "start": 986.04,
      "duration": 6.719
    },
    {
      "text": "the importance of every input Vector",
      "start": 989.6,
      "duration": 4.88
    },
    {
      "text": "with respect to the",
      "start": 992.759,
      "duration": 4.121
    },
    {
      "text": "query that's the question so you have",
      "start": 994.48,
      "duration": 4.56
    },
    {
      "text": "the query vector and you have each of",
      "start": 996.88,
      "duration": 4.639
    },
    {
      "text": "these embedding vectors X1 X2 dot dot",
      "start": 999.04,
      "duration": 5.719
    },
    {
      "text": "dot right up till the final input Vector",
      "start": 1001.519,
      "duration": 6.24
    },
    {
      "text": "how would you find a Score which",
      "start": 1004.759,
      "duration": 5.2
    },
    {
      "text": "quantifies the importance between the",
      "start": 1007.759,
      "duration": 5.241
    },
    {
      "text": "embedding between the query vector and",
      "start": 1009.959,
      "duration": 5.601
    },
    {
      "text": "the input embedding Vector can you try",
      "start": 1013.0,
      "duration": 4.88
    },
    {
      "text": "to think about this intuitively forget",
      "start": 1015.56,
      "duration": 4.32
    },
    {
      "text": "about everything else forget about ml",
      "start": 1017.88,
      "duration": 4.439
    },
    {
      "text": "forget about attention just try to think",
      "start": 1019.88,
      "duration": 3.72
    },
    {
      "text": "from the",
      "start": 1022.319,
      "duration": 4.161
    },
    {
      "text": "basics what is that is that mathematical",
      "start": 1023.6,
      "duration": 5.04
    },
    {
      "text": "operation which you will",
      "start": 1026.48,
      "duration": 5.8
    },
    {
      "text": "consider to find the importance between",
      "start": 1028.64,
      "duration": 5.88
    },
    {
      "text": "two",
      "start": 1032.28,
      "duration": 2.24
    },
    {
      "text": "vectors let me ask you this question in",
      "start": 1035.0,
      "duration": 4.64
    },
    {
      "text": "another way as a hint take a look at",
      "start": 1037.48,
      "duration": 4.24
    },
    {
      "text": "this Vector embedding you know that",
      "start": 1039.64,
      "duration": 4.559
    },
    {
      "text": "vectors are embedded in like this in a",
      "start": 1041.72,
      "duration": 5.199
    },
    {
      "text": "three-dimensional space now let me ask",
      "start": 1044.199,
      "duration": 5.36
    },
    {
      "text": "you we have the query Vector which is",
      "start": 1046.919,
      "duration": 4.12
    },
    {
      "text": "and we have all these other input",
      "start": 1049.559,
      "duration": 4.561
    },
    {
      "text": "vectors for step your with one and",
      "start": 1051.039,
      "duration": 8.0
    },
    {
      "text": "starts how would you find the importance",
      "start": 1054.12,
      "duration": 7.4
    },
    {
      "text": "of all the other vectors with respect to",
      "start": 1059.039,
      "duration": 5.64
    },
    {
      "text": "the query Vector which is",
      "start": 1061.52,
      "duration": 3.159
    },
    {
      "text": "Journey you can pause the video here for",
      "start": 1066.039,
      "duration": 4.64
    },
    {
      "text": "a while while you think about",
      "start": 1068.72,
      "duration": 5.0
    },
    {
      "text": "it let me give you a hint if I rephrase",
      "start": 1070.679,
      "duration": 4.841
    },
    {
      "text": "this question in another manner I'm sure",
      "start": 1073.72,
      "duration": 5.56
    },
    {
      "text": "many of you will be able to answer",
      "start": 1075.52,
      "duration": 5.72
    },
    {
      "text": "what is that mathematical operation",
      "start": 1079.28,
      "duration": 3.72
    },
    {
      "text": "which gives you the alignment between",
      "start": 1081.24,
      "duration": 3.0
    },
    {
      "text": "the two",
      "start": 1083.0,
      "duration": 3.799
    },
    {
      "text": "vectors which mathematical",
      "start": 1084.24,
      "duration": 5.16
    },
    {
      "text": "operation lets you find whether two",
      "start": 1086.799,
      "duration": 4.401
    },
    {
      "text": "vectors are aligned with each other or",
      "start": 1089.4,
      "duration": 5.159
    },
    {
      "text": "whether they are not aligned with each",
      "start": 1091.2,
      "duration": 3.359
    },
    {
      "text": "other keep that thought in your mind now",
      "start": 1095.76,
      "duration": 6.24
    },
    {
      "text": "let me nudge you more towards the answer",
      "start": 1099.159,
      "duration": 6.281
    },
    {
      "text": "now we know that uh the embedding",
      "start": 1102.0,
      "duration": 5.72
    },
    {
      "text": "vectors encode meaning right so if two",
      "start": 1105.44,
      "duration": 4.28
    },
    {
      "text": "vectors are align to each other which",
      "start": 1107.72,
      "duration": 3.959
    },
    {
      "text": "means that they are parallel to each",
      "start": 1109.72,
      "duration": 3.76
    },
    {
      "text": "other or if their angles are closer to",
      "start": 1111.679,
      "duration": 4.081
    },
    {
      "text": "each other like journey and start it",
      "start": 1113.48,
      "duration": 4.96
    },
    {
      "text": "implies that they have some sort of",
      "start": 1115.76,
      "duration": 5.48
    },
    {
      "text": "similarity in their meaning so it makes",
      "start": 1118.44,
      "duration": 4.52
    },
    {
      "text": "sense that more importance should be",
      "start": 1121.24,
      "duration": 4.84
    },
    {
      "text": "paid to start Vector because it seems to",
      "start": 1122.96,
      "duration": 5.4
    },
    {
      "text": "be more aligned with the journey Vector",
      "start": 1126.08,
      "duration": 3.64
    },
    {
      "text": "whereas if you take the vector",
      "start": 1128.36,
      "duration": 3.96
    },
    {
      "text": "corresponding to one this purple Vector",
      "start": 1129.72,
      "duration": 4.24
    },
    {
      "text": "at the bottom you'll see that it's",
      "start": 1132.32,
      "duration": 3.4
    },
    {
      "text": "almost perpendicular to the journey",
      "start": 1133.96,
      "duration": 3.839
    },
    {
      "text": "Vector right the vector one is like this",
      "start": 1135.72,
      "duration": 3.92
    },
    {
      "text": "the vector journey is like this so it's",
      "start": 1137.799,
      "duration": 4.201
    },
    {
      "text": "almost perpendicular which means that",
      "start": 1139.64,
      "duration": 4.399
    },
    {
      "text": "they do not have that much similarity in",
      "start": 1142.0,
      "duration": 4.919
    },
    {
      "text": "meaning so when we assign importance",
      "start": 1144.039,
      "duration": 5.321
    },
    {
      "text": "probably the vector corresponding to one",
      "start": 1146.919,
      "duration": 5.041
    },
    {
      "text": "should have less importance than the",
      "start": 1149.36,
      "duration": 5.12
    },
    {
      "text": "vector corresponding to starts for the",
      "start": 1151.96,
      "duration": 4.56
    },
    {
      "text": "query Journey because starts is more",
      "start": 1154.48,
      "duration": 3.12
    },
    {
      "text": "aligned to",
      "start": 1156.52,
      "duration": 3.279
    },
    {
      "text": "Journey now can you think which",
      "start": 1157.6,
      "duration": 4.0
    },
    {
      "text": "mathematical operation would quantify",
      "start": 1159.799,
      "duration": 4.0
    },
    {
      "text": "this",
      "start": 1161.6,
      "duration": 4.72
    },
    {
      "text": "alignment let me reveal the answer it's",
      "start": 1163.799,
      "duration": 4.921
    },
    {
      "text": "the dot product between the vectors this",
      "start": 1166.32,
      "duration": 4.839
    },
    {
      "text": "this is an awesome idea because Let Me",
      "start": 1168.72,
      "duration": 4.72
    },
    {
      "text": "Now go to Google and type dot product",
      "start": 1171.159,
      "duration": 4.561
    },
    {
      "text": "formula if you see the dot product",
      "start": 1173.44,
      "duration": 5.76
    },
    {
      "text": "formula it basically the if you take the",
      "start": 1175.72,
      "duration": 5.4
    },
    {
      "text": "dot product of the two vectors it's the",
      "start": 1179.2,
      "duration": 3.959
    },
    {
      "text": "product of the magnitude of the vectors",
      "start": 1181.12,
      "duration": 4.16
    },
    {
      "text": "multiplied by the cosine of the angle",
      "start": 1183.159,
      "duration": 5.041
    },
    {
      "text": "between them so if the two vectors are",
      "start": 1185.28,
      "duration": 4.519
    },
    {
      "text": "aligned with each other that means the",
      "start": 1188.2,
      "duration": 3.839
    },
    {
      "text": "angle between them is zero and if the",
      "start": 1189.799,
      "duration": 4.161
    },
    {
      "text": "angle between them is zero cost of 0",
      "start": 1192.039,
      "duration": 4.0
    },
    {
      "text": "will be one so the dot product will be",
      "start": 1193.96,
      "duration": 4.36
    },
    {
      "text": "maximum whereas if the two vectors are",
      "start": 1196.039,
      "duration": 4.241
    },
    {
      "text": "not all aligned with each other that",
      "start": 1198.32,
      "duration": 3.52
    },
    {
      "text": "means they are perpendicular to each",
      "start": 1200.28,
      "duration": 3.639
    },
    {
      "text": "other the angle between the two vectors",
      "start": 1201.84,
      "duration": 3.56
    },
    {
      "text": "is now equal to",
      "start": 1203.919,
      "duration": 5.12
    },
    {
      "text": "90\u00b0 and COS of 90 is equal to",
      "start": 1205.4,
      "duration": 6.2
    },
    {
      "text": "0 so there is no similarity between",
      "start": 1209.039,
      "duration": 4.481
    },
    {
      "text": "these vectors and the dot product is",
      "start": 1211.6,
      "duration": 4.72
    },
    {
      "text": "zero so higher the dot product more",
      "start": 1213.52,
      "duration": 5.0
    },
    {
      "text": "aligned the vectors are lower the dot",
      "start": 1216.32,
      "duration": 4.8
    },
    {
      "text": "product the vectors are not aligned so",
      "start": 1218.52,
      "duration": 4.56
    },
    {
      "text": "the dot product actually encodes the",
      "start": 1221.12,
      "duration": 4.2
    },
    {
      "text": "information about how aligned or how not",
      "start": 1223.08,
      "duration": 5.0
    },
    {
      "text": "aligned the vectors are and that's",
      "start": 1225.32,
      "duration": 4.16
    },
    {
      "text": "exactly",
      "start": 1228.08,
      "duration": 3.599
    },
    {
      "text": "that's exactly what we need so the dot",
      "start": 1229.48,
      "duration": 4.48
    },
    {
      "text": "product between journey and starts might",
      "start": 1231.679,
      "duration": 4.641
    },
    {
      "text": "be more so it's because they are aligned",
      "start": 1233.96,
      "duration": 5.04
    },
    {
      "text": "so what if I use the dot product to find",
      "start": 1236.32,
      "duration": 5.56
    },
    {
      "text": "the attention scores that's the first",
      "start": 1239.0,
      "duration": 4.64
    },
    {
      "text": "key Insight which I want to deliver from",
      "start": 1241.88,
      "duration": 4.159
    },
    {
      "text": "this lecture what if I use the dot",
      "start": 1243.64,
      "duration": 4.399
    },
    {
      "text": "product to find the attention score",
      "start": 1246.039,
      "duration": 4.561
    },
    {
      "text": "between my query vector and the input",
      "start": 1248.039,
      "duration": 5.0
    },
    {
      "text": "Vector this is a key step in",
      "start": 1250.6,
      "duration": 4.199
    },
    {
      "text": "understanding the attention mechanism",
      "start": 1253.039,
      "duration": 3.481
    },
    {
      "text": "the dot product is that fundamental",
      "start": 1254.799,
      "duration": 3.921
    },
    {
      "text": "operation because it encodes the the",
      "start": 1256.52,
      "duration": 4.72
    },
    {
      "text": "meaning of how aligned or not how not or",
      "start": 1258.72,
      "duration": 4.439
    },
    {
      "text": "how far apart the vectors",
      "start": 1261.24,
      "duration": 4.24
    },
    {
      "text": "are and this is exactly what we are",
      "start": 1263.159,
      "duration": 5.241
    },
    {
      "text": "going to do next so the intermediate",
      "start": 1265.48,
      "duration": 4.72
    },
    {
      "text": "attention scores are essentially",
      "start": 1268.4,
      "duration": 5.04
    },
    {
      "text": "calculated between the query token and",
      "start": 1270.2,
      "duration": 5.359
    },
    {
      "text": "each of the input",
      "start": 1273.44,
      "duration": 4.56
    },
    {
      "text": "token and the way the attention scores",
      "start": 1275.559,
      "duration": 4.561
    },
    {
      "text": "are calculated is that we take a DOT",
      "start": 1278.0,
      "duration": 3.88
    },
    {
      "text": "product between the query token and",
      "start": 1280.12,
      "duration": 4.88
    },
    {
      "text": "every other input token why do we take a",
      "start": 1281.88,
      "duration": 4.96
    },
    {
      "text": "DOT product because the dot product",
      "start": 1285.0,
      "duration": 4.44
    },
    {
      "text": "essentially quantifies how much two",
      "start": 1286.84,
      "duration": 4.0
    },
    {
      "text": "vectors are",
      "start": 1289.44,
      "duration": 3.839
    },
    {
      "text": "aligned if two vectors are aligned their",
      "start": 1290.84,
      "duration": 4.28
    },
    {
      "text": "dot product is higher so more attention",
      "start": 1293.279,
      "duration": 4.0
    },
    {
      "text": "should be paid to this pair of vectors",
      "start": 1295.12,
      "duration": 4.6
    },
    {
      "text": "so their attention score will be",
      "start": 1297.279,
      "duration": 5.361
    },
    {
      "text": "higher so in the context of self",
      "start": 1299.72,
      "duration": 4.6
    },
    {
      "text": "attention",
      "start": 1302.64,
      "duration": 4.48
    },
    {
      "text": "mechanisms dot product determines the",
      "start": 1304.32,
      "duration": 5.12
    },
    {
      "text": "extent to which elements of a sequence",
      "start": 1307.12,
      "duration": 5.32
    },
    {
      "text": "attend to one",
      "start": 1309.44,
      "duration": 3.0
    },
    {
      "text": "another this is just a fancy way of",
      "start": 1312.48,
      "duration": 5.079
    },
    {
      "text": "saying how different elements of the",
      "start": 1315.08,
      "duration": 4.719
    },
    {
      "text": "sequence are more Alik with each",
      "start": 1317.559,
      "duration": 5.761
    },
    {
      "text": "other so higher the dot product higher",
      "start": 1319.799,
      "duration": 6.641
    },
    {
      "text": "the dot product higher is the",
      "start": 1323.32,
      "duration": 6.359
    },
    {
      "text": "similarity and the attention scores",
      "start": 1326.44,
      "duration": 5.28
    },
    {
      "text": "between the two elements this is very",
      "start": 1329.679,
      "duration": 4.401
    },
    {
      "text": "important higher the dot product higher",
      "start": 1331.72,
      "duration": 3.92
    },
    {
      "text": "is the similarity between the two",
      "start": 1334.08,
      "duration": 3.719
    },
    {
      "text": "elements or the two vectors and higher",
      "start": 1335.64,
      "duration": 4.88
    },
    {
      "text": "is the attention score a fancy way of",
      "start": 1337.799,
      "duration": 4.561
    },
    {
      "text": "describing two vectors which are aligned",
      "start": 1340.52,
      "duration": 3.68
    },
    {
      "text": "to each other is saying that these two",
      "start": 1342.36,
      "duration": 5.12
    },
    {
      "text": "tokens attend more to each other whereas",
      "start": 1344.2,
      "duration": 4.88
    },
    {
      "text": "if the two vectors are not not aligned",
      "start": 1347.48,
      "duration": 3.52
    },
    {
      "text": "we can say that these two attend less to",
      "start": 1349.08,
      "duration": 4.599
    },
    {
      "text": "each other for example just visually we",
      "start": 1351.0,
      "duration": 4.32
    },
    {
      "text": "can see that journey and starts are",
      "start": 1353.679,
      "duration": 3.6
    },
    {
      "text": "aligned right so these two vectors",
      "start": 1355.32,
      "duration": 4.32
    },
    {
      "text": "attend more to each other so more",
      "start": 1357.279,
      "duration": 4.4
    },
    {
      "text": "attention should be paid to starts when",
      "start": 1359.64,
      "duration": 5.08
    },
    {
      "text": "the query is Journey and if you look at",
      "start": 1361.679,
      "duration": 4.801
    },
    {
      "text": "the vector one and the vector for",
      "start": 1364.72,
      "duration": 3.48
    },
    {
      "text": "Journey you'll see that they are not",
      "start": 1366.48,
      "duration": 3.72
    },
    {
      "text": "aligned they have a 90\u00b0 angle between",
      "start": 1368.2,
      "duration": 4.479
    },
    {
      "text": "them so they do not attend to one",
      "start": 1370.2,
      "duration": 5.479
    },
    {
      "text": "another this is the key idea behind",
      "start": 1372.679,
      "duration": 5.36
    },
    {
      "text": "calculation of the attention scores and",
      "start": 1375.679,
      "duration": 3.961
    },
    {
      "text": "this is exactly what we are going to",
      "start": 1378.039,
      "duration": 3.961
    },
    {
      "text": "implement in the code right now so to",
      "start": 1379.64,
      "duration": 4.279
    },
    {
      "text": "compute the attention score between the",
      "start": 1382.0,
      "duration": 4.48
    },
    {
      "text": "query vector and the input Vector we",
      "start": 1383.919,
      "duration": 4.081
    },
    {
      "text": "simply have to take the dot product",
      "start": 1386.48,
      "duration": 3.199
    },
    {
      "text": "between these two",
      "start": 1388.0,
      "duration": 4.679
    },
    {
      "text": "vectors so let's say the query is inputs",
      "start": 1389.679,
      "duration": 5.961
    },
    {
      "text": "of one why inputs of one because python",
      "start": 1392.679,
      "duration": 5.521
    },
    {
      "text": "has a zero based indexing system and",
      "start": 1395.64,
      "duration": 4.76
    },
    {
      "text": "since our query is the second input",
      "start": 1398.2,
      "duration": 4.959
    },
    {
      "text": "token uh the second input token remember",
      "start": 1400.4,
      "duration": 5.56
    },
    {
      "text": "is for a journey it will be indexed with",
      "start": 1403.159,
      "duration": 5.481
    },
    {
      "text": "one because python has a zero indexing",
      "start": 1405.96,
      "duration": 5.4
    },
    {
      "text": "system so inputs of one will be the",
      "start": 1408.64,
      "duration": 5.639
    },
    {
      "text": "vector for Journey so let's say the",
      "start": 1411.36,
      "duration": 5.919
    },
    {
      "text": "query is the inputs of one and then the",
      "start": 1414.279,
      "duration": 5.081
    },
    {
      "text": "attention scores need to be calculated",
      "start": 1417.279,
      "duration": 3.681
    },
    {
      "text": "so first we initialize the attention",
      "start": 1419.36,
      "duration": 4.12
    },
    {
      "text": "scores as an empty tensor then what",
      "start": 1420.96,
      "duration": 5.319
    },
    {
      "text": "we'll do is that we Loop over the inputs",
      "start": 1423.48,
      "duration": 5.0
    },
    {
      "text": "we'll Loop over the inputs and we'll",
      "start": 1426.279,
      "duration": 4.961
    },
    {
      "text": "take the dot product between every input",
      "start": 1428.48,
      "duration": 5.84
    },
    {
      "text": "vector and the query Vector that's it",
      "start": 1431.24,
      "duration": 4.679
    },
    {
      "text": "and then we'll populate the attention",
      "start": 1434.32,
      "duration": 4.64
    },
    {
      "text": "scores tensor so the first element of",
      "start": 1435.919,
      "duration": 5.321
    },
    {
      "text": "the attention scores tensor is the dot",
      "start": 1438.96,
      "duration": 3.719
    },
    {
      "text": "product between the first input",
      "start": 1441.24,
      "duration": 4.64
    },
    {
      "text": "embedding vector and the query Vector",
      "start": 1442.679,
      "duration": 5.12
    },
    {
      "text": "the second element of the attention",
      "start": 1445.88,
      "duration": 4.08
    },
    {
      "text": "scores tensor is the dot product between",
      "start": 1447.799,
      "duration": 5.041
    },
    {
      "text": "the second input vector and the",
      "start": 1449.96,
      "duration": 5.319
    },
    {
      "text": "embedding Vector similarly the Sixth",
      "start": 1452.84,
      "duration": 5.12
    },
    {
      "text": "Element in the attention score tensor is",
      "start": 1455.279,
      "duration": 5.361
    },
    {
      "text": "the dot product between the sixth input",
      "start": 1457.96,
      "duration": 6.4
    },
    {
      "text": "vector and the query Vector so each of",
      "start": 1460.64,
      "duration": 5.639
    },
    {
      "text": "these element each of the elements of",
      "start": 1464.36,
      "duration": 3.559
    },
    {
      "text": "the attention score is a DOT product",
      "start": 1466.279,
      "duration": 5.161
    },
    {
      "text": "between the input vector and the query",
      "start": 1467.919,
      "duration": 6.0
    },
    {
      "text": "Vector now let us actually look at these",
      "start": 1471.44,
      "duration": 5.0
    },
    {
      "text": "attention scores a bit and try to uh",
      "start": 1473.919,
      "duration": 6.521
    },
    {
      "text": "look at their magnitudes right so which",
      "start": 1476.44,
      "duration": 5.4
    },
    {
      "text": "which of these have the largest",
      "start": 1480.44,
      "duration": 4.16
    },
    {
      "text": "magnitude so we can see that this second",
      "start": 1481.84,
      "duration": 6.199
    },
    {
      "text": "second value the third value and the",
      "start": 1484.6,
      "duration": 5.76
    },
    {
      "text": "sixth value have the largest attention",
      "start": 1488.039,
      "duration": 4.841
    },
    {
      "text": "scores so second third and six so let's",
      "start": 1490.36,
      "duration": 3.76
    },
    {
      "text": "see the which which words they",
      "start": 1492.88,
      "duration": 3.799
    },
    {
      "text": "correspond to so second is the word",
      "start": 1494.12,
      "duration": 5.0
    },
    {
      "text": "journey Third is the word starts and",
      "start": 1496.679,
      "duration": 5.0
    },
    {
      "text": "sixth is the word step so of course",
      "start": 1499.12,
      "duration": 4.24
    },
    {
      "text": "Journey has the high attention score",
      "start": 1501.679,
      "duration": 3.081
    },
    {
      "text": "right because the query itself is",
      "start": 1503.36,
      "duration": 3.48
    },
    {
      "text": "Journey so if the query itself is",
      "start": 1504.76,
      "duration": 3.68
    },
    {
      "text": "Journey it will be aligned with the",
      "start": 1506.84,
      "duration": 3.52
    },
    {
      "text": "vector for Journey and so it will have",
      "start": 1508.44,
      "duration": 5.16
    },
    {
      "text": "the highest dot product but the second",
      "start": 1510.36,
      "duration": 4.88
    },
    {
      "text": "and the third highest dot products are",
      "start": 1513.6,
      "duration": 4.24
    },
    {
      "text": "for starts and step and let's see",
      "start": 1515.24,
      "duration": 4.36
    },
    {
      "text": "whether that follows our",
      "start": 1517.84,
      "duration": 4.24
    },
    {
      "text": "intuition so as we had earlier seen",
      "start": 1519.6,
      "duration": 4.48
    },
    {
      "text": "starts is also very closely aligned with",
      "start": 1522.08,
      "duration": 4.16
    },
    {
      "text": "journey so it makes sense that the dot",
      "start": 1524.08,
      "duration": 3.92
    },
    {
      "text": "product between journey and starts is",
      "start": 1526.24,
      "duration": 4.0
    },
    {
      "text": "higher so the attention score for starts",
      "start": 1528.0,
      "duration": 5.399
    },
    {
      "text": "is higher similarly when you see step",
      "start": 1530.24,
      "duration": 5.679
    },
    {
      "text": "you'll see that step also seems to be",
      "start": 1533.399,
      "duration": 4.081
    },
    {
      "text": "closely aligned with journey their",
      "start": 1535.919,
      "duration": 4.24
    },
    {
      "text": "angles seem to be similar and so the",
      "start": 1537.48,
      "duration": 4.76
    },
    {
      "text": "attention score between step and journey",
      "start": 1540.159,
      "duration": 3.481
    },
    {
      "text": "will also be",
      "start": 1542.24,
      "duration": 3.679
    },
    {
      "text": "higher now let's look at the elements",
      "start": 1543.64,
      "duration": 3.84
    },
    {
      "text": "with the lowest attention score so it",
      "start": 1545.919,
      "duration": 3.88
    },
    {
      "text": "seems to be the fifth element and the",
      "start": 1547.48,
      "duration": 6.88
    },
    {
      "text": "fifth element is the word one and let's",
      "start": 1549.799,
      "duration": 6.441
    },
    {
      "text": "see whether that makes sense with our",
      "start": 1554.36,
      "duration": 3.72
    },
    {
      "text": "intuition so you can see the vector for",
      "start": 1556.24,
      "duration": 4.36
    },
    {
      "text": "the word one and the vector for Journey",
      "start": 1558.08,
      "duration": 4.68
    },
    {
      "text": "almost have a 90\u00b0 angle between them",
      "start": 1560.6,
      "duration": 3.6
    },
    {
      "text": "they are not at all aligned with each",
      "start": 1562.76,
      "duration": 4.039
    },
    {
      "text": "other and that is exactly captured in",
      "start": 1564.2,
      "duration": 4.52
    },
    {
      "text": "the attention score that seems to be the",
      "start": 1566.799,
      "duration": 5.721
    },
    {
      "text": "least between journey and one so every",
      "start": 1568.72,
      "duration": 6.079
    },
    {
      "text": "time you deal with attention scores try",
      "start": 1572.52,
      "duration": 4.44
    },
    {
      "text": "to have a mental map of why exactly is",
      "start": 1574.799,
      "duration": 4.041
    },
    {
      "text": "the attention score higher or why",
      "start": 1576.96,
      "duration": 4.439
    },
    {
      "text": "exactly is the attention score",
      "start": 1578.84,
      "duration": 5.719
    },
    {
      "text": "lower now we'll move to the third",
      "start": 1581.399,
      "duration": 6.52
    },
    {
      "text": "step or the next step rather before we",
      "start": 1584.559,
      "duration": 6.36
    },
    {
      "text": "comp compute the context vector and that",
      "start": 1587.919,
      "duration": 5.561
    },
    {
      "text": "step is",
      "start": 1590.919,
      "duration": 2.561
    },
    {
      "text": "normalization why do we really need",
      "start": 1593.799,
      "duration": 5.161
    },
    {
      "text": "normalization the most important reason",
      "start": 1596.48,
      "duration": 5.48
    },
    {
      "text": "why we need normalization is because of",
      "start": 1598.96,
      "duration": 5.24
    },
    {
      "text": "interpretability what does that mean",
      "start": 1601.96,
      "duration": 4.28
    },
    {
      "text": "well what it means is that when I look",
      "start": 1604.2,
      "duration": 4.32
    },
    {
      "text": "at the attention scores I want to be",
      "start": 1606.24,
      "duration": 4.64
    },
    {
      "text": "able to make statements like okay give",
      "start": 1608.52,
      "duration": 7.159
    },
    {
      "text": "50% attention to starts give uh 20%",
      "start": 1610.88,
      "duration": 7.36
    },
    {
      "text": "attention to step and give only 5% ATT",
      "start": 1615.679,
      "duration": 4.961
    },
    {
      "text": "attention to the vector one so when the",
      "start": 1618.24,
      "duration": 4.319
    },
    {
      "text": "query is Journey I want to make these",
      "start": 1620.64,
      "duration": 3.56
    },
    {
      "text": "kind of interpretable statements in",
      "start": 1622.559,
      "duration": 4.0
    },
    {
      "text": "terms of let's say percentages that if",
      "start": 1624.2,
      "duration": 4.16
    },
    {
      "text": "the query is Journey of course Journey",
      "start": 1626.559,
      "duration": 5.401
    },
    {
      "text": "will receive 20% attention 30% attention",
      "start": 1628.36,
      "duration": 5.16
    },
    {
      "text": "but starts will also receive higher",
      "start": 1631.96,
      "duration": 4.28
    },
    {
      "text": "attention maybe 30% maybe step receives",
      "start": 1633.52,
      "duration": 4.68
    },
    {
      "text": "20% attention and the rest of the",
      "start": 1636.24,
      "duration": 5.12
    },
    {
      "text": "vectors remain receive less percent if",
      "start": 1638.2,
      "duration": 4.92
    },
    {
      "text": "I'm conveying this to someone they will",
      "start": 1641.36,
      "duration": 4.0
    },
    {
      "text": "get a much better idea right if I convey",
      "start": 1643.12,
      "duration": 4.799
    },
    {
      "text": "in terms of these percentages so I want",
      "start": 1645.36,
      "duration": 5.24
    },
    {
      "text": "my attention scores to be interpretable",
      "start": 1647.919,
      "duration": 5.36
    },
    {
      "text": "and they are not right now because if",
      "start": 1650.6,
      "duration": 4.52
    },
    {
      "text": "you sum up the attention scores they are",
      "start": 1653.279,
      "duration": 4.601
    },
    {
      "text": "more than one so we cannot express these",
      "start": 1655.12,
      "duration": 5.039
    },
    {
      "text": "in terms of",
      "start": 1657.88,
      "duration": 4.399
    },
    {
      "text": "percentages and that's why we are now",
      "start": 1660.159,
      "duration": 5.24
    },
    {
      "text": "going to normalize the attention",
      "start": 1662.279,
      "duration": 7.161
    },
    {
      "text": "scores uh now first okay so the main",
      "start": 1665.399,
      "duration": 6.4
    },
    {
      "text": "goal behind the normalization is to",
      "start": 1669.44,
      "duration": 4.52
    },
    {
      "text": "obtain attention weights that sum up to",
      "start": 1671.799,
      "duration": 4.801
    },
    {
      "text": "one that's the main goal and why do we",
      "start": 1673.96,
      "duration": 6.319
    },
    {
      "text": "do this as I mentioned it's useful for",
      "start": 1676.6,
      "duration": 6.24
    },
    {
      "text": "interpretability um and for making",
      "start": 1680.279,
      "duration": 4.681
    },
    {
      "text": "statements like in terms of percentages",
      "start": 1682.84,
      "duration": 4.559
    },
    {
      "text": "how much attention should be given Etc",
      "start": 1684.96,
      "duration": 4.0
    },
    {
      "text": "but the second reason why we do",
      "start": 1687.399,
      "duration": 3.64
    },
    {
      "text": "normalization is because generally it's",
      "start": 1688.96,
      "duration": 4.079
    },
    {
      "text": "good if things are between zero and one",
      "start": 1691.039,
      "duration": 3.52
    },
    {
      "text": "so if the summation of the attention",
      "start": 1693.039,
      "duration": 5.12
    },
    {
      "text": "weights is between zero and one it helps",
      "start": 1694.559,
      "duration": 5.72
    },
    {
      "text": "in the training we are going to do back",
      "start": 1698.159,
      "duration": 4.36
    },
    {
      "text": "propagation later so we need stability",
      "start": 1700.279,
      "duration": 4.64
    },
    {
      "text": "during the training procedure and that's",
      "start": 1702.519,
      "duration": 4.561
    },
    {
      "text": "why normalization is integrated in many",
      "start": 1704.919,
      "duration": 4.041
    },
    {
      "text": "machine learning framework It generally",
      "start": 1707.08,
      "duration": 3.599
    },
    {
      "text": "helps a lot when you are back",
      "start": 1708.96,
      "duration": 3.48
    },
    {
      "text": "propagating and including gradient",
      "start": 1710.679,
      "duration": 3.84
    },
    {
      "text": "descent for",
      "start": 1712.44,
      "duration": 5.52
    },
    {
      "text": "example so uh what is the simplest way",
      "start": 1714.519,
      "duration": 6.561
    },
    {
      "text": "to implement normalization can you try",
      "start": 1717.96,
      "duration": 6.28
    },
    {
      "text": "to think about it remember we want all",
      "start": 1721.08,
      "duration": 5.719
    },
    {
      "text": "of these weights to sum up to one so",
      "start": 1724.24,
      "duration": 4.88
    },
    {
      "text": "what is the best way you can normalize",
      "start": 1726.799,
      "duration": 4.841
    },
    {
      "text": "this you can pause the video for a while",
      "start": 1729.12,
      "duration": 3.88
    },
    {
      "text": "if you",
      "start": 1731.64,
      "duration": 3.639
    },
    {
      "text": "want okay so the simplest way to",
      "start": 1733.0,
      "duration": 4.399
    },
    {
      "text": "normalize this is to just sum up these",
      "start": 1735.279,
      "duration": 4.601
    },
    {
      "text": "weights and then divide every single",
      "start": 1737.399,
      "duration": 3.961
    },
    {
      "text": "element by the",
      "start": 1739.88,
      "duration": 4.12
    },
    {
      "text": "sum so what that will do is that will",
      "start": 1741.36,
      "duration": 5.4
    },
    {
      "text": "make sure that the summation of all the",
      "start": 1744.0,
      "duration": 4.919
    },
    {
      "text": "attention scores will be equal to one",
      "start": 1746.76,
      "duration": 3.84
    },
    {
      "text": "and this is exactly what we are going to",
      "start": 1748.919,
      "duration": 3.401
    },
    {
      "text": "implement as the simplest way to",
      "start": 1750.6,
      "duration": 5.0
    },
    {
      "text": "normalize so what we'll do is that we'll",
      "start": 1752.32,
      "duration": 4.76
    },
    {
      "text": "maintain another tensor which is",
      "start": 1755.6,
      "duration": 3.439
    },
    {
      "text": "attention weights to that will just be",
      "start": 1757.08,
      "duration": 4.0
    },
    {
      "text": "the attention scores divided by the",
      "start": 1759.039,
      "duration": 4.281
    },
    {
      "text": "summation so what will happen is that",
      "start": 1761.08,
      "duration": 4.12
    },
    {
      "text": "every element of the attention score",
      "start": 1763.32,
      "duration": 5.839
    },
    {
      "text": "tensor will be divided by the toal total",
      "start": 1765.2,
      "duration": 6.719
    },
    {
      "text": "summation and so the final tensor which",
      "start": 1769.159,
      "duration": 4.12
    },
    {
      "text": "we have which are also called as the",
      "start": 1771.919,
      "duration": 3.6
    },
    {
      "text": "attention weights will be this it will",
      "start": 1773.279,
      "duration": 3.081
    },
    {
      "text": "be",
      "start": 1775.519,
      "duration": 5.201
    },
    {
      "text": "0455 2278 2249 Etc and you'll see that",
      "start": 1776.36,
      "duration": 6.96
    },
    {
      "text": "they sum up to one here I would like to",
      "start": 1780.72,
      "duration": 5.0
    },
    {
      "text": "mention one terminology and that is the",
      "start": 1783.32,
      "duration": 4.199
    },
    {
      "text": "difference between attention scores and",
      "start": 1785.72,
      "duration": 4.679
    },
    {
      "text": "attention weights both attention scores",
      "start": 1787.519,
      "duration": 4.841
    },
    {
      "text": "and attention weights represent the same",
      "start": 1790.399,
      "duration": 4.361
    },
    {
      "text": "thing intuitively they mean the same the",
      "start": 1792.36,
      "duration": 3.919
    },
    {
      "text": "only difference is that all the",
      "start": 1794.76,
      "duration": 4.12
    },
    {
      "text": "attention weights sum up to one whereas",
      "start": 1796.279,
      "duration": 4.601
    },
    {
      "text": "that's not the case for attention",
      "start": 1798.88,
      "duration": 4.279
    },
    {
      "text": "scores so if you take the summation of",
      "start": 1800.88,
      "duration": 3.72
    },
    {
      "text": "these attention weights you'll see that",
      "start": 1803.159,
      "duration": 3.52
    },
    {
      "text": "it is equal to",
      "start": 1804.6,
      "duration": 4.919
    },
    {
      "text": "one um okay this is great so you might",
      "start": 1806.679,
      "duration": 4.961
    },
    {
      "text": "think awesome what's the next step well",
      "start": 1809.519,
      "duration": 5.0
    },
    {
      "text": "it's not that great because uh there are",
      "start": 1811.64,
      "duration": 5.399
    },
    {
      "text": "better ways to do normalization many of",
      "start": 1814.519,
      "duration": 4.28
    },
    {
      "text": "you might have heard about soft Max",
      "start": 1817.039,
      "duration": 3.88
    },
    {
      "text": "right if you have not it's fine I'm",
      "start": 1818.799,
      "duration": 4.521
    },
    {
      "text": "going to explain right now but when we",
      "start": 1820.919,
      "duration": 5.201
    },
    {
      "text": "consider normalization especially in the",
      "start": 1823.32,
      "duration": 5.199
    },
    {
      "text": "machine learning context it's actually",
      "start": 1826.12,
      "duration": 4.64
    },
    {
      "text": "more common and advisable to use the",
      "start": 1828.519,
      "duration": 4.841
    },
    {
      "text": "softmax function for",
      "start": 1830.76,
      "duration": 5.799
    },
    {
      "text": "normalization and why is this the case",
      "start": 1833.36,
      "duration": 5.039
    },
    {
      "text": "um I think I need to write this down on",
      "start": 1836.559,
      "duration": 3.281
    },
    {
      "text": "the Whiteboard to",
      "start": 1838.399,
      "duration": 5.841
    },
    {
      "text": "explain um why soft Max is preferred",
      "start": 1839.84,
      "duration": 5.8
    },
    {
      "text": "compared to let's say the normal",
      "start": 1844.24,
      "duration": 3.439
    },
    {
      "text": "summation especially when you consider",
      "start": 1845.64,
      "duration": 5.32
    },
    {
      "text": "extreme values so let me take a simple",
      "start": 1847.679,
      "duration": 5.321
    },
    {
      "text": "example right now and I'm going to",
      "start": 1850.96,
      "duration": 3.719
    },
    {
      "text": "switch the color to Black so that you",
      "start": 1853.0,
      "duration": 4.0
    },
    {
      "text": "can see what I'm writing on the screen",
      "start": 1854.679,
      "duration": 4.041
    },
    {
      "text": "so let's say the element which we have",
      "start": 1857.0,
      "duration": 3.36
    },
    {
      "text": "are",
      "start": 1858.72,
      "duration": 9.679
    },
    {
      "text": "1 2 3 and nine uh or let's say",
      "start": 1860.36,
      "duration": 8.039
    },
    {
      "text": "400 let's say these are the",
      "start": 1868.48,
      "duration": 5.079
    },
    {
      "text": "elements",
      "start": 1871.76,
      "duration": 5.48
    },
    {
      "text": "400 now if you do the normal summation",
      "start": 1873.559,
      "duration": 5.6
    },
    {
      "text": "uh and normalize it that way what will",
      "start": 1877.24,
      "duration": 3.919
    },
    {
      "text": "what will happen is that 1 will be",
      "start": 1879.159,
      "duration": 4.0
    },
    {
      "text": "divided by all the entire summation",
      "start": 1881.159,
      "duration": 3.601
    },
    {
      "text": "right so the first element will be 1",
      "start": 1883.159,
      "duration": 6.76
    },
    {
      "text": "divided by 1 + 2 + 3 + 400 which is",
      "start": 1884.76,
      "duration": 5.159
    },
    {
      "text": "406 uh so the denominator here will be",
      "start": 1890.84,
      "duration": 7.04
    },
    {
      "text": "4",
      "start": 1895.36,
      "duration": 7.319
    },
    {
      "text": "0 6 then the last element similarly",
      "start": 1897.88,
      "duration": 7.039
    },
    {
      "text": "would be the highest element which is",
      "start": 1902.679,
      "duration": 3.48
    },
    {
      "text": "the extreme value which we are",
      "start": 1904.919,
      "duration": 3.041
    },
    {
      "text": "considering here this highest element",
      "start": 1906.159,
      "duration": 4.801
    },
    {
      "text": "will be",
      "start": 1907.96,
      "duration": 3.0
    },
    {
      "text": "400 divided by 406",
      "start": 1912.0,
      "duration": 5.2
    },
    {
      "text": "so I'm just writing the denominator",
      "start": 1919.36,
      "duration": 4.0
    },
    {
      "text": "right now yeah so the last element which",
      "start": 1921.159,
      "duration": 4.601
    },
    {
      "text": "is the extreme value will be 400 divided",
      "start": 1923.36,
      "duration": 5.199
    },
    {
      "text": "46 and this element will be 2 divided by",
      "start": 1925.76,
      "duration": 5.519
    },
    {
      "text": "406 and this element will be 3 divided",
      "start": 1928.559,
      "duration": 5.201
    },
    {
      "text": "46 so you might think that okay what is",
      "start": 1931.279,
      "duration": 4.721
    },
    {
      "text": "the problem here the problem here is",
      "start": 1933.76,
      "duration": 5.44
    },
    {
      "text": "that when we look at the inputs 400 is",
      "start": 1936.0,
      "duration": 5.279
    },
    {
      "text": "extremely high right so the",
      "start": 1939.2,
      "duration": 4.079
    },
    {
      "text": "normalization should convey this",
      "start": 1941.279,
      "duration": 5.12
    },
    {
      "text": "information that you should completely",
      "start": 1943.279,
      "duration": 6.12
    },
    {
      "text": "neglect these other values so ideally",
      "start": 1946.399,
      "duration": 5.12
    },
    {
      "text": "when such a situation occurs we want the",
      "start": 1949.399,
      "duration": 4.52
    },
    {
      "text": "normalized values to be zero for these",
      "start": 1951.519,
      "duration": 4.64
    },
    {
      "text": "smaller values and we want the",
      "start": 1953.919,
      "duration": 4.321
    },
    {
      "text": "normalized value to be one for this",
      "start": 1956.159,
      "duration": 4.801
    },
    {
      "text": "extremely high value and that's not the",
      "start": 1958.24,
      "duration": 4.08
    },
    {
      "text": "case when you use the summation",
      "start": 1960.96,
      "duration": 3.559
    },
    {
      "text": "operation when you do the summation this",
      "start": 1962.32,
      "duration": 5.359
    },
    {
      "text": "normalized will be around let's",
      "start": 1964.519,
      "duration": 5.841
    },
    {
      "text": "say uh I not I'm not calculating this",
      "start": 1967.679,
      "duration": 4.761
    },
    {
      "text": "exactly but let's say it's 0",
      "start": 1970.36,
      "duration": 5.159
    },
    {
      "text": "point uh",
      "start": 1972.44,
      "duration": 7.079
    },
    {
      "text": "0 0",
      "start": 1975.519,
      "duration": 4.0
    },
    {
      "text": "25 and let's say this this calculation",
      "start": 1981.0,
      "duration": 5.799
    },
    {
      "text": "for the extreme value is let's say",
      "start": 1984.279,
      "duration": 5.721
    },
    {
      "text": "around uh",
      "start": 1986.799,
      "duration": 3.201
    },
    {
      "text": "Point uh",
      "start": 1991.36,
      "duration": 3.6
    },
    {
      "text": "9 let me write this again so this will",
      "start": 1995.799,
      "duration": 5.24
    },
    {
      "text": "be let's say around",
      "start": 1999.44,
      "duration": 4.599
    },
    {
      "text": "0.9",
      "start": 2001.039,
      "duration": 3.0
    },
    {
      "text": "9 just as an example so you might think",
      "start": 2004.48,
      "duration": 5.039
    },
    {
      "text": "that okay this is almost close to one",
      "start": 2007.6,
      "duration": 3.88
    },
    {
      "text": "right but it should not be almost close",
      "start": 2009.519,
      "duration": 4.04
    },
    {
      "text": "to one it should almost be exactly equal",
      "start": 2011.48,
      "duration": 5.52
    },
    {
      "text": "to one the reason why this is a problem",
      "start": 2013.559,
      "duration": 5.921
    },
    {
      "text": "is that when you get values like this",
      "start": 2017.0,
      "duration": 5.0
    },
    {
      "text": "they are not exactly equal to zero so",
      "start": 2019.48,
      "duration": 4.039
    },
    {
      "text": "when you are doing back propagation and",
      "start": 2022.0,
      "duration": 3.24
    },
    {
      "text": "gradient descent it confuses the",
      "start": 2023.519,
      "duration": 3.76
    },
    {
      "text": "optimizer and the optimizer still gives",
      "start": 2025.24,
      "duration": 4.12
    },
    {
      "text": "enough or weight some weightage to these",
      "start": 2027.279,
      "duration": 4.161
    },
    {
      "text": "values although we should not give any",
      "start": 2029.36,
      "duration": 4.64
    },
    {
      "text": "weightage to these values so ideally the",
      "start": 2031.44,
      "duration": 5.079
    },
    {
      "text": "normalization scheme should be such that",
      "start": 2034.0,
      "duration": 4.32
    },
    {
      "text": "when we normalize the small values",
      "start": 2036.519,
      "duration": 3.76
    },
    {
      "text": "should be close to zero in such a case",
      "start": 2038.32,
      "duration": 3.599
    },
    {
      "text": "and the extremely large values should be",
      "start": 2040.279,
      "duration": 4.161
    },
    {
      "text": "close to one and that is not achieved",
      "start": 2041.919,
      "duration": 4.681
    },
    {
      "text": "through this summation based",
      "start": 2044.44,
      "duration": 4.439
    },
    {
      "text": "normalization however this exact same",
      "start": 2046.6,
      "duration": 4.36
    },
    {
      "text": "thing is achieved if we do a softmax",
      "start": 2048.879,
      "duration": 4.681
    },
    {
      "text": "based normalization so let me explain",
      "start": 2050.96,
      "duration": 4.8
    },
    {
      "text": "what actually happens in the softmax",
      "start": 2053.56,
      "duration": 3.119
    },
    {
      "text": "based",
      "start": 2055.76,
      "duration": 3.319
    },
    {
      "text": "normalization so we currently have the",
      "start": 2056.679,
      "duration": 4.281
    },
    {
      "text": "attention scores like these right we",
      "start": 2059.079,
      "duration": 6.28
    },
    {
      "text": "have X1 X2 dot dot dot up till X6 in the",
      "start": 2060.96,
      "duration": 7.719
    },
    {
      "text": "softmax what happens is that we take the",
      "start": 2065.359,
      "duration": 5.52
    },
    {
      "text": "exponent of every element and then we",
      "start": 2068.679,
      "duration": 4.841
    },
    {
      "text": "divide by the summation of the exponents",
      "start": 2070.879,
      "duration": 5.161
    },
    {
      "text": "so the denominator the summation is e to",
      "start": 2073.52,
      "duration": 6.48
    },
    {
      "text": "X1 plus e to X2 plus e to X3 plus e to",
      "start": 2076.04,
      "duration": 8.0
    },
    {
      "text": "X4 plus e to X5 plus e to X6 so the",
      "start": 2080.0,
      "duration": 6.879
    },
    {
      "text": "first element will be e to X1 divided by",
      "start": 2084.04,
      "duration": 5.52
    },
    {
      "text": "the summation the second element will be",
      "start": 2086.879,
      "duration": 5.401
    },
    {
      "text": "e to X2 divided by the summation and",
      "start": 2089.56,
      "duration": 4.96
    },
    {
      "text": "similarly the last element will be e to",
      "start": 2092.28,
      "duration": 5.2
    },
    {
      "text": "X6 divided by the summation now if you",
      "start": 2094.52,
      "duration": 4.559
    },
    {
      "text": "add add all of these elements together",
      "start": 2097.48,
      "duration": 3.24
    },
    {
      "text": "you'll see that they definitely sum up",
      "start": 2099.079,
      "duration": 3.961
    },
    {
      "text": "to one that's fine but the more",
      "start": 2100.72,
      "duration": 3.68
    },
    {
      "text": "important thing is when you look at",
      "start": 2103.04,
      "duration": 4.6
    },
    {
      "text": "these extreme cases if you in if you use",
      "start": 2104.4,
      "duration": 6.84
    },
    {
      "text": "soft Max 400 now when you do e to 400",
      "start": 2107.64,
      "duration": 6.64
    },
    {
      "text": "that will almost be like Infinity so",
      "start": 2111.24,
      "duration": 4.879
    },
    {
      "text": "this value when normalized will be very",
      "start": 2114.28,
      "duration": 4.92
    },
    {
      "text": "close to one and the smaller values when",
      "start": 2116.119,
      "duration": 4.881
    },
    {
      "text": "normalized using softmax will be very",
      "start": 2119.2,
      "duration": 4.879
    },
    {
      "text": "close to zero so it's much better to use",
      "start": 2121.0,
      "duration": 5.079
    },
    {
      "text": "softmax when dealing with such extreme",
      "start": 2124.079,
      "duration": 5.161
    },
    {
      "text": "values which may occur when we do large",
      "start": 2126.079,
      "duration": 6.561
    },
    {
      "text": "scale uh models like llms that's why",
      "start": 2129.24,
      "duration": 5.119
    },
    {
      "text": "it's much more preferable to be using",
      "start": 2132.64,
      "duration": 5.12
    },
    {
      "text": "soft Max now we can easily code such",
      "start": 2134.359,
      "duration": 6.601
    },
    {
      "text": "kind of a soft Max in Python ourself but",
      "start": 2137.76,
      "duration": 5.12
    },
    {
      "text": "it's much more recommended to use the",
      "start": 2140.96,
      "duration": 4.639
    },
    {
      "text": "implementation provided by py torch so",
      "start": 2142.88,
      "duration": 4.8
    },
    {
      "text": "what py torch does is that instead of",
      "start": 2145.599,
      "duration": 5.081
    },
    {
      "text": "doing e to x divided by the summation it",
      "start": 2147.68,
      "duration": 6.2
    },
    {
      "text": "actually subtracts the maximum",
      "start": 2150.68,
      "duration": 5.84
    },
    {
      "text": "value from each of the values before",
      "start": 2153.88,
      "duration": 5.0
    },
    {
      "text": "doing the exponential operation so the",
      "start": 2156.52,
      "duration": 5.079
    },
    {
      "text": "first element will be e to X1 minus the",
      "start": 2158.88,
      "duration": 7.6
    },
    {
      "text": "maximum value among X1 X2 X3 X4 X5 X6",
      "start": 2161.599,
      "duration": 6.641
    },
    {
      "text": "divided by the summation and the",
      "start": 2166.48,
      "duration": 4.72
    },
    {
      "text": "summation will be e to X1 - Max Plus e",
      "start": 2168.24,
      "duration": 6.0
    },
    {
      "text": "to X2 - Max Etc so the only difference",
      "start": 2171.2,
      "duration": 4.8
    },
    {
      "text": "between pytorch implementation and the",
      "start": 2174.24,
      "duration": 4.0
    },
    {
      "text": "naive normal implementation we saw",
      "start": 2176.0,
      "duration": 4.44
    },
    {
      "text": "before is that pytorch includes this",
      "start": 2178.24,
      "duration": 4.96
    },
    {
      "text": "minus maximum so it subtracts all the",
      "start": 2180.44,
      "duration": 6.36
    },
    {
      "text": "values by the maximum value uh before",
      "start": 2183.2,
      "duration": 5.48
    },
    {
      "text": "doing the exponent IAL",
      "start": 2186.8,
      "duration": 4.64
    },
    {
      "text": "operation now mathematically you'll see",
      "start": 2188.68,
      "duration": 6.12
    },
    {
      "text": "that when you expand this e to X1 minus",
      "start": 2191.44,
      "duration": 5.48
    },
    {
      "text": "the maximum it can also be written as e",
      "start": 2194.8,
      "duration": 5.4
    },
    {
      "text": "to X1 divided e to maximum so the E to",
      "start": 2196.92,
      "duration": 5.24
    },
    {
      "text": "maximum in the numerator and denominator",
      "start": 2200.2,
      "duration": 4.72
    },
    {
      "text": "actually cancels out and the final thing",
      "start": 2202.16,
      "duration": 5.159
    },
    {
      "text": "what we get is actually the same thing",
      "start": 2204.92,
      "duration": 5.28
    },
    {
      "text": "as what we had written previously so you",
      "start": 2207.319,
      "duration": 4.76
    },
    {
      "text": "might be thinking then why is this even",
      "start": 2210.2,
      "duration": 4.56
    },
    {
      "text": "implemented this subtracting by the",
      "start": 2212.079,
      "duration": 5.04
    },
    {
      "text": "maximum the reason it's implemented is",
      "start": 2214.76,
      "duration": 4.64
    },
    {
      "text": "to avoid void numerical instability when",
      "start": 2217.119,
      "duration": 4.361
    },
    {
      "text": "dealing with very large values or very",
      "start": 2219.4,
      "duration": 4.76
    },
    {
      "text": "small values if you have very large",
      "start": 2221.48,
      "duration": 4.879
    },
    {
      "text": "values as in the input or very small",
      "start": 2224.16,
      "duration": 4.64
    },
    {
      "text": "values It generally leads to numerical",
      "start": 2226.359,
      "duration": 4.521
    },
    {
      "text": "instability and leads to errors which",
      "start": 2228.8,
      "duration": 4.559
    },
    {
      "text": "are called overflow errors this is a",
      "start": 2230.88,
      "duration": 4.479
    },
    {
      "text": "computational problem so theoretically",
      "start": 2233.359,
      "duration": 3.801
    },
    {
      "text": "we can get away with this implementation",
      "start": 2235.359,
      "duration": 3.801
    },
    {
      "text": "which is on the screen right now but",
      "start": 2237.16,
      "duration": 4.679
    },
    {
      "text": "when you implement computation in Python",
      "start": 2239.16,
      "duration": 4.4
    },
    {
      "text": "it's much better to subtract with the",
      "start": 2241.839,
      "duration": 4.121
    },
    {
      "text": "maximum to prevent overflow",
      "start": 2243.56,
      "duration": 4.759
    },
    {
      "text": "problems I have seen some companies in",
      "start": 2245.96,
      "duration": 4.119
    },
    {
      "text": "which this is also asked as an interview",
      "start": 2248.319,
      "duration": 3.8
    },
    {
      "text": "question so it's better to be aware of",
      "start": 2250.079,
      "duration": 4.481
    },
    {
      "text": "how pytorch Implement softmax and we'll",
      "start": 2252.119,
      "duration": 4.161
    },
    {
      "text": "be doing both of these implementations",
      "start": 2254.56,
      "duration": 4.08
    },
    {
      "text": "in the code right now so let me jump to",
      "start": 2256.28,
      "duration": 5.12
    },
    {
      "text": "the coding interface as I said the first",
      "start": 2258.64,
      "duration": 4.04
    },
    {
      "text": "thing which we'll be implementing is",
      "start": 2261.4,
      "duration": 3.52
    },
    {
      "text": "naive soft Max without dealing with the",
      "start": 2262.68,
      "duration": 4.28
    },
    {
      "text": "Overflow issues without subtracting the",
      "start": 2264.92,
      "duration": 3.76
    },
    {
      "text": "maximum",
      "start": 2266.96,
      "duration": 4.639
    },
    {
      "text": "value um so what we'll be doing is",
      "start": 2268.68,
      "duration": 5.439
    },
    {
      "text": "simply taking the exponent of each",
      "start": 2271.599,
      "duration": 4.641
    },
    {
      "text": "attention score and dividing by the",
      "start": 2274.119,
      "duration": 4.401
    },
    {
      "text": "summation of the exponent the dimension",
      "start": 2276.24,
      "duration": 5.24
    },
    {
      "text": "equal to Z is used because we are taking",
      "start": 2278.52,
      "duration": 6.12
    },
    {
      "text": "the summation for a full row so we are",
      "start": 2281.48,
      "duration": 4.96
    },
    {
      "text": "essentially summing all the entries in a",
      "start": 2284.64,
      "duration": 4.0
    },
    {
      "text": "row and that's why we have the dimension",
      "start": 2286.44,
      "duration": 4.639
    },
    {
      "text": "equal to zero why are we summing the",
      "start": 2288.64,
      "duration": 5.12
    },
    {
      "text": "entries in a row because if you look at",
      "start": 2291.079,
      "duration": 7.441
    },
    {
      "text": "these Matrix over here um yeah this X1",
      "start": 2293.76,
      "duration": 8.48
    },
    {
      "text": "X2 X3 X4 X5 and X6 this is one row right",
      "start": 2298.52,
      "duration": 5.96
    },
    {
      "text": "and when we calculate the summation we",
      "start": 2302.24,
      "duration": 5.56
    },
    {
      "text": "are going to do e to X1 plus e to X 2",
      "start": 2304.48,
      "duration": 5.52
    },
    {
      "text": "Etc so we are essentially summing all",
      "start": 2307.8,
      "duration": 4.24
    },
    {
      "text": "the entries in a row and that's why we",
      "start": 2310.0,
      "duration": 4.28
    },
    {
      "text": "have to give Dimension equal to zero as",
      "start": 2312.04,
      "duration": 4.16
    },
    {
      "text": "an exercise you can try out Dimension",
      "start": 2314.28,
      "duration": 4.64
    },
    {
      "text": "equal to one and see the error which you",
      "start": 2316.2,
      "duration": 5.159
    },
    {
      "text": "get so this is how we'll implement the",
      "start": 2318.92,
      "duration": 5.199
    },
    {
      "text": "knive soft Max and uh which will",
      "start": 2321.359,
      "duration": 4.401
    },
    {
      "text": "basically take the exponent of each",
      "start": 2324.119,
      "duration": 3.2
    },
    {
      "text": "attention score and divide by the",
      "start": 2325.76,
      "duration": 3.52
    },
    {
      "text": "summation so we'll print out the",
      "start": 2327.319,
      "duration": 3.641
    },
    {
      "text": "attention weights obtained using this",
      "start": 2329.28,
      "duration": 4.799
    },
    {
      "text": "method and you'll see that it's 1385 dot",
      "start": 2330.96,
      "duration": 6.8
    },
    {
      "text": "dot do1 1581 of course these scores are",
      "start": 2334.079,
      "duration": 5.481
    },
    {
      "text": "different than the summation because",
      "start": 2337.76,
      "duration": 4.12
    },
    {
      "text": "here we are using the exponent operation",
      "start": 2339.56,
      "duration": 3.84
    },
    {
      "text": "but if you sum up these scores you'll",
      "start": 2341.88,
      "duration": 3.4
    },
    {
      "text": "see that they also sum to one that's",
      "start": 2343.4,
      "duration": 4.199
    },
    {
      "text": "awesome and now what we'll be doing is",
      "start": 2345.28,
      "duration": 4.319
    },
    {
      "text": "we'll be using the P torch",
      "start": 2347.599,
      "duration": 4.041
    },
    {
      "text": "implementation of soft",
      "start": 2349.599,
      "duration": 4.641
    },
    {
      "text": "Max one more key point to mention is",
      "start": 2351.64,
      "duration": 5.24
    },
    {
      "text": "that the soft Max attention weights",
      "start": 2354.24,
      "duration": 5.0
    },
    {
      "text": "which are obtained are always positive",
      "start": 2356.88,
      "duration": 4.52
    },
    {
      "text": "because we always deal with the exponent",
      "start": 2359.24,
      "duration": 4.92
    },
    {
      "text": "operation and that's positive the",
      "start": 2361.4,
      "duration": 4.64
    },
    {
      "text": "positivity is important to us because it",
      "start": 2364.16,
      "duration": 4.199
    },
    {
      "text": "makes the output interpret so if we want",
      "start": 2366.04,
      "duration": 6.0
    },
    {
      "text": "to say that give 50% attention to this",
      "start": 2368.359,
      "duration": 6.681
    },
    {
      "text": "token give 20% attention to this token",
      "start": 2372.04,
      "duration": 5.72
    },
    {
      "text": "we need all the outputs to be positive",
      "start": 2375.04,
      "duration": 4.4
    },
    {
      "text": "right if it's not",
      "start": 2377.76,
      "duration": 4.88
    },
    {
      "text": "positive well uh it's it becomes very",
      "start": 2379.44,
      "duration": 5.919
    },
    {
      "text": "difficult to interpret in that",
      "start": 2382.64,
      "duration": 6.4
    },
    {
      "text": "case uh now um so note that knife",
      "start": 2385.359,
      "duration": 5.76
    },
    {
      "text": "softmax implementation May encounter",
      "start": 2389.04,
      "duration": 4.0
    },
    {
      "text": "numerical instability problems which I",
      "start": 2391.119,
      "duration": 4.2
    },
    {
      "text": "mentioned to you such as overflow for",
      "start": 2393.04,
      "duration": 4.559
    },
    {
      "text": "very large values and underflow for very",
      "start": 2395.319,
      "duration": 5.401
    },
    {
      "text": "small values therefore in practice it's",
      "start": 2397.599,
      "duration": 5.321
    },
    {
      "text": "always advisable to use the pytorch",
      "start": 2400.72,
      "duration": 3.879
    },
    {
      "text": "implementation of soft",
      "start": 2402.92,
      "duration": 4.399
    },
    {
      "text": "Max so it just one line of command",
      "start": 2404.599,
      "duration": 5.0
    },
    {
      "text": "torch. softmax and we pass in the",
      "start": 2407.319,
      "duration": 5.481
    },
    {
      "text": "attention score tensor and what this one",
      "start": 2409.599,
      "duration": 5.161
    },
    {
      "text": "line of command torge do softmax",
      "start": 2412.8,
      "duration": 3.92
    },
    {
      "text": "actually does is that it",
      "start": 2414.76,
      "duration": 5.12
    },
    {
      "text": "implements um what I'm what I've shown",
      "start": 2416.72,
      "duration": 4.84
    },
    {
      "text": "on the screen right now it implements",
      "start": 2419.88,
      "duration": 5.56
    },
    {
      "text": "this e to X1 minus maximum Etc this it",
      "start": 2421.56,
      "duration": 7.799
    },
    {
      "text": "converts the input into this normalized",
      "start": 2425.44,
      "duration": 6.84
    },
    {
      "text": "format so this is tor. softmax and you",
      "start": 2429.359,
      "duration": 5.201
    },
    {
      "text": "can quickly go to the documentation to",
      "start": 2432.28,
      "duration": 6.44
    },
    {
      "text": "see tor. softmax and you'll see the py",
      "start": 2434.56,
      "duration": 6.44
    },
    {
      "text": "torge documentation for for softmax I'll",
      "start": 2438.72,
      "duration": 6.2
    },
    {
      "text": "be sharing this link also with you uh in",
      "start": 2441.0,
      "duration": 8.079
    },
    {
      "text": "the information section of the YouTube",
      "start": 2444.92,
      "duration": 8.439
    },
    {
      "text": "video okay so this is basically pytorch",
      "start": 2449.079,
      "duration": 6.161
    },
    {
      "text": "softmax and we have got these attention",
      "start": 2453.359,
      "duration": 3.441
    },
    {
      "text": "weights and we'll see that the attention",
      "start": 2455.24,
      "duration": 3.76
    },
    {
      "text": "weight up to one one thing I want to",
      "start": 2456.8,
      "duration": 4.92
    },
    {
      "text": "show is that our knife soft Max results",
      "start": 2459.0,
      "duration": 5.2
    },
    {
      "text": "in this attention weight tensor and the",
      "start": 2461.72,
      "duration": 4.76
    },
    {
      "text": "pytorch softmax also results in the",
      "start": 2464.2,
      "duration": 5.159
    },
    {
      "text": "exact same attention weight tensor since",
      "start": 2466.48,
      "duration": 4.639
    },
    {
      "text": "we don't have any large values or any",
      "start": 2469.359,
      "duration": 4.0
    },
    {
      "text": "small values we don't have any overflow",
      "start": 2471.119,
      "duration": 4.761
    },
    {
      "text": "or underflow issues here and so both our",
      "start": 2473.359,
      "duration": 4.601
    },
    {
      "text": "knife implementation and the pytorch",
      "start": 2475.88,
      "duration": 4.439
    },
    {
      "text": "implementation give the same results but",
      "start": 2477.96,
      "duration": 5.159
    },
    {
      "text": "the reason it always advisable to use py",
      "start": 2480.319,
      "duration": 4.921
    },
    {
      "text": "torch is that later we'll be dealing",
      "start": 2483.119,
      "duration": 5.2
    },
    {
      "text": "with very large parameters and some of",
      "start": 2485.24,
      "duration": 6.28
    },
    {
      "text": "those might be huge so it's better to um",
      "start": 2488.319,
      "duration": 5.561
    },
    {
      "text": "have numerical or better to deal with",
      "start": 2491.52,
      "duration": 3.44
    },
    {
      "text": "numerical",
      "start": 2493.88,
      "duration": 4.56
    },
    {
      "text": "instability awesome so remember the one",
      "start": 2494.96,
      "duration": 5.359
    },
    {
      "text": "reason why we calculated the attention",
      "start": 2498.44,
      "duration": 3.919
    },
    {
      "text": "weights is for interpretability right",
      "start": 2500.319,
      "duration": 4.76
    },
    {
      "text": "now let's try to interpret so the",
      "start": 2502.359,
      "duration": 5.76
    },
    {
      "text": "attention weight for the first Vector is",
      "start": 2505.079,
      "duration": 5.681
    },
    {
      "text": "around .13 the attention weight for the",
      "start": 2508.119,
      "duration": 6.0
    },
    {
      "text": "second is 2379 attention for the third",
      "start": 2510.76,
      "duration": 4.359
    },
    {
      "text": "is",
      "start": 2514.119,
      "duration": 5.121
    },
    {
      "text": "233 Etc this means that we should pay",
      "start": 2515.119,
      "duration": 7.361
    },
    {
      "text": "about 13% attention to the word your",
      "start": 2519.24,
      "duration": 6.0
    },
    {
      "text": "about 23% attention to",
      "start": 2522.48,
      "duration": 6.4
    },
    {
      "text": "Journey about 23% attention to starts",
      "start": 2525.24,
      "duration": 9.079
    },
    {
      "text": "12% attention to with uh with one 10%",
      "start": 2528.88,
      "duration": 8.239
    },
    {
      "text": "attention to one and 15% attention to",
      "start": 2534.319,
      "duration": 5.681
    },
    {
      "text": "step so high attention is being paid to",
      "start": 2537.119,
      "duration": 4.121
    },
    {
      "text": "journey and",
      "start": 2540.0,
      "duration": 3.96
    },
    {
      "text": "starts so as you can see here High",
      "start": 2541.24,
      "duration": 4.359
    },
    {
      "text": "attention is paid to journey and starts",
      "start": 2543.96,
      "duration": 3.359
    },
    {
      "text": "and if someone asks how high we can can",
      "start": 2545.599,
      "duration": 3.081
    },
    {
      "text": "say that well about",
      "start": 2547.319,
      "duration": 4.401
    },
    {
      "text": "20% and low attention is paid to one and",
      "start": 2548.68,
      "duration": 4.919
    },
    {
      "text": "if someone asks how low we can say that",
      "start": 2551.72,
      "duration": 4.68
    },
    {
      "text": "well only 10% we are able to make this",
      "start": 2553.599,
      "duration": 5.081
    },
    {
      "text": "interpretable statements only because we",
      "start": 2556.4,
      "duration": 4.159
    },
    {
      "text": "converted the attention scores into",
      "start": 2558.68,
      "duration": 4.52
    },
    {
      "text": "attention weights and remember that's",
      "start": 2560.559,
      "duration": 4.401
    },
    {
      "text": "the difference between attention scores",
      "start": 2563.2,
      "duration": 4.28
    },
    {
      "text": "and attention weights attention weights",
      "start": 2564.96,
      "duration": 4.48
    },
    {
      "text": "sum up to one so it's much easier to",
      "start": 2567.48,
      "duration": 5.68
    },
    {
      "text": "make this kind of interpretable",
      "start": 2569.44,
      "duration": 3.72
    },
    {
      "text": "statements so we have computed the",
      "start": 2573.48,
      "duration": 5.28
    },
    {
      "text": "attention weights right now and we are",
      "start": 2575.92,
      "duration": 6.76
    },
    {
      "text": "actually ready to move to the next",
      "start": 2578.76,
      "duration": 6.12
    },
    {
      "text": "step which will which is actually the",
      "start": 2582.68,
      "duration": 5.28
    },
    {
      "text": "final step of uh Computing the context",
      "start": 2584.88,
      "duration": 5.88
    },
    {
      "text": "Vector so let me take you to that",
      "start": 2587.96,
      "duration": 5.84
    },
    {
      "text": "portion of the Whiteboard right now so",
      "start": 2590.76,
      "duration": 6.12
    },
    {
      "text": "after Computing the attention weights",
      "start": 2593.8,
      "duration": 5.08
    },
    {
      "text": "now we have actually come to the last",
      "start": 2596.88,
      "duration": 5.679
    },
    {
      "text": "step of finding the context Vector so",
      "start": 2598.88,
      "duration": 5.76
    },
    {
      "text": "let me just show you the image of what",
      "start": 2602.559,
      "duration": 4.121
    },
    {
      "text": "all we have covered up till now so we",
      "start": 2604.64,
      "duration": 4.76
    },
    {
      "text": "had this input query we computed the",
      "start": 2606.68,
      "duration": 5.439
    },
    {
      "text": "attention scores by taking the dot",
      "start": 2609.4,
      "duration": 5.0
    },
    {
      "text": "product between the input query and each",
      "start": 2612.119,
      "duration": 4.561
    },
    {
      "text": "of the embedding vectors and from the",
      "start": 2614.4,
      "duration": 4.08
    },
    {
      "text": "attention scores we actually got the",
      "start": 2616.68,
      "duration": 4.04
    },
    {
      "text": "normalized attention scores which are",
      "start": 2618.48,
      "duration": 4.32
    },
    {
      "text": "called as attention weights and these",
      "start": 2620.72,
      "duration": 4.72
    },
    {
      "text": "attention weights actually sum up to",
      "start": 2622.8,
      "duration": 5.319
    },
    {
      "text": "one awesome so we have reached this",
      "start": 2625.44,
      "duration": 4.919
    },
    {
      "text": "stage right now and now we are ready to",
      "start": 2628.119,
      "duration": 5.401
    },
    {
      "text": "get the context Vector I just want to",
      "start": 2630.359,
      "duration": 5.361
    },
    {
      "text": "intuitively and Visually show you how we",
      "start": 2633.52,
      "duration": 4.2
    },
    {
      "text": "calculate the context vector before",
      "start": 2635.72,
      "duration": 3.599
    },
    {
      "text": "coming to the mathematical",
      "start": 2637.72,
      "duration": 3.599
    },
    {
      "text": "implementation so let's say these are",
      "start": 2639.319,
      "duration": 4.641
    },
    {
      "text": "the embedding vectors for the different",
      "start": 2641.319,
      "duration": 5.321
    },
    {
      "text": "words in the sentence and here I've also",
      "start": 2643.96,
      "duration": 4.359
    },
    {
      "text": "mentioned the relative importance of",
      "start": 2646.64,
      "duration": 4.84
    },
    {
      "text": "each so Journey carries",
      "start": 2648.319,
      "duration": 6.0
    },
    {
      "text": "25% importance because that's the query",
      "start": 2651.48,
      "duration": 4.2
    },
    {
      "text": "so it should carry the highest",
      "start": 2654.319,
      "duration": 4.76
    },
    {
      "text": "importance but starts carries 20%",
      "start": 2655.68,
      "duration": 4.919
    },
    {
      "text": "importance",
      "start": 2659.079,
      "duration": 6.081
    },
    {
      "text": "um then step carries 15% importance and",
      "start": 2660.599,
      "duration": 7.641
    },
    {
      "text": "one with and your carry less importance",
      "start": 2665.16,
      "duration": 5.52
    },
    {
      "text": "now how do we get these how do we use",
      "start": 2668.24,
      "duration": 5.839
    },
    {
      "text": "these attention weights to compute the",
      "start": 2670.68,
      "duration": 6.12
    },
    {
      "text": "ultimate context Vector so the way this",
      "start": 2674.079,
      "duration": 6.881
    },
    {
      "text": "is done is that let's say we use starts",
      "start": 2676.8,
      "duration": 7.72
    },
    {
      "text": "so the starts Vector is now multiplied",
      "start": 2680.96,
      "duration": 5.68
    },
    {
      "text": "by the attention contribution and that",
      "start": 2684.52,
      "duration": 5.0
    },
    {
      "text": "is equal to 02 right because 20% is",
      "start": 2686.64,
      "duration": 6.08
    },
    {
      "text": "important so it's importance is 20% so",
      "start": 2689.52,
      "duration": 6.16
    },
    {
      "text": "the starts Vector is multiplied by 02",
      "start": 2692.72,
      "duration": 5.44
    },
    {
      "text": "which means it scaled by by the",
      "start": 2695.68,
      "duration": 5.399
    },
    {
      "text": "corresponding attention attention weight",
      "start": 2698.16,
      "duration": 5.399
    },
    {
      "text": "so the starts Vector is multiplied by 02",
      "start": 2701.079,
      "duration": 4.561
    },
    {
      "text": "so it will be scaled down like this the",
      "start": 2703.559,
      "duration": 4.081
    },
    {
      "text": "width vect width Vector is scaled down",
      "start": 2705.64,
      "duration": 6.0
    },
    {
      "text": "by 15 because it carries 15% importance",
      "start": 2707.64,
      "duration": 6.719
    },
    {
      "text": "the step Vector carries 15% importance",
      "start": 2711.64,
      "duration": 4.919
    },
    {
      "text": "the journey Vector carries 25%",
      "start": 2714.359,
      "duration": 4.0
    },
    {
      "text": "importance so it will it will be scaled",
      "start": 2716.559,
      "duration": 3.481
    },
    {
      "text": "down by",
      "start": 2718.359,
      "duration": 4.521
    },
    {
      "text": "1/4 uh and the one vector carries only",
      "start": 2720.04,
      "duration": 5.039
    },
    {
      "text": "10% importance so it will be scaled down",
      "start": 2722.88,
      "duration": 5.92
    },
    {
      "text": "by a lot which is about 1110th now what",
      "start": 2725.079,
      "duration": 5.961
    },
    {
      "text": "will and similarly your carries 15%",
      "start": 2728.8,
      "duration": 4.319
    },
    {
      "text": "importance so it will be scaled down by",
      "start": 2731.04,
      "duration": 3.44
    },
    {
      "text": "multiplied",
      "start": 2733.119,
      "duration": 5.921
    },
    {
      "text": "by5 so what we do is that we multiply",
      "start": 2734.48,
      "duration": 6.68
    },
    {
      "text": "each of the input embedding vector by",
      "start": 2739.04,
      "duration": 3.92
    },
    {
      "text": "the corresponding attention weights and",
      "start": 2741.16,
      "duration": 4.36
    },
    {
      "text": "we scale them down that much so now we",
      "start": 2742.96,
      "duration": 5.32
    },
    {
      "text": "have uh the multiplied weights for each",
      "start": 2745.52,
      "duration": 4.2
    },
    {
      "text": "of these and we take the vector",
      "start": 2748.28,
      "duration": 3.72
    },
    {
      "text": "summation of all of these and you add",
      "start": 2749.72,
      "duration": 4.0
    },
    {
      "text": "the vector summation and that gives the",
      "start": 2752.0,
      "duration": 4.16
    },
    {
      "text": "final context Vector like this so this",
      "start": 2753.72,
      "duration": 6.04
    },
    {
      "text": "is now my cont context Vector for",
      "start": 2756.16,
      "duration": 3.6
    },
    {
      "text": "Journey so this is the context",
      "start": 2760.88,
      "duration": 4.199
    },
    {
      "text": "Vector let me write it down",
      "start": 2767.44,
      "duration": 6.679
    },
    {
      "text": "again okay so let me explain this again",
      "start": 2770.52,
      "duration": 5.72
    },
    {
      "text": "so we have calculate we have multiplied",
      "start": 2774.119,
      "duration": 4.96
    },
    {
      "text": "each of the input embedding Vector with",
      "start": 2776.24,
      "duration": 5.04
    },
    {
      "text": "the corresponding attention weight and",
      "start": 2779.079,
      "duration": 4.721
    },
    {
      "text": "we have got these six vectors right we",
      "start": 2781.28,
      "duration": 6.12
    },
    {
      "text": "sum up all of these six vectors and that",
      "start": 2783.8,
      "duration": 6.72
    },
    {
      "text": "uh now describes the context vector and",
      "start": 2787.4,
      "duration": 6.04
    },
    {
      "text": "this I'm just writing here context this",
      "start": 2790.52,
      "duration": 6.319
    },
    {
      "text": "is the context Vector for the embedding",
      "start": 2793.44,
      "duration": 4.399
    },
    {
      "text": "Vector of",
      "start": 2796.839,
      "duration": 3.321
    },
    {
      "text": "Journey now look at how this context",
      "start": 2797.839,
      "duration": 4.961
    },
    {
      "text": "Vector is calculated the context Vector",
      "start": 2800.16,
      "duration": 4.64
    },
    {
      "text": "has some contributions from all other",
      "start": 2802.8,
      "duration": 4.4
    },
    {
      "text": "vectors so it's an enriched embedding",
      "start": 2804.8,
      "duration": 5.759
    },
    {
      "text": "Vector it has 25% contribution from the",
      "start": 2807.2,
      "duration": 5.52
    },
    {
      "text": "embedding Vector but it has all the",
      "start": 2810.559,
      "duration": 4.361
    },
    {
      "text": "other contribution from other vectors",
      "start": 2812.72,
      "duration": 3.879
    },
    {
      "text": "and those contributions symbolize",
      "start": 2814.92,
      "duration": 4.199
    },
    {
      "text": "something those contributions symbolize",
      "start": 2816.599,
      "duration": 4.081
    },
    {
      "text": "how much importance is given to the",
      "start": 2819.119,
      "duration": 4.561
    },
    {
      "text": "other vectors for example we have about",
      "start": 2820.68,
      "duration": 5.84
    },
    {
      "text": "20% contribution from starts because the",
      "start": 2823.68,
      "duration": 5.72
    },
    {
      "text": "attention weight to starts is 0.2 we",
      "start": 2826.52,
      "duration": 5.36
    },
    {
      "text": "have only 10% contribution from one",
      "start": 2829.4,
      "duration": 5.52
    },
    {
      "text": "because the attention weight for one is",
      "start": 2831.88,
      "duration": 5.679
    },
    {
      "text": "0.1 and that's why context vectors are",
      "start": 2834.92,
      "duration": 5.04
    },
    {
      "text": "so important I wanted to show this to",
      "start": 2837.559,
      "duration": 4.401
    },
    {
      "text": "you visually because the context Vector",
      "start": 2839.96,
      "duration": 4.68
    },
    {
      "text": "which will calculate for modern llms for",
      "start": 2841.96,
      "duration": 5.52
    },
    {
      "text": "large scale models like GPT they carry",
      "start": 2844.64,
      "duration": 5.0
    },
    {
      "text": "the exact same meaning they are enriched",
      "start": 2847.48,
      "duration": 4.76
    },
    {
      "text": "embedding vectors so the context Vector",
      "start": 2849.64,
      "duration": 5.16
    },
    {
      "text": "for Journey looks like this and we can",
      "start": 2852.24,
      "duration": 4.72
    },
    {
      "text": "also see this in code so I have just",
      "start": 2854.8,
      "duration": 5.039
    },
    {
      "text": "written a small code to find the to plot",
      "start": 2856.96,
      "duration": 4.399
    },
    {
      "text": "the context Vector I'll show the",
      "start": 2859.839,
      "duration": 3.48
    },
    {
      "text": "mathematical derivation but for now just",
      "start": 2861.359,
      "duration": 4.121
    },
    {
      "text": "take a look at this context Vector for",
      "start": 2863.319,
      "duration": 5.401
    },
    {
      "text": "Journey which has been shown in Red so",
      "start": 2865.48,
      "duration": 4.8
    },
    {
      "text": "the vector embedding for Journey has",
      "start": 2868.72,
      "duration": 4.28
    },
    {
      "text": "been shown in green but the context",
      "start": 2870.28,
      "duration": 4.16
    },
    {
      "text": "Vector is shown in red which is the",
      "start": 2873.0,
      "duration": 3.16
    },
    {
      "text": "summation of all the other vectors which",
      "start": 2874.44,
      "duration": 4.08
    },
    {
      "text": "I just showed to you and this is how it",
      "start": 2876.16,
      "duration": 4.56
    },
    {
      "text": "looks like in the three-dimensional",
      "start": 2878.52,
      "duration": 4.52
    },
    {
      "text": "space ultimately we are interested in",
      "start": 2880.72,
      "duration": 4.639
    },
    {
      "text": "these context vectors and it was only",
      "start": 2883.04,
      "duration": 4.44
    },
    {
      "text": "possible to get this context Vector",
      "start": 2885.359,
      "duration": 4.24
    },
    {
      "text": "because of the attention mechanism",
      "start": 2887.48,
      "duration": 3.839
    },
    {
      "text": "that's why the attention mechanism is so",
      "start": 2889.599,
      "duration": 3.601
    },
    {
      "text": "important we would have been stuck at",
      "start": 2891.319,
      "duration": 3.881
    },
    {
      "text": "the embedding Vector if we did not have",
      "start": 2893.2,
      "duration": 3.84
    },
    {
      "text": "the attention",
      "start": 2895.2,
      "duration": 3.919
    },
    {
      "text": "mechanism now let us look at the",
      "start": 2897.04,
      "duration": 3.48
    },
    {
      "text": "mathematical",
      "start": 2899.119,
      "duration": 3.801
    },
    {
      "text": "representation um regarding how we",
      "start": 2900.52,
      "duration": 6.039
    },
    {
      "text": "actually compute the context Vector from",
      "start": 2902.92,
      "duration": 6.159
    },
    {
      "text": "the attention weights and if you have",
      "start": 2906.559,
      "duration": 4.881
    },
    {
      "text": "understood the Whiteboard description",
      "start": 2909.079,
      "duration": 3.961
    },
    {
      "text": "which I just showed you understanding",
      "start": 2911.44,
      "duration": 3.32
    },
    {
      "text": "the this mathematical operation will",
      "start": 2913.04,
      "duration": 4.2
    },
    {
      "text": "actually be very",
      "start": 2914.76,
      "duration": 5.28
    },
    {
      "text": "easy okay so we have reached this stage",
      "start": 2917.24,
      "duration": 4.24
    },
    {
      "text": "now where we have computed these",
      "start": 2920.04,
      "duration": 3.88
    },
    {
      "text": "attention weights and after Computing",
      "start": 2921.48,
      "duration": 4.879
    },
    {
      "text": "the normalized attention weights what",
      "start": 2923.92,
      "duration": 4.679
    },
    {
      "text": "we'll do is that we'll compute the",
      "start": 2926.359,
      "duration": 4.24
    },
    {
      "text": "context vector and currently we're",
      "start": 2928.599,
      "duration": 3.361
    },
    {
      "text": "looking at the context Vector for",
      "start": 2930.599,
      "duration": 4.361
    },
    {
      "text": "Journey so it's Z2 and to do that we'll",
      "start": 2931.96,
      "duration": 5.119
    },
    {
      "text": "multiply all the embedded in input",
      "start": 2934.96,
      "duration": 4.28
    },
    {
      "text": "tokens with the corresponding attention",
      "start": 2937.079,
      "duration": 5.161
    },
    {
      "text": "weights this is very important and so",
      "start": 2939.24,
      "duration": 4.64
    },
    {
      "text": "that was the scaling which I showed you",
      "start": 2942.24,
      "duration": 3.64
    },
    {
      "text": "in the figure and then we will sum up",
      "start": 2943.88,
      "duration": 3.56
    },
    {
      "text": "all of the resultant",
      "start": 2945.88,
      "duration": 3.76
    },
    {
      "text": "vectors when we sum up all of the",
      "start": 2947.44,
      "duration": 4.2
    },
    {
      "text": "resultant vectors that will give us the",
      "start": 2949.64,
      "duration": 5.28
    },
    {
      "text": "final context Vector for the token",
      "start": 2951.64,
      "duration": 5.8
    },
    {
      "text": "journey and this has been showed in this",
      "start": 2954.92,
      "duration": 5.72
    },
    {
      "text": "schematic right now let me just rub rub",
      "start": 2957.44,
      "duration": 5.119
    },
    {
      "text": "this here so that I can explain this to",
      "start": 2960.64,
      "duration": 3.64
    },
    {
      "text": "you in a better",
      "start": 2962.559,
      "duration": 4.361
    },
    {
      "text": "manner okay so we have the attention",
      "start": 2964.28,
      "duration": 5.88
    },
    {
      "text": "weights for every token right so we",
      "start": 2966.92,
      "duration": 7.0
    },
    {
      "text": "have1 2 Etc so what we'll do is that for",
      "start": 2970.16,
      "duration": 6.12
    },
    {
      "text": "the first input embedding we'll multiply",
      "start": 2973.92,
      "duration": 4.08
    },
    {
      "text": "the attention weight with this",
      "start": 2976.28,
      "duration": 4.519
    },
    {
      "text": "Vector for the second input embedding",
      "start": 2978.0,
      "duration": 4.599
    },
    {
      "text": "we'll multiply the attention weight for",
      "start": 2980.799,
      "duration": 3.8
    },
    {
      "text": "the second input embedding with the",
      "start": 2982.599,
      "duration": 4.601
    },
    {
      "text": "second Vector we'll multiply the third",
      "start": 2984.599,
      "duration": 5.121
    },
    {
      "text": "attention weight with the third input",
      "start": 2987.2,
      "duration": 5.119
    },
    {
      "text": "embedding and similarly we'll multiply",
      "start": 2989.72,
      "duration": 4.079
    },
    {
      "text": "the sixth attention weight with the",
      "start": 2992.319,
      "duration": 3.28
    },
    {
      "text": "sixth input embedding this is scaling",
      "start": 2993.799,
      "duration": 3.56
    },
    {
      "text": "down the vectors in the victorial",
      "start": 2995.599,
      "duration": 4.121
    },
    {
      "text": "representation and then we'll add all of",
      "start": 2997.359,
      "duration": 5.0
    },
    {
      "text": "them together when we add all of them",
      "start": 2999.72,
      "duration": 4.079
    },
    {
      "text": "together we'll ultimately get the",
      "start": 3002.359,
      "duration": 4.24
    },
    {
      "text": "context Vector for Journey and this is",
      "start": 3003.799,
      "duration": 4.961
    },
    {
      "text": "the final answer this is the context",
      "start": 3006.599,
      "duration": 4.441
    },
    {
      "text": "Vector for Journey and I've have plotted",
      "start": 3008.76,
      "duration": 4.52
    },
    {
      "text": "this context Vector over here which has",
      "start": 3011.04,
      "duration": 4.12
    },
    {
      "text": "been calculated through this uh",
      "start": 3013.28,
      "duration": 3.68
    },
    {
      "text": "mathematical operation which I just",
      "start": 3015.16,
      "duration": 3.24
    },
    {
      "text": "showed you on the",
      "start": 3016.96,
      "duration": 3.8
    },
    {
      "text": "screen and now we'll be implementing",
      "start": 3018.4,
      "duration": 4.439
    },
    {
      "text": "this operation in Python to calculate",
      "start": 3020.76,
      "duration": 4.2
    },
    {
      "text": "the context vector and it's pretty",
      "start": 3022.839,
      "duration": 3.921
    },
    {
      "text": "simple it's only two to three lines of",
      "start": 3024.96,
      "duration": 3.92
    },
    {
      "text": "code first we have the query which is",
      "start": 3026.76,
      "duration": 4.599
    },
    {
      "text": "the inputs index by one because the word",
      "start": 3028.88,
      "duration": 4.679
    },
    {
      "text": "which we are looking at is Journey then",
      "start": 3031.359,
      "duration": 5.281
    },
    {
      "text": "we initialize uh tensor context Vector",
      "start": 3033.559,
      "duration": 5.121
    },
    {
      "text": "two why two because we are looking at",
      "start": 3036.64,
      "duration": 3.719
    },
    {
      "text": "the second token journey and we are",
      "start": 3038.68,
      "duration": 4.919
    },
    {
      "text": "finding the context Vector for that so",
      "start": 3040.359,
      "duration": 4.641
    },
    {
      "text": "what we'll be doing is that we'll be",
      "start": 3043.599,
      "duration": 3.281
    },
    {
      "text": "looping through all the",
      "start": 3045.0,
      "duration": 4.88
    },
    {
      "text": "inputs and uh what we'll be doing is",
      "start": 3046.88,
      "duration": 5.8
    },
    {
      "text": "that we'll scale each input with the",
      "start": 3049.88,
      "duration": 4.64
    },
    {
      "text": "corresponding attention weight and then",
      "start": 3052.68,
      "duration": 3.76
    },
    {
      "text": "we'll add all the scaled vectors",
      "start": 3054.52,
      "duration": 3.839
    },
    {
      "text": "together to give the final context",
      "start": 3056.44,
      "duration": 4.48
    },
    {
      "text": "Vector that's it so let's say we are",
      "start": 3058.359,
      "duration": 4.641
    },
    {
      "text": "looking at the first input Vector which",
      "start": 3060.92,
      "duration": 3.72
    },
    {
      "text": "is the first input embedding we'll",
      "start": 3063.0,
      "duration": 3.24
    },
    {
      "text": "multiply it with the first attention",
      "start": 3064.64,
      "duration": 3.56
    },
    {
      "text": "weight then we'll look at the second",
      "start": 3066.24,
      "duration": 3.92
    },
    {
      "text": "input Vector we'll multiply it with the",
      "start": 3068.2,
      "duration": 3.919
    },
    {
      "text": "second attention weight then we'll look",
      "start": 3070.16,
      "duration": 4.36
    },
    {
      "text": "at the sixth input Vector at the end and",
      "start": 3072.119,
      "duration": 4.0
    },
    {
      "text": "multiply it with the sixth attention",
      "start": 3074.52,
      "duration": 3.36
    },
    {
      "text": "weight and we'll add all of these",
      "start": 3076.119,
      "duration": 3.881
    },
    {
      "text": "vectors together which ultimately leads",
      "start": 3077.88,
      "duration": 4.6
    },
    {
      "text": "to the final context vector and that's",
      "start": 3080.0,
      "duration": 4.4
    },
    {
      "text": "the one which I've showed here in the",
      "start": 3082.48,
      "duration": 4.319
    },
    {
      "text": "red arrow I've also shown the any",
      "start": 3084.4,
      "duration": 4.679
    },
    {
      "text": "context here as a ping Dot and how it's",
      "start": 3086.799,
      "duration": 4.641
    },
    {
      "text": "different from the other",
      "start": 3089.079,
      "duration": 4.921
    },
    {
      "text": "vectors awesome so we have reached this",
      "start": 3091.44,
      "duration": 4.24
    },
    {
      "text": "step where we have calculated the",
      "start": 3094.0,
      "duration": 4.76
    },
    {
      "text": "context Vector for Journey right however",
      "start": 3095.68,
      "duration": 5.8
    },
    {
      "text": "the task is not yet over because we have",
      "start": 3098.76,
      "duration": 5.16
    },
    {
      "text": "to calculate a similar context Vector",
      "start": 3101.48,
      "duration": 4.599
    },
    {
      "text": "for all the other tokens right we have",
      "start": 3103.92,
      "duration": 4.399
    },
    {
      "text": "to calculate the similar context Vector",
      "start": 3106.079,
      "duration": 5.841
    },
    {
      "text": "for your journey starts with one step",
      "start": 3108.319,
      "duration": 5.48
    },
    {
      "text": "all of these six",
      "start": 3111.92,
      "duration": 4.08
    },
    {
      "text": "words and now if you have understood",
      "start": 3113.799,
      "duration": 3.8
    },
    {
      "text": "this computer comput which we did for",
      "start": 3116.0,
      "duration": 4.28
    },
    {
      "text": "Journey we can actually extend the exact",
      "start": 3117.599,
      "duration": 4.96
    },
    {
      "text": "similar computation to compute the",
      "start": 3120.28,
      "duration": 4.6
    },
    {
      "text": "attention weight and context Vector for",
      "start": 3122.559,
      "duration": 4.841
    },
    {
      "text": "all the other",
      "start": 3124.88,
      "duration": 5.04
    },
    {
      "text": "inputs and this is actually represented",
      "start": 3127.4,
      "duration": 4.28
    },
    {
      "text": "very nicely with this which is called as",
      "start": 3129.92,
      "duration": 4.439
    },
    {
      "text": "the attention weight Matrix so what the",
      "start": 3131.68,
      "duration": 4.399
    },
    {
      "text": "Matrix which you're seeing on the screen",
      "start": 3134.359,
      "duration": 3.361
    },
    {
      "text": "right now is called as the attention",
      "start": 3136.079,
      "duration": 4.76
    },
    {
      "text": "weight Matrix and let me explain uh this",
      "start": 3137.72,
      "duration": 4.92
    },
    {
      "text": "Matrix in a very simple",
      "start": 3140.839,
      "duration": 5.361
    },
    {
      "text": "manner so if you look at the rows each",
      "start": 3142.64,
      "duration": 6.52
    },
    {
      "text": "row represents the attention weights for",
      "start": 3146.2,
      "duration": 5.56
    },
    {
      "text": "one particular word So currently we have",
      "start": 3149.16,
      "duration": 4.159
    },
    {
      "text": "calculated the attention weights for",
      "start": 3151.76,
      "duration": 2.64
    },
    {
      "text": "Journey",
      "start": 3153.319,
      "duration": 6.561
    },
    {
      "text": "right so the first value here 13 is the",
      "start": 3154.4,
      "duration": 7.36
    },
    {
      "text": "attention score or the attention weight",
      "start": 3159.88,
      "duration": 3.8
    },
    {
      "text": "between journey and",
      "start": 3161.76,
      "duration": 5.4
    },
    {
      "text": "your the second value here 23 is the",
      "start": 3163.68,
      "duration": 5.919
    },
    {
      "text": "attention score or the attention weight",
      "start": 3167.16,
      "duration": 5.28
    },
    {
      "text": "between journey and",
      "start": 3169.599,
      "duration": 6.161
    },
    {
      "text": "journey the second value here is the",
      "start": 3172.44,
      "duration": 5.04
    },
    {
      "text": "attention",
      "start": 3175.76,
      "duration": 5.68
    },
    {
      "text": "weight between journey and",
      "start": 3177.48,
      "duration": 3.96
    },
    {
      "text": "starts the fourth value here is the",
      "start": 3182.16,
      "duration": 7.159
    },
    {
      "text": "attention weight let me yeah the fourth",
      "start": 3185.079,
      "duration": 7.201
    },
    {
      "text": "value here is the attention weight",
      "start": 3189.319,
      "duration": 6.8
    },
    {
      "text": "between journey and",
      "start": 3192.28,
      "duration": 6.519
    },
    {
      "text": "width the fifth value here is the",
      "start": 3196.119,
      "duration": 5.041
    },
    {
      "text": "attention weight between journey and one",
      "start": 3198.799,
      "duration": 3.841
    },
    {
      "text": "and the sixth value here is the",
      "start": 3201.16,
      "duration": 3.04
    },
    {
      "text": "attention weight between journey and",
      "start": 3202.64,
      "duration": 4.159
    },
    {
      "text": "step so these are the sixth atten ention",
      "start": 3204.2,
      "duration": 5.399
    },
    {
      "text": "weights which we also computed over here",
      "start": 3206.799,
      "duration": 4.601
    },
    {
      "text": "so these are the six attention weights",
      "start": 3209.599,
      "duration": 3.361
    },
    {
      "text": "which have been computed here we have",
      "start": 3211.4,
      "duration": 3.12
    },
    {
      "text": "just rounded off the values so the",
      "start": 3212.96,
      "duration": 4.359
    },
    {
      "text": "values might not be exactly similar but",
      "start": 3214.52,
      "duration": 4.76
    },
    {
      "text": "these are the uh these are the six",
      "start": 3217.319,
      "duration": 4.441
    },
    {
      "text": "attention",
      "start": 3219.28,
      "duration": 6.279
    },
    {
      "text": "weights okay now um let",
      "start": 3221.76,
      "duration": 7.68
    },
    {
      "text": "us go next okay so similarly what we",
      "start": 3225.559,
      "duration": 6.441
    },
    {
      "text": "have to do is we have to find",
      "start": 3229.44,
      "duration": 4.72
    },
    {
      "text": "essentially similar attention weights",
      "start": 3232.0,
      "duration": 4.44
    },
    {
      "text": "for all the other words like let's say",
      "start": 3234.16,
      "duration": 5.0
    },
    {
      "text": "if we look at starts we have to find six",
      "start": 3236.44,
      "duration": 5.0
    },
    {
      "text": "attention weights for starts we have to",
      "start": 3239.16,
      "duration": 4.32
    },
    {
      "text": "find six attention weights for width we",
      "start": 3241.44,
      "duration": 4.48
    },
    {
      "text": "have to find six attention uh weights",
      "start": 3243.48,
      "duration": 4.72
    },
    {
      "text": "for step and we have to find six",
      "start": 3245.92,
      "duration": 4.56
    },
    {
      "text": "attention weights for your and",
      "start": 3248.2,
      "duration": 5.599
    },
    {
      "text": "one so for every every query we have to",
      "start": 3250.48,
      "duration": 5.92
    },
    {
      "text": "find six attention weights so all of",
      "start": 3253.799,
      "duration": 4.601
    },
    {
      "text": "these which I'm highlighting with star",
      "start": 3256.4,
      "duration": 4.199
    },
    {
      "text": "right now all of these are the queries",
      "start": 3258.4,
      "duration": 4.08
    },
    {
      "text": "currently we only looked at the journey",
      "start": 3260.599,
      "duration": 4.48
    },
    {
      "text": "query but now we have to essentially",
      "start": 3262.48,
      "duration": 4.879
    },
    {
      "text": "replicate the exact same computation for",
      "start": 3265.079,
      "duration": 4.361
    },
    {
      "text": "all the other queries as",
      "start": 3267.359,
      "duration": 4.881
    },
    {
      "text": "well so how will we do this let's say if",
      "start": 3269.44,
      "duration": 5.159
    },
    {
      "text": "the query is Step we'll find the",
      "start": 3272.24,
      "duration": 4.16
    },
    {
      "text": "attention weight between step and all",
      "start": 3274.599,
      "duration": 4.641
    },
    {
      "text": "the other words and then we'll find the",
      "start": 3276.4,
      "duration": 4.719
    },
    {
      "text": "context vector by doing the summation",
      "start": 3279.24,
      "duration": 4.28
    },
    {
      "text": "operation like we did at the end for the",
      "start": 3281.119,
      "duration": 4.881
    },
    {
      "text": "query of",
      "start": 3283.52,
      "duration": 4.92
    },
    {
      "text": "Journey so essentially we are going to",
      "start": 3286.0,
      "duration": 5.0
    },
    {
      "text": "follow the exact same steps as before",
      "start": 3288.44,
      "duration": 5.6
    },
    {
      "text": "for all the other tokens also we are",
      "start": 3291.0,
      "duration": 4.68
    },
    {
      "text": "first going to compute the attention",
      "start": 3294.04,
      "duration": 4.079
    },
    {
      "text": "scores then we are going to compute the",
      "start": 3295.68,
      "duration": 4.72
    },
    {
      "text": "attention weights and then we are going",
      "start": 3298.119,
      "duration": 4.881
    },
    {
      "text": "to compute the context Vector remember",
      "start": 3300.4,
      "duration": 4.919
    },
    {
      "text": "these are the exact same steps which we",
      "start": 3303.0,
      "duration": 6.64
    },
    {
      "text": "followed uh for uh the query of Journey",
      "start": 3305.319,
      "duration": 6.28
    },
    {
      "text": "and these are the exact same steps which",
      "start": 3309.64,
      "duration": 4.679
    },
    {
      "text": "we will follow for other queries as well",
      "start": 3311.599,
      "duration": 4.601
    },
    {
      "text": "so let me take you through code right",
      "start": 3314.319,
      "duration": 5.8
    },
    {
      "text": "now and let us start uh",
      "start": 3316.2,
      "duration": 6.76
    },
    {
      "text": "implementing the attention or let us",
      "start": 3320.119,
      "duration": 5.841
    },
    {
      "text": "start calculating the these three steps",
      "start": 3322.96,
      "duration": 6.8
    },
    {
      "text": "for the other queries as well so as we",
      "start": 3325.96,
      "duration": 6.119
    },
    {
      "text": "discussed we have to follow three steps",
      "start": 3329.76,
      "duration": 4.72
    },
    {
      "text": "the first step is to find the attention",
      "start": 3332.079,
      "duration": 5.04
    },
    {
      "text": "scores and remember how do we find the",
      "start": 3334.48,
      "duration": 4.72
    },
    {
      "text": "attention score if we have a particular",
      "start": 3337.119,
      "duration": 4.641
    },
    {
      "text": "query we'll just take the do product of",
      "start": 3339.2,
      "duration": 5.56
    },
    {
      "text": "that with all the other input vectors so",
      "start": 3341.76,
      "duration": 5.64
    },
    {
      "text": "let's say if the query is in inputs",
      "start": 3344.76,
      "duration": 5.279
    },
    {
      "text": "we'll take the dot product of the query",
      "start": 3347.4,
      "duration": 5.48
    },
    {
      "text": "with all the other vectors in the input",
      "start": 3350.039,
      "duration": 5.201
    },
    {
      "text": "so one way to find the attention scores",
      "start": 3352.88,
      "duration": 4.199
    },
    {
      "text": "is to just Loop through the input two",
      "start": 3355.24,
      "duration": 4.68
    },
    {
      "text": "times and essentially find the dot",
      "start": 3357.079,
      "duration": 5.96
    },
    {
      "text": "product uh I'll show you what that",
      "start": 3359.92,
      "duration": 4.96
    },
    {
      "text": "actually",
      "start": 3363.039,
      "duration": 6.241
    },
    {
      "text": "means uh right so let me rub these these",
      "start": 3364.88,
      "duration": 6.88
    },
    {
      "text": "things over here so that I can show you",
      "start": 3369.28,
      "duration": 4.519
    },
    {
      "text": "this one method of finding the attention",
      "start": 3371.76,
      "duration": 4.92
    },
    {
      "text": "scores so let's say you Loop over the",
      "start": 3373.799,
      "duration": 5.641
    },
    {
      "text": "input Vector right so the first you'll",
      "start": 3376.68,
      "duration": 4.6
    },
    {
      "text": "encounter your then you will find the",
      "start": 3379.44,
      "duration": 4.879
    },
    {
      "text": "dot product between your and all these",
      "start": 3381.28,
      "duration": 5.88
    },
    {
      "text": "other uh all the other",
      "start": 3384.319,
      "duration": 7.441
    },
    {
      "text": "inputs so that will be uh the result of",
      "start": 3387.16,
      "duration": 8.399
    },
    {
      "text": "the first inner loop so what you do is",
      "start": 3391.76,
      "duration": 6.16
    },
    {
      "text": "that first you fix an i in the outer",
      "start": 3395.559,
      "duration": 4.201
    },
    {
      "text": "loop and in the Inner Loop you go",
      "start": 3397.92,
      "duration": 3.96
    },
    {
      "text": "through the input entirely so when we",
      "start": 3399.76,
      "duration": 4.16
    },
    {
      "text": "fix an i in the outer loop it means we",
      "start": 3401.88,
      "duration": 4.6
    },
    {
      "text": "fix this sarey then we go through the",
      "start": 3403.92,
      "duration": 5.08
    },
    {
      "text": "inner loop entirely and find these six",
      "start": 3406.48,
      "duration": 5.079
    },
    {
      "text": "dot products now change the outer loop",
      "start": 3409.0,
      "duration": 4.039
    },
    {
      "text": "so then the outer loop changes to",
      "start": 3411.559,
      "duration": 3.56
    },
    {
      "text": "journey and then similarly find the dot",
      "start": 3413.039,
      "duration": 3.881
    },
    {
      "text": "product between Journey and all the",
      "start": 3415.119,
      "duration": 4.121
    },
    {
      "text": "other vectors now change the outer loop",
      "start": 3416.92,
      "duration": 4.919
    },
    {
      "text": "once more so similarly we'll change the",
      "start": 3419.24,
      "duration": 4.68
    },
    {
      "text": "outer loop and in each outer loop we'll",
      "start": 3421.839,
      "duration": 4.24
    },
    {
      "text": "go through the inner loop so that is",
      "start": 3423.92,
      "duration": 3.96
    },
    {
      "text": "essentially finding the dot products",
      "start": 3426.079,
      "duration": 4.0
    },
    {
      "text": "which are the attention",
      "start": 3427.88,
      "duration": 4.64
    },
    {
      "text": "scores uh the problem with this approach",
      "start": 3430.079,
      "duration": 5.28
    },
    {
      "text": "is that this will take a lot of",
      "start": 3432.52,
      "duration": 5.48
    },
    {
      "text": "computational time so if you look at the",
      "start": 3435.359,
      "duration": 6.361
    },
    {
      "text": "output tensor so this is a 6x6 and each",
      "start": 3438.0,
      "duration": 5.76
    },
    {
      "text": "element in this tensor represents an",
      "start": 3441.72,
      "duration": 3.8
    },
    {
      "text": "attention score between two pairs of",
      "start": 3443.76,
      "duration": 5.279
    },
    {
      "text": "inputs so for example this 6x6 Matrix",
      "start": 3445.52,
      "duration": 4.92
    },
    {
      "text": "which you just saw in the code is very",
      "start": 3449.039,
      "duration": 3.601
    },
    {
      "text": "similar to this here I'm showing the",
      "start": 3450.44,
      "duration": 4.639
    },
    {
      "text": "normalized attention scores but even the",
      "start": 3452.64,
      "duration": 5.439
    },
    {
      "text": "attention scores look like the 6x6 so if",
      "start": 3455.079,
      "duration": 5.321
    },
    {
      "text": "you look at the first row all of those",
      "start": 3458.079,
      "duration": 4.401
    },
    {
      "text": "are the dot products between the first",
      "start": 3460.4,
      "duration": 4.32
    },
    {
      "text": "query and all the other queries if you",
      "start": 3462.48,
      "duration": 4.2
    },
    {
      "text": "look at the second row all of these are",
      "start": 3464.72,
      "duration": 3.48
    },
    {
      "text": "the dot products between the second",
      "start": 3466.68,
      "duration": 6.56
    },
    {
      "text": "query and uh all the other inputs so",
      "start": 3468.2,
      "duration": 8.0
    },
    {
      "text": "this second this second row actually",
      "start": 3473.24,
      "duration": 6.48
    },
    {
      "text": "will be exactly same to the attention uh",
      "start": 3476.2,
      "duration": 5.359
    },
    {
      "text": "scores which we had calculated earlier",
      "start": 3479.72,
      "duration": 5.28
    },
    {
      "text": "see because the second row is",
      "start": 3481.559,
      "duration": 7.28
    },
    {
      "text": "9444 1.49 ETC so if you look at the",
      "start": 3485.0,
      "duration": 7.52
    },
    {
      "text": "second row here that is also 9544",
      "start": 3488.839,
      "duration": 6.24
    },
    {
      "text": "1.49 because the second row represents",
      "start": 3492.52,
      "duration": 5.4
    },
    {
      "text": "the dot product between the second query",
      "start": 3495.079,
      "duration": 5.201
    },
    {
      "text": "and all the other input vectors",
      "start": 3497.92,
      "duration": 4.24
    },
    {
      "text": "similarly the last row represents the",
      "start": 3500.28,
      "duration": 3.92
    },
    {
      "text": "dot product between the last query and",
      "start": 3502.16,
      "duration": 5.639
    },
    {
      "text": "all the other input vectors",
      "start": 3504.2,
      "duration": 6.04
    },
    {
      "text": "okay now here we have used two for Loops",
      "start": 3507.799,
      "duration": 3.601
    },
    {
      "text": "right and that's not very",
      "start": 3510.24,
      "duration": 3.48
    },
    {
      "text": "computationally efficient for Loops are",
      "start": 3511.4,
      "duration": 4.48
    },
    {
      "text": "generally quite slow and that's the",
      "start": 3513.72,
      "duration": 4.399
    },
    {
      "text": "reason why matrix multiplication needs",
      "start": 3515.88,
      "duration": 4.679
    },
    {
      "text": "to be understood the reason I say that",
      "start": 3518.119,
      "duration": 4.401
    },
    {
      "text": "linear algebra is actually the core",
      "start": 3520.559,
      "duration": 4.401
    },
    {
      "text": "Foundation of every machine learning",
      "start": 3522.52,
      "duration": 5.0
    },
    {
      "text": "concept which you want to master is this",
      "start": 3524.96,
      "duration": 4.2
    },
    {
      "text": "for someone who does not know about",
      "start": 3527.52,
      "duration": 3.72
    },
    {
      "text": "linear algebra and matrix multiplication",
      "start": 3529.16,
      "duration": 3.879
    },
    {
      "text": "they'll just do these two rounds of four",
      "start": 3531.24,
      "duration": 4.319
    },
    {
      "text": "Loops but if you actually know linear",
      "start": 3533.039,
      "duration": 5.08
    },
    {
      "text": "algebra you'll see that instead of doing",
      "start": 3535.559,
      "duration": 4.361
    },
    {
      "text": "this you can just take the uh",
      "start": 3538.119,
      "duration": 4.48
    },
    {
      "text": "multiplication of inputs and the",
      "start": 3539.92,
      "duration": 4.52
    },
    {
      "text": "transpose of the inputs and you will",
      "start": 3542.599,
      "duration": 3.641
    },
    {
      "text": "actually get the exact same",
      "start": 3544.44,
      "duration": 4.08
    },
    {
      "text": "answer so what this does is that you",
      "start": 3546.24,
      "duration": 6.2
    },
    {
      "text": "take the input uh input Matrix and what",
      "start": 3548.52,
      "duration": 6.48
    },
    {
      "text": "the input Matrix looks like is this you",
      "start": 3552.44,
      "duration": 4.44
    },
    {
      "text": "take the input Matrix and then you",
      "start": 3555.0,
      "duration": 3.799
    },
    {
      "text": "multiply with the transpose of the input",
      "start": 3556.88,
      "duration": 4.56
    },
    {
      "text": "Matrix and you'll get the exact same",
      "start": 3558.799,
      "duration": 4.681
    },
    {
      "text": "answer as",
      "start": 3561.44,
      "duration": 4.76
    },
    {
      "text": "uh you'll take you'll get the exact same",
      "start": 3563.48,
      "duration": 5.079
    },
    {
      "text": "answer as doing this dot product in the",
      "start": 3566.2,
      "duration": 5.32
    },
    {
      "text": "for Loop format and you can verify this",
      "start": 3568.559,
      "duration": 5.641
    },
    {
      "text": "so if you just take the product between",
      "start": 3571.52,
      "duration": 6.24
    },
    {
      "text": "inputs Matrix and the inputs transpose",
      "start": 3574.2,
      "duration": 6.44
    },
    {
      "text": "what we'll see is the exact same thing",
      "start": 3577.76,
      "duration": 5.44
    },
    {
      "text": "as the previous answer why because when",
      "start": 3580.64,
      "duration": 4.399
    },
    {
      "text": "we multiply two matrices what it",
      "start": 3583.2,
      "duration": 4.44
    },
    {
      "text": "essentially does is it just computes a",
      "start": 3585.039,
      "duration": 4.361
    },
    {
      "text": "bunch of dot products between the rows",
      "start": 3587.64,
      "duration": 3.56
    },
    {
      "text": "of the first Matrix and The Columns of",
      "start": 3589.4,
      "duration": 3.84
    },
    {
      "text": "the second Matrix which is exactly what",
      "start": 3591.2,
      "duration": 4.44
    },
    {
      "text": "we are doing here in these two for Loops",
      "start": 3593.24,
      "duration": 4.48
    },
    {
      "text": "it just that this matrix multiplication",
      "start": 3595.64,
      "duration": 4.24
    },
    {
      "text": "operation is much more efficient than",
      "start": 3597.72,
      "duration": 6.28
    },
    {
      "text": "using these two for Loops so Step One is",
      "start": 3599.88,
      "duration": 5.679
    },
    {
      "text": "completed right now we have found the",
      "start": 3604.0,
      "duration": 3.599
    },
    {
      "text": "attention scores I hope you have",
      "start": 3605.559,
      "duration": 4.681
    },
    {
      "text": "understood why this is a 6x6 Matrix here",
      "start": 3607.599,
      "duration": 5.121
    },
    {
      "text": "and why each what each row represents",
      "start": 3610.24,
      "duration": 4.68
    },
    {
      "text": "each row represents the dot product",
      "start": 3612.72,
      "duration": 5.8
    },
    {
      "text": "between that particular query and all",
      "start": 3614.92,
      "duration": 7.159
    },
    {
      "text": "the other input um input embedding",
      "start": 3618.52,
      "duration": 5.76
    },
    {
      "text": "vectors now we'll implement the",
      "start": 3622.079,
      "duration": 4.321
    },
    {
      "text": "normalization so remember how we did",
      "start": 3624.28,
      "duration": 4.44
    },
    {
      "text": "normalization here we did",
      "start": 3626.4,
      "duration": 6.0
    },
    {
      "text": "torch. soft Max right so similarly what",
      "start": 3628.72,
      "duration": 5.319
    },
    {
      "text": "we are going to do here is we are going",
      "start": 3632.4,
      "duration": 4.439
    },
    {
      "text": "to do torge do softmax of this attention",
      "start": 3634.039,
      "duration": 5.721
    },
    {
      "text": "scores Matrix and what this will do is",
      "start": 3636.839,
      "duration": 4.601
    },
    {
      "text": "that it will implement the soft Max",
      "start": 3639.76,
      "duration": 5.079
    },
    {
      "text": "operation to each row so the first row",
      "start": 3641.44,
      "duration": 5.359
    },
    {
      "text": "we'll do the soft Max like we learned",
      "start": 3644.839,
      "duration": 3.561
    },
    {
      "text": "before then the second row we do the",
      "start": 3646.799,
      "duration": 4.24
    },
    {
      "text": "softmax like we learned before similarly",
      "start": 3648.4,
      "duration": 4.56
    },
    {
      "text": "the last row will do the soft Max like",
      "start": 3651.039,
      "duration": 4.32
    },
    {
      "text": "we learned before so if you look at each",
      "start": 3652.96,
      "duration": 4.879
    },
    {
      "text": "individual row you'll see that entries",
      "start": 3655.359,
      "duration": 4.96
    },
    {
      "text": "of each row sum up to one so you can",
      "start": 3657.839,
      "duration": 6.601
    },
    {
      "text": "look at the second row here1 385 2379",
      "start": 3660.319,
      "duration": 6.961
    },
    {
      "text": "it's the same U attention weights which",
      "start": 3664.44,
      "duration": 5.96
    },
    {
      "text": "we have got for the",
      "start": 3667.28,
      "duration": 5.519
    },
    {
      "text": "journey",
      "start": 3670.4,
      "duration": 5.199
    },
    {
      "text": "query uh one key thing to mention here",
      "start": 3672.799,
      "duration": 5.841
    },
    {
      "text": "is that what is the dim parameter over",
      "start": 3675.599,
      "duration": 6.0
    },
    {
      "text": "here so the dim here I'm saying minus",
      "start": 3678.64,
      "duration": 5.36
    },
    {
      "text": "one and the reason is explained below",
      "start": 3681.599,
      "duration": 4.52
    },
    {
      "text": "the dim parameter in functions like like",
      "start": 3684.0,
      "duration": 4.799
    },
    {
      "text": "tor. softmax specifies the dimension of",
      "start": 3686.119,
      "duration": 5.2
    },
    {
      "text": "the input function input tensor along",
      "start": 3688.799,
      "duration": 5.28
    },
    {
      "text": "which the function will be computed so",
      "start": 3691.319,
      "duration": 5.48
    },
    {
      "text": "by setting dim equal to minus1 here we",
      "start": 3694.079,
      "duration": 5.921
    },
    {
      "text": "are instructing the softmax function to",
      "start": 3696.799,
      "duration": 5.721
    },
    {
      "text": "apply the normalization along the last",
      "start": 3700.0,
      "duration": 4.72
    },
    {
      "text": "dimension of the attention score",
      "start": 3702.52,
      "duration": 4.76
    },
    {
      "text": "tensor and what is the last dimension of",
      "start": 3704.72,
      "duration": 4.2
    },
    {
      "text": "the attention score tensor it's",
      "start": 3707.28,
      "duration": 4.2
    },
    {
      "text": "essentially The Columns so if the",
      "start": 3708.92,
      "duration": 4.879
    },
    {
      "text": "attention scores is a 2d tensor it's a",
      "start": 3711.48,
      "duration": 5.359
    },
    {
      "text": "2d 6x6 tensor right and it has the shape",
      "start": 3713.799,
      "duration": 5.76
    },
    {
      "text": "of rows and columns uh the last",
      "start": 3716.839,
      "duration": 4.681
    },
    {
      "text": "Dimension is the column so Dimension",
      "start": 3719.559,
      "duration": 4.121
    },
    {
      "text": "equal to minus one will normalize across",
      "start": 3721.52,
      "duration": 3.519
    },
    {
      "text": "the",
      "start": 3723.68,
      "duration": 4.2
    },
    {
      "text": "columns so what what will happen is that",
      "start": 3725.039,
      "duration": 5.121
    },
    {
      "text": "for the first row look at the columns so",
      "start": 3727.88,
      "duration": 3.56
    },
    {
      "text": "this is the first column this is the",
      "start": 3730.16,
      "duration": 3.48
    },
    {
      "text": "second column actually I have to show",
      "start": 3731.44,
      "duration": 3.879
    },
    {
      "text": "here this is the First Column this is",
      "start": 3733.64,
      "duration": 3.04
    },
    {
      "text": "the second column this is the third",
      "start": 3735.319,
      "duration": 4.161
    },
    {
      "text": "column so we are normalizing essentially",
      "start": 3736.68,
      "duration": 4.52
    },
    {
      "text": "along the columns right because we are",
      "start": 3739.48,
      "duration": 3.559
    },
    {
      "text": "going to take the exponent of what all",
      "start": 3741.2,
      "duration": 3.399
    },
    {
      "text": "is there in the First Column second",
      "start": 3743.039,
      "duration": 3.441
    },
    {
      "text": "column third column Etc we are going to",
      "start": 3744.599,
      "duration": 4.72
    },
    {
      "text": "sum these exponents that's why it's very",
      "start": 3746.48,
      "duration": 4.16
    },
    {
      "text": "important",
      "start": 3749.319,
      "duration": 5.321
    },
    {
      "text": "to uh write this dim equal to minus1",
      "start": 3750.64,
      "duration": 6.28
    },
    {
      "text": "because we are normalizing across a",
      "start": 3754.64,
      "duration": 5.28
    },
    {
      "text": "column um and that's why the values in",
      "start": 3756.92,
      "duration": 5.24
    },
    {
      "text": "one row sum up to one since we are",
      "start": 3759.92,
      "duration": 5.199
    },
    {
      "text": "normalizing in the each column that's",
      "start": 3762.16,
      "duration": 4.919
    },
    {
      "text": "why the values in each row sum up to one",
      "start": 3765.119,
      "duration": 4.0
    },
    {
      "text": "it's very important to note that so",
      "start": 3767.079,
      "duration": 4.121
    },
    {
      "text": "Dimension equal to minus1 means that we",
      "start": 3769.119,
      "duration": 3.92
    },
    {
      "text": "have to apply the normalization along",
      "start": 3771.2,
      "duration": 3.96
    },
    {
      "text": "the last Dimension and for a two",
      "start": 3773.039,
      "duration": 3.921
    },
    {
      "text": "dimensional t sensor like this the last",
      "start": 3775.16,
      "duration": 4.399
    },
    {
      "text": "Dimension is the columns so the soft Max",
      "start": 3776.96,
      "duration": 5.24
    },
    {
      "text": "will be applied across the columns and",
      "start": 3779.559,
      "duration": 4.681
    },
    {
      "text": "that's why for each row you will see",
      "start": 3782.2,
      "duration": 4.359
    },
    {
      "text": "that all the entries sum up to",
      "start": 3784.24,
      "duration": 4.799
    },
    {
      "text": "one so these are the attention weights",
      "start": 3786.559,
      "duration": 4.361
    },
    {
      "text": "which we have calculated and the last",
      "start": 3789.039,
      "duration": 4.52
    },
    {
      "text": "step which is very important is",
      "start": 3790.92,
      "duration": 6.0
    },
    {
      "text": "calculating the context vector and uh I",
      "start": 3793.559,
      "duration": 5.921
    },
    {
      "text": "want to uh show some things to you but",
      "start": 3796.92,
      "duration": 4.439
    },
    {
      "text": "before that let's verify that all the",
      "start": 3799.48,
      "duration": 4.119
    },
    {
      "text": "rows indeed sum up to one in this",
      "start": 3801.359,
      "duration": 4.561
    },
    {
      "text": "attention weights so what I'm doing here",
      "start": 3803.599,
      "duration": 4.041
    },
    {
      "text": "is that I'm looking at the second row",
      "start": 3805.92,
      "duration": 4.639
    },
    {
      "text": "here and I'm just going to sum up to one",
      "start": 3807.64,
      "duration": 4.52
    },
    {
      "text": "and I'm just going to sum up the entries",
      "start": 3810.559,
      "duration": 4.24
    },
    {
      "text": "of the second row and you will see that",
      "start": 3812.16,
      "duration": 5.639
    },
    {
      "text": "uh the second row sums up to one and I",
      "start": 3814.799,
      "duration": 4.721
    },
    {
      "text": "have also included a print statement",
      "start": 3817.799,
      "duration": 4.121
    },
    {
      "text": "below which prints out the summation of",
      "start": 3819.52,
      "duration": 4.64
    },
    {
      "text": "all the rows and you'll see that the",
      "start": 3821.92,
      "duration": 4.24
    },
    {
      "text": "first row the second row similarly the",
      "start": 3824.16,
      "duration": 3.959
    },
    {
      "text": "sixth row all of the rows essentially",
      "start": 3826.16,
      "duration": 4.04
    },
    {
      "text": "sum up to one this means that the",
      "start": 3828.119,
      "duration": 4.881
    },
    {
      "text": "softmax operation has been employed in a",
      "start": 3830.2,
      "duration": 5.639
    },
    {
      "text": "correct manner for you to explore this",
      "start": 3833.0,
      "duration": 4.799
    },
    {
      "text": "dim further you can try with dimm equal",
      "start": 3835.839,
      "duration": 4.081
    },
    {
      "text": "to Z dim equal to 1",
      "start": 3837.799,
      "duration": 4.52
    },
    {
      "text": "also from the errors you will learn a",
      "start": 3839.92,
      "duration": 4.52
    },
    {
      "text": "lot these small details are very",
      "start": 3842.319,
      "duration": 4.76
    },
    {
      "text": "important other students who just apply",
      "start": 3844.44,
      "duration": 6.04
    },
    {
      "text": "llm Lang chain and just focus on",
      "start": 3847.079,
      "duration": 5.681
    },
    {
      "text": "deployment will never focus on these",
      "start": 3850.48,
      "duration": 4.119
    },
    {
      "text": "Minor Details like what is this dim",
      "start": 3852.76,
      "duration": 4.4
    },
    {
      "text": "operator over here Etc but I believe the",
      "start": 3854.599,
      "duration": 4.281
    },
    {
      "text": "devil always lies in the",
      "start": 3857.16,
      "duration": 4.199
    },
    {
      "text": "details so the students who understand",
      "start": 3858.88,
      "duration": 5.399
    },
    {
      "text": "these Basics will really Master large",
      "start": 3861.359,
      "duration": 5.161
    },
    {
      "text": "language models much more than other",
      "start": 3864.279,
      "duration": 4.8
    },
    {
      "text": "students now we come to the final step",
      "start": 3866.52,
      "duration": 4.079
    },
    {
      "text": "which is essentially Computing the",
      "start": 3869.079,
      "duration": 4.441
    },
    {
      "text": "context vectors right and I will take",
      "start": 3870.599,
      "duration": 5.2
    },
    {
      "text": "you to code but the final step is",
      "start": 3873.52,
      "duration": 4.599
    },
    {
      "text": "actually implemented in elegant one line",
      "start": 3875.799,
      "duration": 5.24
    },
    {
      "text": "of code let me take you to the final",
      "start": 3878.119,
      "duration": 4.561
    },
    {
      "text": "step before what we had done in the",
      "start": 3881.039,
      "duration": 4.401
    },
    {
      "text": "final step remember what we simply did",
      "start": 3882.68,
      "duration": 5.76
    },
    {
      "text": "was uh we just",
      "start": 3885.44,
      "duration": 6.599
    },
    {
      "text": "uh where was that yeah in the final step",
      "start": 3888.44,
      "duration": 5.159
    },
    {
      "text": "what we simply did was we just",
      "start": 3892.039,
      "duration": 4.56
    },
    {
      "text": "multiplied the attention weights",
      "start": 3893.599,
      "duration": 4.081
    },
    {
      "text": "for",
      "start": 3896.599,
      "duration": 4.44
    },
    {
      "text": "each Vector for each input Vector with",
      "start": 3897.68,
      "duration": 5.639
    },
    {
      "text": "that corresponding Vector right I can",
      "start": 3901.039,
      "duration": 5.56
    },
    {
      "text": "show this to you in the Whiteboard",
      "start": 3903.319,
      "duration": 6.881
    },
    {
      "text": "also okay so what we did for the final",
      "start": 3906.599,
      "duration": 5.921
    },
    {
      "text": "step of the context Vector was something",
      "start": 3910.2,
      "duration": 5.76
    },
    {
      "text": "like this yeah so what we did was we we",
      "start": 3912.52,
      "duration": 5.559
    },
    {
      "text": "got the attention weights for",
      "start": 3915.96,
      "duration": 5.0
    },
    {
      "text": "each input embedding vector and we",
      "start": 3918.079,
      "duration": 5.0
    },
    {
      "text": "multiplied those attention weights with",
      "start": 3920.96,
      "duration": 4.28
    },
    {
      "text": "each with the corresponding input vector",
      "start": 3923.079,
      "duration": 5.401
    },
    {
      "text": "and we those up now this is exactly what",
      "start": 3925.24,
      "duration": 5.2
    },
    {
      "text": "we have to do for the",
      "start": 3928.48,
      "duration": 4.799
    },
    {
      "text": "other uh for the other tokens also but",
      "start": 3930.44,
      "duration": 5.119
    },
    {
      "text": "we have to do this in a matrix manner",
      "start": 3933.279,
      "duration": 4.08
    },
    {
      "text": "because we cannot just keep on looping",
      "start": 3935.559,
      "duration": 4.56
    },
    {
      "text": "over and use for loops and there is a",
      "start": 3937.359,
      "duration": 4.641
    },
    {
      "text": "very elegant Matrix operation which",
      "start": 3940.119,
      "duration": 3.761
    },
    {
      "text": "actually helps us calculate the context",
      "start": 3942.0,
      "duration": 5.48
    },
    {
      "text": "vectors it's just one line of matrix",
      "start": 3943.88,
      "duration": 7.8
    },
    {
      "text": "product essentially we have to multiply",
      "start": 3947.48,
      "duration": 9.0
    },
    {
      "text": "the uh attention scores Matrix or the",
      "start": 3951.68,
      "duration": 6.76
    },
    {
      "text": "attention weight Matrix with the input",
      "start": 3956.48,
      "duration": 4.52
    },
    {
      "text": "right and we have to do some summations",
      "start": 3958.44,
      "duration": 4.28
    },
    {
      "text": "can you think of the matrix",
      "start": 3961.0,
      "duration": 3.24
    },
    {
      "text": "multiplication operation which will",
      "start": 3962.72,
      "duration": 3.68
    },
    {
      "text": "directly give us this",
      "start": 3964.24,
      "duration": 4.92
    },
    {
      "text": "answer um it's fine if you don't know",
      "start": 3966.4,
      "duration": 6.32
    },
    {
      "text": "the answer but the simplified matrix",
      "start": 3969.16,
      "duration": 5.679
    },
    {
      "text": "multiplication is just essentially",
      "start": 3972.72,
      "duration": 3.76
    },
    {
      "text": "multiplying the attention weights with",
      "start": 3974.839,
      "duration": 3.0
    },
    {
      "text": "the inputs that's",
      "start": 3976.48,
      "duration": 4.68
    },
    {
      "text": "it uh this last step of finding the",
      "start": 3977.839,
      "duration": 5.24
    },
    {
      "text": "context Vector is just taking this",
      "start": 3981.16,
      "duration": 4.199
    },
    {
      "text": "attention weight Matrix and multiplying",
      "start": 3983.079,
      "duration": 4.561
    },
    {
      "text": "it with the input Matrix and the claim",
      "start": 3985.359,
      "duration": 3.92
    },
    {
      "text": "is that it will give us the context",
      "start": 3987.64,
      "duration": 3.6
    },
    {
      "text": "vectors it will give us the six context",
      "start": 3989.279,
      "duration": 3.961
    },
    {
      "text": "vectors which we are looking for",
      "start": 3991.24,
      "duration": 3.879
    },
    {
      "text": "remember we need a context Vector for",
      "start": 3993.24,
      "duration": 3.44
    },
    {
      "text": "every token right and there are six",
      "start": 3995.119,
      "duration": 4.361
    },
    {
      "text": "tokens so here are the six context",
      "start": 3996.68,
      "duration": 5.48
    },
    {
      "text": "vectors now I'm going to try to explain",
      "start": 3999.48,
      "duration": 5.119
    },
    {
      "text": "why this matrix multiplication operation",
      "start": 4002.16,
      "duration": 4.8
    },
    {
      "text": "really works so let's go to the",
      "start": 4004.599,
      "duration": 4.121
    },
    {
      "text": "Whiteboard once",
      "start": 4006.96,
      "duration": 4.8
    },
    {
      "text": "more all right so this is the first",
      "start": 4008.72,
      "duration": 5.079
    },
    {
      "text": "Matrix which we have and that's the",
      "start": 4011.76,
      "duration": 5.4
    },
    {
      "text": "attention weights right here",
      "start": 4013.799,
      "duration": 5.24
    },
    {
      "text": "and this is the second Matrix which we",
      "start": 4017.16,
      "duration": 4.32
    },
    {
      "text": "have which is the",
      "start": 4019.039,
      "duration": 5.681
    },
    {
      "text": "inputs uh so keep in mind here that the",
      "start": 4021.48,
      "duration": 5.559
    },
    {
      "text": "attention weights is a 6x6 Matrix so we",
      "start": 4024.72,
      "duration": 4.839
    },
    {
      "text": "have six rows and six columns and the",
      "start": 4027.039,
      "duration": 6.201
    },
    {
      "text": "inputs is a 6x3 matrix now we have",
      "start": 4029.559,
      "duration": 6.841
    },
    {
      "text": "already looked at how to find the uh",
      "start": 4033.24,
      "duration": 5.16
    },
    {
      "text": "context Vector for the",
      "start": 4036.4,
      "duration": 5.52
    },
    {
      "text": "second uh for the second row right which",
      "start": 4038.4,
      "duration": 4.959
    },
    {
      "text": "which essentially corresponds to the",
      "start": 4041.92,
      "duration": 4.879
    },
    {
      "text": "word journey and uh so let's see what we",
      "start": 4043.359,
      "duration": 6.68
    },
    {
      "text": "exactly did here so the final attention",
      "start": 4046.799,
      "duration": 7.081
    },
    {
      "text": "Matrix will be a 6x3 matrix because so",
      "start": 4050.039,
      "duration": 5.841
    },
    {
      "text": "sorry the final context Vector Matrix",
      "start": 4053.88,
      "duration": 4.719
    },
    {
      "text": "will be a 6x3 matrix because every row",
      "start": 4055.88,
      "duration": 5.6
    },
    {
      "text": "of this will be a context Vector so the",
      "start": 4058.599,
      "duration": 4.68
    },
    {
      "text": "first row will be the context Vector for",
      "start": 4061.48,
      "duration": 3.879
    },
    {
      "text": "the first word the second row will be",
      "start": 4063.279,
      "duration": 4.76
    },
    {
      "text": "the context Vector for the second word",
      "start": 4065.359,
      "duration": 4.321
    },
    {
      "text": "so let's look at the second word which",
      "start": 4068.039,
      "duration": 4.641
    },
    {
      "text": "is essentially the context Vector for",
      "start": 4069.68,
      "duration": 7.439
    },
    {
      "text": "Journey now uh if we take a product of",
      "start": 4072.68,
      "duration": 6.28
    },
    {
      "text": "these two Matrix so let's say if we take",
      "start": 4077.119,
      "duration": 3.361
    },
    {
      "text": "the product of the attention weight",
      "start": 4078.96,
      "duration": 3.879
    },
    {
      "text": "Matrix and the input Matrix first let's",
      "start": 4080.48,
      "duration": 4.639
    },
    {
      "text": "check the dimensions so this is a 6x6",
      "start": 4082.839,
      "duration": 6.161
    },
    {
      "text": "Matrix and the inputs is a 6x3 so 6X 6",
      "start": 4085.119,
      "duration": 6.44
    },
    {
      "text": "can be multiplied with 6x3 so taking the",
      "start": 4089.0,
      "duration": 4.559
    },
    {
      "text": "product is completely possible and it",
      "start": 4091.559,
      "duration": 4.041
    },
    {
      "text": "will result in a 6x3",
      "start": 4093.559,
      "duration": 4.321
    },
    {
      "text": "matrix uh so let's look at the second",
      "start": 4095.6,
      "duration": 4.599
    },
    {
      "text": "row if you look at the second row it",
      "start": 4097.88,
      "duration": 5.04
    },
    {
      "text": "will be uh something like we we'll take",
      "start": 4100.199,
      "duration": 6.52
    },
    {
      "text": "the second row uh so the second row",
      "start": 4102.92,
      "duration": 6.08
    },
    {
      "text": "First Column would be the dot product",
      "start": 4106.719,
      "duration": 3.881
    },
    {
      "text": "between the second row of this and the",
      "start": 4109.0,
      "duration": 3.88
    },
    {
      "text": "First Column of this the second row",
      "start": 4110.6,
      "duration": 4.159
    },
    {
      "text": "second column will be the dot product of",
      "start": 4112.88,
      "duration": 4.08
    },
    {
      "text": "this with the second column of this and",
      "start": 4114.759,
      "duration": 3.92
    },
    {
      "text": "the second row third column will be the",
      "start": 4116.96,
      "duration": 3.6
    },
    {
      "text": "dot product of this second row and the",
      "start": 4118.679,
      "duration": 3.801
    },
    {
      "text": "third column here so that's what I've",
      "start": 4120.56,
      "duration": 4.08
    },
    {
      "text": "written here in the output",
      "start": 4122.48,
      "duration": 5.04
    },
    {
      "text": "Matrix so the first element of the",
      "start": 4124.64,
      "duration": 4.84
    },
    {
      "text": "second row will be the dot product",
      "start": 4127.52,
      "duration": 3.799
    },
    {
      "text": "between the second row and the First",
      "start": 4129.48,
      "duration": 4.6
    },
    {
      "text": "Column the second element of the second",
      "start": 4131.319,
      "duration": 5.681
    },
    {
      "text": "row will be the dot product between the",
      "start": 4134.08,
      "duration": 5.239
    },
    {
      "text": "uh second row and the second column and",
      "start": 4137.0,
      "duration": 4.56
    },
    {
      "text": "the third element of the second row will",
      "start": 4139.319,
      "duration": 5.36
    },
    {
      "text": "be the dot product between the third row",
      "start": 4141.56,
      "duration": 5.08
    },
    {
      "text": "of the first Matrix and the third column",
      "start": 4144.679,
      "duration": 4.881
    },
    {
      "text": "of the second Matrix right now when you",
      "start": 4146.64,
      "duration": 4.559
    },
    {
      "text": "compute these dot",
      "start": 4149.56,
      "duration": 5.199
    },
    {
      "text": "products uh very surprisingly you will",
      "start": 4151.199,
      "duration": 5.801
    },
    {
      "text": "see that the answer is actually equal to",
      "start": 4154.759,
      "duration": 5.761
    },
    {
      "text": "this the answer is",
      "start": 4157.0,
      "duration": 7.839
    },
    {
      "text": "138 the answer is actually 138 which is",
      "start": 4160.52,
      "duration": 7.0
    },
    {
      "text": "138 multiplied by the first row over",
      "start": 4164.839,
      "duration": 7.201
    },
    {
      "text": "here plus 237 multiplied by the second",
      "start": 4167.52,
      "duration": 7.719
    },
    {
      "text": "row over here plus 233 which is",
      "start": 4172.04,
      "duration": 5.92
    },
    {
      "text": "multiplied by the third row over here",
      "start": 4175.239,
      "duration": 7.281
    },
    {
      "text": "Etc uh it's just a trick of matricis but",
      "start": 4177.96,
      "duration": 7.16
    },
    {
      "text": "what it it turns out that this second",
      "start": 4182.52,
      "duration": 6.679
    },
    {
      "text": "row second row can also be represented",
      "start": 4185.12,
      "duration": 7.32
    },
    {
      "text": "by this formulation where you take",
      "start": 4189.199,
      "duration": 6.681
    },
    {
      "text": "the uh first element of this",
      "start": 4192.44,
      "duration": 6.56
    },
    {
      "text": "second row multiply it with the first",
      "start": 4195.88,
      "duration": 5.88
    },
    {
      "text": "row of the input Matrix plus the second",
      "start": 4199.0,
      "duration": 5.159
    },
    {
      "text": "element of the second row multiply with",
      "start": 4201.76,
      "duration": 4.919
    },
    {
      "text": "the second row of the input Matrix plus",
      "start": 4204.159,
      "duration": 5.201
    },
    {
      "text": "the third element multiply it with the",
      "start": 4206.679,
      "duration": 4.961
    },
    {
      "text": "third row of the input Matrix can you",
      "start": 4209.36,
      "duration": 4.24
    },
    {
      "text": "see what we are essentially doing here",
      "start": 4211.64,
      "duration": 3.84
    },
    {
      "text": "we essentially scaling every input",
      "start": 4213.6,
      "duration": 4.44
    },
    {
      "text": "Vector right we take the first input",
      "start": 4215.48,
      "duration": 4.8
    },
    {
      "text": "Vector we take the first input Vector we",
      "start": 4218.04,
      "duration": 5.4
    },
    {
      "text": "scale it by 138 we take the second input",
      "start": 4220.28,
      "duration": 5.56
    },
    {
      "text": "Vector we scale it by 237",
      "start": 4223.44,
      "duration": 4.2
    },
    {
      "text": "we take the third input Vector we scale",
      "start": 4225.84,
      "duration": 4.839
    },
    {
      "text": "it by 233 isn't this the exact same",
      "start": 4227.64,
      "duration": 5.84
    },
    {
      "text": "scaling operation which we saw uh when",
      "start": 4230.679,
      "duration": 4.961
    },
    {
      "text": "we looked at the visual representation",
      "start": 4233.48,
      "duration": 5.64
    },
    {
      "text": "of the uh context Vector calculations",
      "start": 4235.64,
      "duration": 5.519
    },
    {
      "text": "remember we have seen seen the scaling",
      "start": 4239.12,
      "duration": 3.8
    },
    {
      "text": "operation to calculate the context",
      "start": 4241.159,
      "duration": 3.881
    },
    {
      "text": "Vector here where we had taken each of",
      "start": 4242.92,
      "duration": 4.36
    },
    {
      "text": "the input vectors and we had scaled it",
      "start": 4245.04,
      "duration": 5.4
    },
    {
      "text": "by the attention weight values and then",
      "start": 4247.28,
      "duration": 5.24
    },
    {
      "text": "we summ them to find the final context",
      "start": 4250.44,
      "duration": 4.2
    },
    {
      "text": "Vector this is the exact same thing",
      "start": 4252.52,
      "duration": 4.24
    },
    {
      "text": "which I which we are doing over here so",
      "start": 4254.64,
      "duration": 5.16
    },
    {
      "text": "when we take the product of the uh when",
      "start": 4256.76,
      "duration": 4.6
    },
    {
      "text": "we take the product of the attention",
      "start": 4259.8,
      "duration": 4.0
    },
    {
      "text": "weights and the inputs another way to",
      "start": 4261.36,
      "duration": 3.92
    },
    {
      "text": "look at it is that if you look at the",
      "start": 4263.8,
      "duration": 4.48
    },
    {
      "text": "second row it's actually scaling the",
      "start": 4265.28,
      "duration": 6.52
    },
    {
      "text": "first input by 138 scaling the second",
      "start": 4268.28,
      "duration": 6.36
    },
    {
      "text": "input by 237 dot dot dot and scaling the",
      "start": 4271.8,
      "duration": 4.359
    },
    {
      "text": "sixth input by",
      "start": 4274.64,
      "duration": 4.24
    },
    {
      "text": "0158 so it's the exact same operation as",
      "start": 4276.159,
      "duration": 4.601
    },
    {
      "text": "we performed before for finding the",
      "start": 4278.88,
      "duration": 4.6
    },
    {
      "text": "context vector and that's why finding",
      "start": 4280.76,
      "duration": 4.76
    },
    {
      "text": "the context vectors is as simple as",
      "start": 4283.48,
      "duration": 3.6
    },
    {
      "text": "multiplying the attention weights with",
      "start": 4285.52,
      "duration": 4.159
    },
    {
      "text": "the inputs the first row of this answer",
      "start": 4287.08,
      "duration": 5.159
    },
    {
      "text": "will give you the context Vector for the",
      "start": 4289.679,
      "duration": 6.121
    },
    {
      "text": "first uh input embedding Vector the",
      "start": 4292.239,
      "duration": 5.92
    },
    {
      "text": "second row will give you the context",
      "start": 4295.8,
      "duration": 4.28
    },
    {
      "text": "Vector for the second token the third",
      "start": 4298.159,
      "duration": 3.921
    },
    {
      "text": "row of this product will give you the",
      "start": 4300.08,
      "duration": 4.119
    },
    {
      "text": "context Vector for the second token for",
      "start": 4302.08,
      "duration": 4.24
    },
    {
      "text": "the third token and right up till the",
      "start": 4304.199,
      "duration": 4.04
    },
    {
      "text": "very end the sixth row will give you the",
      "start": 4306.32,
      "duration": 5.04
    },
    {
      "text": "context Vector for the final token and",
      "start": 4308.239,
      "duration": 4.841
    },
    {
      "text": "this is how the product between the",
      "start": 4311.36,
      "duration": 3.64
    },
    {
      "text": "attention weights and the inputs will",
      "start": 4313.08,
      "duration": 4.36
    },
    {
      "text": "give give you the final context Vector",
      "start": 4315.0,
      "duration": 4.679
    },
    {
      "text": "Matrix which contains the context Vector",
      "start": 4317.44,
      "duration": 3.759
    },
    {
      "text": "for all of the tokens which you are",
      "start": 4319.679,
      "duration": 2.441
    },
    {
      "text": "looking",
      "start": 4321.199,
      "duration": 4.601
    },
    {
      "text": "for and uh with this final calculation",
      "start": 4322.12,
      "duration": 7.2
    },
    {
      "text": "we calculate the context Vector for all",
      "start": 4325.8,
      "duration": 5.24
    },
    {
      "text": "um of the input",
      "start": 4329.32,
      "duration": 3.919
    },
    {
      "text": "tokens and this is exactly what I've",
      "start": 4331.04,
      "duration": 5.28
    },
    {
      "text": "have tried to do here so finally uh we",
      "start": 4333.239,
      "duration": 4.801
    },
    {
      "text": "generate a tensor which is called as the",
      "start": 4336.32,
      "duration": 4.64
    },
    {
      "text": "all context vectors and we multiply the",
      "start": 4338.04,
      "duration": 4.679
    },
    {
      "text": "attention weight Matrix with the input",
      "start": 4340.96,
      "duration": 4.4
    },
    {
      "text": "Matrix and then we get this all context",
      "start": 4342.719,
      "duration": 5.361
    },
    {
      "text": "Vector tensor and if you look at the",
      "start": 4345.36,
      "duration": 7.96
    },
    {
      "text": "second row here 4419 6515 56 you'll see",
      "start": 4348.08,
      "duration": 8.28
    },
    {
      "text": "that this is exactly the same value of",
      "start": 4353.32,
      "duration": 4.879
    },
    {
      "text": "the context Vector which we had obtained",
      "start": 4356.36,
      "duration": 5.359
    },
    {
      "text": "over here uh when we looked at Journey",
      "start": 4358.199,
      "duration": 5.681
    },
    {
      "text": "so this again implies that whatever we",
      "start": 4361.719,
      "duration": 4.721
    },
    {
      "text": "are doing here uh with the matrix",
      "start": 4363.88,
      "duration": 4.72
    },
    {
      "text": "multiplication is leading to the correct",
      "start": 4366.44,
      "duration": 4.64
    },
    {
      "text": "answer so based on this result we can",
      "start": 4368.6,
      "duration": 4.28
    },
    {
      "text": "see that the previously calculated",
      "start": 4371.08,
      "duration": 4.559
    },
    {
      "text": "context Vector 2 for journey matches the",
      "start": 4372.88,
      "duration": 5.92
    },
    {
      "text": "second row in the in this tensor exactly",
      "start": 4375.639,
      "duration": 5.08
    },
    {
      "text": "so remember this operation to get the",
      "start": 4378.8,
      "duration": 4.24
    },
    {
      "text": "final context Vector we just multiply",
      "start": 4380.719,
      "duration": 4.48
    },
    {
      "text": "the attention weights with the inputs if",
      "start": 4383.04,
      "duration": 3.8
    },
    {
      "text": "you did not understand the matrix",
      "start": 4385.199,
      "duration": 3.081
    },
    {
      "text": "multiplication which I showed on the",
      "start": 4386.84,
      "duration": 4.52
    },
    {
      "text": "Whiteboard I encourage you to do it on a",
      "start": 4388.28,
      "duration": 5.959
    },
    {
      "text": "piece of paper because this last matrix",
      "start": 4391.36,
      "duration": 4.76
    },
    {
      "text": "multiplication is very important to get",
      "start": 4394.239,
      "duration": 3.561
    },
    {
      "text": "the context Vector we just have to",
      "start": 4396.12,
      "duration": 4.119
    },
    {
      "text": "multiply the attention weight Matrix",
      "start": 4397.8,
      "duration": 3.76
    },
    {
      "text": "with the input",
      "start": 4400.239,
      "duration": 3.841
    },
    {
      "text": "Matrix and what all you are learning",
      "start": 4401.56,
      "duration": 4.76
    },
    {
      "text": "right now will Direct L extend to the",
      "start": 4404.08,
      "duration": 4.76
    },
    {
      "text": "key query and value concept which we'll",
      "start": 4406.32,
      "duration": 4.48
    },
    {
      "text": "cover when we when we come to causal",
      "start": 4408.84,
      "duration": 4.08
    },
    {
      "text": "attention and multi-head attention and",
      "start": 4410.8,
      "duration": 4.2
    },
    {
      "text": "even in the next lecture in the next",
      "start": 4412.92,
      "duration": 3.84
    },
    {
      "text": "lecture we are going to look at this",
      "start": 4415.0,
      "duration": 3.48
    },
    {
      "text": "exact mechanism but with trainable",
      "start": 4416.76,
      "duration": 4.12
    },
    {
      "text": "weights then we'll come to the concept",
      "start": 4418.48,
      "duration": 4.92
    },
    {
      "text": "of key query and value but these",
      "start": 4420.88,
      "duration": 4.359
    },
    {
      "text": "operations which we are looking at here",
      "start": 4423.4,
      "duration": 4.52
    },
    {
      "text": "so for example uh here we are taking the",
      "start": 4425.239,
      "duration": 4.48
    },
    {
      "text": "matrix product between attention weights",
      "start": 4427.92,
      "duration": 4.239
    },
    {
      "text": "and inputs right in the key query value",
      "start": 4429.719,
      "duration": 4.041
    },
    {
      "text": "this will be replaced this inputs will",
      "start": 4432.159,
      "duration": 4.0
    },
    {
      "text": "be replaced by value",
      "start": 4433.76,
      "duration": 5.32
    },
    {
      "text": "we'll also have a key and a query but",
      "start": 4436.159,
      "duration": 4.641
    },
    {
      "text": "the underlying intuition and the",
      "start": 4439.08,
      "duration": 4.119
    },
    {
      "text": "underlying mechanism is exactly the same",
      "start": 4440.8,
      "duration": 4.04
    },
    {
      "text": "so if you understand what's going on",
      "start": 4443.199,
      "duration": 3.681
    },
    {
      "text": "here you'll really understand key query",
      "start": 4444.84,
      "duration": 4.04
    },
    {
      "text": "value very",
      "start": 4446.88,
      "duration": 4.48
    },
    {
      "text": "easily okay one last thing which I want",
      "start": 4448.88,
      "duration": 5.04
    },
    {
      "text": "to cover um at the end of today's",
      "start": 4451.36,
      "duration": 4.92
    },
    {
      "text": "lecture is that okay so you might think",
      "start": 4453.92,
      "duration": 5.799
    },
    {
      "text": "that we already then find the context",
      "start": 4456.28,
      "duration": 5.879
    },
    {
      "text": "vectors like this right then what's the",
      "start": 4459.719,
      "duration": 5.48
    },
    {
      "text": "need for trainable weights",
      "start": 4462.159,
      "duration": 5.121
    },
    {
      "text": "we just take the dot product and we then",
      "start": 4465.199,
      "duration": 4.201
    },
    {
      "text": "find these context",
      "start": 4467.28,
      "duration": 4.68
    },
    {
      "text": "vectors the main problem with the",
      "start": 4469.4,
      "duration": 5.4
    },
    {
      "text": "current approach is that think about how",
      "start": 4471.96,
      "duration": 5.6
    },
    {
      "text": "we found the attention weights to find",
      "start": 4474.8,
      "duration": 4.8
    },
    {
      "text": "the attention weights all we did was to",
      "start": 4477.56,
      "duration": 4.48
    },
    {
      "text": "just take the dot product right so",
      "start": 4479.6,
      "duration": 5.44
    },
    {
      "text": "currently uh in our world the reason why",
      "start": 4482.04,
      "duration": 4.92
    },
    {
      "text": "we are giving more attention to starts",
      "start": 4485.04,
      "duration": 4.599
    },
    {
      "text": "is that the alignment between starts and",
      "start": 4486.96,
      "duration": 5.4
    },
    {
      "text": "journey is maximum so the only reason",
      "start": 4489.639,
      "duration": 4.321
    },
    {
      "text": "why we are giving more attention to",
      "start": 4492.36,
      "duration": 3.0
    },
    {
      "text": "starts is because because it",
      "start": 4493.96,
      "duration": 3.52
    },
    {
      "text": "semantically matches with",
      "start": 4495.36,
      "duration": 4.44
    },
    {
      "text": "journey because we are only getting the",
      "start": 4497.48,
      "duration": 4.0
    },
    {
      "text": "attention scores and attention weight",
      "start": 4499.8,
      "duration": 4.28
    },
    {
      "text": "from the dot product however that is not",
      "start": 4501.48,
      "duration": 5.88
    },
    {
      "text": "correct right because two vectors might",
      "start": 4504.08,
      "duration": 5.84
    },
    {
      "text": "not be semantically aligned but maybe",
      "start": 4507.36,
      "duration": 5.12
    },
    {
      "text": "they are more important in the context",
      "start": 4509.92,
      "duration": 4.92
    },
    {
      "text": "of the current sentence so for example",
      "start": 4512.48,
      "duration": 4.92
    },
    {
      "text": "journey and one are not semantically",
      "start": 4514.84,
      "duration": 5.48
    },
    {
      "text": "related to each other but what if in the",
      "start": 4517.4,
      "duration": 6.12
    },
    {
      "text": "current context one is the is the vector",
      "start": 4520.32,
      "duration": 5.56
    },
    {
      "text": "which is more important",
      "start": 4523.52,
      "duration": 4.92
    },
    {
      "text": "so apart from the meaning you also need",
      "start": 4525.88,
      "duration": 4.2
    },
    {
      "text": "to capture the information of the",
      "start": 4528.44,
      "duration": 3.64
    },
    {
      "text": "context right what is happening in the",
      "start": 4530.08,
      "duration": 3.0
    },
    {
      "text": "current",
      "start": 4532.08,
      "duration": 3.52
    },
    {
      "text": "sentence and without trainable weights",
      "start": 4533.08,
      "duration": 4.32
    },
    {
      "text": "it's not going to happen we are not",
      "start": 4535.6,
      "duration": 4.24
    },
    {
      "text": "going to capture the context effectively",
      "start": 4537.4,
      "duration": 4.239
    },
    {
      "text": "right now we did manage to capture the",
      "start": 4539.84,
      "duration": 4.399
    },
    {
      "text": "context somewhat but we only give",
      "start": 4541.639,
      "duration": 4.841
    },
    {
      "text": "attention to Words which are similar in",
      "start": 4544.239,
      "duration": 5.4
    },
    {
      "text": "meaning to the query but even if a word",
      "start": 4546.48,
      "duration": 5.679
    },
    {
      "text": "is not similar in meaning it still might",
      "start": 4549.639,
      "duration": 4.56
    },
    {
      "text": "deserve attention in the context of the",
      "start": 4552.159,
      "duration": 3.401
    },
    {
      "text": "current sentence",
      "start": 4554.199,
      "duration": 4.0
    },
    {
      "text": "so let's take a simple example",
      "start": 4555.56,
      "duration": 6.119
    },
    {
      "text": "here okay so the example is the cat sat",
      "start": 4558.199,
      "duration": 6.361
    },
    {
      "text": "on the mat because it is warm and let's",
      "start": 4561.679,
      "duration": 5.921
    },
    {
      "text": "say our query is warm so in the first",
      "start": 4564.56,
      "duration": 5.04
    },
    {
      "text": "case let's say we do not use trainable",
      "start": 4567.6,
      "duration": 3.36
    },
    {
      "text": "weights like what we have done in",
      "start": 4569.6,
      "duration": 3.4
    },
    {
      "text": "today's lecture if we don't use",
      "start": 4570.96,
      "duration": 4.719
    },
    {
      "text": "trainable weight we only take the dot",
      "start": 4573.0,
      "duration": 4.56
    },
    {
      "text": "product between the query warm and each",
      "start": 4575.679,
      "duration": 4.881
    },
    {
      "text": "words embedding and we'll find that warm",
      "start": 4577.56,
      "duration": 5.159
    },
    {
      "text": "is most similar to itself and maybe",
      "start": 4580.56,
      "duration": 4.76
    },
    {
      "text": "somewhat related to mat",
      "start": 4582.719,
      "duration": 5.0
    },
    {
      "text": "words like the cat and sat might have",
      "start": 4585.32,
      "duration": 4.2
    },
    {
      "text": "low similarity scores because they are",
      "start": 4587.719,
      "duration": 4.721
    },
    {
      "text": "not semantically related to or so with",
      "start": 4589.52,
      "duration": 4.48
    },
    {
      "text": "with this so if we don't consider",
      "start": 4592.44,
      "duration": 3.799
    },
    {
      "text": "trainable weights we'll only look at",
      "start": 4594.0,
      "duration": 3.8
    },
    {
      "text": "Words which are more similar to this",
      "start": 4596.239,
      "duration": 4.241
    },
    {
      "text": "query which is warm now with trainable",
      "start": 4597.8,
      "duration": 4.6
    },
    {
      "text": "weights the model can learn that warm",
      "start": 4600.48,
      "duration": 4.239
    },
    {
      "text": "should pay more attention to mat even if",
      "start": 4602.4,
      "duration": 5.16
    },
    {
      "text": "mat is not semantically related to warm",
      "start": 4604.719,
      "duration": 4.44
    },
    {
      "text": "so what will happen without trainable",
      "start": 4607.56,
      "duration": 4.28
    },
    {
      "text": "weights is that mat and warm might be",
      "start": 4609.159,
      "duration": 4.52
    },
    {
      "text": "vectors which are like this which have a",
      "start": 4611.84,
      "duration": 4.52
    },
    {
      "text": "90\u00b0 angle and they might not be related",
      "start": 4613.679,
      "duration": 4.801
    },
    {
      "text": "because their meaning is not related but",
      "start": 4616.36,
      "duration": 3.68
    },
    {
      "text": "that does not mean we should not pay",
      "start": 4618.48,
      "duration": 4.04
    },
    {
      "text": "attention to mat because in this context",
      "start": 4620.04,
      "duration": 4.44
    },
    {
      "text": "probably Matt is the most important if",
      "start": 4622.52,
      "duration": 4.199
    },
    {
      "text": "the query is warm because the mat is",
      "start": 4624.48,
      "duration": 4.84
    },
    {
      "text": "warm but the meaning of mat and warm are",
      "start": 4626.719,
      "duration": 5.761
    },
    {
      "text": "not not related right that's why we need",
      "start": 4629.32,
      "duration": 6.08
    },
    {
      "text": "trainable weights with trainable weights",
      "start": 4632.48,
      "duration": 5.28
    },
    {
      "text": "the model can learn that warm should pay",
      "start": 4635.4,
      "duration": 4.759
    },
    {
      "text": "more attention to Matt even if mat isn't",
      "start": 4637.76,
      "duration": 4.2
    },
    {
      "text": "semantically similar to warm in",
      "start": 4640.159,
      "duration": 4.52
    },
    {
      "text": "traditional embedding space so this is",
      "start": 4641.96,
      "duration": 4.96
    },
    {
      "text": "where important the trainable weight",
      "start": 4644.679,
      "duration": 4.881
    },
    {
      "text": "allows the model to learn that warm",
      "start": 4646.92,
      "duration": 4.799
    },
    {
      "text": "often follows mat in context like this",
      "start": 4649.56,
      "duration": 4.4
    },
    {
      "text": "one and that's how it captures long",
      "start": 4651.719,
      "duration": 4.601
    },
    {
      "text": "range dependencies that is the reason",
      "start": 4653.96,
      "duration": 4.4
    },
    {
      "text": "why we need trainable weights without",
      "start": 4656.32,
      "duration": 4.359
    },
    {
      "text": "trainable weights this meaning would be",
      "start": 4658.36,
      "duration": 4.0
    },
    {
      "text": "lost and we would only be looking at",
      "start": 4660.679,
      "duration": 4.681
    },
    {
      "text": "Words which are similar to The query by",
      "start": 4662.36,
      "duration": 5.279
    },
    {
      "text": "trainable weights we get more more of",
      "start": 4665.36,
      "duration": 4.68
    },
    {
      "text": "this information that okay Matt might",
      "start": 4667.639,
      "duration": 4.6
    },
    {
      "text": "not be related in meaning but in the",
      "start": 4670.04,
      "duration": 4.199
    },
    {
      "text": "current context mat is the word which is",
      "start": 4672.239,
      "duration": 5.241
    },
    {
      "text": "more important because the mat is warm",
      "start": 4674.239,
      "duration": 5.241
    },
    {
      "text": "this is how trainable weights allow us",
      "start": 4677.48,
      "duration": 4.679
    },
    {
      "text": "to capture context and in the next",
      "start": 4679.48,
      "duration": 4.679
    },
    {
      "text": "lecture we'll specifically devote the",
      "start": 4682.159,
      "duration": 5.0
    },
    {
      "text": "next lecture to uh the simplified self",
      "start": 4684.159,
      "duration": 4.921
    },
    {
      "text": "attention mechanism but with trainable",
      "start": 4687.159,
      "duration": 5.08
    },
    {
      "text": "weights so here is the lecture notes for",
      "start": 4689.08,
      "duration": 4.639
    },
    {
      "text": "the next lecture self attention",
      "start": 4692.239,
      "duration": 3.44
    },
    {
      "text": "mechanism with trainable weights we'll",
      "start": 4693.719,
      "duration": 4.96
    },
    {
      "text": "introduce the concept of key query value",
      "start": 4695.679,
      "duration": 5.56
    },
    {
      "text": "and then slowly we'll move to the",
      "start": 4698.679,
      "duration": 7.401
    },
    {
      "text": "concept of uh causal attention and then",
      "start": 4701.239,
      "duration": 6.641
    },
    {
      "text": "we'll move to the concept of multi-head",
      "start": 4706.08,
      "duration": 4.079
    },
    {
      "text": "attention so up till now we have covered",
      "start": 4707.88,
      "duration": 4.48
    },
    {
      "text": "simplified self attention I know this",
      "start": 4710.159,
      "duration": 4.08
    },
    {
      "text": "lecture became a bit long but it was",
      "start": 4712.36,
      "duration": 5.319
    },
    {
      "text": "very important because uh I have seen no",
      "start": 4714.239,
      "duration": 5.801
    },
    {
      "text": "other lecture or no other material which",
      "start": 4717.679,
      "duration": 4.96
    },
    {
      "text": "covers this much detail visually",
      "start": 4720.04,
      "duration": 4.56
    },
    {
      "text": "theoretically and in code about the",
      "start": 4722.639,
      "duration": 3.961
    },
    {
      "text": "attention mechanism I could have",
      "start": 4724.6,
      "duration": 3.96
    },
    {
      "text": "directly jumped to key query and value",
      "start": 4726.6,
      "duration": 4.24
    },
    {
      "text": "which will come later but then you would",
      "start": 4728.56,
      "duration": 4.76
    },
    {
      "text": "not have understood the meaning but this",
      "start": 4730.84,
      "duration": 4.64
    },
    {
      "text": "lecture allowed me to build your",
      "start": 4733.32,
      "duration": 4.48
    },
    {
      "text": "intuition I hope you're liking these set",
      "start": 4735.48,
      "duration": 3.96
    },
    {
      "text": "of lectures if you have any doubts or",
      "start": 4737.8,
      "duration": 3.8
    },
    {
      "text": "any questions please ask in the YouTube",
      "start": 4739.44,
      "duration": 3.799
    },
    {
      "text": "comment section and I'll be happy to",
      "start": 4741.6,
      "duration": 4.44
    },
    {
      "text": "reply thank you so much everyone and I",
      "start": 4743.239,
      "duration": 4.561
    },
    {
      "text": "really encourage you to take notes while",
      "start": 4746.04,
      "duration": 3.4
    },
    {
      "text": "I'm making these lectures I'll also",
      "start": 4747.8,
      "duration": 4.879
    },
    {
      "text": "share this code file with you um thanks",
      "start": 4749.44,
      "duration": 4.88
    },
    {
      "text": "everyone and I look forward to seeing",
      "start": 4752.679,
      "duration": 5.321
    },
    {
      "text": "you in the next lecture",
      "start": 4754.32,
      "duration": 3.68
    }
  ],
  "full_text": "[Music] hello everyone welcome to this lecture in the build large language models from scratch Series in the last lecture we looked at the intuition behind the attention mechanism and we saw the limitations in the recurrent neural networks which prompted The Invention which is at the heart of the attention mechanism in today's lecture we are going to look at the the mathematical foundations behind attention and we are also going to code out a very simplified version of the attention mechanism from scratch in Python so I'm calling this simplified U simplified self attention mechanism without trainable weights before we proceed to the content of the today's lecture here are the lecture notes which we covered previously in our introductory lecture on attention so if you have not seen this lecture I highly encourage you to go through the lecture to understand the intuition that way you will appreciate today's lecture much more in the last lecture I also showed I also told you the plan which I have to cover attention in this series of lectures I strongly believe that it is impossible to cover everything related to the attention mechanism in one video or even two videos so I've planned a set of uh three to four different videos to cover this entire concept the the way I'll be doing this is that first in today's lecture I'll be going through the simplified self attention mechanism without any trainable weights just to introduce the broad idea then in the next lecture we'll move to self attention with trainable weights the way it's done in modern llms including GPT then we'll move to causal attention and then finally we'll move to multi-head attention all of these will be separate series of lectures and in each lecture we'll I'll show you the mathematical foundations on this white board and then we'll code out the attention mechanism showed in that lecture from scratch in Python so if you have got this overall workflow let me go to the lecture notes of today's lecture and we'll get started awesome so the main goal which we have today is to implement a simple variant of the self attention mechanism which does not have any trainable weights which is free from any trainable weights so to motivate today's lecture let's start with a simple sentence in the whole of today's lecture we'll be dealing with this sentence which is your journey starts with one step now if this sentence is given to a large language model there are number of things which are done first we will be pre-processing this sentence we'll take this sentence we'll convert it into individual tokens GPT uses the bite pair encoder which is a subw tokenizer so it will convert these tokens and then then we'll have token IDs for each of these tokens then that token ID will be converted into a vector representation which is also called as a vector embedding so each of these word will have a vector embedding in a higher dimensional space when we deal with llms like GPT the dimensional space can be as high as 500 700 or even more than thousand dimensional Vector space but for the sake of today's demonstration we'll be considering a three-dimensional Vector space so here's how these vectors can look like for each word in the input sentence we have a vector in the three-dimensional space these vectors are not just normal vectors they capture the semantic meaning of each word so Words which are semantically related to each other like let's say cat and puppy cat and kitten dog and puppy will be closer to each other um so you might be thinking that okay this is pretty cool and this can be the input to the large language model then why do we need attention the main problem here is that let's say if we look at Journey what the vector embedding does is that the high dimensional projection of this word does capture the meaning of this word itself but it does not contain any information about how this word Journey relates to other words in the sentence how much is the relative importance between your and journey starts and journey with and journey one and journey and step with respect to Journey the embedding Vector does not contain any of this information and this information is very crucial for us to know if this sentence appears in a large body of text and we want to predict the next word we really need to know the context we need to know that which word in this sentence is closely related to Journey let's say that will really help us predict the next word in other words we need to know how much attention should we pay to each each of these words when we look at Journey that's where attention mechanism comes into the picture so the whole goal of the attention mechanism is to take the embedding Vector for uh take the vector embedding for Journey let's say and then transform it into another Vector which is called as the context Vector the context Vector can be thought of as an enriched embedding Vector because it contains much more information it not only contains the semantic meaning which is the embedding Vector also contains but it also contains information about how that given word relates to other words in the sentence and uh this contextual information really helps a lot in predicting the next word in large language model tasks one such context Vector will be generated from each of the embedding vectors so if you look at all of the embedding vectors which I have shown here we'll have a corresponding context Vector for each of these embedding vector and that is the main purpose or that is the main goal of attention mechanism even the simplified attention mechanism which we are going to consider today and even the complex multi-head attention which is used in modern large language models the goal of all of these mechanisms is the same we need to convert the embedding Vector into context vectors which are then inputs to the llms so let's get started with accomplishing this aim of converting the embedding vectors into context vectors so here's the input Vector which we have your journey starts with one step each of this each of the tokens is converted into a threedimensional vector it's called as a vector embedding and the notation which we are going to use is we are going to denote X for the inputs so X1 will be the vector representation of the first token X2 will be the vector representation of the second token X3 will be the vector representation of the third token and these will be threedimensional tokens for the sake of Simplicity throughout this lecture when I use the word token and word I'll use tokens and words interchangeably normally one token is not equal to one word because GPT uses a subword tokenizer but for the sake of Simplicity I'm going to use tokens and words interchangeably in today's lecture so let's say if we look at Journey the input embedding Vector is X2 two because it is the second word our main aim uh in this lecture is to essentially create a context Vector so the context Vector for X2 will be denoted by zed2 similarly we need one such context Vector for all the input vectors and to compute the context Vector from the embedding Vector we'll need to have information about how much in importance needs to be given to X1 which is the word your how much information needs to be given to X3 and how much information needs to be given to all the other word when we are looking at Journey For example and this is captured by this metric which is called as attention weights so we need to decide how much attention we we need to give to each input token when we are Computing the context Vector for any uh embedding vector and based on these attention weights we will finally derive the context Vector which is denoted by Zed so we'll see mathematically how to uh manipulate the attention weights the input vector VOR we'll also see what are attention scores and how to get the context Vector for each of the given embedding vectors but I hope until now you have understood the task or the aim which we have in today's lecture and as I mentioned the representation or the notation which will be which we will be following is that for the inputs we'll be using X so X of one will be the token embedding one which is the vector representation of the first token X of two will be the token embedding two which is the vector representation of of the second token Journey Etc so these are three dimensional embeddings in the example which we are considering in today's lecture now uh I have just written down the goal which I was talking about the goal of today's lecture or the goal of any attention mechanism is to basically calculate a context Vector for each element in the input and as I mentioned before the context Vector can be thought of as an enriched embedding Vector which also contains information about how that particular word relates to other words in the sentence okay so let us get started and we'll dive into code right now so here's the Jupiter notebook file and I'll be sharing this file with you and uh we'll get started implementing a simplified attention mechanism in Python so let's start taking a deep dive into this code so uh we are going to consider threedimensional embedding vectors and the the reason we are choosing this small Dimension is just for illustration purposes to ensure that uh we do not consider an very complex example and confuse everyone but the learnings which we are going to have today can easily be extended to a higher dimensional Vector space also so we have the inputs tensor which is inputs equal to torch do tensor and for every input token we have a embedding Vector which is a three-dimensional vector Vector so for the token or the word y we have this three-dimensional Vector for the word Journey we have this three-dimensional Vector for the word starts we have this threedimensional Vector right up to step we have this three-dimensional Vector so there are six threedimensional vectors which we are considering over here and uh I have just plotted these vectors here in in the threedimensional vector space for you to have a look these Vector embeddings capture the semantic meaning so journey and starts will be bit more closer to each other because they are semantically more related than let's say journey and other words so this is how the vectors look like in 3D space uh there is no reason to plot this graph you can do this entire exercise without these visuals but the reason I wanted to show you this is because when I learned about Vector embeddings for the first time I was really fascinated by the concept that words can be represented as vectors uh it seemed very surprising to me that how can words be represented mathematically as vectors but when I saw this I really was I really liked the concept so visual understanding helps and that's why I'm showing you this plot so if someone is not familiar with natural language processing and your intuition is not developed in this space such plots can really help you visualize things which you will never forget so Vector embedding becomes much more easier if you have visuals like these okay now let's move to the next step uh first let's look at this tensor and let's try to understand what each row and each column of this tensor actually represents so each row of this tensor represents each token so the first row is the first token or the first word Etc and each column represents that particular Vector Dimension there are three dimensions and hence there are three columns okay now uh we'll be moving to the next aspect which is discussing a bit about query and what exactly are query um so now what we'll be doing is that we'll be finding a context Vector for each element so if you look at um this sentence right here we'll be finding a context Vector for each of the embedding vectors but for the sake of demonstration we are going to start with journey so we are going to exclusively look at Journey right now and we are going to find the context Vector for Journey and then we are going to to apply the same analysis to all the other words in this sentence uh so now as I mentioned we'll be focusing on the second element which is X of two why two because it's the second element of X so X1 is the first element X2 is the second element so we are looking at X2 because we are looking at the word Journey the element or the token which we are looking at right now it's also called as the query and since we are looking looking at the token Journey that becomes the query this uh terminology will also show up later when you look when we look at multi-head attention mechanism but for now uh just keep in mind that we are looking at this uh token journey and that becomes our query and the corresponding context Vector for this query which is X2 will be Z of two and that is essentially an embedding which contains information about X2 and all the other input elements so this context Vector not only contains information about that particular word which is Journey but it also contains information about all the other input elements again I would like to emphasize here that in this lecture we are not looking at trainable weights later we are going to add trainable weights which is how it's actually done in llms because this helps the llms to understand the context text much better and learn in a better way uh okay now we have this task that we we have a query which is the word journey and the task is to convert this the embedding Vector for journey into a context Vector the first task the first step of implementing this task is to compute the intermediate values W which are also referred to as the attention scores so the first task is to basically do the following we have a query which is the word journey and we have all these other input words right now what we have to quantify basically is that how much importance should be paid to each of these other words or how much attention should be paid to each of the input word for the query journey and this is Quantified by a mathematical metric which is called as the attention score so the attention score exist exists between the query and every input Vector so there will be an attention score between the query and the first input Vector there will be an attention score between the query and the second input Vector there will be an attention score between the query and the third input vector and finally there will be an attention score between the query and the final input Vector the first step of getting to the context Vector is to find these attention scores which will help us understand how much importance should be given to each of the tokens in the input okay for a moment assume that you do not know anything about large language models you do not know anything about machine learning or natural language processing just assume that you are a student who is Guided by intuition and Mathematics and try to think of this question you have this input query X2 and you have these other embedding input embedding vectors how would you find the importance of every input Vector with respect to the query that's the question so you have the query vector and you have each of these embedding vectors X1 X2 dot dot dot right up till the final input Vector how would you find a Score which quantifies the importance between the embedding between the query vector and the input embedding Vector can you try to think about this intuitively forget about everything else forget about ml forget about attention just try to think from the basics what is that is that mathematical operation which you will consider to find the importance between two vectors let me ask you this question in another way as a hint take a look at this Vector embedding you know that vectors are embedded in like this in a three-dimensional space now let me ask you we have the query Vector which is and we have all these other input vectors for step your with one and starts how would you find the importance of all the other vectors with respect to the query Vector which is Journey you can pause the video here for a while while you think about it let me give you a hint if I rephrase this question in another manner I'm sure many of you will be able to answer what is that mathematical operation which gives you the alignment between the two vectors which mathematical operation lets you find whether two vectors are aligned with each other or whether they are not aligned with each other keep that thought in your mind now let me nudge you more towards the answer now we know that uh the embedding vectors encode meaning right so if two vectors are align to each other which means that they are parallel to each other or if their angles are closer to each other like journey and start it implies that they have some sort of similarity in their meaning so it makes sense that more importance should be paid to start Vector because it seems to be more aligned with the journey Vector whereas if you take the vector corresponding to one this purple Vector at the bottom you'll see that it's almost perpendicular to the journey Vector right the vector one is like this the vector journey is like this so it's almost perpendicular which means that they do not have that much similarity in meaning so when we assign importance probably the vector corresponding to one should have less importance than the vector corresponding to starts for the query Journey because starts is more aligned to Journey now can you think which mathematical operation would quantify this alignment let me reveal the answer it's the dot product between the vectors this this is an awesome idea because Let Me Now go to Google and type dot product formula if you see the dot product formula it basically the if you take the dot product of the two vectors it's the product of the magnitude of the vectors multiplied by the cosine of the angle between them so if the two vectors are aligned with each other that means the angle between them is zero and if the angle between them is zero cost of 0 will be one so the dot product will be maximum whereas if the two vectors are not all aligned with each other that means they are perpendicular to each other the angle between the two vectors is now equal to 90\u00b0 and COS of 90 is equal to 0 so there is no similarity between these vectors and the dot product is zero so higher the dot product more aligned the vectors are lower the dot product the vectors are not aligned so the dot product actually encodes the information about how aligned or how not aligned the vectors are and that's exactly that's exactly what we need so the dot product between journey and starts might be more so it's because they are aligned so what if I use the dot product to find the attention scores that's the first key Insight which I want to deliver from this lecture what if I use the dot product to find the attention score between my query vector and the input Vector this is a key step in understanding the attention mechanism the dot product is that fundamental operation because it encodes the the meaning of how aligned or not how not or how far apart the vectors are and this is exactly what we are going to do next so the intermediate attention scores are essentially calculated between the query token and each of the input token and the way the attention scores are calculated is that we take a DOT product between the query token and every other input token why do we take a DOT product because the dot product essentially quantifies how much two vectors are aligned if two vectors are aligned their dot product is higher so more attention should be paid to this pair of vectors so their attention score will be higher so in the context of self attention mechanisms dot product determines the extent to which elements of a sequence attend to one another this is just a fancy way of saying how different elements of the sequence are more Alik with each other so higher the dot product higher the dot product higher is the similarity and the attention scores between the two elements this is very important higher the dot product higher is the similarity between the two elements or the two vectors and higher is the attention score a fancy way of describing two vectors which are aligned to each other is saying that these two tokens attend more to each other whereas if the two vectors are not not aligned we can say that these two attend less to each other for example just visually we can see that journey and starts are aligned right so these two vectors attend more to each other so more attention should be paid to starts when the query is Journey and if you look at the vector one and the vector for Journey you'll see that they are not aligned they have a 90\u00b0 angle between them so they do not attend to one another this is the key idea behind calculation of the attention scores and this is exactly what we are going to implement in the code right now so to compute the attention score between the query vector and the input Vector we simply have to take the dot product between these two vectors so let's say the query is inputs of one why inputs of one because python has a zero based indexing system and since our query is the second input token uh the second input token remember is for a journey it will be indexed with one because python has a zero indexing system so inputs of one will be the vector for Journey so let's say the query is the inputs of one and then the attention scores need to be calculated so first we initialize the attention scores as an empty tensor then what we'll do is that we Loop over the inputs we'll Loop over the inputs and we'll take the dot product between every input vector and the query Vector that's it and then we'll populate the attention scores tensor so the first element of the attention scores tensor is the dot product between the first input embedding vector and the query Vector the second element of the attention scores tensor is the dot product between the second input vector and the embedding Vector similarly the Sixth Element in the attention score tensor is the dot product between the sixth input vector and the query Vector so each of these element each of the elements of the attention score is a DOT product between the input vector and the query Vector now let us actually look at these attention scores a bit and try to uh look at their magnitudes right so which which of these have the largest magnitude so we can see that this second second value the third value and the sixth value have the largest attention scores so second third and six so let's see the which which words they correspond to so second is the word journey Third is the word starts and sixth is the word step so of course Journey has the high attention score right because the query itself is Journey so if the query itself is Journey it will be aligned with the vector for Journey and so it will have the highest dot product but the second and the third highest dot products are for starts and step and let's see whether that follows our intuition so as we had earlier seen starts is also very closely aligned with journey so it makes sense that the dot product between journey and starts is higher so the attention score for starts is higher similarly when you see step you'll see that step also seems to be closely aligned with journey their angles seem to be similar and so the attention score between step and journey will also be higher now let's look at the elements with the lowest attention score so it seems to be the fifth element and the fifth element is the word one and let's see whether that makes sense with our intuition so you can see the vector for the word one and the vector for Journey almost have a 90\u00b0 angle between them they are not at all aligned with each other and that is exactly captured in the attention score that seems to be the least between journey and one so every time you deal with attention scores try to have a mental map of why exactly is the attention score higher or why exactly is the attention score lower now we'll move to the third step or the next step rather before we comp compute the context vector and that step is normalization why do we really need normalization the most important reason why we need normalization is because of interpretability what does that mean well what it means is that when I look at the attention scores I want to be able to make statements like okay give 50% attention to starts give uh 20% attention to step and give only 5% ATT attention to the vector one so when the query is Journey I want to make these kind of interpretable statements in terms of let's say percentages that if the query is Journey of course Journey will receive 20% attention 30% attention but starts will also receive higher attention maybe 30% maybe step receives 20% attention and the rest of the vectors remain receive less percent if I'm conveying this to someone they will get a much better idea right if I convey in terms of these percentages so I want my attention scores to be interpretable and they are not right now because if you sum up the attention scores they are more than one so we cannot express these in terms of percentages and that's why we are now going to normalize the attention scores uh now first okay so the main goal behind the normalization is to obtain attention weights that sum up to one that's the main goal and why do we do this as I mentioned it's useful for interpretability um and for making statements like in terms of percentages how much attention should be given Etc but the second reason why we do normalization is because generally it's good if things are between zero and one so if the summation of the attention weights is between zero and one it helps in the training we are going to do back propagation later so we need stability during the training procedure and that's why normalization is integrated in many machine learning framework It generally helps a lot when you are back propagating and including gradient descent for example so uh what is the simplest way to implement normalization can you try to think about it remember we want all of these weights to sum up to one so what is the best way you can normalize this you can pause the video for a while if you want okay so the simplest way to normalize this is to just sum up these weights and then divide every single element by the sum so what that will do is that will make sure that the summation of all the attention scores will be equal to one and this is exactly what we are going to implement as the simplest way to normalize so what we'll do is that we'll maintain another tensor which is attention weights to that will just be the attention scores divided by the summation so what will happen is that every element of the attention score tensor will be divided by the toal total summation and so the final tensor which we have which are also called as the attention weights will be this it will be 0455 2278 2249 Etc and you'll see that they sum up to one here I would like to mention one terminology and that is the difference between attention scores and attention weights both attention scores and attention weights represent the same thing intuitively they mean the same the only difference is that all the attention weights sum up to one whereas that's not the case for attention scores so if you take the summation of these attention weights you'll see that it is equal to one um okay this is great so you might think awesome what's the next step well it's not that great because uh there are better ways to do normalization many of you might have heard about soft Max right if you have not it's fine I'm going to explain right now but when we consider normalization especially in the machine learning context it's actually more common and advisable to use the softmax function for normalization and why is this the case um I think I need to write this down on the Whiteboard to explain um why soft Max is preferred compared to let's say the normal summation especially when you consider extreme values so let me take a simple example right now and I'm going to switch the color to Black so that you can see what I'm writing on the screen so let's say the element which we have are 1 2 3 and nine uh or let's say 400 let's say these are the elements 400 now if you do the normal summation uh and normalize it that way what will what will happen is that 1 will be divided by all the entire summation right so the first element will be 1 divided by 1 + 2 + 3 + 400 which is 406 uh so the denominator here will be 4 0 6 then the last element similarly would be the highest element which is the extreme value which we are considering here this highest element will be 400 divided by 406 so I'm just writing the denominator right now yeah so the last element which is the extreme value will be 400 divided 46 and this element will be 2 divided by 406 and this element will be 3 divided 46 so you might think that okay what is the problem here the problem here is that when we look at the inputs 400 is extremely high right so the normalization should convey this information that you should completely neglect these other values so ideally when such a situation occurs we want the normalized values to be zero for these smaller values and we want the normalized value to be one for this extremely high value and that's not the case when you use the summation operation when you do the summation this normalized will be around let's say uh I not I'm not calculating this exactly but let's say it's 0 point uh 0 0 25 and let's say this this calculation for the extreme value is let's say around uh Point uh 9 let me write this again so this will be let's say around 0.9 9 just as an example so you might think that okay this is almost close to one right but it should not be almost close to one it should almost be exactly equal to one the reason why this is a problem is that when you get values like this they are not exactly equal to zero so when you are doing back propagation and gradient descent it confuses the optimizer and the optimizer still gives enough or weight some weightage to these values although we should not give any weightage to these values so ideally the normalization scheme should be such that when we normalize the small values should be close to zero in such a case and the extremely large values should be close to one and that is not achieved through this summation based normalization however this exact same thing is achieved if we do a softmax based normalization so let me explain what actually happens in the softmax based normalization so we currently have the attention scores like these right we have X1 X2 dot dot dot up till X6 in the softmax what happens is that we take the exponent of every element and then we divide by the summation of the exponents so the denominator the summation is e to X1 plus e to X2 plus e to X3 plus e to X4 plus e to X5 plus e to X6 so the first element will be e to X1 divided by the summation the second element will be e to X2 divided by the summation and similarly the last element will be e to X6 divided by the summation now if you add add all of these elements together you'll see that they definitely sum up to one that's fine but the more important thing is when you look at these extreme cases if you in if you use soft Max 400 now when you do e to 400 that will almost be like Infinity so this value when normalized will be very close to one and the smaller values when normalized using softmax will be very close to zero so it's much better to use softmax when dealing with such extreme values which may occur when we do large scale uh models like llms that's why it's much more preferable to be using soft Max now we can easily code such kind of a soft Max in Python ourself but it's much more recommended to use the implementation provided by py torch so what py torch does is that instead of doing e to x divided by the summation it actually subtracts the maximum value from each of the values before doing the exponential operation so the first element will be e to X1 minus the maximum value among X1 X2 X3 X4 X5 X6 divided by the summation and the summation will be e to X1 - Max Plus e to X2 - Max Etc so the only difference between pytorch implementation and the naive normal implementation we saw before is that pytorch includes this minus maximum so it subtracts all the values by the maximum value uh before doing the exponent IAL operation now mathematically you'll see that when you expand this e to X1 minus the maximum it can also be written as e to X1 divided e to maximum so the E to maximum in the numerator and denominator actually cancels out and the final thing what we get is actually the same thing as what we had written previously so you might be thinking then why is this even implemented this subtracting by the maximum the reason it's implemented is to avoid void numerical instability when dealing with very large values or very small values if you have very large values as in the input or very small values It generally leads to numerical instability and leads to errors which are called overflow errors this is a computational problem so theoretically we can get away with this implementation which is on the screen right now but when you implement computation in Python it's much better to subtract with the maximum to prevent overflow problems I have seen some companies in which this is also asked as an interview question so it's better to be aware of how pytorch Implement softmax and we'll be doing both of these implementations in the code right now so let me jump to the coding interface as I said the first thing which we'll be implementing is naive soft Max without dealing with the Overflow issues without subtracting the maximum value um so what we'll be doing is simply taking the exponent of each attention score and dividing by the summation of the exponent the dimension equal to Z is used because we are taking the summation for a full row so we are essentially summing all the entries in a row and that's why we have the dimension equal to zero why are we summing the entries in a row because if you look at these Matrix over here um yeah this X1 X2 X3 X4 X5 and X6 this is one row right and when we calculate the summation we are going to do e to X1 plus e to X 2 Etc so we are essentially summing all the entries in a row and that's why we have to give Dimension equal to zero as an exercise you can try out Dimension equal to one and see the error which you get so this is how we'll implement the knive soft Max and uh which will basically take the exponent of each attention score and divide by the summation so we'll print out the attention weights obtained using this method and you'll see that it's 1385 dot dot do1 1581 of course these scores are different than the summation because here we are using the exponent operation but if you sum up these scores you'll see that they also sum to one that's awesome and now what we'll be doing is we'll be using the P torch implementation of soft Max one more key point to mention is that the soft Max attention weights which are obtained are always positive because we always deal with the exponent operation and that's positive the positivity is important to us because it makes the output interpret so if we want to say that give 50% attention to this token give 20% attention to this token we need all the outputs to be positive right if it's not positive well uh it's it becomes very difficult to interpret in that case uh now um so note that knife softmax implementation May encounter numerical instability problems which I mentioned to you such as overflow for very large values and underflow for very small values therefore in practice it's always advisable to use the pytorch implementation of soft Max so it just one line of command torch. softmax and we pass in the attention score tensor and what this one line of command torge do softmax actually does is that it implements um what I'm what I've shown on the screen right now it implements this e to X1 minus maximum Etc this it converts the input into this normalized format so this is tor. softmax and you can quickly go to the documentation to see tor. softmax and you'll see the py torge documentation for for softmax I'll be sharing this link also with you uh in the information section of the YouTube video okay so this is basically pytorch softmax and we have got these attention weights and we'll see that the attention weight up to one one thing I want to show is that our knife soft Max results in this attention weight tensor and the pytorch softmax also results in the exact same attention weight tensor since we don't have any large values or any small values we don't have any overflow or underflow issues here and so both our knife implementation and the pytorch implementation give the same results but the reason it always advisable to use py torch is that later we'll be dealing with very large parameters and some of those might be huge so it's better to um have numerical or better to deal with numerical instability awesome so remember the one reason why we calculated the attention weights is for interpretability right now let's try to interpret so the attention weight for the first Vector is around .13 the attention weight for the second is 2379 attention for the third is 233 Etc this means that we should pay about 13% attention to the word your about 23% attention to Journey about 23% attention to starts 12% attention to with uh with one 10% attention to one and 15% attention to step so high attention is being paid to journey and starts so as you can see here High attention is paid to journey and starts and if someone asks how high we can can say that well about 20% and low attention is paid to one and if someone asks how low we can say that well only 10% we are able to make this interpretable statements only because we converted the attention scores into attention weights and remember that's the difference between attention scores and attention weights attention weights sum up to one so it's much easier to make this kind of interpretable statements so we have computed the attention weights right now and we are actually ready to move to the next step which will which is actually the final step of uh Computing the context Vector so let me take you to that portion of the Whiteboard right now so after Computing the attention weights now we have actually come to the last step of finding the context Vector so let me just show you the image of what all we have covered up till now so we had this input query we computed the attention scores by taking the dot product between the input query and each of the embedding vectors and from the attention scores we actually got the normalized attention scores which are called as attention weights and these attention weights actually sum up to one awesome so we have reached this stage right now and now we are ready to get the context Vector I just want to intuitively and Visually show you how we calculate the context vector before coming to the mathematical implementation so let's say these are the embedding vectors for the different words in the sentence and here I've also mentioned the relative importance of each so Journey carries 25% importance because that's the query so it should carry the highest importance but starts carries 20% importance um then step carries 15% importance and one with and your carry less importance now how do we get these how do we use these attention weights to compute the ultimate context Vector so the way this is done is that let's say we use starts so the starts Vector is now multiplied by the attention contribution and that is equal to 02 right because 20% is important so it's importance is 20% so the starts Vector is multiplied by 02 which means it scaled by by the corresponding attention attention weight so the starts Vector is multiplied by 02 so it will be scaled down like this the width vect width Vector is scaled down by 15 because it carries 15% importance the step Vector carries 15% importance the journey Vector carries 25% importance so it will it will be scaled down by 1/4 uh and the one vector carries only 10% importance so it will be scaled down by a lot which is about 1110th now what will and similarly your carries 15% importance so it will be scaled down by multiplied by5 so what we do is that we multiply each of the input embedding vector by the corresponding attention weights and we scale them down that much so now we have uh the multiplied weights for each of these and we take the vector summation of all of these and you add the vector summation and that gives the final context Vector like this so this is now my cont context Vector for Journey so this is the context Vector let me write it down again okay so let me explain this again so we have calculate we have multiplied each of the input embedding Vector with the corresponding attention weight and we have got these six vectors right we sum up all of these six vectors and that uh now describes the context vector and this I'm just writing here context this is the context Vector for the embedding Vector of Journey now look at how this context Vector is calculated the context Vector has some contributions from all other vectors so it's an enriched embedding Vector it has 25% contribution from the embedding Vector but it has all the other contribution from other vectors and those contributions symbolize something those contributions symbolize how much importance is given to the other vectors for example we have about 20% contribution from starts because the attention weight to starts is 0.2 we have only 10% contribution from one because the attention weight for one is 0.1 and that's why context vectors are so important I wanted to show this to you visually because the context Vector which will calculate for modern llms for large scale models like GPT they carry the exact same meaning they are enriched embedding vectors so the context Vector for Journey looks like this and we can also see this in code so I have just written a small code to find the to plot the context Vector I'll show the mathematical derivation but for now just take a look at this context Vector for Journey which has been shown in Red so the vector embedding for Journey has been shown in green but the context Vector is shown in red which is the summation of all the other vectors which I just showed to you and this is how it looks like in the three-dimensional space ultimately we are interested in these context vectors and it was only possible to get this context Vector because of the attention mechanism that's why the attention mechanism is so important we would have been stuck at the embedding Vector if we did not have the attention mechanism now let us look at the mathematical representation um regarding how we actually compute the context Vector from the attention weights and if you have understood the Whiteboard description which I just showed you understanding the this mathematical operation will actually be very easy okay so we have reached this stage now where we have computed these attention weights and after Computing the normalized attention weights what we'll do is that we'll compute the context vector and currently we're looking at the context Vector for Journey so it's Z2 and to do that we'll multiply all the embedded in input tokens with the corresponding attention weights this is very important and so that was the scaling which I showed you in the figure and then we will sum up all of the resultant vectors when we sum up all of the resultant vectors that will give us the final context Vector for the token journey and this has been showed in this schematic right now let me just rub rub this here so that I can explain this to you in a better manner okay so we have the attention weights for every token right so we have1 2 Etc so what we'll do is that for the first input embedding we'll multiply the attention weight with this Vector for the second input embedding we'll multiply the attention weight for the second input embedding with the second Vector we'll multiply the third attention weight with the third input embedding and similarly we'll multiply the sixth attention weight with the sixth input embedding this is scaling down the vectors in the victorial representation and then we'll add all of them together when we add all of them together we'll ultimately get the context Vector for Journey and this is the final answer this is the context Vector for Journey and I've have plotted this context Vector over here which has been calculated through this uh mathematical operation which I just showed you on the screen and now we'll be implementing this operation in Python to calculate the context vector and it's pretty simple it's only two to three lines of code first we have the query which is the inputs index by one because the word which we are looking at is Journey then we initialize uh tensor context Vector two why two because we are looking at the second token journey and we are finding the context Vector for that so what we'll be doing is that we'll be looping through all the inputs and uh what we'll be doing is that we'll scale each input with the corresponding attention weight and then we'll add all the scaled vectors together to give the final context Vector that's it so let's say we are looking at the first input Vector which is the first input embedding we'll multiply it with the first attention weight then we'll look at the second input Vector we'll multiply it with the second attention weight then we'll look at the sixth input Vector at the end and multiply it with the sixth attention weight and we'll add all of these vectors together which ultimately leads to the final context vector and that's the one which I've showed here in the red arrow I've also shown the any context here as a ping Dot and how it's different from the other vectors awesome so we have reached this step where we have calculated the context Vector for Journey right however the task is not yet over because we have to calculate a similar context Vector for all the other tokens right we have to calculate the similar context Vector for your journey starts with one step all of these six words and now if you have understood this computer comput which we did for Journey we can actually extend the exact similar computation to compute the attention weight and context Vector for all the other inputs and this is actually represented very nicely with this which is called as the attention weight Matrix so what the Matrix which you're seeing on the screen right now is called as the attention weight Matrix and let me explain uh this Matrix in a very simple manner so if you look at the rows each row represents the attention weights for one particular word So currently we have calculated the attention weights for Journey right so the first value here 13 is the attention score or the attention weight between journey and your the second value here 23 is the attention score or the attention weight between journey and journey the second value here is the attention weight between journey and starts the fourth value here is the attention weight let me yeah the fourth value here is the attention weight between journey and width the fifth value here is the attention weight between journey and one and the sixth value here is the attention weight between journey and step so these are the sixth atten ention weights which we also computed over here so these are the six attention weights which have been computed here we have just rounded off the values so the values might not be exactly similar but these are the uh these are the six attention weights okay now um let us go next okay so similarly what we have to do is we have to find essentially similar attention weights for all the other words like let's say if we look at starts we have to find six attention weights for starts we have to find six attention weights for width we have to find six attention uh weights for step and we have to find six attention weights for your and one so for every every query we have to find six attention weights so all of these which I'm highlighting with star right now all of these are the queries currently we only looked at the journey query but now we have to essentially replicate the exact same computation for all the other queries as well so how will we do this let's say if the query is Step we'll find the attention weight between step and all the other words and then we'll find the context vector by doing the summation operation like we did at the end for the query of Journey so essentially we are going to follow the exact same steps as before for all the other tokens also we are first going to compute the attention scores then we are going to compute the attention weights and then we are going to compute the context Vector remember these are the exact same steps which we followed uh for uh the query of Journey and these are the exact same steps which we will follow for other queries as well so let me take you through code right now and let us start uh implementing the attention or let us start calculating the these three steps for the other queries as well so as we discussed we have to follow three steps the first step is to find the attention scores and remember how do we find the attention score if we have a particular query we'll just take the do product of that with all the other input vectors so let's say if the query is in inputs we'll take the dot product of the query with all the other vectors in the input so one way to find the attention scores is to just Loop through the input two times and essentially find the dot product uh I'll show you what that actually means uh right so let me rub these these things over here so that I can show you this one method of finding the attention scores so let's say you Loop over the input Vector right so the first you'll encounter your then you will find the dot product between your and all these other uh all the other inputs so that will be uh the result of the first inner loop so what you do is that first you fix an i in the outer loop and in the Inner Loop you go through the input entirely so when we fix an i in the outer loop it means we fix this sarey then we go through the inner loop entirely and find these six dot products now change the outer loop so then the outer loop changes to journey and then similarly find the dot product between Journey and all the other vectors now change the outer loop once more so similarly we'll change the outer loop and in each outer loop we'll go through the inner loop so that is essentially finding the dot products which are the attention scores uh the problem with this approach is that this will take a lot of computational time so if you look at the output tensor so this is a 6x6 and each element in this tensor represents an attention score between two pairs of inputs so for example this 6x6 Matrix which you just saw in the code is very similar to this here I'm showing the normalized attention scores but even the attention scores look like the 6x6 so if you look at the first row all of those are the dot products between the first query and all the other queries if you look at the second row all of these are the dot products between the second query and uh all the other inputs so this second this second row actually will be exactly same to the attention uh scores which we had calculated earlier see because the second row is 9444 1.49 ETC so if you look at the second row here that is also 9544 1.49 because the second row represents the dot product between the second query and all the other input vectors similarly the last row represents the dot product between the last query and all the other input vectors okay now here we have used two for Loops right and that's not very computationally efficient for Loops are generally quite slow and that's the reason why matrix multiplication needs to be understood the reason I say that linear algebra is actually the core Foundation of every machine learning concept which you want to master is this for someone who does not know about linear algebra and matrix multiplication they'll just do these two rounds of four Loops but if you actually know linear algebra you'll see that instead of doing this you can just take the uh multiplication of inputs and the transpose of the inputs and you will actually get the exact same answer so what this does is that you take the input uh input Matrix and what the input Matrix looks like is this you take the input Matrix and then you multiply with the transpose of the input Matrix and you'll get the exact same answer as uh you'll take you'll get the exact same answer as doing this dot product in the for Loop format and you can verify this so if you just take the product between inputs Matrix and the inputs transpose what we'll see is the exact same thing as the previous answer why because when we multiply two matrices what it essentially does is it just computes a bunch of dot products between the rows of the first Matrix and The Columns of the second Matrix which is exactly what we are doing here in these two for Loops it just that this matrix multiplication operation is much more efficient than using these two for Loops so Step One is completed right now we have found the attention scores I hope you have understood why this is a 6x6 Matrix here and why each what each row represents each row represents the dot product between that particular query and all the other input um input embedding vectors now we'll implement the normalization so remember how we did normalization here we did torch. soft Max right so similarly what we are going to do here is we are going to do torge do softmax of this attention scores Matrix and what this will do is that it will implement the soft Max operation to each row so the first row we'll do the soft Max like we learned before then the second row we do the softmax like we learned before similarly the last row will do the soft Max like we learned before so if you look at each individual row you'll see that entries of each row sum up to one so you can look at the second row here1 385 2379 it's the same U attention weights which we have got for the journey query uh one key thing to mention here is that what is the dim parameter over here so the dim here I'm saying minus one and the reason is explained below the dim parameter in functions like like tor. softmax specifies the dimension of the input function input tensor along which the function will be computed so by setting dim equal to minus1 here we are instructing the softmax function to apply the normalization along the last dimension of the attention score tensor and what is the last dimension of the attention score tensor it's essentially The Columns so if the attention scores is a 2d tensor it's a 2d 6x6 tensor right and it has the shape of rows and columns uh the last Dimension is the column so Dimension equal to minus one will normalize across the columns so what what will happen is that for the first row look at the columns so this is the first column this is the second column actually I have to show here this is the First Column this is the second column this is the third column so we are normalizing essentially along the columns right because we are going to take the exponent of what all is there in the First Column second column third column Etc we are going to sum these exponents that's why it's very important to uh write this dim equal to minus1 because we are normalizing across a column um and that's why the values in one row sum up to one since we are normalizing in the each column that's why the values in each row sum up to one it's very important to note that so Dimension equal to minus1 means that we have to apply the normalization along the last Dimension and for a two dimensional t sensor like this the last Dimension is the columns so the soft Max will be applied across the columns and that's why for each row you will see that all the entries sum up to one so these are the attention weights which we have calculated and the last step which is very important is calculating the context vector and uh I want to uh show some things to you but before that let's verify that all the rows indeed sum up to one in this attention weights so what I'm doing here is that I'm looking at the second row here and I'm just going to sum up to one and I'm just going to sum up the entries of the second row and you will see that uh the second row sums up to one and I have also included a print statement below which prints out the summation of all the rows and you'll see that the first row the second row similarly the sixth row all of the rows essentially sum up to one this means that the softmax operation has been employed in a correct manner for you to explore this dim further you can try with dimm equal to Z dim equal to 1 also from the errors you will learn a lot these small details are very important other students who just apply llm Lang chain and just focus on deployment will never focus on these Minor Details like what is this dim operator over here Etc but I believe the devil always lies in the details so the students who understand these Basics will really Master large language models much more than other students now we come to the final step which is essentially Computing the context vectors right and I will take you to code but the final step is actually implemented in elegant one line of code let me take you to the final step before what we had done in the final step remember what we simply did was uh we just uh where was that yeah in the final step what we simply did was we just multiplied the attention weights for each Vector for each input Vector with that corresponding Vector right I can show this to you in the Whiteboard also okay so what we did for the final step of the context Vector was something like this yeah so what we did was we we got the attention weights for each input embedding vector and we multiplied those attention weights with each with the corresponding input vector and we those up now this is exactly what we have to do for the other uh for the other tokens also but we have to do this in a matrix manner because we cannot just keep on looping over and use for loops and there is a very elegant Matrix operation which actually helps us calculate the context vectors it's just one line of matrix product essentially we have to multiply the uh attention scores Matrix or the attention weight Matrix with the input right and we have to do some summations can you think of the matrix multiplication operation which will directly give us this answer um it's fine if you don't know the answer but the simplified matrix multiplication is just essentially multiplying the attention weights with the inputs that's it uh this last step of finding the context Vector is just taking this attention weight Matrix and multiplying it with the input Matrix and the claim is that it will give us the context vectors it will give us the six context vectors which we are looking for remember we need a context Vector for every token right and there are six tokens so here are the six context vectors now I'm going to try to explain why this matrix multiplication operation really works so let's go to the Whiteboard once more all right so this is the first Matrix which we have and that's the attention weights right here and this is the second Matrix which we have which is the inputs uh so keep in mind here that the attention weights is a 6x6 Matrix so we have six rows and six columns and the inputs is a 6x3 matrix now we have already looked at how to find the uh context Vector for the second uh for the second row right which which essentially corresponds to the word journey and uh so let's see what we exactly did here so the final attention Matrix will be a 6x3 matrix because so sorry the final context Vector Matrix will be a 6x3 matrix because every row of this will be a context Vector so the first row will be the context Vector for the first word the second row will be the context Vector for the second word so let's look at the second word which is essentially the context Vector for Journey now uh if we take a product of these two Matrix so let's say if we take the product of the attention weight Matrix and the input Matrix first let's check the dimensions so this is a 6x6 Matrix and the inputs is a 6x3 so 6X 6 can be multiplied with 6x3 so taking the product is completely possible and it will result in a 6x3 matrix uh so let's look at the second row if you look at the second row it will be uh something like we we'll take the second row uh so the second row First Column would be the dot product between the second row of this and the First Column of this the second row second column will be the dot product of this with the second column of this and the second row third column will be the dot product of this second row and the third column here so that's what I've written here in the output Matrix so the first element of the second row will be the dot product between the second row and the First Column the second element of the second row will be the dot product between the uh second row and the second column and the third element of the second row will be the dot product between the third row of the first Matrix and the third column of the second Matrix right now when you compute these dot products uh very surprisingly you will see that the answer is actually equal to this the answer is 138 the answer is actually 138 which is 138 multiplied by the first row over here plus 237 multiplied by the second row over here plus 233 which is multiplied by the third row over here Etc uh it's just a trick of matricis but what it it turns out that this second row second row can also be represented by this formulation where you take the uh first element of this second row multiply it with the first row of the input Matrix plus the second element of the second row multiply with the second row of the input Matrix plus the third element multiply it with the third row of the input Matrix can you see what we are essentially doing here we essentially scaling every input Vector right we take the first input Vector we take the first input Vector we scale it by 138 we take the second input Vector we scale it by 237 we take the third input Vector we scale it by 233 isn't this the exact same scaling operation which we saw uh when we looked at the visual representation of the uh context Vector calculations remember we have seen seen the scaling operation to calculate the context Vector here where we had taken each of the input vectors and we had scaled it by the attention weight values and then we summ them to find the final context Vector this is the exact same thing which I which we are doing over here so when we take the product of the uh when we take the product of the attention weights and the inputs another way to look at it is that if you look at the second row it's actually scaling the first input by 138 scaling the second input by 237 dot dot dot and scaling the sixth input by 0158 so it's the exact same operation as we performed before for finding the context vector and that's why finding the context vectors is as simple as multiplying the attention weights with the inputs the first row of this answer will give you the context Vector for the first uh input embedding Vector the second row will give you the context Vector for the second token the third row of this product will give you the context Vector for the second token for the third token and right up till the very end the sixth row will give you the context Vector for the final token and this is how the product between the attention weights and the inputs will give give you the final context Vector Matrix which contains the context Vector for all of the tokens which you are looking for and uh with this final calculation we calculate the context Vector for all um of the input tokens and this is exactly what I've have tried to do here so finally uh we generate a tensor which is called as the all context vectors and we multiply the attention weight Matrix with the input Matrix and then we get this all context Vector tensor and if you look at the second row here 4419 6515 56 you'll see that this is exactly the same value of the context Vector which we had obtained over here uh when we looked at Journey so this again implies that whatever we are doing here uh with the matrix multiplication is leading to the correct answer so based on this result we can see that the previously calculated context Vector 2 for journey matches the second row in the in this tensor exactly so remember this operation to get the final context Vector we just multiply the attention weights with the inputs if you did not understand the matrix multiplication which I showed on the Whiteboard I encourage you to do it on a piece of paper because this last matrix multiplication is very important to get the context Vector we just have to multiply the attention weight Matrix with the input Matrix and what all you are learning right now will Direct L extend to the key query and value concept which we'll cover when we when we come to causal attention and multi-head attention and even in the next lecture in the next lecture we are going to look at this exact mechanism but with trainable weights then we'll come to the concept of key query and value but these operations which we are looking at here so for example uh here we are taking the matrix product between attention weights and inputs right in the key query value this will be replaced this inputs will be replaced by value we'll also have a key and a query but the underlying intuition and the underlying mechanism is exactly the same so if you understand what's going on here you'll really understand key query value very easily okay one last thing which I want to cover um at the end of today's lecture is that okay so you might think that we already then find the context vectors like this right then what's the need for trainable weights we just take the dot product and we then find these context vectors the main problem with the current approach is that think about how we found the attention weights to find the attention weights all we did was to just take the dot product right so currently uh in our world the reason why we are giving more attention to starts is that the alignment between starts and journey is maximum so the only reason why we are giving more attention to starts is because because it semantically matches with journey because we are only getting the attention scores and attention weight from the dot product however that is not correct right because two vectors might not be semantically aligned but maybe they are more important in the context of the current sentence so for example journey and one are not semantically related to each other but what if in the current context one is the is the vector which is more important so apart from the meaning you also need to capture the information of the context right what is happening in the current sentence and without trainable weights it's not going to happen we are not going to capture the context effectively right now we did manage to capture the context somewhat but we only give attention to Words which are similar in meaning to the query but even if a word is not similar in meaning it still might deserve attention in the context of the current sentence so let's take a simple example here okay so the example is the cat sat on the mat because it is warm and let's say our query is warm so in the first case let's say we do not use trainable weights like what we have done in today's lecture if we don't use trainable weight we only take the dot product between the query warm and each words embedding and we'll find that warm is most similar to itself and maybe somewhat related to mat words like the cat and sat might have low similarity scores because they are not semantically related to or so with with this so if we don't consider trainable weights we'll only look at Words which are more similar to this query which is warm now with trainable weights the model can learn that warm should pay more attention to mat even if mat is not semantically related to warm so what will happen without trainable weights is that mat and warm might be vectors which are like this which have a 90\u00b0 angle and they might not be related because their meaning is not related but that does not mean we should not pay attention to mat because in this context probably Matt is the most important if the query is warm because the mat is warm but the meaning of mat and warm are not not related right that's why we need trainable weights with trainable weights the model can learn that warm should pay more attention to Matt even if mat isn't semantically similar to warm in traditional embedding space so this is where important the trainable weight allows the model to learn that warm often follows mat in context like this one and that's how it captures long range dependencies that is the reason why we need trainable weights without trainable weights this meaning would be lost and we would only be looking at Words which are similar to The query by trainable weights we get more more of this information that okay Matt might not be related in meaning but in the current context mat is the word which is more important because the mat is warm this is how trainable weights allow us to capture context and in the next lecture we'll specifically devote the next lecture to uh the simplified self attention mechanism but with trainable weights so here is the lecture notes for the next lecture self attention mechanism with trainable weights we'll introduce the concept of key query value and then slowly we'll move to the concept of uh causal attention and then we'll move to the concept of multi-head attention so up till now we have covered simplified self attention I know this lecture became a bit long but it was very important because uh I have seen no other lecture or no other material which covers this much detail visually theoretically and in code about the attention mechanism I could have directly jumped to key query and value which will come later but then you would not have understood the meaning but this lecture allowed me to build your intuition I hope you're liking these set of lectures if you have any doubts or any questions please ask in the YouTube comment section and I'll be happy to reply thank you so much everyone and I really encourage you to take notes while I'm making these lectures I'll also share this code file with you um thanks everyone and I look forward to seeing you in the next lecture"
}