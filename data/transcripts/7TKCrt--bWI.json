{
  "video": {
    "video_id": "7TKCrt--bWI",
    "title": "Measuring the LLM loss function",
    "duration": 3374.0,
    "index": 25
  },
  "segments": [
    {
      "text": "[Music]",
      "start": 0.0,
      "duration": 7.12
    },
    {
      "text": "hello everyone and uh welcome to this",
      "start": 7.6,
      "duration": 4.48
    },
    {
      "text": "lecture in the build large language",
      "start": 10.16,
      "duration": 5.16
    },
    {
      "text": "models from scratch Series today we are",
      "start": 12.08,
      "duration": 5.6
    },
    {
      "text": "going to look at a very important topic",
      "start": 15.32,
      "duration": 5.16
    },
    {
      "text": "and that is regarding loss functions",
      "start": 17.68,
      "duration": 5.4
    },
    {
      "text": "when you look at machine learning and",
      "start": 20.48,
      "duration": 4.44
    },
    {
      "text": "traditional machine learning models such",
      "start": 23.08,
      "duration": 4.4
    },
    {
      "text": "as regression and classification the",
      "start": 24.92,
      "duration": 4.64
    },
    {
      "text": "loss function for these type of models",
      "start": 27.48,
      "duration": 4.639
    },
    {
      "text": "are pretty well defined for regression",
      "start": 29.56,
      "duration": 4.679
    },
    {
      "text": "you generally use the least Square type",
      "start": 32.119,
      "duration": 4.841
    },
    {
      "text": "of a loss mean square error for",
      "start": 34.239,
      "duration": 4.881
    },
    {
      "text": "classification you use cross entropy",
      "start": 36.96,
      "duration": 5.2
    },
    {
      "text": "cross entropy loss hinge loss",
      "start": 39.12,
      "duration": 5.56
    },
    {
      "text": "Etc however the question is that when we",
      "start": 42.16,
      "duration": 4.96
    },
    {
      "text": "come to large language models how do we",
      "start": 44.68,
      "duration": 5.08
    },
    {
      "text": "Define the loss function how do we",
      "start": 47.12,
      "duration": 4.52
    },
    {
      "text": "measure whe whether the large language",
      "start": 49.76,
      "duration": 4.2
    },
    {
      "text": "model is doing a good job or whether",
      "start": 51.64,
      "duration": 5.0
    },
    {
      "text": "it's not performing well let's find out",
      "start": 53.96,
      "duration": 3.84
    },
    {
      "text": "in today's",
      "start": 56.64,
      "duration": 4.12
    },
    {
      "text": "lecture so up till now in this series we",
      "start": 57.8,
      "duration": 5.8
    },
    {
      "text": "have covered stage one of building a",
      "start": 60.76,
      "duration": 5.2
    },
    {
      "text": "large language model from scratch in",
      "start": 63.6,
      "duration": 4.96
    },
    {
      "text": "stage one we essentially covered three",
      "start": 65.96,
      "duration": 4.88
    },
    {
      "text": "subm modules data preparation and",
      "start": 68.56,
      "duration": 5.48
    },
    {
      "text": "sampling attention mechanism and llm",
      "start": 70.84,
      "duration": 5.12
    },
    {
      "text": "architecture in each of these",
      "start": 74.04,
      "duration": 4.24
    },
    {
      "text": "subcomponents we divide we devoted a",
      "start": 75.96,
      "duration": 5.6
    },
    {
      "text": "huge number of lectures and whiteboard",
      "start": 78.28,
      "duration": 6.159
    },
    {
      "text": "notes and also coding so that by the",
      "start": 81.56,
      "duration": 4.96
    },
    {
      "text": "time you have reached this stage I hope",
      "start": 84.439,
      "duration": 4.68
    },
    {
      "text": "the building blocks of how to construct",
      "start": 86.52,
      "duration": 5.959
    },
    {
      "text": "a large language models are clear to you",
      "start": 89.119,
      "duration": 5.441
    },
    {
      "text": "in particular till the last lecture we",
      "start": 92.479,
      "duration": 5.161
    },
    {
      "text": "have seen this GPT or the llm",
      "start": 94.56,
      "duration": 5.36
    },
    {
      "text": "architecture which we have built from",
      "start": 97.64,
      "duration": 5.32
    },
    {
      "text": "scratch and up till now we we are at a",
      "start": 99.92,
      "duration": 5.68
    },
    {
      "text": "stage where the GPT architecture which",
      "start": 102.96,
      "duration": 4.28
    },
    {
      "text": "we have built and whose code we have",
      "start": 105.6,
      "duration": 4.199
    },
    {
      "text": "written takes an input it takes an input",
      "start": 107.24,
      "duration": 5.8
    },
    {
      "text": "text and it returns an output we also",
      "start": 109.799,
      "duration": 5.68
    },
    {
      "text": "saw how this output is converted into",
      "start": 113.04,
      "duration": 5.96
    },
    {
      "text": "the next word prediction task so our GPT",
      "start": 115.479,
      "duration": 6.401
    },
    {
      "text": "model up till this stage or our large",
      "start": 119.0,
      "duration": 4.88
    },
    {
      "text": "language model we which we have set out",
      "start": 121.88,
      "duration": 5.32
    },
    {
      "text": "to build is at a level where it takes an",
      "start": 123.88,
      "duration": 5.879
    },
    {
      "text": "input as a text and it can predict next",
      "start": 127.2,
      "duration": 5.24
    },
    {
      "text": "tokens or it can predict next words",
      "start": 129.759,
      "duration": 4.961
    },
    {
      "text": "that's awesome right but now we want to",
      "start": 132.44,
      "duration": 5.079
    },
    {
      "text": "make it better and better and better in",
      "start": 134.72,
      "duration": 5.12
    },
    {
      "text": "particular if you remember uh where we",
      "start": 137.519,
      "duration": 4.72
    },
    {
      "text": "ended the previous lecture on llm",
      "start": 139.84,
      "duration": 5.679
    },
    {
      "text": "architecture we got this kind of the so",
      "start": 142.239,
      "duration": 6.241
    },
    {
      "text": "we gave the input as hello I am and the",
      "start": 145.519,
      "duration": 4.561
    },
    {
      "text": "next tokens which we got are or",
      "start": 148.48,
      "duration": 3.039
    },
    {
      "text": "something completely",
      "start": 150.08,
      "duration": 5.0
    },
    {
      "text": "random so now from this lecture onwards",
      "start": 151.519,
      "duration": 5.161
    },
    {
      "text": "what we'll be doing is that we'll be",
      "start": 155.08,
      "duration": 3.439
    },
    {
      "text": "looking at stage",
      "start": 156.68,
      "duration": 4.16
    },
    {
      "text": "two stage two of building a large",
      "start": 158.519,
      "duration": 4.601
    },
    {
      "text": "language model just centers around one",
      "start": 160.84,
      "duration": 4.36
    },
    {
      "text": "word and I'm just going to write that",
      "start": 163.12,
      "duration": 6.479
    },
    {
      "text": "one word here and that is called as",
      "start": 165.2,
      "duration": 4.399
    },
    {
      "text": "training we need to train our large",
      "start": 169.959,
      "duration": 4.64
    },
    {
      "text": "language model so that the next token",
      "start": 172.44,
      "duration": 4.24
    },
    {
      "text": "which it predicts makes",
      "start": 174.599,
      "duration": 4.36
    },
    {
      "text": "sense the first step of the training",
      "start": 176.68,
      "duration": 4.24
    },
    {
      "text": "procedure is that you you need to ask",
      "start": 178.959,
      "duration": 4.84
    },
    {
      "text": "yourself okay let's say my large",
      "start": 180.92,
      "duration": 6.12
    },
    {
      "text": "language model is has is giving me an",
      "start": 183.799,
      "duration": 6.52
    },
    {
      "text": "output and I know it does not look good",
      "start": 187.04,
      "duration": 5.32
    },
    {
      "text": "how do I capture this qualitative",
      "start": 190.319,
      "duration": 4.721
    },
    {
      "text": "intuition of not of knowing that it does",
      "start": 192.36,
      "duration": 6.28
    },
    {
      "text": "not look good into a quantitative matric",
      "start": 195.04,
      "duration": 5.24
    },
    {
      "text": "and that quantitative matric is",
      "start": 198.64,
      "duration": 4.48
    },
    {
      "text": "generally the loss",
      "start": 200.28,
      "duration": 2.84
    },
    {
      "text": "function so if we can define a loss",
      "start": 204.28,
      "duration": 5.28
    },
    {
      "text": "function which quantifies how good or",
      "start": 206.76,
      "duration": 6.08
    },
    {
      "text": "how bad our llm performance is we can",
      "start": 209.56,
      "duration": 6.12
    },
    {
      "text": "aim to minimize the loss so that the llm",
      "start": 212.84,
      "duration": 5.44
    },
    {
      "text": "can do better and better and better and",
      "start": 215.68,
      "duration": 4.68
    },
    {
      "text": "once we construct a loss function it",
      "start": 218.28,
      "duration": 5.039
    },
    {
      "text": "opens the door for integrating gradient",
      "start": 220.36,
      "duration": 4.76
    },
    {
      "text": "descent based back propagation",
      "start": 223.319,
      "duration": 3.881
    },
    {
      "text": "algorithms which we have already learned",
      "start": 225.12,
      "duration": 5.0
    },
    {
      "text": "in neural networks so then we convert",
      "start": 227.2,
      "duration": 4.72
    },
    {
      "text": "the problem of training a large language",
      "start": 230.12,
      "duration": 3.679
    },
    {
      "text": "model into a problem which we have",
      "start": 231.92,
      "duration": 4.0
    },
    {
      "text": "previously solved",
      "start": 233.799,
      "duration": 4.841
    },
    {
      "text": "before so what we are going to do in",
      "start": 235.92,
      "duration": 4.56
    },
    {
      "text": "these set of lectures we are going to",
      "start": 238.64,
      "duration": 3.84
    },
    {
      "text": "look at this entire pipeline of seven",
      "start": 240.48,
      "duration": 4.039
    },
    {
      "text": "steps which I have shown over here in",
      "start": 242.48,
      "duration": 3.599
    },
    {
      "text": "today's lecture we are going to look at",
      "start": 244.519,
      "duration": 4.321
    },
    {
      "text": "the first two steps here we are going to",
      "start": 246.079,
      "duration": 5.52
    },
    {
      "text": "look at how text is generated which we",
      "start": 248.84,
      "duration": 4.599
    },
    {
      "text": "have already seen before but I want to",
      "start": 251.599,
      "duration": 4.6
    },
    {
      "text": "quickly recap it in case some of you",
      "start": 253.439,
      "duration": 5.52
    },
    {
      "text": "have come to this lecture directly and",
      "start": 256.199,
      "duration": 4.481
    },
    {
      "text": "then we are also going to look at text",
      "start": 258.959,
      "duration": 3.761
    },
    {
      "text": "evaluation which means we are going to",
      "start": 260.68,
      "duration": 4.239
    },
    {
      "text": "define a loss function in this lecture",
      "start": 262.72,
      "duration": 4.8
    },
    {
      "text": "itself and we are going to see how that",
      "start": 264.919,
      "duration": 5.081
    },
    {
      "text": "loss function quantifies the loss L",
      "start": 267.52,
      "duration": 5.88
    },
    {
      "text": "between what our llm has generated and",
      "start": 270.0,
      "duration": 6.919
    },
    {
      "text": "the good output which we actually want",
      "start": 273.4,
      "duration": 5.16
    },
    {
      "text": "in the subsequent lectures we'll be",
      "start": 276.919,
      "duration": 5.041
    },
    {
      "text": "looking at taking an entire data set",
      "start": 278.56,
      "duration": 5.639
    },
    {
      "text": "feeding it into the llm getting the",
      "start": 281.96,
      "duration": 4.079
    },
    {
      "text": "output and finding the training and the",
      "start": 284.199,
      "duration": 4.201
    },
    {
      "text": "validation losses will then generate the",
      "start": 286.039,
      "duration": 4.201
    },
    {
      "text": "llm training function that's the back",
      "start": 288.4,
      "duration": 4.079
    },
    {
      "text": "propagation I was talking about and then",
      "start": 290.24,
      "duration": 4.64
    },
    {
      "text": "towards the end of this series on",
      "start": 292.479,
      "duration": 4.321
    },
    {
      "text": "training llms we'll also load",
      "start": 294.88,
      "duration": 4.24
    },
    {
      "text": "pre-trained weights from open AI all",
      "start": 296.8,
      "duration": 4.839
    },
    {
      "text": "that will come next for now let's just",
      "start": 299.12,
      "duration": 4.44
    },
    {
      "text": "start with these initial two goals for",
      "start": 301.639,
      "duration": 4.12
    },
    {
      "text": "today's lecture and that is text",
      "start": 303.56,
      "duration": 5.479
    },
    {
      "text": "generation using llms and text",
      "start": 305.759,
      "duration": 6.081
    },
    {
      "text": "evaluation first I want to quickly recap",
      "start": 309.039,
      "duration": 6.041
    },
    {
      "text": "how can we use the llm or the gpt2",
      "start": 311.84,
      "duration": 4.84
    },
    {
      "text": "architecture which we have trained so",
      "start": 315.08,
      "duration": 5.36
    },
    {
      "text": "far to generate text okay so the way it",
      "start": 316.68,
      "duration": 7.519
    },
    {
      "text": "works is that we have to specify a GPT",
      "start": 320.44,
      "duration": 5.759
    },
    {
      "text": "configuration uh and this GPT",
      "start": 324.199,
      "duration": 3.921
    },
    {
      "text": "configuration or the llm configuration",
      "start": 326.199,
      "duration": 4.241
    },
    {
      "text": "contains many parameters first is our",
      "start": 328.12,
      "duration": 5.4
    },
    {
      "text": "vocabulary size this is the length of",
      "start": 330.44,
      "duration": 5.44
    },
    {
      "text": "the number of tokens we have in our",
      "start": 333.52,
      "duration": 5.0
    },
    {
      "text": "vocabulary so we are using the same",
      "start": 335.88,
      "duration": 5.8
    },
    {
      "text": "tokenizer which gpt2 and in fact all",
      "start": 338.52,
      "duration": 5.119
    },
    {
      "text": "open AI models used and that's called as",
      "start": 341.68,
      "duration": 3.519
    },
    {
      "text": "tick",
      "start": 343.639,
      "duration": 4.161
    },
    {
      "text": "token so this is thck token and what it",
      "start": 345.199,
      "duration": 5.201
    },
    {
      "text": "does is that it uses a bite pair encoder",
      "start": 347.8,
      "duration": 4.679
    },
    {
      "text": "and it creates a vocabulary of tokens",
      "start": 350.4,
      "duration": 4.799
    },
    {
      "text": "like this the vocabulary size which gpt2",
      "start": 352.479,
      "duration": 3.84
    },
    {
      "text": "had is",
      "start": 355.199,
      "duration": 3.681
    },
    {
      "text": "50257 and that's the same size which we",
      "start": 356.319,
      "duration": 4.32
    },
    {
      "text": "have when we conr constructed our large",
      "start": 358.88,
      "duration": 4.36
    },
    {
      "text": "language model we have to specify a",
      "start": 360.639,
      "duration": 5.321
    },
    {
      "text": "context length gpt2 used a context",
      "start": 363.24,
      "duration": 3.799
    },
    {
      "text": "length of",
      "start": 365.96,
      "duration": 4.4
    },
    {
      "text": "1024 but for the purposes of easy",
      "start": 367.039,
      "duration": 5.801
    },
    {
      "text": "calculation which you can train on your",
      "start": 370.36,
      "duration": 4.72
    },
    {
      "text": "own local machine in less than 2 to 3",
      "start": 372.84,
      "duration": 4.88
    },
    {
      "text": "minutes we are using a context length of",
      "start": 375.08,
      "duration": 5.679
    },
    {
      "text": "256 keep in mind that you can just use",
      "start": 377.72,
      "duration": 4.879
    },
    {
      "text": "the same code for a longer context",
      "start": 380.759,
      "duration": 4.16
    },
    {
      "text": "length as well but just make sure that",
      "start": 382.599,
      "duration": 4.44
    },
    {
      "text": "you have enough compute power and memory",
      "start": 384.919,
      "duration": 4.761
    },
    {
      "text": "to execute the code then we have the",
      "start": 387.039,
      "duration": 4.72
    },
    {
      "text": "embedding Dimension which is which",
      "start": 389.68,
      "duration": 3.359
    },
    {
      "text": "basically means that the token",
      "start": 391.759,
      "duration": 3.121
    },
    {
      "text": "embeddings need to be projected into",
      "start": 393.039,
      "duration": 4.28
    },
    {
      "text": "higher dimensional space to capture the",
      "start": 394.88,
      "duration": 3.719
    },
    {
      "text": "semantic",
      "start": 397.319,
      "duration": 3.44
    },
    {
      "text": "meaning then we have the number of",
      "start": 398.599,
      "duration": 4.201
    },
    {
      "text": "attention heads so these are the number",
      "start": 400.759,
      "duration": 4.121
    },
    {
      "text": "of self attention mechanism blocks we",
      "start": 402.8,
      "duration": 4.959
    },
    {
      "text": "have within one Transformer why do I say",
      "start": 404.88,
      "duration": 4.719
    },
    {
      "text": "within one Transformer because there is",
      "start": 407.759,
      "duration": 3.72
    },
    {
      "text": "not one Transformer there are many",
      "start": 409.599,
      "duration": 4.361
    },
    {
      "text": "Transformers and the end layer specifies",
      "start": 411.479,
      "duration": 5.081
    },
    {
      "text": "how many Transformer blocks we have",
      "start": 413.96,
      "duration": 5.639
    },
    {
      "text": "dropout rate is can be set to zero also",
      "start": 416.56,
      "duration": 5.319
    },
    {
      "text": "this is the Dropout layer parameter and",
      "start": 419.599,
      "duration": 5.28
    },
    {
      "text": "query key value bias is basically uh",
      "start": 421.879,
      "duration": 4.801
    },
    {
      "text": "when we initialize the weight metries",
      "start": 424.879,
      "duration": 4.0
    },
    {
      "text": "for the query key and value we don't",
      "start": 426.68,
      "duration": 5.04
    },
    {
      "text": "want the bias term if any of these terms",
      "start": 428.879,
      "duration": 5.16
    },
    {
      "text": "are looking unfamiliar to you right now",
      "start": 431.72,
      "duration": 5.159
    },
    {
      "text": "please revise the previous lectures we",
      "start": 434.039,
      "duration": 7.681
    },
    {
      "text": "have had on uh llm Basics attention",
      "start": 436.879,
      "duration": 6.921
    },
    {
      "text": "mechanism and GPT",
      "start": 441.72,
      "duration": 4.68
    },
    {
      "text": "architecture awesome so the GPT model",
      "start": 443.8,
      "duration": 4.28
    },
    {
      "text": "which we built earlier looks something",
      "start": 446.4,
      "duration": 3.76
    },
    {
      "text": "like this what this model model does is",
      "start": 448.08,
      "duration": 4.32
    },
    {
      "text": "that it takes in the input tokens it",
      "start": 450.16,
      "duration": 4.759
    },
    {
      "text": "converts them into token embeddings adds",
      "start": 452.4,
      "duration": 4.6
    },
    {
      "text": "positional embeddings to it then we have",
      "start": 454.919,
      "duration": 5.081
    },
    {
      "text": "a Dropout layer this output upti layer",
      "start": 457.0,
      "duration": 5.16
    },
    {
      "text": "passes through the Transformer block",
      "start": 460.0,
      "duration": 3.8
    },
    {
      "text": "after we come out of the Transformer",
      "start": 462.16,
      "duration": 3.479
    },
    {
      "text": "block we have a final normalization",
      "start": 463.8,
      "duration": 5.64
    },
    {
      "text": "layer and then we return the logits now",
      "start": 465.639,
      "duration": 6.361
    },
    {
      "text": "these logits which are returned consists",
      "start": 469.44,
      "duration": 5.84
    },
    {
      "text": "of the tokens and for each token we have",
      "start": 472.0,
      "duration": 4.84
    },
    {
      "text": "uh the number of tokens equal to",
      "start": 475.28,
      "duration": 3.759
    },
    {
      "text": "vocabulary size sorry for each token we",
      "start": 476.84,
      "duration": 4.079
    },
    {
      "text": "have the number of columns equal to the",
      "start": 479.039,
      "duration": 4.28
    },
    {
      "text": "vocabulary size I'll come to this part",
      "start": 480.919,
      "duration": 5.4
    },
    {
      "text": "later and I'll explain to you again what",
      "start": 483.319,
      "duration": 5.72
    },
    {
      "text": "we have done in this part of the code uh",
      "start": 486.319,
      "duration": 6.0
    },
    {
      "text": "so that it's revised for you okay great",
      "start": 489.039,
      "duration": 5.081
    },
    {
      "text": "now let's start looking at the first",
      "start": 492.319,
      "duration": 4.361
    },
    {
      "text": "section of today's lecture okay so let's",
      "start": 494.12,
      "duration": 4.12
    },
    {
      "text": "start with the first part of today's",
      "start": 496.68,
      "duration": 4.16
    },
    {
      "text": "lecture remember that to to get the loss",
      "start": 498.24,
      "duration": 4.88
    },
    {
      "text": "function we need the",
      "start": 500.84,
      "duration": 5.84
    },
    {
      "text": "input from the input we'll get the",
      "start": 503.12,
      "duration": 6.44
    },
    {
      "text": "predicted values and we should already",
      "start": 506.68,
      "duration": 4.479
    },
    {
      "text": "know the True Values which I'm going to",
      "start": 509.56,
      "duration": 4.24
    },
    {
      "text": "call as Target values in today's lecture",
      "start": 511.159,
      "duration": 4.68
    },
    {
      "text": "whenever I use the word Target remember",
      "start": 513.8,
      "duration": 4.359
    },
    {
      "text": "that it stands for the True Values and",
      "start": 515.839,
      "duration": 4.12
    },
    {
      "text": "ultimately the loss function will be",
      "start": 518.159,
      "duration": 3.641
    },
    {
      "text": "determined by the predicted and the",
      "start": 519.959,
      "duration": 4.481
    },
    {
      "text": "target values and how close they are so",
      "start": 521.8,
      "duration": 4.479
    },
    {
      "text": "in the first section let's look at the",
      "start": 524.44,
      "duration": 4.16
    },
    {
      "text": "inputs and let's look at the targets",
      "start": 526.279,
      "duration": 4.921
    },
    {
      "text": "which are the True Values okay so the",
      "start": 528.6,
      "duration": 4.64
    },
    {
      "text": "input to the large language model which",
      "start": 531.2,
      "duration": 4.319
    },
    {
      "text": "we are building comes in a format like",
      "start": 533.24,
      "duration": 5.64
    },
    {
      "text": "this the input is a tensor and uh the",
      "start": 535.519,
      "duration": 5.041
    },
    {
      "text": "number of Ro rows here correspond to the",
      "start": 538.88,
      "duration": 3.72
    },
    {
      "text": "number of batches so I have two batches",
      "start": 540.56,
      "duration": 3.92
    },
    {
      "text": "over here and that's why there are two",
      "start": 542.6,
      "duration": 5.12
    },
    {
      "text": "rows uh so two batches corresponding to",
      "start": 544.48,
      "duration": 5.88
    },
    {
      "text": "the two rows I'm writing two over",
      "start": 547.72,
      "duration": 5.04
    },
    {
      "text": "here the first row corresponds to the",
      "start": 550.36,
      "duration": 4.159
    },
    {
      "text": "first batch the second row corresponds",
      "start": 552.76,
      "duration": 4.0
    },
    {
      "text": "to the second batch now you'll see that",
      "start": 554.519,
      "duration": 4.201
    },
    {
      "text": "every row has three tokens here which is",
      "start": 556.76,
      "duration": 4.079
    },
    {
      "text": "usually set by our context size and I'm",
      "start": 558.72,
      "duration": 3.799
    },
    {
      "text": "just assuming context size equal to",
      "start": 560.839,
      "duration": 4.321
    },
    {
      "text": "three for this example so for the first",
      "start": 562.519,
      "duration": 5.841
    },
    {
      "text": "batch the input tokens are every effort",
      "start": 565.16,
      "duration": 5.799
    },
    {
      "text": "moves for the second badge the input",
      "start": 568.36,
      "duration": 5.56
    },
    {
      "text": "tokens are I really like now the thing",
      "start": 570.959,
      "duration": 5.761
    },
    {
      "text": "is based on these inputs our task is to",
      "start": 573.92,
      "duration": 6.56
    },
    {
      "text": "predict the next next word right and uh",
      "start": 576.72,
      "duration": 5.84
    },
    {
      "text": "so for targets you must be thinking that",
      "start": 580.48,
      "duration": 5.12
    },
    {
      "text": "the target should only be two token IDs",
      "start": 582.56,
      "duration": 5.24
    },
    {
      "text": "for the first batch we need a token ID",
      "start": 585.6,
      "duration": 4.679
    },
    {
      "text": "for the second batch we need a token ID",
      "start": 587.8,
      "duration": 4.76
    },
    {
      "text": "but now look at this target tensor over",
      "start": 590.279,
      "duration": 4.841
    },
    {
      "text": "here it has two rows corresponding to",
      "start": 592.56,
      "duration": 4.959
    },
    {
      "text": "the two batches that's fine but why does",
      "start": 595.12,
      "duration": 4.76
    },
    {
      "text": "the first row have three values and why",
      "start": 597.519,
      "duration": 4.44
    },
    {
      "text": "does the second row have three",
      "start": 599.88,
      "duration": 4.32
    },
    {
      "text": "values the reason is because whenever",
      "start": 601.959,
      "duration": 3.921
    },
    {
      "text": "you look at the input like this there",
      "start": 604.2,
      "duration": 3.879
    },
    {
      "text": "are actually three there are actually",
      "start": 605.88,
      "duration": 4.72
    },
    {
      "text": "three uh prediction tasks which are",
      "start": 608.079,
      "duration": 5.241
    },
    {
      "text": "happening here when every is the input",
      "start": 610.6,
      "duration": 4.52
    },
    {
      "text": "effort should be the output that's why",
      "start": 613.32,
      "duration": 4.199
    },
    {
      "text": "the first Target is 3626 which",
      "start": 615.12,
      "duration": 5.04
    },
    {
      "text": "corresponds to effort when every effort",
      "start": 617.519,
      "duration": 6.44
    },
    {
      "text": "is the input which means that 16833 and",
      "start": 620.16,
      "duration": 7.44
    },
    {
      "text": "3626 are input 61 0 is the output which",
      "start": 623.959,
      "duration": 6.56
    },
    {
      "text": "is moves and and then finally when every",
      "start": 627.6,
      "duration": 5.2
    },
    {
      "text": "effort moves is an input the output",
      "start": 630.519,
      "duration": 4.961
    },
    {
      "text": "should be U so the correct answer which",
      "start": 632.8,
      "duration": 6.32
    },
    {
      "text": "we want is U over here similarly for I",
      "start": 635.48,
      "duration": 5.919
    },
    {
      "text": "really like there are three prediction",
      "start": 639.12,
      "duration": 5.839
    },
    {
      "text": "tasks when I is the input uh really is",
      "start": 641.399,
      "duration": 6.201
    },
    {
      "text": "the output when I really is the input",
      "start": 644.959,
      "duration": 5.12
    },
    {
      "text": "like is the output and when I really",
      "start": 647.6,
      "duration": 4.28
    },
    {
      "text": "like is the input chocolate is the",
      "start": 650.079,
      "duration": 4.121
    },
    {
      "text": "output so we want the output to be",
      "start": 651.88,
      "duration": 4.36
    },
    {
      "text": "chocolate in this case but then there",
      "start": 654.2,
      "duration": 3.72
    },
    {
      "text": "are three prediction tasks here and",
      "start": 656.24,
      "duration": 3.76
    },
    {
      "text": "that's why the target tensor consist of",
      "start": 657.92,
      "duration": 3.919
    },
    {
      "text": "two rows first row corresponds to the",
      "start": 660.0,
      "duration": 3.88
    },
    {
      "text": "first batch second row corresponds to",
      "start": 661.839,
      "duration": 3.601
    },
    {
      "text": "the second batch and then we have three",
      "start": 663.88,
      "duration": 3.72
    },
    {
      "text": "values I hope you have understood why",
      "start": 665.44,
      "duration": 3.639
    },
    {
      "text": "there are three values in the Target",
      "start": 667.6,
      "duration": 3.64
    },
    {
      "text": "tensor because that will be very",
      "start": 669.079,
      "duration": 4.681
    },
    {
      "text": "important to keep in mind as we move",
      "start": 671.24,
      "duration": 4.96
    },
    {
      "text": "forward ultimately we are just concerned",
      "start": 673.76,
      "duration": 3.92
    },
    {
      "text": "about the last word right which is this",
      "start": 676.2,
      "duration": 3.92
    },
    {
      "text": "345 that is the token ID corresponding",
      "start": 677.68,
      "duration": 7.599
    },
    {
      "text": "to U uh token ID corresponding to U and",
      "start": 680.12,
      "duration": 7.56
    },
    {
      "text": "11311 that's the token ID essentially",
      "start": 685.279,
      "duration": 4.321
    },
    {
      "text": "corresponding to Chocolate but but the",
      "start": 687.68,
      "duration": 3.959
    },
    {
      "text": "first two are also important for the",
      "start": 689.6,
      "duration": 5.08
    },
    {
      "text": "initial prediction tasks in this input",
      "start": 691.639,
      "duration": 4.801
    },
    {
      "text": "okay so the these are the True Values",
      "start": 694.68,
      "duration": 4.0
    },
    {
      "text": "which we want so this tensor are the",
      "start": 696.44,
      "duration": 5.199
    },
    {
      "text": "True Values true prediction values if",
      "start": 698.68,
      "duration": 4.92
    },
    {
      "text": "our large language model is doing an",
      "start": 701.639,
      "duration": 4.481
    },
    {
      "text": "amazing job for these two inputs the",
      "start": 703.6,
      "duration": 5.2
    },
    {
      "text": "prediction should be the first tensor of",
      "start": 706.12,
      "duration": 4.839
    },
    {
      "text": "the output should look like this the",
      "start": 708.8,
      "duration": 3.96
    },
    {
      "text": "second of the out the second row of the",
      "start": 710.959,
      "duration": 4.161
    },
    {
      "text": "output should look like this but of",
      "start": 712.76,
      "duration": 4.519
    },
    {
      "text": "course initially when the large language",
      "start": 715.12,
      "duration": 5.0
    },
    {
      "text": "model is initialized randomly it won't",
      "start": 717.279,
      "duration": 4.921
    },
    {
      "text": "produce these outputs which we want",
      "start": 720.12,
      "duration": 5.24
    },
    {
      "text": "we'll need to train it okay so we have",
      "start": 722.2,
      "duration": 4.879
    },
    {
      "text": "looked at the input and we have looked",
      "start": 725.36,
      "duration": 4.4
    },
    {
      "text": "at the Target now let's see the outputs",
      "start": 727.079,
      "duration": 6.2
    },
    {
      "text": "which are the predicted values of our",
      "start": 729.76,
      "duration": 3.519
    },
    {
      "text": "llm so this is the input which we saw",
      "start": 733.68,
      "duration": 7.04
    },
    {
      "text": "over here right uh 16833 3626",
      "start": 736.24,
      "duration": 6.959
    },
    {
      "text": "610 then that's the first batch the",
      "start": 740.72,
      "duration": 5.32
    },
    {
      "text": "second batch is 40 1107 and",
      "start": 743.199,
      "duration": 6.521
    },
    {
      "text": "588 so the this input will go to GPT",
      "start": 746.04,
      "duration": 6.08
    },
    {
      "text": "model and what is the GPT model the GPT",
      "start": 749.72,
      "duration": 4.44
    },
    {
      "text": "model looks something like this so the",
      "start": 752.12,
      "duration": 4.839
    },
    {
      "text": "input which I told you goes over",
      "start": 754.16,
      "duration": 5.4
    },
    {
      "text": "here and then it passes through all of",
      "start": 756.959,
      "duration": 4.401
    },
    {
      "text": "these steps and then we have an output",
      "start": 759.56,
      "duration": 5.8
    },
    {
      "text": "tensor which is also called as the logit",
      "start": 761.36,
      "duration": 4.0
    },
    {
      "text": "tensor okay so the first step as to what",
      "start": 765.68,
      "duration": 4.68
    },
    {
      "text": "happens in the GPT model is that the",
      "start": 768.6,
      "duration": 4.239
    },
    {
      "text": "tokenizer is converts the tokens into",
      "start": 770.36,
      "duration": 5.839
    },
    {
      "text": "token IDs then the token IDs pass",
      "start": 772.839,
      "duration": 4.921
    },
    {
      "text": "through all of these steps and we get",
      "start": 776.199,
      "duration": 4.281
    },
    {
      "text": "the logic sensor and finally the logits",
      "start": 777.76,
      "duration": 4.8
    },
    {
      "text": "are converted back into token IDs which",
      "start": 780.48,
      "duration": 3.76
    },
    {
      "text": "are the next",
      "start": 782.56,
      "duration": 4.0
    },
    {
      "text": "predictions uh of our large language",
      "start": 784.24,
      "duration": 4.88
    },
    {
      "text": "model this entire sequence is encoded in",
      "start": 786.56,
      "duration": 4.2
    },
    {
      "text": "this figure here which I want to explain",
      "start": 789.12,
      "duration": 3.719
    },
    {
      "text": "thoroughly first we start with a",
      "start": 790.76,
      "duration": 3.96
    },
    {
      "text": "vocabulary and for the sake of",
      "start": 792.839,
      "duration": 3.481
    },
    {
      "text": "Simplicity let's assume that the",
      "start": 794.72,
      "duration": 3.679
    },
    {
      "text": "vocabulary just has seven elements",
      "start": 796.32,
      "duration": 5.04
    },
    {
      "text": "remember in the actual example which we",
      "start": 798.399,
      "duration": 4.641
    },
    {
      "text": "are going to consider the vocabulary",
      "start": 801.36,
      "duration": 4.279
    },
    {
      "text": "actually has 50257",
      "start": 803.04,
      "duration": 5.0
    },
    {
      "text": "elements but I'm just illustrating this",
      "start": 805.639,
      "duration": 4.2
    },
    {
      "text": "diagram for the sake of Simplicity where",
      "start": 808.04,
      "duration": 3.0
    },
    {
      "text": "there are seven elements in the",
      "start": 809.839,
      "duration": 3.881
    },
    {
      "text": "vocabulary right and the input text",
      "start": 811.04,
      "duration": 4.88
    },
    {
      "text": "let's say is just one batch for now and",
      "start": 813.72,
      "duration": 4.76
    },
    {
      "text": "every effort moves that's the input",
      "start": 815.92,
      "duration": 5.52
    },
    {
      "text": "right the first step is to use our",
      "start": 818.48,
      "duration": 4.88
    },
    {
      "text": "vocabulary to map the input text to",
      "start": 821.44,
      "duration": 5.399
    },
    {
      "text": "token IDs so every is token ID number",
      "start": 823.36,
      "duration": 6.839
    },
    {
      "text": "two effort is token ID number one and",
      "start": 826.839,
      "duration": 6.0
    },
    {
      "text": "moves is token ID number four so First",
      "start": 830.199,
      "duration": 4.44
    },
    {
      "text": "We Take These tokens and map them into",
      "start": 832.839,
      "duration": 5.24
    },
    {
      "text": "token IDs 2 1 and four the second step",
      "start": 834.639,
      "duration": 5.481
    },
    {
      "text": "is that we'll pass all of this through",
      "start": 838.079,
      "duration": 4.641
    },
    {
      "text": "the GPT",
      "start": 840.12,
      "duration": 2.6
    },
    {
      "text": "model and the GPT model will give uh a",
      "start": 843.04,
      "duration": 7.28
    },
    {
      "text": "logit sensor and to the logit sensor we",
      "start": 847.24,
      "duration": 5.88
    },
    {
      "text": "apply the soft Max distribution and",
      "start": 850.32,
      "duration": 5.28
    },
    {
      "text": "we'll get this output Matrix look at",
      "start": 853.12,
      "duration": 4.88
    },
    {
      "text": "this output Matrix here I'll just write",
      "start": 855.6,
      "duration": 6.28
    },
    {
      "text": "it Dimensions here so this output Matrix",
      "start": 858.0,
      "duration": 6.72
    },
    {
      "text": "essentially uh it has",
      "start": 861.88,
      "duration": 5.44
    },
    {
      "text": "uh let me use a different color it has",
      "start": 864.72,
      "duration": 4.919
    },
    {
      "text": "three rows the first row corresponds to",
      "start": 867.32,
      "duration": 4.48
    },
    {
      "text": "every the second row corresponds to",
      "start": 869.639,
      "duration": 4.081
    },
    {
      "text": "effort and the third row corresponds to",
      "start": 871.8,
      "duration": 4.24
    },
    {
      "text": "moves now you'll see that the number of",
      "start": 873.72,
      "duration": 6.039
    },
    {
      "text": "columns are equal to 7 1 2 3 4 5 6 and",
      "start": 876.04,
      "duration": 7.32
    },
    {
      "text": "7even and we have some values see every",
      "start": 879.759,
      "duration": 5.401
    },
    {
      "text": "row has some values and there are seven",
      "start": 883.36,
      "duration": 3.96
    },
    {
      "text": "columns corresponding to every row",
      "start": 885.16,
      "duration": 4.32
    },
    {
      "text": "basically the values in each row",
      "start": 887.32,
      "duration": 3.959
    },
    {
      "text": "corresponds to the probability of what",
      "start": 889.48,
      "duration": 4.159
    },
    {
      "text": "the next token will be so let's look at",
      "start": 891.279,
      "duration": 4.48
    },
    {
      "text": "the first row values and I'll just write",
      "start": 893.639,
      "duration": 5.801
    },
    {
      "text": "them over here for your reference 0.1 6",
      "start": 895.759,
      "duration": 5.121
    },
    {
      "text": "2",
      "start": 899.44,
      "duration": 4.88
    },
    {
      "text": "0.05 0 0.02 and",
      "start": 900.88,
      "duration": 5.84
    },
    {
      "text": "0.01 these are the values corresponding",
      "start": 904.32,
      "duration": 4.879
    },
    {
      "text": "to every now what these values means is",
      "start": 906.72,
      "duration": 4.479
    },
    {
      "text": "that when every is the",
      "start": 909.199,
      "duration": 4.681
    },
    {
      "text": "input what is the probability for the",
      "start": 911.199,
      "duration": 5.521
    },
    {
      "text": "next token being the output and 1 2 3 4",
      "start": 913.88,
      "duration": 5.519
    },
    {
      "text": "5 6 7 corresponds to a effort every",
      "start": 916.72,
      "duration": 5.44
    },
    {
      "text": "forward move Z and Zoo so let me write",
      "start": 919.399,
      "duration": 4.44
    },
    {
      "text": "the",
      "start": 922.16,
      "duration": 4.32
    },
    {
      "text": "a",
      "start": 923.839,
      "duration": 6.641
    },
    {
      "text": "effort every",
      "start": 926.48,
      "duration": 4.0
    },
    {
      "text": "forward",
      "start": 932.279,
      "duration": 5.48
    },
    {
      "text": "moves U and Zoo this is the vocabulary",
      "start": 934.36,
      "duration": 6.64
    },
    {
      "text": "right now every value here in this row",
      "start": 937.759,
      "duration": 5.56
    },
    {
      "text": "corresponds to the probability of what",
      "start": 941.0,
      "duration": 4.519
    },
    {
      "text": "the next token will be if every is an",
      "start": 943.319,
      "duration": 4.88
    },
    {
      "text": "input so let's Analyze This probability",
      "start": 945.519,
      "duration": 4.56
    },
    {
      "text": "if every is an input the probability",
      "start": 948.199,
      "duration": 4.64
    },
    {
      "text": "that the next token is a is just 10% or",
      "start": 950.079,
      "duration": 5.961
    },
    {
      "text": "0.1 there is 60% chance of the next",
      "start": 952.839,
      "duration": 5.92
    },
    {
      "text": "token being effort 20% chance of the",
      "start": 956.04,
      "duration": 5.479
    },
    {
      "text": "next token me every Etc so then what we",
      "start": 958.759,
      "duration": 5.121
    },
    {
      "text": "do is that we look at the index with the",
      "start": 961.519,
      "duration": 4.601
    },
    {
      "text": "highest probability which is this 6 over",
      "start": 963.88,
      "duration": 4.28
    },
    {
      "text": "here and so we know that the next token",
      "start": 966.12,
      "duration": 3.199
    },
    {
      "text": "is equal to",
      "start": 968.16,
      "duration": 3.64
    },
    {
      "text": "effort now when every effort is the",
      "start": 969.319,
      "duration": 4.96
    },
    {
      "text": "input we again look at the second row so",
      "start": 971.8,
      "duration": 4.519
    },
    {
      "text": "we again look at the second row and find",
      "start": 974.279,
      "duration": 3.56
    },
    {
      "text": "the index which has the highest",
      "start": 976.319,
      "duration": 3.96
    },
    {
      "text": "probability and that corresponds to",
      "start": 977.839,
      "duration": 5.201
    },
    {
      "text": "moves so this is index number four and",
      "start": 980.279,
      "duration": 5.321
    },
    {
      "text": "that corresponds to moves over here then",
      "start": 983.04,
      "duration": 5.159
    },
    {
      "text": "we look at the third row so every effort",
      "start": 985.6,
      "duration": 4.599
    },
    {
      "text": "moves when that's the input what's the",
      "start": 988.199,
      "duration": 4.361
    },
    {
      "text": "output and this is the main prediction",
      "start": 990.199,
      "duration": 3.961
    },
    {
      "text": "task because it actually predicts the",
      "start": 992.56,
      "duration": 3.719
    },
    {
      "text": "next token and if you look at the third",
      "start": 994.16,
      "duration": 4.44
    },
    {
      "text": "row you look at the index which has the",
      "start": 996.279,
      "duration": 4.12
    },
    {
      "text": "maximum probability and that's index",
      "start": 998.6,
      "duration": 4.44
    },
    {
      "text": "number five which is here and we know",
      "start": 1000.399,
      "duration": 4.321
    },
    {
      "text": "that index number five corresponds to",
      "start": 1003.04,
      "duration": 4.919
    },
    {
      "text": "you so we know that the next uh token is",
      "start": 1004.72,
      "duration": 5.679
    },
    {
      "text": "you so remember when I told you that",
      "start": 1007.959,
      "duration": 4.32
    },
    {
      "text": "when the input has three tokens like",
      "start": 1010.399,
      "duration": 4.24
    },
    {
      "text": "this there are three prediction tasks",
      "start": 1012.279,
      "duration": 4.441
    },
    {
      "text": "for the first prediction task the output",
      "start": 1014.639,
      "duration": 4.56
    },
    {
      "text": "is index number one because that",
      "start": 1016.72,
      "duration": 4.599
    },
    {
      "text": "corresponds to the highest probability",
      "start": 1019.199,
      "duration": 3.841
    },
    {
      "text": "and the token which it corresponds to is",
      "start": 1021.319,
      "duration": 5.281
    },
    {
      "text": "effort for index number two or sorry for",
      "start": 1023.04,
      "duration": 5.84
    },
    {
      "text": "the prediction task number two the input",
      "start": 1026.6,
      "duration": 4.719
    },
    {
      "text": "is every effort and the output is index",
      "start": 1028.88,
      "duration": 5.079
    },
    {
      "text": "four which corresponds to moves and for",
      "start": 1031.319,
      "duration": 4.681
    },
    {
      "text": "the third prediction tasks the input is",
      "start": 1033.959,
      "duration": 4.36
    },
    {
      "text": "every effort moves and the output is",
      "start": 1036.0,
      "duration": 5.559
    },
    {
      "text": "five which corresponds to U so when you",
      "start": 1038.319,
      "duration": 7.12
    },
    {
      "text": "give this input every effort moves to",
      "start": 1041.559,
      "duration": 7.48
    },
    {
      "text": "the uh large language model the output",
      "start": 1045.439,
      "duration": 7.561
    },
    {
      "text": "is a sequence of indexes 1 4 and five",
      "start": 1049.039,
      "duration": 6.841
    },
    {
      "text": "what this output means is that one so if",
      "start": 1053.0,
      "duration": 4.88
    },
    {
      "text": "you look at one over here it means that",
      "start": 1055.88,
      "duration": 4.96
    },
    {
      "text": "when every is an input effort is the",
      "start": 1057.88,
      "duration": 5.6
    },
    {
      "text": "output when every effort is the input",
      "start": 1060.84,
      "duration": 4.36
    },
    {
      "text": "moves is the output and when every",
      "start": 1063.48,
      "duration": 4.0
    },
    {
      "text": "effort moves is the input U is the",
      "start": 1065.2,
      "duration": 4.52
    },
    {
      "text": "output so this is what is actually",
      "start": 1067.48,
      "duration": 4.4
    },
    {
      "text": "happening within the GPT model itself I",
      "start": 1069.72,
      "duration": 4.0
    },
    {
      "text": "just gave you a five minute crash course",
      "start": 1071.88,
      "duration": 4.48
    },
    {
      "text": "of what we learned in four to five hours",
      "start": 1073.72,
      "duration": 5.12
    },
    {
      "text": "in the previous set of videos so if any",
      "start": 1076.36,
      "duration": 5.319
    },
    {
      "text": "any of this you're finding hard I would",
      "start": 1078.84,
      "duration": 4.68
    },
    {
      "text": "again highly encourage you to go through",
      "start": 1081.679,
      "duration": 4.281
    },
    {
      "text": "all the previous videos so that you",
      "start": 1083.52,
      "duration": 4.48
    },
    {
      "text": "understand this better for now even if",
      "start": 1085.96,
      "duration": 4.959
    },
    {
      "text": "you get an intuition of what's going on",
      "start": 1088.0,
      "duration": 4.799
    },
    {
      "text": "here it's fine and you can follow along",
      "start": 1090.919,
      "duration": 4.161
    },
    {
      "text": "till the next part right so here what I",
      "start": 1092.799,
      "duration": 4.441
    },
    {
      "text": "want to illustrate to you is that this",
      "start": 1095.08,
      "duration": 3.839
    },
    {
      "text": "process which I've shown here we have",
      "start": 1097.24,
      "duration": 4.439
    },
    {
      "text": "the input when it goes to a GPT model we",
      "start": 1098.919,
      "duration": 5.601
    },
    {
      "text": "have the output token IDs and uh I've",
      "start": 1101.679,
      "duration": 5.201
    },
    {
      "text": "told you how we get the output token",
      "start": 1104.52,
      "duration": 4.519
    },
    {
      "text": "IDs so if you have understood the",
      "start": 1106.88,
      "duration": 4.159
    },
    {
      "text": "process let's say these are the output",
      "start": 1109.039,
      "duration": 4.481
    },
    {
      "text": "token IDs which we have obtained",
      "start": 1111.039,
      "duration": 4.481
    },
    {
      "text": "remember for actually gpt2 the",
      "start": 1113.52,
      "duration": 5.32
    },
    {
      "text": "vocabulary size is 5257 right so the",
      "start": 1115.52,
      "duration": 6.32
    },
    {
      "text": "range of the output token ID is is 0 to",
      "start": 1118.84,
      "duration": 6.16
    },
    {
      "text": "50257 here the vocabulary size was 7 so",
      "start": 1121.84,
      "duration": 5.839
    },
    {
      "text": "the range was 0 to 7 so if you look at",
      "start": 1125.0,
      "duration": 5.799
    },
    {
      "text": "batch number one uh the output tokens",
      "start": 1127.679,
      "duration": 6.641
    },
    {
      "text": "are 16657 339 and",
      "start": 1130.799,
      "duration": 6.041
    },
    {
      "text": "42826 U and let's look at the inputs",
      "start": 1134.32,
      "duration": 5.32
    },
    {
      "text": "right every effort moves",
      "start": 1136.84,
      "duration": 5.199
    },
    {
      "text": "so",
      "start": 1139.64,
      "duration": 4.8
    },
    {
      "text": "every effort and",
      "start": 1142.039,
      "duration": 4.76
    },
    {
      "text": "moves what this output essentially",
      "start": 1144.44,
      "duration": 4.84
    },
    {
      "text": "conveys is that if every is an input the",
      "start": 1146.799,
      "duration": 4.841
    },
    {
      "text": "token corresponding to the Token ID of",
      "start": 1149.28,
      "duration": 5.759
    },
    {
      "text": "16657 is the output when every effort is",
      "start": 1151.64,
      "duration": 5.519
    },
    {
      "text": "the input the token corresponding to",
      "start": 1155.039,
      "duration": 4.681
    },
    {
      "text": "token ID 339 is the output and when",
      "start": 1157.159,
      "duration": 4.601
    },
    {
      "text": "every effort moves is the input the",
      "start": 1159.72,
      "duration": 3.839
    },
    {
      "text": "token corresponding to the Token ID",
      "start": 1161.76,
      "duration": 4.88
    },
    {
      "text": "42826 is the output and what do we want",
      "start": 1163.559,
      "duration": 5.161
    },
    {
      "text": "these token IDs to be we want these",
      "start": 1166.64,
      "duration": 4.039
    },
    {
      "text": "token IDs to be as close to the Target",
      "start": 1168.72,
      "duration": 4.52
    },
    {
      "text": "token IDs which we saw look at these",
      "start": 1170.679,
      "duration": 4.201
    },
    {
      "text": "Target token",
      "start": 1173.24,
      "duration": 4.52
    },
    {
      "text": "IDs the token IDs which we have obtained",
      "start": 1174.88,
      "duration": 4.96
    },
    {
      "text": "right now are not the actual token IDs",
      "start": 1177.76,
      "duration": 3.64
    },
    {
      "text": "because the training has not yet",
      "start": 1179.84,
      "duration": 4.04
    },
    {
      "text": "happened so these are the output token",
      "start": 1181.4,
      "duration": 4.96
    },
    {
      "text": "IDs and we want these output token IDs",
      "start": 1183.88,
      "duration": 4.52
    },
    {
      "text": "to be as close to the Target token IDs",
      "start": 1186.36,
      "duration": 3.799
    },
    {
      "text": "as possible that's the whole goal of",
      "start": 1188.4,
      "duration": 3.92
    },
    {
      "text": "today's lecture and so we are going to",
      "start": 1190.159,
      "duration": 3.801
    },
    {
      "text": "then Define the loss function between",
      "start": 1192.32,
      "duration": 4.64
    },
    {
      "text": "the outputs and the targets so these are",
      "start": 1193.96,
      "duration": 4.76
    },
    {
      "text": "the output token IDs for batch number",
      "start": 1196.96,
      "duration": 3.68
    },
    {
      "text": "number one and here you can see that",
      "start": 1198.72,
      "duration": 3.76
    },
    {
      "text": "these are the three output token IDs for",
      "start": 1200.64,
      "duration": 6.32
    },
    {
      "text": "batch number two where the input is uh I",
      "start": 1202.48,
      "duration": 7.24
    },
    {
      "text": "really like so",
      "start": 1206.96,
      "duration": 6.839
    },
    {
      "text": "I really like and for these outputs we",
      "start": 1209.72,
      "duration": 6.6
    },
    {
      "text": "have to compare these outputs to these",
      "start": 1213.799,
      "duration": 4.681
    },
    {
      "text": "True Values for the second",
      "start": 1216.32,
      "duration": 4.16
    },
    {
      "text": "batch I hope you have you have",
      "start": 1218.48,
      "duration": 5.36
    },
    {
      "text": "understood the goal of today's lecture",
      "start": 1220.48,
      "duration": 5.36
    },
    {
      "text": "um up till now we have understood what",
      "start": 1223.84,
      "duration": 4.24
    },
    {
      "text": "are the inputs the shape in which inputs",
      "start": 1225.84,
      "duration": 5.12
    },
    {
      "text": "will be given to the model what are the",
      "start": 1228.08,
      "duration": 4.64
    },
    {
      "text": "True Values which I'm also calling as",
      "start": 1230.96,
      "duration": 4.16
    },
    {
      "text": "targets and we understood that what are",
      "start": 1232.72,
      "duration": 3.16
    },
    {
      "text": "the",
      "start": 1235.12,
      "duration": 4.12
    },
    {
      "text": "output uh output token IDs for batch",
      "start": 1235.88,
      "duration": 5.84
    },
    {
      "text": "number one and batch number two and to",
      "start": 1239.24,
      "duration": 4.799
    },
    {
      "text": "get these output token IDs the inputs",
      "start": 1241.72,
      "duration": 4.68
    },
    {
      "text": "have to go through a huge Transformer",
      "start": 1244.039,
      "duration": 3.961
    },
    {
      "text": "block and it has to go through this",
      "start": 1246.4,
      "duration": 3.88
    },
    {
      "text": "whole GPT architecture then we get the",
      "start": 1248.0,
      "duration": 4.72
    },
    {
      "text": "output Logics we apply soft Max and we",
      "start": 1250.28,
      "duration": 4.759
    },
    {
      "text": "get this tensor like this this one",
      "start": 1252.72,
      "duration": 4.199
    },
    {
      "text": "tensor essentially contains all the",
      "start": 1255.039,
      "duration": 3.841
    },
    {
      "text": "information you take a look at at this",
      "start": 1256.919,
      "duration": 5.081
    },
    {
      "text": "tensor and then you extract the indexes",
      "start": 1258.88,
      "duration": 5.44
    },
    {
      "text": "with the highest probability in each row",
      "start": 1262.0,
      "duration": 4.559
    },
    {
      "text": "that is essentially your",
      "start": 1264.32,
      "duration": 5.68
    },
    {
      "text": "output okay and now what we are going to",
      "start": 1266.559,
      "duration": 5.321
    },
    {
      "text": "do next is that we are going to then",
      "start": 1270.0,
      "duration": 3.72
    },
    {
      "text": "find the loss between the targets and",
      "start": 1271.88,
      "duration": 4.08
    },
    {
      "text": "the output before coming to the next",
      "start": 1273.72,
      "duration": 4.68
    },
    {
      "text": "step I want to go to code and really",
      "start": 1275.96,
      "duration": 4.24
    },
    {
      "text": "code everything what we have learned so",
      "start": 1278.4,
      "duration": 4.04
    },
    {
      "text": "far so that the understanding is clear",
      "start": 1280.2,
      "duration": 5.359
    },
    {
      "text": "and so that you also understand uh what",
      "start": 1282.44,
      "duration": 5.08
    },
    {
      "text": "is really going on in the",
      "start": 1285.559,
      "duration": 4.761
    },
    {
      "text": "code I already explained to you the",
      "start": 1287.52,
      "duration": 4.2
    },
    {
      "text": "configuration which we are going to be",
      "start": 1290.32,
      "duration": 4.56
    },
    {
      "text": "using right vocabulary size 50257 the",
      "start": 1291.72,
      "duration": 5.16
    },
    {
      "text": "context length of",
      "start": 1294.88,
      "duration": 5.88
    },
    {
      "text": "256 uh embedding dimension of",
      "start": 1296.88,
      "duration": 6.88
    },
    {
      "text": "768 number of attention heads 12 number",
      "start": 1300.76,
      "duration": 5.12
    },
    {
      "text": "of Transformer blocks equal to 12",
      "start": 1303.76,
      "duration": 4.88
    },
    {
      "text": "dropout rate 0.1 and the query key value",
      "start": 1305.88,
      "duration": 4.279
    },
    {
      "text": "bias equal to",
      "start": 1308.64,
      "duration": 4.6
    },
    {
      "text": "false awesome now the next thing what we",
      "start": 1310.159,
      "duration": 5.281
    },
    {
      "text": "are going to do is that we are going to",
      "start": 1313.24,
      "duration": 5.08
    },
    {
      "text": "uh construct our model what this model",
      "start": 1315.44,
      "duration": 5.119
    },
    {
      "text": "is basically it's an instance of the GPT",
      "start": 1318.32,
      "duration": 4.839
    },
    {
      "text": "model class and this is the GPT model",
      "start": 1320.559,
      "duration": 4.961
    },
    {
      "text": "class which we have defined previously",
      "start": 1323.159,
      "duration": 4.12
    },
    {
      "text": "uh to get a visual representation of",
      "start": 1325.52,
      "duration": 3.56
    },
    {
      "text": "what this class does is that it takes in",
      "start": 1327.279,
      "duration": 3.76
    },
    {
      "text": "the input and then it converts the",
      "start": 1329.08,
      "duration": 4.04
    },
    {
      "text": "inputs into this logic",
      "start": 1331.039,
      "duration": 5.281
    },
    {
      "text": "sensor that's so all of these steps",
      "start": 1333.12,
      "duration": 5.12
    },
    {
      "text": "which have been written here are",
      "start": 1336.32,
      "duration": 4.04
    },
    {
      "text": "actually encoded visually in this",
      "start": 1338.24,
      "duration": 4.16
    },
    {
      "text": "Transformer block or in this GPT model",
      "start": 1340.36,
      "duration": 4.12
    },
    {
      "text": "architecture which I've seen the blue",
      "start": 1342.4,
      "duration": 4.399
    },
    {
      "text": "color which I'm highlighting right now",
      "start": 1344.48,
      "duration": 4.04
    },
    {
      "text": "with the orange pen that's the transform",
      "start": 1346.799,
      "duration": 3.36
    },
    {
      "text": "for block and it's the heart of the",
      "start": 1348.52,
      "duration": 5.0
    },
    {
      "text": "entire GPT architecture so here we are",
      "start": 1350.159,
      "duration": 5.201
    },
    {
      "text": "creating an instance of this model so",
      "start": 1353.52,
      "duration": 3.84
    },
    {
      "text": "that when we pass an input to this model",
      "start": 1355.36,
      "duration": 4.919
    },
    {
      "text": "we get the logic sensor as the",
      "start": 1357.36,
      "duration": 5.72
    },
    {
      "text": "output one note is that we reduce the",
      "start": 1360.279,
      "duration": 6.361
    },
    {
      "text": "context length of only 256 tokens here",
      "start": 1363.08,
      "duration": 5.479
    },
    {
      "text": "although we know that the gpt2 model",
      "start": 1366.64,
      "duration": 4.919
    },
    {
      "text": "uses 1024 tokens the reason is because",
      "start": 1368.559,
      "duration": 4.521
    },
    {
      "text": "you all will be able to follow and",
      "start": 1371.559,
      "duration": 4.521
    },
    {
      "text": "execute the code on your laptop computer",
      "start": 1373.08,
      "duration": 4.839
    },
    {
      "text": "and we reduce the computational resource",
      "start": 1376.08,
      "duration": 4.959
    },
    {
      "text": "requirements that way okay there are two",
      "start": 1377.919,
      "duration": 5.24
    },
    {
      "text": "more functions which I want to define",
      "start": 1381.039,
      "duration": 4.801
    },
    {
      "text": "the first is text to token IDs what this",
      "start": 1383.159,
      "duration": 5.041
    },
    {
      "text": "will do is that whenever we get any text",
      "start": 1385.84,
      "duration": 4.36
    },
    {
      "text": "which is the input we'll just convert it",
      "start": 1388.2,
      "duration": 5.32
    },
    {
      "text": "into token IDs and this uses this tick",
      "start": 1390.2,
      "duration": 6.8
    },
    {
      "text": "token tokenizer to convert the text into",
      "start": 1393.52,
      "duration": 6.0
    },
    {
      "text": "token IDs and there is also token IDs to",
      "start": 1397.0,
      "duration": 4.52
    },
    {
      "text": "text which is the reverse function so",
      "start": 1399.52,
      "duration": 4.36
    },
    {
      "text": "basically when we get any token ID we'll",
      "start": 1401.52,
      "duration": 4.24
    },
    {
      "text": "convert it back into",
      "start": 1403.88,
      "duration": 5.279
    },
    {
      "text": "text so uh if every effort moves you is",
      "start": 1405.76,
      "duration": 5.08
    },
    {
      "text": "the text and when you pass it through",
      "start": 1409.159,
      "duration": 3.921
    },
    {
      "text": "the tokenizer you'll get the token ID is",
      "start": 1410.84,
      "duration": 4.76
    },
    {
      "text": "corresponding to this and then you can",
      "start": 1413.08,
      "duration": 4.44
    },
    {
      "text": "even decode the token IDs and it will",
      "start": 1415.6,
      "duration": 4.36
    },
    {
      "text": "get you back the same text just as a",
      "start": 1417.52,
      "duration": 4.12
    },
    {
      "text": "proof that our model is working what",
      "start": 1419.96,
      "duration": 4.52
    },
    {
      "text": "I've done over here is that uh I have",
      "start": 1421.64,
      "duration": 4.76
    },
    {
      "text": "use this generate text simple which is",
      "start": 1424.48,
      "duration": 3.439
    },
    {
      "text": "another function we have defined in the",
      "start": 1426.4,
      "duration": 3.36
    },
    {
      "text": "previous lecture no need to worry about",
      "start": 1427.919,
      "duration": 3.76
    },
    {
      "text": "this right now but what this function",
      "start": 1429.76,
      "duration": 4.24
    },
    {
      "text": "does is that it takes in our model and",
      "start": 1431.679,
      "duration": 4.12
    },
    {
      "text": "we predict the maximum number of new",
      "start": 1434.0,
      "duration": 4.12
    },
    {
      "text": "tokens the number of input tokens is",
      "start": 1435.799,
      "duration": 4.601
    },
    {
      "text": "four every effort moves you and then it",
      "start": 1438.12,
      "duration": 4.88
    },
    {
      "text": "generates 10 new",
      "start": 1440.4,
      "duration": 5.24
    },
    {
      "text": "tokens um so here you can see the we",
      "start": 1443.0,
      "duration": 4.48
    },
    {
      "text": "have printed the output text and this",
      "start": 1445.64,
      "duration": 3.68
    },
    {
      "text": "shows that the output text consists of",
      "start": 1447.48,
      "duration": 4.84
    },
    {
      "text": "four input tokens and 10 output tokens",
      "start": 1449.32,
      "duration": 4.8
    },
    {
      "text": "remember one word is not necessarily",
      "start": 1452.32,
      "duration": 3.68
    },
    {
      "text": "equal to one token because we are using",
      "start": 1454.12,
      "duration": 4.039
    },
    {
      "text": "the bite pair encoder where we even have",
      "start": 1456.0,
      "duration": 4.4
    },
    {
      "text": "subwords and characters which can be",
      "start": 1458.159,
      "duration": 5.041
    },
    {
      "text": "individual tokens so up till now as we",
      "start": 1460.4,
      "duration": 4.999
    },
    {
      "text": "see the model does not produce good text",
      "start": 1463.2,
      "duration": 5.04
    },
    {
      "text": "because it has not been trained yet and",
      "start": 1465.399,
      "duration": 4.721
    },
    {
      "text": "uh so what we'll be doing is that how to",
      "start": 1468.24,
      "duration": 5.4
    },
    {
      "text": "measure what what good text is so that's",
      "start": 1470.12,
      "duration": 5.32
    },
    {
      "text": "why we have to define the loss function",
      "start": 1473.64,
      "duration": 3.8
    },
    {
      "text": "right now let's start with the main",
      "start": 1475.44,
      "duration": 4.56
    },
    {
      "text": "implementation in today's lecture and",
      "start": 1477.44,
      "duration": 5.16
    },
    {
      "text": "these are the inputs so the first input",
      "start": 1480.0,
      "duration": 4.48
    },
    {
      "text": "as as I told you on the Whiteboard is",
      "start": 1482.6,
      "duration": 4.079
    },
    {
      "text": "every effort moves these are the token",
      "start": 1484.48,
      "duration": 4.679
    },
    {
      "text": "IDs corresponding to the input text and",
      "start": 1486.679,
      "duration": 5.281
    },
    {
      "text": "the second input is I really like and",
      "start": 1489.159,
      "duration": 4.801
    },
    {
      "text": "the token IDs which are corresponding to",
      "start": 1491.96,
      "duration": 5.56
    },
    {
      "text": "the second input text are 40 1107 and",
      "start": 1493.96,
      "duration": 5.079
    },
    {
      "text": "588",
      "start": 1497.52,
      "duration": 3.639
    },
    {
      "text": "great and the targets are essentially",
      "start": 1499.039,
      "duration": 6.24
    },
    {
      "text": "3626 610 345 these are the True Values",
      "start": 1501.159,
      "duration": 6.561
    },
    {
      "text": "for the this for the first batch for the",
      "start": 1505.279,
      "duration": 5.52
    },
    {
      "text": "second batch the targets are 1107 588",
      "start": 1507.72,
      "duration": 4.16
    },
    {
      "text": "and",
      "start": 1510.799,
      "duration": 4.36
    },
    {
      "text": "11311 take a note that the targets are",
      "start": 1511.88,
      "duration": 5.799
    },
    {
      "text": "the input shifted by one so if you take",
      "start": 1515.159,
      "duration": 5.4
    },
    {
      "text": "the inputs and shift it or just take the",
      "start": 1517.679,
      "duration": 5.041
    },
    {
      "text": "last two values here and add a new value",
      "start": 1520.559,
      "duration": 4.161
    },
    {
      "text": "that's the targets this is because there",
      "start": 1522.72,
      "duration": 4.24
    },
    {
      "text": "are three input output",
      "start": 1524.72,
      "duration": 4.52
    },
    {
      "text": "pairs okay now now once we get the",
      "start": 1526.96,
      "duration": 4.599
    },
    {
      "text": "inputs what we are going to do is that",
      "start": 1529.24,
      "duration": 3.64
    },
    {
      "text": "we have the inputs and we have the",
      "start": 1531.559,
      "duration": 3.041
    },
    {
      "text": "targets but we have not yet generated",
      "start": 1532.88,
      "duration": 4.159
    },
    {
      "text": "the outputs to generate the outputs what",
      "start": 1534.6,
      "duration": 4.24
    },
    {
      "text": "we'll do now is that we'll pass in the",
      "start": 1537.039,
      "duration": 4.441
    },
    {
      "text": "input through this GPT model we'll pass",
      "start": 1538.84,
      "duration": 4.48
    },
    {
      "text": "the input through this GPT model block",
      "start": 1541.48,
      "duration": 3.84
    },
    {
      "text": "and generate the output token IDs in",
      "start": 1543.32,
      "duration": 3.959
    },
    {
      "text": "this in this kind of a format which I've",
      "start": 1545.32,
      "duration": 5.16
    },
    {
      "text": "shown you okay so the first step is that",
      "start": 1547.279,
      "duration": 5.481
    },
    {
      "text": "remember that when you pass the input",
      "start": 1550.48,
      "duration": 4.679
    },
    {
      "text": "through the GPT block it returns this",
      "start": 1552.76,
      "duration": 5.76
    },
    {
      "text": "logits and which are not converted into",
      "start": 1555.159,
      "duration": 5.441
    },
    {
      "text": "normalized 0 to one format we have not",
      "start": 1558.52,
      "duration": 5.2
    },
    {
      "text": "applied soft Max so the first St step is",
      "start": 1560.6,
      "duration": 5.199
    },
    {
      "text": "that to get these Logics and pass them",
      "start": 1563.72,
      "duration": 4.88
    },
    {
      "text": "through a soft Max uh always keep an eye",
      "start": 1565.799,
      "duration": 6.88
    },
    {
      "text": "out for Dimensions why is it 2A 3A",
      "start": 1568.6,
      "duration": 7.48
    },
    {
      "text": "5257 the reason is because everything",
      "start": 1572.679,
      "duration": 5.521
    },
    {
      "text": "can be explained through this",
      "start": 1576.08,
      "duration": 4.599
    },
    {
      "text": "diagram look when the vocabulary is so",
      "start": 1578.2,
      "duration": 5.4
    },
    {
      "text": "let me first rub many things here just",
      "start": 1580.679,
      "duration": 6.36
    },
    {
      "text": "so that Things become a bit easier to",
      "start": 1583.6,
      "duration": 6.439
    },
    {
      "text": "understand",
      "start": 1587.039,
      "duration": 3.0
    },
    {
      "text": "okay so look when the vocabulary size",
      "start": 1590.32,
      "duration": 4.44
    },
    {
      "text": "was equal to 7 over here take a look at",
      "start": 1592.039,
      "duration": 5.64
    },
    {
      "text": "the form of this logic sensor so if the",
      "start": 1594.76,
      "duration": 4.96
    },
    {
      "text": "vocabulary size was equal to seven for",
      "start": 1597.679,
      "duration": 4.88
    },
    {
      "text": "one batch the logic sensor size was",
      "start": 1599.72,
      "duration": 6.199
    },
    {
      "text": "three rows and seven columns right now",
      "start": 1602.559,
      "duration": 5.041
    },
    {
      "text": "if there were two batches the size would",
      "start": 1605.919,
      "duration": 5.24
    },
    {
      "text": "have been 2 comma 3 comma 7 that's the",
      "start": 1607.6,
      "duration": 5.679
    },
    {
      "text": "general size of this logic sensor so",
      "start": 1611.159,
      "duration": 4.4
    },
    {
      "text": "it's batch",
      "start": 1613.279,
      "duration": 5.361
    },
    {
      "text": "size batch size the second is the number",
      "start": 1615.559,
      "duration": 4.921
    },
    {
      "text": "of",
      "start": 1618.64,
      "duration": 4.639
    },
    {
      "text": "tokens and the third is the",
      "start": 1620.48,
      "duration": 5.84
    },
    {
      "text": "vocabulary Dimension so in this case the",
      "start": 1623.279,
      "duration": 4.801
    },
    {
      "text": "vocabulary Dimension was seven so there",
      "start": 1626.32,
      "duration": 4.44
    },
    {
      "text": "are seven columns in our case there are",
      "start": 1628.08,
      "duration": 5.479
    },
    {
      "text": "50257 columns so this third dimension",
      "start": 1630.76,
      "duration": 4.799
    },
    {
      "text": "will be",
      "start": 1633.559,
      "duration": 5.401
    },
    {
      "text": "5257 so here we see that the probab the",
      "start": 1635.559,
      "duration": 5.6
    },
    {
      "text": "logic tensor is converted into a tensor",
      "start": 1638.96,
      "duration": 4.52
    },
    {
      "text": "of probabilities whose shape is batch",
      "start": 1641.159,
      "duration": 4.321
    },
    {
      "text": "size number of tokens and vocabulary",
      "start": 1643.48,
      "duration": 6.439
    },
    {
      "text": "size so that will be 2 comma 3A",
      "start": 1645.48,
      "duration": 6.919
    },
    {
      "text": "50257 great and now what we'll do as I",
      "start": 1649.919,
      "duration": 4.24
    },
    {
      "text": "mentioned to you the next step is that",
      "start": 1652.399,
      "duration": 5.681
    },
    {
      "text": "we extract from each row the index which",
      "start": 1654.159,
      "duration": 6.12
    },
    {
      "text": "has the maximum value and that will be",
      "start": 1658.08,
      "duration": 5.199
    },
    {
      "text": "done using the AR Max function and these",
      "start": 1660.279,
      "duration": 5.12
    },
    {
      "text": "indexes with the maximum value that's",
      "start": 1663.279,
      "duration": 5.921
    },
    {
      "text": "our output from each given batch so what",
      "start": 1665.399,
      "duration": 5.321
    },
    {
      "text": "we are now going to do is that we are",
      "start": 1669.2,
      "duration": 4.359
    },
    {
      "text": "going to do torch. AR Max you can even",
      "start": 1670.72,
      "duration": 6.04
    },
    {
      "text": "go to the P Tor documentation and see",
      "start": 1673.559,
      "duration": 5.921
    },
    {
      "text": "what argmax does Arc Max basically",
      "start": 1676.76,
      "duration": 5.6
    },
    {
      "text": "Returns the indexes of the maximum value",
      "start": 1679.48,
      "duration": 5.52
    },
    {
      "text": "so we are doing tor. AR Max Dimension",
      "start": 1682.36,
      "duration": 4.319
    },
    {
      "text": "equal to minus one which is along the",
      "start": 1685.0,
      "duration": 3.799
    },
    {
      "text": "column so what this will do is that it",
      "start": 1686.679,
      "duration": 3.761
    },
    {
      "text": "will look at all the values in the",
      "start": 1688.799,
      "duration": 3.681
    },
    {
      "text": "columns and give the index of the value",
      "start": 1690.44,
      "duration": 4.119
    },
    {
      "text": "which is the give the index which has",
      "start": 1692.48,
      "duration": 4.919
    },
    {
      "text": "the maximum value and so here we see for",
      "start": 1694.559,
      "duration": 5.521
    },
    {
      "text": "the first batch there are this is the",
      "start": 1697.399,
      "duration": 5.841
    },
    {
      "text": "first tokens output second tokens output",
      "start": 1700.08,
      "duration": 5.36
    },
    {
      "text": "third tokens output index remember these",
      "start": 1703.24,
      "duration": 5.72
    },
    {
      "text": "are token IDs for the second batch this",
      "start": 1705.44,
      "duration": 6.56
    },
    {
      "text": "is the first tokens token ID output this",
      "start": 1708.96,
      "duration": 5.0
    },
    {
      "text": "is the second output token ID this is",
      "start": 1712.0,
      "duration": 4.48
    },
    {
      "text": "the third output token ID and these",
      "start": 1713.96,
      "duration": 4.88
    },
    {
      "text": "output token IDs are the ones which we",
      "start": 1716.48,
      "duration": 4.319
    },
    {
      "text": "want as close as possible to these",
      "start": 1718.84,
      "duration": 4.679
    },
    {
      "text": "Target token IDs that's the main goal",
      "start": 1720.799,
      "duration": 4.401
    },
    {
      "text": "and for this we are going to construct a",
      "start": 1723.519,
      "duration": 4.561
    },
    {
      "text": "loss function so now actually let's",
      "start": 1725.2,
      "duration": 5.04
    },
    {
      "text": "decode these tokens remember we have",
      "start": 1728.08,
      "duration": 5.04
    },
    {
      "text": "already written a function which decodes",
      "start": 1730.24,
      "duration": 5.52
    },
    {
      "text": "uh um here we have written a function",
      "start": 1733.12,
      "duration": 5.159
    },
    {
      "text": "which converts token IDs into text right",
      "start": 1735.76,
      "duration": 4.399
    },
    {
      "text": "let's decode these tokens in the output",
      "start": 1738.279,
      "duration": 4.561
    },
    {
      "text": "and let's see what they mean so we have",
      "start": 1740.159,
      "duration": 4.64
    },
    {
      "text": "passed our input into a GPT model and we",
      "start": 1742.84,
      "duration": 3.839
    },
    {
      "text": "have got these outputs and let's decode",
      "start": 1744.799,
      "duration": 2.88
    },
    {
      "text": "these",
      "start": 1746.679,
      "duration": 5.36
    },
    {
      "text": "now so if you decode for the first batch",
      "start": 1747.679,
      "duration": 8.24
    },
    {
      "text": "the output the input is effort moves you",
      "start": 1752.039,
      "duration": 7.281
    },
    {
      "text": "and uh the output is armed H Netflix",
      "start": 1755.919,
      "duration": 5.24
    },
    {
      "text": "completely random output which does not",
      "start": 1759.32,
      "duration": 4.52
    },
    {
      "text": "make sense we want this output to be",
      "start": 1761.159,
      "duration": 5.201
    },
    {
      "text": "every effort moves",
      "start": 1763.84,
      "duration": 5.88
    },
    {
      "text": "you uh every effort moves you yeah so",
      "start": 1766.36,
      "duration": 5.76
    },
    {
      "text": "input so the target is every effort",
      "start": 1769.72,
      "duration": 4.839
    },
    {
      "text": "moves you sorry effort moves you correct",
      "start": 1772.12,
      "duration": 4.279
    },
    {
      "text": "this is the true value so we want the",
      "start": 1774.559,
      "duration": 4.321
    },
    {
      "text": "target to be effort moves you but the",
      "start": 1776.399,
      "duration": 4.561
    },
    {
      "text": "actual output which we have got is armed",
      "start": 1778.88,
      "duration": 3.32
    },
    {
      "text": "H",
      "start": 1780.96,
      "duration": 3.8
    },
    {
      "text": "Netflix so if you look at the first",
      "start": 1782.2,
      "duration": 4.719
    },
    {
      "text": "batch you can see that there's a huge",
      "start": 1784.76,
      "duration": 3.68
    },
    {
      "text": "difference between the targets and the",
      "start": 1786.919,
      "duration": 4.321
    },
    {
      "text": "outputs right which makes sense",
      "start": 1788.44,
      "duration": 6.56
    },
    {
      "text": "because well uh we have not yet",
      "start": 1791.24,
      "duration": 5.799
    },
    {
      "text": "optimized we have not even defined the",
      "start": 1795.0,
      "duration": 3.6
    },
    {
      "text": "loss function we have not try to",
      "start": 1797.039,
      "duration": 4.161
    },
    {
      "text": "minimize the loss function so ideally",
      "start": 1798.6,
      "duration": 4.36
    },
    {
      "text": "now what we'll do is that we'll Define a",
      "start": 1801.2,
      "duration": 4.079
    },
    {
      "text": "loss function between the targets and",
      "start": 1802.96,
      "duration": 4.28
    },
    {
      "text": "the outputs from both the batches so",
      "start": 1805.279,
      "duration": 5.361
    },
    {
      "text": "that the target output and the so that",
      "start": 1807.24,
      "duration": 6.52
    },
    {
      "text": "the target text and the output text will",
      "start": 1810.64,
      "duration": 5.8
    },
    {
      "text": "be as close as possible to each",
      "start": 1813.76,
      "duration": 5.96
    },
    {
      "text": "other great now that actually motivates",
      "start": 1816.44,
      "duration": 6.0
    },
    {
      "text": "us to go to our next section in today's",
      "start": 1819.72,
      "duration": 4.4
    },
    {
      "text": "lecture which is actually defining the",
      "start": 1822.44,
      "duration": 4.76
    },
    {
      "text": "loss between the targets and the output",
      "start": 1824.12,
      "duration": 5.399
    },
    {
      "text": "so now let let's see what's going on",
      "start": 1827.2,
      "duration": 5.12
    },
    {
      "text": "here so we have the inputs and this is",
      "start": 1829.519,
      "duration": 5.481
    },
    {
      "text": "the probability tensor which indicates",
      "start": 1832.32,
      "duration": 5.44
    },
    {
      "text": "what's the probability of the next token",
      "start": 1835.0,
      "duration": 4.159
    },
    {
      "text": "for every input",
      "start": 1837.76,
      "duration": 4.08
    },
    {
      "text": "token what I've marked in blue over here",
      "start": 1839.159,
      "duration": 6.0
    },
    {
      "text": "is the target indexes so if we want the",
      "start": 1841.84,
      "duration": 5.24
    },
    {
      "text": "actual answer to be that for every it",
      "start": 1845.159,
      "duration": 3.721
    },
    {
      "text": "should be effort for every effort it",
      "start": 1847.08,
      "duration": 3.719
    },
    {
      "text": "should be moves and for every effort",
      "start": 1848.88,
      "duration": 4.919
    },
    {
      "text": "moves it should be U the the correct",
      "start": 1850.799,
      "duration": 5.081
    },
    {
      "text": "answer of the indices which we want is 1",
      "start": 1853.799,
      "duration": 4.76
    },
    {
      "text": "four and five these are the true",
      "start": 1855.88,
      "duration": 8.639
    },
    {
      "text": "values uh so the token IDs here are 3626",
      "start": 1858.559,
      "duration": 10.881
    },
    {
      "text": "610 and 345 but uh in the sample code",
      "start": 1864.519,
      "duration": 6.721
    },
    {
      "text": "which I've shown over here we just has",
      "start": 1869.44,
      "duration": 4.28
    },
    {
      "text": "seven uh we just has seven vocabulary",
      "start": 1871.24,
      "duration": 4.76
    },
    {
      "text": "size the token IDs which are 1 four and",
      "start": 1873.72,
      "duration": 5.12
    },
    {
      "text": "five those are the correct ones so now",
      "start": 1876.0,
      "duration": 4.36
    },
    {
      "text": "one thing which I would like to mention",
      "start": 1878.84,
      "duration": 4.199
    },
    {
      "text": "is that let's say these indexes which",
      "start": 1880.36,
      "duration": 4.52
    },
    {
      "text": "are marked with the star right now these",
      "start": 1883.039,
      "duration": 3.841
    },
    {
      "text": "are the actual indexes which we want",
      "start": 1884.88,
      "duration": 3.639
    },
    {
      "text": "right",
      "start": 1886.88,
      "duration": 3.48
    },
    {
      "text": "but if you see the values corresponding",
      "start": 1888.519,
      "duration": 3.921
    },
    {
      "text": "to those those values are not the",
      "start": 1890.36,
      "duration": 4.159
    },
    {
      "text": "highest because our llm is not trained",
      "start": 1892.44,
      "duration": 4.56
    },
    {
      "text": "yet so what I'm going to do now is that",
      "start": 1894.519,
      "duration": 4.921
    },
    {
      "text": "I'm going to collect the probabilities",
      "start": 1897.0,
      "duration": 4.2
    },
    {
      "text": "at these indexes so I'm going to take",
      "start": 1899.44,
      "duration": 4.079
    },
    {
      "text": "this index I'm going to take this index",
      "start": 1901.2,
      "duration": 4.4
    },
    {
      "text": "and I'm going to take this index and I'm",
      "start": 1903.519,
      "duration": 4.28
    },
    {
      "text": "going to find the probabilities at these",
      "start": 1905.6,
      "duration": 5.16
    },
    {
      "text": "these indexes or these indices rather",
      "start": 1907.799,
      "duration": 4.801
    },
    {
      "text": "and I'm going to collect these together",
      "start": 1910.76,
      "duration": 4.08
    },
    {
      "text": "so for example for the first batch what",
      "start": 1912.6,
      "duration": 5.64
    },
    {
      "text": "I collect can be 144",
      "start": 1914.84,
      "duration": 6.319
    },
    {
      "text": "and3 these are the set of probabilities",
      "start": 1918.24,
      "duration": 4.6
    },
    {
      "text": "and for the second batch I'll collect",
      "start": 1921.159,
      "duration": 3.48
    },
    {
      "text": "another set of three probabilities like",
      "start": 1922.84,
      "duration": 4.319
    },
    {
      "text": "this this is exactly what we are doing",
      "start": 1924.639,
      "duration": 4.721
    },
    {
      "text": "in the actual problem in the actual",
      "start": 1927.159,
      "duration": 4.161
    },
    {
      "text": "problem we know that the target indexes",
      "start": 1929.36,
      "duration": 4.0
    },
    {
      "text": "are these so what I'll do is that I look",
      "start": 1931.32,
      "duration": 5.319
    },
    {
      "text": "at my probability tensor and I'll get",
      "start": 1933.36,
      "duration": 5.72
    },
    {
      "text": "the probability which is corresponding",
      "start": 1936.639,
      "duration": 4.801
    },
    {
      "text": "to these indexes I know it will not be",
      "start": 1939.08,
      "duration": 4.8
    },
    {
      "text": "maximum because my llm is not optimized",
      "start": 1941.44,
      "duration": 3.76
    },
    {
      "text": "but I'll just write down these",
      "start": 1943.88,
      "duration": 4.2
    },
    {
      "text": "probabilities for now so uh I I have",
      "start": 1945.2,
      "duration": 4.599
    },
    {
      "text": "these Target indices and let me call",
      "start": 1948.08,
      "duration": 5.439
    },
    {
      "text": "them i11 i12 I13 for the first batch and",
      "start": 1949.799,
      "duration": 7.441
    },
    {
      "text": "i21 i22 and i23 for the second",
      "start": 1953.519,
      "duration": 6.64
    },
    {
      "text": "batch so what I'm uh so what I'll be",
      "start": 1957.24,
      "duration": 6.08
    },
    {
      "text": "doing now is that I'll be uh looking at",
      "start": 1960.159,
      "duration": 4.76
    },
    {
      "text": "batch one and I'll be looking at badge",
      "start": 1963.32,
      "duration": 3.56
    },
    {
      "text": "two and I'll find the value",
      "start": 1964.919,
      "duration": 3.841
    },
    {
      "text": "corresponding to these indices in the",
      "start": 1966.88,
      "duration": 5.36
    },
    {
      "text": "probabilities tensor so in this in in",
      "start": 1968.76,
      "duration": 5.279
    },
    {
      "text": "the probability tensor which look which",
      "start": 1972.24,
      "duration": 3.679
    },
    {
      "text": "will look something like this for the",
      "start": 1974.039,
      "duration": 3.961
    },
    {
      "text": "50257",
      "start": 1975.919,
      "duration": 4.0
    },
    {
      "text": "vocabulary size also I'll find the",
      "start": 1978.0,
      "duration": 4.279
    },
    {
      "text": "probabilities corresponding to i11 i12",
      "start": 1979.919,
      "duration": 4.961
    },
    {
      "text": "and I13 for batch number one and I'll",
      "start": 1982.279,
      "duration": 5.0
    },
    {
      "text": "find the probabilities corresponding to",
      "start": 1984.88,
      "duration": 6.36
    },
    {
      "text": "i21 i22 and i23 for batch number two so",
      "start": 1987.279,
      "duration": 5.52
    },
    {
      "text": "for batch number one I'll aggregate",
      "start": 1991.24,
      "duration": 3.12
    },
    {
      "text": "these probabilities together and they",
      "start": 1992.799,
      "duration": 4.961
    },
    {
      "text": "look like this P11 p12 p13 for batch two",
      "start": 1994.36,
      "duration": 5.0
    },
    {
      "text": "I'll aggregate these probabilities and",
      "start": 1997.76,
      "duration": 5.2
    },
    {
      "text": "they look like p21 P22 and p23 remember",
      "start": 1999.36,
      "duration": 5.08
    },
    {
      "text": "these are the probabilities which are",
      "start": 2002.96,
      "duration": 4.959
    },
    {
      "text": "not maximum right now the goal of",
      "start": 2004.44,
      "duration": 5.16
    },
    {
      "text": "training is that to get all of these",
      "start": 2007.919,
      "duration": 4.401
    },
    {
      "text": "values as close to one as possible and",
      "start": 2009.6,
      "duration": 4.48
    },
    {
      "text": "why do we want all these values close to",
      "start": 2012.32,
      "duration": 3.68
    },
    {
      "text": "one as possible because then we'll make",
      "start": 2014.08,
      "duration": 4.68
    },
    {
      "text": "sure that the output indexes which have",
      "start": 2016.0,
      "duration": 5.12
    },
    {
      "text": "the maximum probabilities will be closer",
      "start": 2018.76,
      "duration": 7.399
    },
    {
      "text": "to i11 i12 I13 i21 i22 and i23 which are",
      "start": 2021.12,
      "duration": 6.2
    },
    {
      "text": "my",
      "start": 2026.159,
      "duration": 4.64
    },
    {
      "text": "targets so the goal of the llm",
      "start": 2027.32,
      "duration": 5.8
    },
    {
      "text": "Performing better now is reduced to this",
      "start": 2030.799,
      "duration": 4.641
    },
    {
      "text": "problem that I want these probabilities",
      "start": 2033.12,
      "duration": 4.32
    },
    {
      "text": "to be as close to one as possible I want",
      "start": 2035.44,
      "duration": 4.239
    },
    {
      "text": "all six probabilities to be as close to",
      "start": 2037.44,
      "duration": 4.479
    },
    {
      "text": "one why are there six probabilities",
      "start": 2039.679,
      "duration": 4.24
    },
    {
      "text": "because there are two batches each batch",
      "start": 2041.919,
      "duration": 3.961
    },
    {
      "text": "has three tokens so there are three",
      "start": 2043.919,
      "duration": 4.081
    },
    {
      "text": "prediction tasks so that's why there are",
      "start": 2045.88,
      "duration": 3.92
    },
    {
      "text": "three probabilities for the first batch",
      "start": 2048.0,
      "duration": 3.879
    },
    {
      "text": "three probabilities for the second",
      "start": 2049.8,
      "duration": 4.24
    },
    {
      "text": "batch so let me merge these",
      "start": 2051.879,
      "duration": 3.8
    },
    {
      "text": "probabilities together and then this",
      "start": 2054.04,
      "duration": 4.0
    },
    {
      "text": "will be P11 p12",
      "start": 2055.679,
      "duration": 6.641
    },
    {
      "text": "p13 and this will be p21 P22 and",
      "start": 2058.04,
      "duration": 7.359
    },
    {
      "text": "p23 now what I want is I want all of",
      "start": 2062.32,
      "duration": 7.72
    },
    {
      "text": "these P11 p12 p13 p21 P22 and p23 I want",
      "start": 2065.399,
      "duration": 6.841
    },
    {
      "text": "all of these to be as close to one as",
      "start": 2070.04,
      "duration": 5.039
    },
    {
      "text": "possible how do I enforce this",
      "start": 2072.24,
      "duration": 4.72
    },
    {
      "text": "mathematically first let's see the",
      "start": 2075.079,
      "duration": 4.76
    },
    {
      "text": "workflow so we had the logit tensor we",
      "start": 2076.96,
      "duration": 4.679
    },
    {
      "text": "converted it into probabilities through",
      "start": 2079.839,
      "duration": 4.76
    },
    {
      "text": "soft Max and then we had the target",
      "start": 2081.639,
      "duration": 6.081
    },
    {
      "text": "probabilities tensor what this target",
      "start": 2084.599,
      "duration": 5.121
    },
    {
      "text": "probabilities means is that we have the",
      "start": 2087.72,
      "duration": 4.199
    },
    {
      "text": "IND indices corresponding to the Target",
      "start": 2089.72,
      "duration": 5.439
    },
    {
      "text": "values so we have these indexes i11 i12",
      "start": 2091.919,
      "duration": 6.721
    },
    {
      "text": "I13 i21 i22 i23 these Target",
      "start": 2095.159,
      "duration": 6.001
    },
    {
      "text": "probabilities are just the merge merge",
      "start": 2098.64,
      "duration": 5.04
    },
    {
      "text": "Target probabilities which is",
      "start": 2101.16,
      "duration": 6.32
    },
    {
      "text": "P11 p12 dot dot dot up to p23 these are",
      "start": 2103.68,
      "duration": 6.36
    },
    {
      "text": "the six probabilities and we want all of",
      "start": 2107.48,
      "duration": 5.52
    },
    {
      "text": "these to be as close to one as possible",
      "start": 2110.04,
      "duration": 5.64
    },
    {
      "text": "to all of you students who have studied",
      "start": 2113.0,
      "duration": 5.0
    },
    {
      "text": "uh classification and the loss this",
      "start": 2115.68,
      "duration": 4.36
    },
    {
      "text": "problem would be familiar since we are",
      "start": 2118.0,
      "duration": 4.04
    },
    {
      "text": "dealing with probabilities it's natural",
      "start": 2120.04,
      "duration": 4.039
    },
    {
      "text": "that logarithms and cross entropy will",
      "start": 2122.04,
      "duration": 4.4
    },
    {
      "text": "come into the picture so instead of",
      "start": 2124.079,
      "duration": 3.601
    },
    {
      "text": "directly dealing with this number",
      "start": 2126.44,
      "duration": 3.08
    },
    {
      "text": "numbers it's much better mathematically",
      "start": 2127.68,
      "duration": 4.04
    },
    {
      "text": "and from an optimization P perspective",
      "start": 2129.52,
      "duration": 4.319
    },
    {
      "text": "to just take the logarithm of these",
      "start": 2131.72,
      "duration": 4.72
    },
    {
      "text": "values and it comes out to be this and",
      "start": 2133.839,
      "duration": 4.601
    },
    {
      "text": "then we take the average of all of these",
      "start": 2136.44,
      "duration": 4.12
    },
    {
      "text": "logarithm values and it comes out to be",
      "start": 2138.44,
      "duration": 4.24
    },
    {
      "text": "this and then we take the negative of",
      "start": 2140.56,
      "duration": 6.44
    },
    {
      "text": "this average so that's 10.77%",
      "start": 2142.68,
      "duration": 4.32
    },
    {
      "text": "Target probabilities and we find the",
      "start": 2158.079,
      "duration": 4.401
    },
    {
      "text": "mean of so we take the then the log of",
      "start": 2160.56,
      "duration": 6.44
    },
    {
      "text": "this so we take log P11 log p12 and the",
      "start": 2162.48,
      "duration": 6.2
    },
    {
      "text": "last is Log",
      "start": 2167.0,
      "duration": 4.319
    },
    {
      "text": "p23 and then what we do is that we find",
      "start": 2168.68,
      "duration": 5.52
    },
    {
      "text": "the mean of this",
      "start": 2171.319,
      "duration": 7.201
    },
    {
      "text": "mean and uh so then that will be Sigma",
      "start": 2174.2,
      "duration": 7.6
    },
    {
      "text": "which will be the summation of log 1 one",
      "start": 2178.52,
      "duration": 6.64
    },
    {
      "text": "summation of log P11",
      "start": 2181.8,
      "duration": 8.799
    },
    {
      "text": "DOT log p23 divided 6 and then I'll just",
      "start": 2185.16,
      "duration": 7.84
    },
    {
      "text": "take the negative of",
      "start": 2190.599,
      "duration": 6.881
    },
    {
      "text": "this and that's called the negative log",
      "start": 2193.0,
      "duration": 4.48
    },
    {
      "text": "likelihood so then that will be negative",
      "start": 2198.72,
      "duration": 4.68
    },
    {
      "text": "of summation of log of",
      "start": 2201.119,
      "duration": 5.881
    },
    {
      "text": "P11 dot dot dot up till log of",
      "start": 2203.4,
      "duration": 8.52
    },
    {
      "text": "p23 divided 6 so now see we want",
      "start": 2207.0,
      "duration": 9.76
    },
    {
      "text": "uh we want P11 to be close to uh one we",
      "start": 2211.92,
      "duration": 6.88
    },
    {
      "text": "want p all the probabilities to be close",
      "start": 2216.76,
      "duration": 4.28
    },
    {
      "text": "to one so we essentially want this",
      "start": 2218.8,
      "duration": 4.48
    },
    {
      "text": "negative log likelihood to be as less as",
      "start": 2221.04,
      "duration": 4.6
    },
    {
      "text": "possible so this cross entropy loss we",
      "start": 2223.28,
      "duration": 4.28
    },
    {
      "text": "want to be as low as possible and as",
      "start": 2225.64,
      "duration": 7.16
    },
    {
      "text": "close to zero as possible so uh why are",
      "start": 2227.56,
      "duration": 6.6
    },
    {
      "text": "we taking",
      "start": 2232.8,
      "duration": 5.0
    },
    {
      "text": "the negative log likelihood the reason",
      "start": 2234.16,
      "duration": 5.439
    },
    {
      "text": "we take the negative log likelihood is",
      "start": 2237.8,
      "duration": 3.88
    },
    {
      "text": "because it just makes more sense if you",
      "start": 2239.599,
      "duration": 4.52
    },
    {
      "text": "if you don't take the negative value",
      "start": 2241.68,
      "duration": 6.12
    },
    {
      "text": "then uh the loss function would look",
      "start": 2244.119,
      "duration": 6.081
    },
    {
      "text": "reverse of this the loss function would",
      "start": 2247.8,
      "duration": 3.96
    },
    {
      "text": "look reverse of this and then we'll have",
      "start": 2250.2,
      "duration": 3.44
    },
    {
      "text": "to reframe the problem as trying to",
      "start": 2251.76,
      "duration": 4.64
    },
    {
      "text": "maximize the loss instead it just makes",
      "start": 2253.64,
      "duration": 6.719
    },
    {
      "text": "more intuitive sense if the negative",
      "start": 2256.4,
      "duration": 7.439
    },
    {
      "text": "looks like this and now the whole",
      "start": 2260.359,
      "duration": 3.48
    },
    {
      "text": "goal instead it makes more physical and",
      "start": 2265.64,
      "duration": 5.08
    },
    {
      "text": "intuitive sense if the log likelihood",
      "start": 2268.52,
      "duration": 4.44
    },
    {
      "text": "looks like this and now our whole goal",
      "start": 2270.72,
      "duration": 5.56
    },
    {
      "text": "is to bring down this loss as low as",
      "start": 2272.96,
      "duration": 5.32
    },
    {
      "text": "possible what will happen if this loss",
      "start": 2276.28,
      "duration": 4.839
    },
    {
      "text": "is brought down to let's say zero if the",
      "start": 2278.28,
      "duration": 4.64
    },
    {
      "text": "loss is brought down to zero which means",
      "start": 2281.119,
      "duration": 4.121
    },
    {
      "text": "that the mean which means this value is",
      "start": 2282.92,
      "duration": 4.439
    },
    {
      "text": "Clos to one which means that there is a",
      "start": 2285.24,
      "duration": 5.44
    },
    {
      "text": "higher chance that P11 p12 p13 up till",
      "start": 2287.359,
      "duration": 5.881
    },
    {
      "text": "p23 are equal to one that is exactly",
      "start": 2290.68,
      "duration": 4.8
    },
    {
      "text": "what we want the goal of training is to",
      "start": 2293.24,
      "duration": 5.359
    },
    {
      "text": "get these values as close to one as",
      "start": 2295.48,
      "duration": 6.44
    },
    {
      "text": "possible so that is the main uh main",
      "start": 2298.599,
      "duration": 5.601
    },
    {
      "text": "hope now so initially let's say when we",
      "start": 2301.92,
      "duration": 5.36
    },
    {
      "text": "start the training procedure our value",
      "start": 2304.2,
      "duration": 6.68
    },
    {
      "text": "uh the x value whose log we are going to",
      "start": 2307.28,
      "duration": 5.72
    },
    {
      "text": "take that will not be close to one it",
      "start": 2310.88,
      "duration": 3.88
    },
    {
      "text": "will be somewhere maybe here it will be",
      "start": 2313.0,
      "duration": 3.44
    },
    {
      "text": "somewhere 0 to one and somewhere here",
      "start": 2314.76,
      "duration": 4.559
    },
    {
      "text": "the goal is to go here",
      "start": 2316.44,
      "duration": 5.0
    },
    {
      "text": "slowly so that the mean of the",
      "start": 2319.319,
      "duration": 5.441
    },
    {
      "text": "probabilities will be equal to",
      "start": 2321.44,
      "duration": 7.32
    },
    {
      "text": "1 okay so this is the negative log",
      "start": 2324.76,
      "duration": 6.68
    },
    {
      "text": "likelihood and it's also called as",
      "start": 2328.76,
      "duration": 5.079
    },
    {
      "text": "the uh it's also called as the cross",
      "start": 2331.44,
      "duration": 5.639
    },
    {
      "text": "entropy loss and uh we want to minimize",
      "start": 2333.839,
      "duration": 5.081
    },
    {
      "text": "this cross cross entropy loss as much as",
      "start": 2337.079,
      "duration": 3.881
    },
    {
      "text": "possible that will ensure that the",
      "start": 2338.92,
      "duration": 4.64
    },
    {
      "text": "indexes at which our",
      "start": 2340.96,
      "duration": 5.52
    },
    {
      "text": "tokens uh indexes at which we predict",
      "start": 2343.56,
      "duration": 4.72
    },
    {
      "text": "the next token that matches with the",
      "start": 2346.48,
      "duration": 4.44
    },
    {
      "text": "target indexes so then the output and",
      "start": 2348.28,
      "duration": 3.319
    },
    {
      "text": "the",
      "start": 2350.92,
      "duration": 3.36
    },
    {
      "text": "target indexes will match closely to",
      "start": 2351.599,
      "duration": 4.52
    },
    {
      "text": "each other and then we know that the",
      "start": 2354.28,
      "duration": 3.88
    },
    {
      "text": "large language model is actually doing a",
      "start": 2356.119,
      "duration": 4.521
    },
    {
      "text": "very good job so this is exactly what we",
      "start": 2358.16,
      "duration": 4.72
    },
    {
      "text": "are going to do right now in the code",
      "start": 2360.64,
      "duration": 3.8
    },
    {
      "text": "one thing which I would like to mention",
      "start": 2362.88,
      "duration": 4.12
    },
    {
      "text": "about the cross entropy loss is that the",
      "start": 2364.44,
      "duration": 4.56
    },
    {
      "text": "cross entropy loss essentially measures",
      "start": 2367.0,
      "duration": 3.88
    },
    {
      "text": "the difference between two probability",
      "start": 2369.0,
      "duration": 3.96
    },
    {
      "text": "distributions so here what we are",
      "start": 2370.88,
      "duration": 3.68
    },
    {
      "text": "actually doing is that we are just",
      "start": 2372.96,
      "duration": 3.92
    },
    {
      "text": "adding discrete probabilities but in a",
      "start": 2374.56,
      "duration": 4.24
    },
    {
      "text": "sense this can also be called as the",
      "start": 2376.88,
      "duration": 4.0
    },
    {
      "text": "categorical or I should call it the",
      "start": 2378.8,
      "duration": 3.4
    },
    {
      "text": "cross entropy",
      "start": 2380.88,
      "duration": 3.88
    },
    {
      "text": "loss so now let's see the sequence of",
      "start": 2382.2,
      "duration": 4.6
    },
    {
      "text": "steps which we are going to implement",
      "start": 2384.76,
      "duration": 4.52
    },
    {
      "text": "the logit tensor which is a tensor of",
      "start": 2386.8,
      "duration": 5.0
    },
    {
      "text": "probabilities uh has the shape of 2 into",
      "start": 2389.28,
      "duration": 3.92
    },
    {
      "text": "3x",
      "start": 2391.8,
      "duration": 4.4
    },
    {
      "text": "50257 why because it looks something",
      "start": 2393.2,
      "duration": 5.28
    },
    {
      "text": "like this so every effort moves and then",
      "start": 2396.2,
      "duration": 3.84
    },
    {
      "text": "we have",
      "start": 2398.48,
      "duration": 5.359
    },
    {
      "text": "50257 uh so this is the first batch and",
      "start": 2400.04,
      "duration": 5.84
    },
    {
      "text": "this is the second batch what we are",
      "start": 2403.839,
      "duration": 3.441
    },
    {
      "text": "going to do is that we are going to",
      "start": 2405.88,
      "duration": 4.959
    },
    {
      "text": "flatten this so that these two first",
      "start": 2407.28,
      "duration": 7.52
    },
    {
      "text": "batch and second batch are merged",
      "start": 2410.839,
      "duration": 6.48
    },
    {
      "text": "together right so these are so this is",
      "start": 2414.8,
      "duration": 4.6
    },
    {
      "text": "my output tensor right now every effort",
      "start": 2417.319,
      "duration": 4.04
    },
    {
      "text": "moves you I really like it's a merging",
      "start": 2419.4,
      "duration": 5.36
    },
    {
      "text": "of these two batches and my target so my",
      "start": 2421.359,
      "duration": 6.321
    },
    {
      "text": "target is 2x3 why is it 2 by three",
      "start": 2424.76,
      "duration": 6.0
    },
    {
      "text": "because for the first batch this is the",
      "start": 2427.68,
      "duration": 5.8
    },
    {
      "text": "target index of the first input every",
      "start": 2430.76,
      "duration": 4.599
    },
    {
      "text": "this is the target index of every effort",
      "start": 2433.48,
      "duration": 3.599
    },
    {
      "text": "and this is the target IND index of",
      "start": 2435.359,
      "duration": 4.841
    },
    {
      "text": "every effort moves 107 is the target",
      "start": 2437.079,
      "duration": 6.721
    },
    {
      "text": "index of I so it will be really 588 is",
      "start": 2440.2,
      "duration": 6.68
    },
    {
      "text": "the target index of I really and 11311",
      "start": 2443.8,
      "duration": 5.36
    },
    {
      "text": "is the target index of I really like so",
      "start": 2446.88,
      "duration": 4.68
    },
    {
      "text": "then chocolate then what we are going to",
      "start": 2449.16,
      "duration": 4.24
    },
    {
      "text": "do is we are going to flatten this out",
      "start": 2451.56,
      "duration": 4.36
    },
    {
      "text": "so then this will be 3626",
      "start": 2453.4,
      "duration": 7.56
    },
    {
      "text": "610 uh 345 107 588 and",
      "start": 2455.92,
      "duration": 8.24
    },
    {
      "text": "11311 now ideally what we need is that",
      "start": 2460.96,
      "duration": 5.56
    },
    {
      "text": "we are going to look at every year and",
      "start": 2464.16,
      "duration": 4.48
    },
    {
      "text": "we are going to look at the index",
      "start": 2466.52,
      "duration": 4.52
    },
    {
      "text": "corresponding to 3626 so let's say if",
      "start": 2468.64,
      "duration": 4.679
    },
    {
      "text": "this is the index corresponding to 3626",
      "start": 2471.04,
      "duration": 4.88
    },
    {
      "text": "this will be P11 for effort we are going",
      "start": 2473.319,
      "duration": 4.601
    },
    {
      "text": "to look at the index corresponding to",
      "start": 2475.92,
      "duration": 4.48
    },
    {
      "text": "610 let's say it's this one so that will",
      "start": 2477.92,
      "duration": 6.56
    },
    {
      "text": "be p12 similarly for like which is the",
      "start": 2480.4,
      "duration": 5.959
    },
    {
      "text": "last we going to look at the index",
      "start": 2484.48,
      "duration": 4.04
    },
    {
      "text": "corresponding to 1 311 so let's say this",
      "start": 2486.359,
      "duration": 3.601
    },
    {
      "text": "is this so this is",
      "start": 2488.52,
      "duration": 4.44
    },
    {
      "text": "p23 so what we'll get is that we we'll",
      "start": 2489.96,
      "duration": 7.76
    },
    {
      "text": "get P11 p12 p13 p21 P22 p23 we'll take",
      "start": 2492.96,
      "duration": 7.08
    },
    {
      "text": "the log of them we'll add take the mean",
      "start": 2497.72,
      "duration": 4.52
    },
    {
      "text": "and then do the negative this whole",
      "start": 2500.04,
      "duration": 4.44
    },
    {
      "text": "process is encapsulated in this just one",
      "start": 2502.24,
      "duration": 5.839
    },
    {
      "text": "line of code tor. nn. functional cross",
      "start": 2504.48,
      "duration": 6.8
    },
    {
      "text": "entropy logits so this is the logits",
      "start": 2508.079,
      "duration": 6.121
    },
    {
      "text": "flat I'm calling this logic flat tensor",
      "start": 2511.28,
      "duration": 4.48
    },
    {
      "text": "and the second argument here is the",
      "start": 2514.2,
      "duration": 6.08
    },
    {
      "text": "target flat what this uh this code will",
      "start": 2515.76,
      "duration": 6.76
    },
    {
      "text": "do is that tor. nl. function cross",
      "start": 2520.28,
      "duration": 5.079
    },
    {
      "text": "entropy logit flat comma targets flat",
      "start": 2522.52,
      "duration": 4.76
    },
    {
      "text": "first it will convert this logits tensor",
      "start": 2525.359,
      "duration": 4.48
    },
    {
      "text": "into a tensor of probabilities until now",
      "start": 2527.28,
      "duration": 5.24
    },
    {
      "text": "the logits is is not does not represent",
      "start": 2529.839,
      "duration": 5.041
    },
    {
      "text": "probabilities so it this function will",
      "start": 2532.52,
      "duration": 5.079
    },
    {
      "text": "apply soft Max to the Lo logits and then",
      "start": 2534.88,
      "duration": 4.84
    },
    {
      "text": "what it will do is that as I told you",
      "start": 2537.599,
      "duration": 4.201
    },
    {
      "text": "before it will find the negative log",
      "start": 2539.72,
      "duration": 4.639
    },
    {
      "text": "likelihood so first it",
      "start": 2541.8,
      "duration": 6.039
    },
    {
      "text": "will uh find the probabilities P11 p12",
      "start": 2544.359,
      "duration": 7.441
    },
    {
      "text": "p13 p21 P22 and p23 in this logic flat",
      "start": 2547.839,
      "duration": 5.961
    },
    {
      "text": "tensor corresponding to the index",
      "start": 2551.8,
      "duration": 4.92
    },
    {
      "text": "indices in the Target flat tensor it",
      "start": 2553.8,
      "duration": 4.64
    },
    {
      "text": "will take the negative log and then it",
      "start": 2556.72,
      "duration": 4.16
    },
    {
      "text": "will find the mean this one line of code",
      "start": 2558.44,
      "duration": 4.919
    },
    {
      "text": "is going to do all of the steps for us",
      "start": 2560.88,
      "duration": 4.36
    },
    {
      "text": "that's why python is so powerful in one",
      "start": 2563.359,
      "duration": 4.321
    },
    {
      "text": "line of code we'll not only convert this",
      "start": 2565.24,
      "duration": 6.2
    },
    {
      "text": "Logics into uh a tensor of probabilities",
      "start": 2567.68,
      "duration": 5.879
    },
    {
      "text": "through soft Max but we will also get",
      "start": 2571.44,
      "duration": 4.72
    },
    {
      "text": "the negative log likelihood and that's",
      "start": 2573.559,
      "duration": 4.161
    },
    {
      "text": "the final loss function which we are",
      "start": 2576.16,
      "duration": 3.84
    },
    {
      "text": "looking for so this one line of code is",
      "start": 2577.72,
      "duration": 3.839
    },
    {
      "text": "going to give us the loss between the",
      "start": 2580.0,
      "duration": 4.2
    },
    {
      "text": "output and the Target and that's why",
      "start": 2581.559,
      "duration": 6.081
    },
    {
      "text": "it's a very important piece of code uh",
      "start": 2584.2,
      "duration": 5.8
    },
    {
      "text": "to do all of these steps in just one",
      "start": 2587.64,
      "duration": 4.719
    },
    {
      "text": "line of code I think it's pretty amazing",
      "start": 2590.0,
      "duration": 4.48
    },
    {
      "text": "but what I want you all to focus on is",
      "start": 2592.359,
      "duration": 4.76
    },
    {
      "text": "what we really are doing over here what",
      "start": 2594.48,
      "duration": 4.56
    },
    {
      "text": "we are essentially doing is",
      "start": 2597.119,
      "duration": 4.601
    },
    {
      "text": "that we have inputs and those inputs are",
      "start": 2599.04,
      "duration": 5.2
    },
    {
      "text": "every effort moves and I really like we",
      "start": 2601.72,
      "duration": 4.08
    },
    {
      "text": "are passing them through the GPT",
      "start": 2604.24,
      "duration": 4.119
    },
    {
      "text": "architecture and then we get this logits",
      "start": 2605.8,
      "duration": 5.08
    },
    {
      "text": "we get this logit tensor what we are",
      "start": 2608.359,
      "duration": 4.321
    },
    {
      "text": "going to do with this logit tensor is",
      "start": 2610.88,
      "duration": 3.959
    },
    {
      "text": "that we'll first merge them merge this",
      "start": 2612.68,
      "duration": 4.52
    },
    {
      "text": "tensor merge the batches and call it",
      "start": 2614.839,
      "duration": 4.641
    },
    {
      "text": "logits flat then what we'll do is that",
      "start": 2617.2,
      "duration": 4.639
    },
    {
      "text": "corresponding to every token we are",
      "start": 2619.48,
      "duration": 4.839
    },
    {
      "text": "going to look at the indexes which",
      "start": 2621.839,
      "duration": 5.0
    },
    {
      "text": "corresponds to the Target and then we",
      "start": 2624.319,
      "duration": 4.921
    },
    {
      "text": "are going to find the probabilities now",
      "start": 2626.839,
      "duration": 4.401
    },
    {
      "text": "in an Ideal World these probabilities",
      "start": 2629.24,
      "duration": 3.4
    },
    {
      "text": "will be close to",
      "start": 2631.24,
      "duration": 3.599
    },
    {
      "text": "one because that would mean that the",
      "start": 2632.64,
      "duration": 5.719
    },
    {
      "text": "output and the in uh Target will match",
      "start": 2634.839,
      "duration": 6.48
    },
    {
      "text": "but since the llm is not optimized these",
      "start": 2638.359,
      "duration": 4.881
    },
    {
      "text": "probabilities which correspond to the",
      "start": 2641.319,
      "duration": 4.601
    },
    {
      "text": "Target tensor indices will be probably",
      "start": 2643.24,
      "duration": 4.68
    },
    {
      "text": "be very small and our goal is to bring",
      "start": 2645.92,
      "duration": 3.72
    },
    {
      "text": "all of these probabilities close to one",
      "start": 2647.92,
      "duration": 4.76
    },
    {
      "text": "as possible so that's why we'll uh take",
      "start": 2649.64,
      "duration": 4.8
    },
    {
      "text": "the negative log of these probabilities",
      "start": 2652.68,
      "duration": 3.399
    },
    {
      "text": "and take the mean that's the cross",
      "start": 2654.44,
      "duration": 4.119
    },
    {
      "text": "entropy loss and through one line of",
      "start": 2656.079,
      "duration": 5.801
    },
    {
      "text": "code cross entropy tor. nn. functional",
      "start": 2658.559,
      "duration": 5.441
    },
    {
      "text": "doc cross entropy between these two",
      "start": 2661.88,
      "duration": 3.92
    },
    {
      "text": "tensors we are going to implement this",
      "start": 2664.0,
      "duration": 3.839
    },
    {
      "text": "exact same operation which I just",
      "start": 2665.8,
      "duration": 4.48
    },
    {
      "text": "described in words right now so you can",
      "start": 2667.839,
      "duration": 4.52
    },
    {
      "text": "even check this and let me type it",
      "start": 2670.28,
      "duration": 3.88
    },
    {
      "text": "torch.nn",
      "start": 2672.359,
      "duration": 5.161
    },
    {
      "text": "do functional cross",
      "start": 2674.16,
      "duration": 7.24
    },
    {
      "text": "entropy and you can check it finds the",
      "start": 2677.52,
      "duration": 5.76
    },
    {
      "text": "cross entropy loss between the input",
      "start": 2681.4,
      "duration": 5.4
    },
    {
      "text": "Logics and the target this is exactly um",
      "start": 2683.28,
      "duration": 5.88
    },
    {
      "text": "what we did right",
      "start": 2686.8,
      "duration": 5.039
    },
    {
      "text": "now okay now I'm going to implement this",
      "start": 2689.16,
      "duration": 4.48
    },
    {
      "text": "same thing in code we are going to find",
      "start": 2691.839,
      "duration": 4.961
    },
    {
      "text": "the cross entropy loss in code okay so",
      "start": 2693.64,
      "duration": 4.959
    },
    {
      "text": "for first what we are going to do is",
      "start": 2696.8,
      "duration": 5.08
    },
    {
      "text": "that uh we are going to find the P1 P2",
      "start": 2698.599,
      "duration": 7.72
    },
    {
      "text": "P3 so we are going to find P11 p12 p13",
      "start": 2701.88,
      "duration": 8.439
    },
    {
      "text": "and p21 P22 and p23 that's the first",
      "start": 2706.319,
      "duration": 6.361
    },
    {
      "text": "step so remember here in the sequence of",
      "start": 2710.319,
      "duration": 4.681
    },
    {
      "text": "steps over here first we find these",
      "start": 2712.68,
      "duration": 5.12
    },
    {
      "text": "Target probabilities right uh and to",
      "start": 2715.0,
      "duration": 4.44
    },
    {
      "text": "find these Target probabilities what",
      "start": 2717.8,
      "duration": 4.2
    },
    {
      "text": "we'll do is that we will",
      "start": 2719.44,
      "duration": 5.72
    },
    {
      "text": "uh we'll take the target indexes which",
      "start": 2722.0,
      "duration": 6.48
    },
    {
      "text": "are uh the indexes corresponding to the",
      "start": 2725.16,
      "duration": 5.76
    },
    {
      "text": "True Values and what we are going to do",
      "start": 2728.48,
      "duration": 4.92
    },
    {
      "text": "is that we are going to uh take the",
      "start": 2730.92,
      "duration": 4.52
    },
    {
      "text": "probas so let me see what the probab",
      "start": 2733.4,
      "duration": 5.4
    },
    {
      "text": "probas is so this probas is the",
      "start": 2735.44,
      "duration": 5.8
    },
    {
      "text": "output uh this probas is the output",
      "start": 2738.8,
      "duration": 4.84
    },
    {
      "text": "probability tensor and what we are going",
      "start": 2741.24,
      "duration": 4.28
    },
    {
      "text": "to do is that we are going to find the",
      "start": 2743.64,
      "duration": 3.88
    },
    {
      "text": "token probabilities corresponding to the",
      "start": 2745.52,
      "duration": 4.68
    },
    {
      "text": "Target indexes and to do that what we",
      "start": 2747.52,
      "duration": 4.36
    },
    {
      "text": "are going to do is that we are going to",
      "start": 2750.2,
      "duration": 3.919
    },
    {
      "text": "use this line of code and what this will",
      "start": 2751.88,
      "duration": 3.92
    },
    {
      "text": "just do is that it will take the target",
      "start": 2754.119,
      "duration": 4.881
    },
    {
      "text": "indices and it will index or it will",
      "start": 2755.8,
      "duration": 5.08
    },
    {
      "text": "look at this probas which is the tensor",
      "start": 2759.0,
      "duration": 4.04
    },
    {
      "text": "of output probabilities and then it will",
      "start": 2760.88,
      "duration": 4.679
    },
    {
      "text": "look for those particular indexes so",
      "start": 2763.04,
      "duration": 4.279
    },
    {
      "text": "basically what it will be doing is that",
      "start": 2765.559,
      "duration": 5.481
    },
    {
      "text": "for zero it that's P11 that will be this",
      "start": 2767.319,
      "duration": 7.481
    },
    {
      "text": "for one that will be P1 two and for two",
      "start": 2771.04,
      "duration": 6.24
    },
    {
      "text": "that will be p13 similarly when we look",
      "start": 2774.8,
      "duration": 5.559
    },
    {
      "text": "at the second batch over here um what we",
      "start": 2777.28,
      "duration": 5.12
    },
    {
      "text": "are doing over here is",
      "start": 2780.359,
      "duration": 5.161
    },
    {
      "text": "that uh that this is one which means",
      "start": 2782.4,
      "duration": 5.6
    },
    {
      "text": "that it's going to look at",
      "start": 2785.52,
      "duration": 4.839
    },
    {
      "text": "the second batch so one way to",
      "start": 2788.0,
      "duration": 3.68
    },
    {
      "text": "understand what's going on here is to",
      "start": 2790.359,
      "duration": 4.081
    },
    {
      "text": "look at the probas and try to see the",
      "start": 2791.68,
      "duration": 5.76
    },
    {
      "text": "dimensions so uh let me write it here",
      "start": 2794.44,
      "duration": 4.84
    },
    {
      "text": "again for reference just so that your",
      "start": 2797.44,
      "duration": 4.32
    },
    {
      "text": "understanding is clear so the probas is",
      "start": 2799.28,
      "duration": 5.12
    },
    {
      "text": "actually it will look something like",
      "start": 2801.76,
      "duration": 7.44
    },
    {
      "text": "this first let me write it for the first",
      "start": 2804.4,
      "duration": 6.439
    },
    {
      "text": "every",
      "start": 2809.2,
      "duration": 6.32
    },
    {
      "text": "effort moves right and then",
      "start": 2810.839,
      "duration": 6.641
    },
    {
      "text": "uh this is",
      "start": 2815.52,
      "duration": 5.599
    },
    {
      "text": "50257 the number of",
      "start": 2817.48,
      "duration": 3.639
    },
    {
      "text": "columns and then second is",
      "start": 2822.119,
      "duration": 4.121
    },
    {
      "text": "I really",
      "start": 2826.319,
      "duration": 5.841
    },
    {
      "text": "like and the size of this so this whole",
      "start": 2829.04,
      "duration": 6.799
    },
    {
      "text": "thing is the probas tensor which is the",
      "start": 2832.16,
      "duration": 5.56
    },
    {
      "text": "probability tensor and the size of this",
      "start": 2835.839,
      "duration": 3.561
    },
    {
      "text": "is so we have two batches we have three",
      "start": 2837.72,
      "duration": 4.28
    },
    {
      "text": "tokens and",
      "start": 2839.4,
      "duration": 2.6
    },
    {
      "text": "50257 so what we are essentially doing",
      "start": 2842.68,
      "duration": 5.04
    },
    {
      "text": "here is that",
      "start": 2845.04,
      "duration": 4.36
    },
    {
      "text": "uh let",
      "start": 2847.72,
      "duration": 4.2
    },
    {
      "text": "me scroll down below yeah what we are",
      "start": 2849.4,
      "duration": 5.48
    },
    {
      "text": "doing here is that in this line text idx",
      "start": 2851.92,
      "duration": 4.56
    },
    {
      "text": "is equal to zero which means that we are",
      "start": 2854.88,
      "duration": 4.0
    },
    {
      "text": "first looking at the first batch so we",
      "start": 2856.48,
      "duration": 6.2
    },
    {
      "text": "are looking at this batch and then what",
      "start": 2858.88,
      "duration": 5.36
    },
    {
      "text": "we are doing is that we are looking at",
      "start": 2862.68,
      "duration": 6.0
    },
    {
      "text": "row 0o row one and row two and from row",
      "start": 2864.24,
      "duration": 8.72
    },
    {
      "text": "0o we'll get the um we'll get the so",
      "start": 2868.68,
      "duration": 5.84
    },
    {
      "text": "let's look at the Target answer we'll",
      "start": 2872.96,
      "duration": 3.68
    },
    {
      "text": "get the index corresponding we'll get",
      "start": 2874.52,
      "duration": 4.48
    },
    {
      "text": "the value corresponding to 3626 index",
      "start": 2876.64,
      "duration": 3.8
    },
    {
      "text": "from Row one we'll get the value",
      "start": 2879.0,
      "duration": 4.2
    },
    {
      "text": "corresponding to 61 0 and from row two",
      "start": 2880.44,
      "duration": 5.04
    },
    {
      "text": "we'll get the value corresponding to",
      "start": 2883.2,
      "duration": 5.2
    },
    {
      "text": "345 now similarly here what we are doing",
      "start": 2885.48,
      "duration": 5.04
    },
    {
      "text": "here is that we are looking at so text",
      "start": 2888.4,
      "duration": 3.8
    },
    {
      "text": "idx equal to 1 which means we are",
      "start": 2890.52,
      "duration": 3.48
    },
    {
      "text": "looking at the second batch we are",
      "start": 2892.2,
      "duration": 5.0
    },
    {
      "text": "looking at the second batch and then uh",
      "start": 2894.0,
      "duration": 5.599
    },
    {
      "text": "row zero Row one and row two so then",
      "start": 2897.2,
      "duration": 5.159
    },
    {
      "text": "we'll look at the targets tensor again",
      "start": 2899.599,
      "duration": 4.441
    },
    {
      "text": "and for row zero we'll take the value",
      "start": 2902.359,
      "duration": 4.841
    },
    {
      "text": "corresponding to index 1107 for Row one",
      "start": 2904.04,
      "duration": 4.72
    },
    {
      "text": "we'll take the value corresponding to",
      "start": 2907.2,
      "duration": 4.24
    },
    {
      "text": "index 588 and for row two we'll take the",
      "start": 2908.76,
      "duration": 5.16
    },
    {
      "text": "value corresponding to index",
      "start": 2911.44,
      "duration": 4.56
    },
    {
      "text": "11311 so that is",
      "start": 2913.92,
      "duration": 6.28
    },
    {
      "text": "essentially uh p21 P22 and p23 so these",
      "start": 2916.0,
      "duration": 9.04
    },
    {
      "text": "three values are the P11 p12 P2 p13 and",
      "start": 2920.2,
      "duration": 7.96
    },
    {
      "text": "these three values are p21 P22 and p23",
      "start": 2925.04,
      "duration": 5.079
    },
    {
      "text": "the whole goal is to get these values as",
      "start": 2928.16,
      "duration": 4.399
    },
    {
      "text": "close to one as possible right then what",
      "start": 2930.119,
      "duration": 4.48
    },
    {
      "text": "we'll do in the Second Step as I told",
      "start": 2932.559,
      "duration": 5.401
    },
    {
      "text": "you uh over here once we get these P11",
      "start": 2934.599,
      "duration": 7.161
    },
    {
      "text": "p12 p13 p21 P22 p23 we are going to",
      "start": 2937.96,
      "duration": 6.159
    },
    {
      "text": "merge these together right so that's",
      "start": 2941.76,
      "duration": 3.68
    },
    {
      "text": "what's written here we are going to",
      "start": 2944.119,
      "duration": 3.48
    },
    {
      "text": "concatenate these six values together",
      "start": 2945.44,
      "duration": 3.8
    },
    {
      "text": "and you can see that they appear like",
      "start": 2947.599,
      "duration": 3.96
    },
    {
      "text": "this the next step is that we are going",
      "start": 2949.24,
      "duration": 3.839
    },
    {
      "text": "to take the",
      "start": 2951.559,
      "duration": 4.401
    },
    {
      "text": "log uh actually after concatenation yeah",
      "start": 2953.079,
      "duration": 4.72
    },
    {
      "text": "we take the log over here and then we",
      "start": 2955.96,
      "duration": 4.2
    },
    {
      "text": "print these log values then we'll take",
      "start": 2957.799,
      "duration": 4.8
    },
    {
      "text": "the mean of these log values and then we",
      "start": 2960.16,
      "duration": 4.12
    },
    {
      "text": "are going to do the negative of the log",
      "start": 2962.599,
      "duration": 4.681
    },
    {
      "text": "likelihood",
      "start": 2964.28,
      "duration": 3.0
    },
    {
      "text": "uh so as I told you we can also not do",
      "start": 2967.68,
      "duration": 4.8
    },
    {
      "text": "the log likelihood and just do positive",
      "start": 2970.72,
      "duration": 4.359
    },
    {
      "text": "log likelihood but then that would mean",
      "start": 2972.48,
      "duration": 4.48
    },
    {
      "text": "maximizing the loss that does not make",
      "start": 2975.079,
      "duration": 3.24
    },
    {
      "text": "sense so in deep learning it's",
      "start": 2976.96,
      "duration": 3.44
    },
    {
      "text": "conventional to use negative log lik Le",
      "start": 2978.319,
      "duration": 4.401
    },
    {
      "text": "so that we minimize the loss right so",
      "start": 2980.4,
      "duration": 4.24
    },
    {
      "text": "this is now the negative log likelihood",
      "start": 2982.72,
      "duration": 5.2
    },
    {
      "text": "loss value which we ultimately need to",
      "start": 2984.64,
      "duration": 5.8
    },
    {
      "text": "minimize now as I told you there's a",
      "start": 2987.92,
      "duration": 5.32
    },
    {
      "text": "much simpler way of doing this using the",
      "start": 2990.44,
      "duration": 5.6
    },
    {
      "text": "torch.nn do functional cross entropy the",
      "start": 2993.24,
      "duration": 4.48
    },
    {
      "text": "first step of this is to flatten the",
      "start": 2996.04,
      "duration": 3.68
    },
    {
      "text": "logits and the targets and to use this",
      "start": 2997.72,
      "duration": 4.28
    },
    {
      "text": "one line of code so that's what I'm",
      "start": 2999.72,
      "duration": 4.24
    },
    {
      "text": "going to do now I'm going to take the",
      "start": 3002.0,
      "duration": 6.559
    },
    {
      "text": "logits which was earlier 2A 3A 5257 and",
      "start": 3003.96,
      "duration": 6.119
    },
    {
      "text": "I'm going to flatten it I'm going to",
      "start": 3008.559,
      "duration": 3.841
    },
    {
      "text": "flatten the these first two Dimensions",
      "start": 3010.079,
      "duration": 4.561
    },
    {
      "text": "together so that it's 6A",
      "start": 3012.4,
      "duration": 4.88
    },
    {
      "text": "5257 this is exactly what we saw over",
      "start": 3014.64,
      "duration": 5.439
    },
    {
      "text": "here look at this it's 6A",
      "start": 3017.28,
      "duration": 5.64
    },
    {
      "text": "5257 batch one and batch two are merged",
      "start": 3020.079,
      "duration": 5.401
    },
    {
      "text": "together then what I'm going to do is",
      "start": 3022.92,
      "duration": 4.08
    },
    {
      "text": "that I'm going to look at the Target",
      "start": 3025.48,
      "duration": 3.879
    },
    {
      "text": "stenor and I'm going to flatten it so if",
      "start": 3027.0,
      "duration": 4.599
    },
    {
      "text": "the target stenor is this two rows and",
      "start": 3029.359,
      "duration": 4.361
    },
    {
      "text": "three columns I'm going to merge these",
      "start": 3031.599,
      "duration": 4.041
    },
    {
      "text": "two rows into six",
      "start": 3033.72,
      "duration": 4.44
    },
    {
      "text": "values and then I'm going to just write",
      "start": 3035.64,
      "duration": 4.84
    },
    {
      "text": "one line of code this one line of code",
      "start": 3038.16,
      "duration": 4.52
    },
    {
      "text": "will first convert these Logics into a",
      "start": 3040.48,
      "duration": 5.119
    },
    {
      "text": "soft Max and then it will find the",
      "start": 3042.68,
      "duration": 4.56
    },
    {
      "text": "values corresponding to the Target",
      "start": 3045.599,
      "duration": 3.881
    },
    {
      "text": "indexes then it will take the log of",
      "start": 3047.24,
      "duration": 4.0
    },
    {
      "text": "these the mean and then the negative log",
      "start": 3049.48,
      "duration": 3.879
    },
    {
      "text": "likelihood all of this will be done and",
      "start": 3051.24,
      "duration": 4.079
    },
    {
      "text": "I will get my categorical cross entropy",
      "start": 3053.359,
      "duration": 4.24
    },
    {
      "text": "loss of 10 remember I want to take this",
      "start": 3055.319,
      "duration": 4.48
    },
    {
      "text": "loss as close to zero as possible that's",
      "start": 3057.599,
      "duration": 4.561
    },
    {
      "text": "the goal so when we train the large",
      "start": 3059.799,
      "duration": 4.04
    },
    {
      "text": "language models later the goal is to",
      "start": 3062.16,
      "duration": 3.6
    },
    {
      "text": "bring this categorical cross entropy",
      "start": 3063.839,
      "duration": 3.641
    },
    {
      "text": "loss to as low as",
      "start": 3065.76,
      "duration": 3.92
    },
    {
      "text": "possible before we end the lecture I",
      "start": 3067.48,
      "duration": 4.0
    },
    {
      "text": "want to show you one last thing there is",
      "start": 3069.68,
      "duration": 6.2
    },
    {
      "text": "another major of loss and uh that's like",
      "start": 3071.48,
      "duration": 6.24
    },
    {
      "text": "cross entropy itself but a minor",
      "start": 3075.88,
      "duration": 4.32
    },
    {
      "text": "modification and that's called as",
      "start": 3077.72,
      "duration": 5.2
    },
    {
      "text": "perplexity so perplexity actually",
      "start": 3080.2,
      "duration": 5.0
    },
    {
      "text": "measures how well the probability",
      "start": 3082.92,
      "duration": 4.639
    },
    {
      "text": "distribution predicted by by the model",
      "start": 3085.2,
      "duration": 4.32
    },
    {
      "text": "matches the actual distribution of words",
      "start": 3087.559,
      "duration": 4.441
    },
    {
      "text": "in the data set and so in that sense it",
      "start": 3089.52,
      "duration": 5.039
    },
    {
      "text": "is more interpretable and it's more",
      "start": 3092.0,
      "duration": 4.599
    },
    {
      "text": "interpretable way of understanding model",
      "start": 3094.559,
      "duration": 4.8
    },
    {
      "text": "uncertainty in predicting the next token",
      "start": 3096.599,
      "duration": 4.801
    },
    {
      "text": "and I'll show you why in a minute",
      "start": 3099.359,
      "duration": 4.601
    },
    {
      "text": "remember that lower per perplexity score",
      "start": 3101.4,
      "duration": 4.199
    },
    {
      "text": "also means better",
      "start": 3103.96,
      "duration": 3.76
    },
    {
      "text": "predictions so the formula for",
      "start": 3105.599,
      "duration": 4.681
    },
    {
      "text": "perplexity is just to find the exponent",
      "start": 3107.72,
      "duration": 3.52
    },
    {
      "text": "of the",
      "start": 3110.28,
      "duration": 3.92
    },
    {
      "text": "loss and surprisingly this simple uh",
      "start": 3111.24,
      "duration": 5.4
    },
    {
      "text": "modification leads to a lot more",
      "start": 3114.2,
      "duration": 5.08
    },
    {
      "text": "intuition so if the exponent of the loss",
      "start": 3116.64,
      "duration": 4.8
    },
    {
      "text": "in our case is",
      "start": 3119.28,
      "duration": 5.12
    },
    {
      "text": "48725 it means that the model is roughly",
      "start": 3121.44,
      "duration": 5.48
    },
    {
      "text": "as uncertain as if it had to choose the",
      "start": 3124.4,
      "duration": 5.88
    },
    {
      "text": "next token randomly from about 48725",
      "start": 3126.92,
      "duration": 5.8
    },
    {
      "text": "tokens in the vocabulary so the number",
      "start": 3130.28,
      "duration": 6.36
    },
    {
      "text": "of tokens in the vocabulary is 50 uh 257",
      "start": 3132.72,
      "duration": 6.56
    },
    {
      "text": "I think now what this means is that if",
      "start": 3136.64,
      "duration": 5.88
    },
    {
      "text": "the input is uh",
      "start": 3139.28,
      "duration": 6.92
    },
    {
      "text": "every effort moves right every effort",
      "start": 3142.52,
      "duration": 6.52
    },
    {
      "text": "moves is the input currently our llm is",
      "start": 3146.2,
      "duration": 4.72
    },
    {
      "text": "at a stage that to predict the next",
      "start": 3149.04,
      "duration": 6.079
    },
    {
      "text": "token that's as as if we have to choose",
      "start": 3150.92,
      "duration": 7.439
    },
    {
      "text": "between 48725 tokens that's pretty",
      "start": 3155.119,
      "duration": 5.361
    },
    {
      "text": "pretty bad right it means that there is",
      "start": 3158.359,
      "duration": 3.881
    },
    {
      "text": "a lot of uncertainty in predicting the",
      "start": 3160.48,
      "duration": 4.52
    },
    {
      "text": "next token if the perplexity score was",
      "start": 3162.24,
      "duration": 4.52
    },
    {
      "text": "equal to two that's pretty good which",
      "start": 3165.0,
      "duration": 3.799
    },
    {
      "text": "means that we just need to predict",
      "start": 3166.76,
      "duration": 5.12
    },
    {
      "text": "between two tokens so that means our llm",
      "start": 3168.799,
      "duration": 5.441
    },
    {
      "text": "is very accurate but in this case the",
      "start": 3171.88,
      "duration": 4.919
    },
    {
      "text": "perplexity score is 48725",
      "start": 3174.24,
      "duration": 6.359
    },
    {
      "text": "48725 which means that 4 48,000 tokens",
      "start": 3176.799,
      "duration": 5.841
    },
    {
      "text": "are kind of equally likely to become the",
      "start": 3180.599,
      "duration": 4.401
    },
    {
      "text": "next token which means that our model is",
      "start": 3182.64,
      "duration": 5.159
    },
    {
      "text": "not good at all uh you can see how it is",
      "start": 3185.0,
      "duration": 4.359
    },
    {
      "text": "more interpretable than getting a",
      "start": 3187.799,
      "duration": 3.52
    },
    {
      "text": "categorical cross entropy loss of equal",
      "start": 3189.359,
      "duration": 4.48
    },
    {
      "text": "to 10 when I get a categorical cross",
      "start": 3191.319,
      "duration": 4.441
    },
    {
      "text": "entropy loss of 10 I don't really know",
      "start": 3193.839,
      "duration": 3.76
    },
    {
      "text": "how it relates to my vocabulary size",
      "start": 3195.76,
      "duration": 5.839
    },
    {
      "text": "also but if you get a uh entropy or a",
      "start": 3197.599,
      "duration": 5.801
    },
    {
      "text": "perplexity score of",
      "start": 3201.599,
      "duration": 4.281
    },
    {
      "text": "48725 you can kind of relate it to your",
      "start": 3203.4,
      "duration": 5.0
    },
    {
      "text": "vocabulary size and make interpretable",
      "start": 3205.88,
      "duration": 5.719
    },
    {
      "text": "predictions like these that the 48,000",
      "start": 3208.4,
      "duration": 5.28
    },
    {
      "text": "next token 48,000 tokens in our",
      "start": 3211.599,
      "duration": 4.801
    },
    {
      "text": "vocabulary of 50,000 are equally likely",
      "start": 3213.68,
      "duration": 5.28
    },
    {
      "text": "to be the next token and that's pretty",
      "start": 3216.4,
      "duration": 4.919
    },
    {
      "text": "bad because our llm is not yet",
      "start": 3218.96,
      "duration": 4.839
    },
    {
      "text": "trained so this brings us to the end of",
      "start": 3221.319,
      "duration": 4.76
    },
    {
      "text": "today's lecture where basically what we",
      "start": 3223.799,
      "duration": 5.201
    },
    {
      "text": "did is we took at we uh we first started",
      "start": 3226.079,
      "duration": 4.76
    },
    {
      "text": "with the",
      "start": 3229.0,
      "duration": 4.48
    },
    {
      "text": "inputs then we started with the True",
      "start": 3230.839,
      "duration": 5.081
    },
    {
      "text": "Values which were known as the targets",
      "start": 3233.48,
      "duration": 4.56
    },
    {
      "text": "and then what what we did was we found",
      "start": 3235.92,
      "duration": 3.879
    },
    {
      "text": "the output values this was kind of a",
      "start": 3238.04,
      "duration": 4.16
    },
    {
      "text": "revision we got the Logics tensor we",
      "start": 3239.799,
      "duration": 4.121
    },
    {
      "text": "converted it into a tensor of",
      "start": 3242.2,
      "duration": 3.76
    },
    {
      "text": "probabilities and then we got these",
      "start": 3243.92,
      "duration": 5.199
    },
    {
      "text": "output tokens so then we Tred to find",
      "start": 3245.96,
      "duration": 5.04
    },
    {
      "text": "the loss between the targets and the",
      "start": 3249.119,
      "duration": 3.72
    },
    {
      "text": "output using",
      "start": 3251.0,
      "duration": 5.88
    },
    {
      "text": "the cross entropy loss and we ultimately",
      "start": 3252.839,
      "duration": 6.041
    },
    {
      "text": "saw that using one single line of code",
      "start": 3256.88,
      "duration": 5.0
    },
    {
      "text": "torge nn. functional. cross entropy we",
      "start": 3258.88,
      "duration": 4.64
    },
    {
      "text": "can find the loss between the loged",
      "start": 3261.88,
      "duration": 5.04
    },
    {
      "text": "sensor and the target tensor",
      "start": 3263.52,
      "duration": 5.24
    },
    {
      "text": "as the lecture concluded we even looked",
      "start": 3266.92,
      "duration": 3.639
    },
    {
      "text": "at another way of measuring loss which",
      "start": 3268.76,
      "duration": 3.72
    },
    {
      "text": "was called as perplexity which is much",
      "start": 3270.559,
      "duration": 4.481
    },
    {
      "text": "more intuitive and the way perplexity is",
      "start": 3272.48,
      "duration": 5.079
    },
    {
      "text": "calculated it just e rais to loss which",
      "start": 3275.04,
      "duration": 5.64
    },
    {
      "text": "is exponent of loss and I also mentioned",
      "start": 3277.559,
      "duration": 5.52
    },
    {
      "text": "the perplexity concept over here in our",
      "start": 3280.68,
      "duration": 5.04
    },
    {
      "text": "case the loss was 10.79 and the",
      "start": 3283.079,
      "duration": 5.081
    },
    {
      "text": "perplexity value was",
      "start": 3285.72,
      "duration": 5.24
    },
    {
      "text": "48725 and this is usually more",
      "start": 3288.16,
      "duration": 5.32
    },
    {
      "text": "interpretable awesome in the next",
      "start": 3290.96,
      "duration": 4.119
    },
    {
      "text": "lecture what we are going to do is that",
      "start": 3293.48,
      "duration": 3.48
    },
    {
      "text": "we are going to take an actual",
      "start": 3295.079,
      "duration": 4.48
    },
    {
      "text": "data set from a book which is called the",
      "start": 3296.96,
      "duration": 5.0
    },
    {
      "text": "verdict and first we'll tokenize this",
      "start": 3299.559,
      "duration": 4.8
    },
    {
      "text": "data set we'll divide it into input",
      "start": 3301.96,
      "duration": 5.119
    },
    {
      "text": "output input output Pairs and then we",
      "start": 3304.359,
      "duration": 4.76
    },
    {
      "text": "are going to get the llm output and we",
      "start": 3307.079,
      "duration": 4.561
    },
    {
      "text": "are going to find the loss function or",
      "start": 3309.119,
      "duration": 5.921
    },
    {
      "text": "the loss value for this entire uh data",
      "start": 3311.64,
      "duration": 5.919
    },
    {
      "text": "set until now in this lecture we just",
      "start": 3315.04,
      "duration": 5.319
    },
    {
      "text": "looked at uh in the code we just looked",
      "start": 3317.559,
      "duration": 5.8
    },
    {
      "text": "at two sample inputs right we looked at",
      "start": 3320.359,
      "duration": 4.76
    },
    {
      "text": "um let me yeah we looked at these two",
      "start": 3323.359,
      "duration": 3.881
    },
    {
      "text": "inputs in the next lecture we'll be",
      "start": 3325.119,
      "duration": 4.44
    },
    {
      "text": "scaling this up and look at an entire",
      "start": 3327.24,
      "duration": 4.799
    },
    {
      "text": "text data set so next lecture will be a",
      "start": 3329.559,
      "duration": 5.361
    },
    {
      "text": "lot of fun and uh we'll run the entire",
      "start": 3332.039,
      "duration": 6.601
    },
    {
      "text": "architecture on this um Hands-On example",
      "start": 3334.92,
      "duration": 5.48
    },
    {
      "text": "you can replace this example with any",
      "start": 3338.64,
      "duration": 3.36
    },
    {
      "text": "book which you like Harry Potter book",
      "start": 3340.4,
      "duration": 3.919
    },
    {
      "text": "any other book so next lecture is going",
      "start": 3342.0,
      "duration": 4.599
    },
    {
      "text": "to be a lot of fun I hope you all are",
      "start": 3344.319,
      "duration": 4.121
    },
    {
      "text": "liking these lectures we have now",
      "start": 3346.599,
      "duration": 6.161
    },
    {
      "text": "started a new module which is on U the",
      "start": 3348.44,
      "duration": 6.52
    },
    {
      "text": "training large language model and we are",
      "start": 3352.76,
      "duration": 4.92
    },
    {
      "text": "slowly making a lot of progress on in",
      "start": 3354.96,
      "duration": 4.32
    },
    {
      "text": "building the large language models we",
      "start": 3357.68,
      "duration": 4.2
    },
    {
      "text": "have already finished stage one now we",
      "start": 3359.28,
      "duration": 4.319
    },
    {
      "text": "are on stage two and rapidly moving",
      "start": 3361.88,
      "duration": 4.159
    },
    {
      "text": "towards completion thanks a lot everyone",
      "start": 3363.599,
      "duration": 4.161
    },
    {
      "text": "and I look forward to seeing you in the",
      "start": 3366.039,
      "duration": 4.961
    },
    {
      "text": "next lecture",
      "start": 3367.76,
      "duration": 3.24
    }
  ],
  "full_text": "[Music] hello everyone and uh welcome to this lecture in the build large language models from scratch Series today we are going to look at a very important topic and that is regarding loss functions when you look at machine learning and traditional machine learning models such as regression and classification the loss function for these type of models are pretty well defined for regression you generally use the least Square type of a loss mean square error for classification you use cross entropy cross entropy loss hinge loss Etc however the question is that when we come to large language models how do we Define the loss function how do we measure whe whether the large language model is doing a good job or whether it's not performing well let's find out in today's lecture so up till now in this series we have covered stage one of building a large language model from scratch in stage one we essentially covered three subm modules data preparation and sampling attention mechanism and llm architecture in each of these subcomponents we divide we devoted a huge number of lectures and whiteboard notes and also coding so that by the time you have reached this stage I hope the building blocks of how to construct a large language models are clear to you in particular till the last lecture we have seen this GPT or the llm architecture which we have built from scratch and up till now we we are at a stage where the GPT architecture which we have built and whose code we have written takes an input it takes an input text and it returns an output we also saw how this output is converted into the next word prediction task so our GPT model up till this stage or our large language model we which we have set out to build is at a level where it takes an input as a text and it can predict next tokens or it can predict next words that's awesome right but now we want to make it better and better and better in particular if you remember uh where we ended the previous lecture on llm architecture we got this kind of the so we gave the input as hello I am and the next tokens which we got are or something completely random so now from this lecture onwards what we'll be doing is that we'll be looking at stage two stage two of building a large language model just centers around one word and I'm just going to write that one word here and that is called as training we need to train our large language model so that the next token which it predicts makes sense the first step of the training procedure is that you you need to ask yourself okay let's say my large language model is has is giving me an output and I know it does not look good how do I capture this qualitative intuition of not of knowing that it does not look good into a quantitative matric and that quantitative matric is generally the loss function so if we can define a loss function which quantifies how good or how bad our llm performance is we can aim to minimize the loss so that the llm can do better and better and better and once we construct a loss function it opens the door for integrating gradient descent based back propagation algorithms which we have already learned in neural networks so then we convert the problem of training a large language model into a problem which we have previously solved before so what we are going to do in these set of lectures we are going to look at this entire pipeline of seven steps which I have shown over here in today's lecture we are going to look at the first two steps here we are going to look at how text is generated which we have already seen before but I want to quickly recap it in case some of you have come to this lecture directly and then we are also going to look at text evaluation which means we are going to define a loss function in this lecture itself and we are going to see how that loss function quantifies the loss L between what our llm has generated and the good output which we actually want in the subsequent lectures we'll be looking at taking an entire data set feeding it into the llm getting the output and finding the training and the validation losses will then generate the llm training function that's the back propagation I was talking about and then towards the end of this series on training llms we'll also load pre-trained weights from open AI all that will come next for now let's just start with these initial two goals for today's lecture and that is text generation using llms and text evaluation first I want to quickly recap how can we use the llm or the gpt2 architecture which we have trained so far to generate text okay so the way it works is that we have to specify a GPT configuration uh and this GPT configuration or the llm configuration contains many parameters first is our vocabulary size this is the length of the number of tokens we have in our vocabulary so we are using the same tokenizer which gpt2 and in fact all open AI models used and that's called as tick token so this is thck token and what it does is that it uses a bite pair encoder and it creates a vocabulary of tokens like this the vocabulary size which gpt2 had is 50257 and that's the same size which we have when we conr constructed our large language model we have to specify a context length gpt2 used a context length of 1024 but for the purposes of easy calculation which you can train on your own local machine in less than 2 to 3 minutes we are using a context length of 256 keep in mind that you can just use the same code for a longer context length as well but just make sure that you have enough compute power and memory to execute the code then we have the embedding Dimension which is which basically means that the token embeddings need to be projected into higher dimensional space to capture the semantic meaning then we have the number of attention heads so these are the number of self attention mechanism blocks we have within one Transformer why do I say within one Transformer because there is not one Transformer there are many Transformers and the end layer specifies how many Transformer blocks we have dropout rate is can be set to zero also this is the Dropout layer parameter and query key value bias is basically uh when we initialize the weight metries for the query key and value we don't want the bias term if any of these terms are looking unfamiliar to you right now please revise the previous lectures we have had on uh llm Basics attention mechanism and GPT architecture awesome so the GPT model which we built earlier looks something like this what this model model does is that it takes in the input tokens it converts them into token embeddings adds positional embeddings to it then we have a Dropout layer this output upti layer passes through the Transformer block after we come out of the Transformer block we have a final normalization layer and then we return the logits now these logits which are returned consists of the tokens and for each token we have uh the number of tokens equal to vocabulary size sorry for each token we have the number of columns equal to the vocabulary size I'll come to this part later and I'll explain to you again what we have done in this part of the code uh so that it's revised for you okay great now let's start looking at the first section of today's lecture okay so let's start with the first part of today's lecture remember that to to get the loss function we need the input from the input we'll get the predicted values and we should already know the True Values which I'm going to call as Target values in today's lecture whenever I use the word Target remember that it stands for the True Values and ultimately the loss function will be determined by the predicted and the target values and how close they are so in the first section let's look at the inputs and let's look at the targets which are the True Values okay so the input to the large language model which we are building comes in a format like this the input is a tensor and uh the number of Ro rows here correspond to the number of batches so I have two batches over here and that's why there are two rows uh so two batches corresponding to the two rows I'm writing two over here the first row corresponds to the first batch the second row corresponds to the second batch now you'll see that every row has three tokens here which is usually set by our context size and I'm just assuming context size equal to three for this example so for the first batch the input tokens are every effort moves for the second badge the input tokens are I really like now the thing is based on these inputs our task is to predict the next next word right and uh so for targets you must be thinking that the target should only be two token IDs for the first batch we need a token ID for the second batch we need a token ID but now look at this target tensor over here it has two rows corresponding to the two batches that's fine but why does the first row have three values and why does the second row have three values the reason is because whenever you look at the input like this there are actually three there are actually three uh prediction tasks which are happening here when every is the input effort should be the output that's why the first Target is 3626 which corresponds to effort when every effort is the input which means that 16833 and 3626 are input 61 0 is the output which is moves and and then finally when every effort moves is an input the output should be U so the correct answer which we want is U over here similarly for I really like there are three prediction tasks when I is the input uh really is the output when I really is the input like is the output and when I really like is the input chocolate is the output so we want the output to be chocolate in this case but then there are three prediction tasks here and that's why the target tensor consist of two rows first row corresponds to the first batch second row corresponds to the second batch and then we have three values I hope you have understood why there are three values in the Target tensor because that will be very important to keep in mind as we move forward ultimately we are just concerned about the last word right which is this 345 that is the token ID corresponding to U uh token ID corresponding to U and 11311 that's the token ID essentially corresponding to Chocolate but but the first two are also important for the initial prediction tasks in this input okay so the these are the True Values which we want so this tensor are the True Values true prediction values if our large language model is doing an amazing job for these two inputs the prediction should be the first tensor of the output should look like this the second of the out the second row of the output should look like this but of course initially when the large language model is initialized randomly it won't produce these outputs which we want we'll need to train it okay so we have looked at the input and we have looked at the Target now let's see the outputs which are the predicted values of our llm so this is the input which we saw over here right uh 16833 3626 610 then that's the first batch the second batch is 40 1107 and 588 so the this input will go to GPT model and what is the GPT model the GPT model looks something like this so the input which I told you goes over here and then it passes through all of these steps and then we have an output tensor which is also called as the logit tensor okay so the first step as to what happens in the GPT model is that the tokenizer is converts the tokens into token IDs then the token IDs pass through all of these steps and we get the logic sensor and finally the logits are converted back into token IDs which are the next predictions uh of our large language model this entire sequence is encoded in this figure here which I want to explain thoroughly first we start with a vocabulary and for the sake of Simplicity let's assume that the vocabulary just has seven elements remember in the actual example which we are going to consider the vocabulary actually has 50257 elements but I'm just illustrating this diagram for the sake of Simplicity where there are seven elements in the vocabulary right and the input text let's say is just one batch for now and every effort moves that's the input right the first step is to use our vocabulary to map the input text to token IDs so every is token ID number two effort is token ID number one and moves is token ID number four so First We Take These tokens and map them into token IDs 2 1 and four the second step is that we'll pass all of this through the GPT model and the GPT model will give uh a logit sensor and to the logit sensor we apply the soft Max distribution and we'll get this output Matrix look at this output Matrix here I'll just write it Dimensions here so this output Matrix essentially uh it has uh let me use a different color it has three rows the first row corresponds to every the second row corresponds to effort and the third row corresponds to moves now you'll see that the number of columns are equal to 7 1 2 3 4 5 6 and 7even and we have some values see every row has some values and there are seven columns corresponding to every row basically the values in each row corresponds to the probability of what the next token will be so let's look at the first row values and I'll just write them over here for your reference 0.1 6 2 0.05 0 0.02 and 0.01 these are the values corresponding to every now what these values means is that when every is the input what is the probability for the next token being the output and 1 2 3 4 5 6 7 corresponds to a effort every forward move Z and Zoo so let me write the a effort every forward moves U and Zoo this is the vocabulary right now every value here in this row corresponds to the probability of what the next token will be if every is an input so let's Analyze This probability if every is an input the probability that the next token is a is just 10% or 0.1 there is 60% chance of the next token being effort 20% chance of the next token me every Etc so then what we do is that we look at the index with the highest probability which is this 6 over here and so we know that the next token is equal to effort now when every effort is the input we again look at the second row so we again look at the second row and find the index which has the highest probability and that corresponds to moves so this is index number four and that corresponds to moves over here then we look at the third row so every effort moves when that's the input what's the output and this is the main prediction task because it actually predicts the next token and if you look at the third row you look at the index which has the maximum probability and that's index number five which is here and we know that index number five corresponds to you so we know that the next uh token is you so remember when I told you that when the input has three tokens like this there are three prediction tasks for the first prediction task the output is index number one because that corresponds to the highest probability and the token which it corresponds to is effort for index number two or sorry for the prediction task number two the input is every effort and the output is index four which corresponds to moves and for the third prediction tasks the input is every effort moves and the output is five which corresponds to U so when you give this input every effort moves to the uh large language model the output is a sequence of indexes 1 4 and five what this output means is that one so if you look at one over here it means that when every is an input effort is the output when every effort is the input moves is the output and when every effort moves is the input U is the output so this is what is actually happening within the GPT model itself I just gave you a five minute crash course of what we learned in four to five hours in the previous set of videos so if any any of this you're finding hard I would again highly encourage you to go through all the previous videos so that you understand this better for now even if you get an intuition of what's going on here it's fine and you can follow along till the next part right so here what I want to illustrate to you is that this process which I've shown here we have the input when it goes to a GPT model we have the output token IDs and uh I've told you how we get the output token IDs so if you have understood the process let's say these are the output token IDs which we have obtained remember for actually gpt2 the vocabulary size is 5257 right so the range of the output token ID is is 0 to 50257 here the vocabulary size was 7 so the range was 0 to 7 so if you look at batch number one uh the output tokens are 16657 339 and 42826 U and let's look at the inputs right every effort moves so every effort and moves what this output essentially conveys is that if every is an input the token corresponding to the Token ID of 16657 is the output when every effort is the input the token corresponding to token ID 339 is the output and when every effort moves is the input the token corresponding to the Token ID 42826 is the output and what do we want these token IDs to be we want these token IDs to be as close to the Target token IDs which we saw look at these Target token IDs the token IDs which we have obtained right now are not the actual token IDs because the training has not yet happened so these are the output token IDs and we want these output token IDs to be as close to the Target token IDs as possible that's the whole goal of today's lecture and so we are going to then Define the loss function between the outputs and the targets so these are the output token IDs for batch number number one and here you can see that these are the three output token IDs for batch number two where the input is uh I really like so I really like and for these outputs we have to compare these outputs to these True Values for the second batch I hope you have you have understood the goal of today's lecture um up till now we have understood what are the inputs the shape in which inputs will be given to the model what are the True Values which I'm also calling as targets and we understood that what are the output uh output token IDs for batch number one and batch number two and to get these output token IDs the inputs have to go through a huge Transformer block and it has to go through this whole GPT architecture then we get the output Logics we apply soft Max and we get this tensor like this this one tensor essentially contains all the information you take a look at at this tensor and then you extract the indexes with the highest probability in each row that is essentially your output okay and now what we are going to do next is that we are going to then find the loss between the targets and the output before coming to the next step I want to go to code and really code everything what we have learned so far so that the understanding is clear and so that you also understand uh what is really going on in the code I already explained to you the configuration which we are going to be using right vocabulary size 50257 the context length of 256 uh embedding dimension of 768 number of attention heads 12 number of Transformer blocks equal to 12 dropout rate 0.1 and the query key value bias equal to false awesome now the next thing what we are going to do is that we are going to uh construct our model what this model is basically it's an instance of the GPT model class and this is the GPT model class which we have defined previously uh to get a visual representation of what this class does is that it takes in the input and then it converts the inputs into this logic sensor that's so all of these steps which have been written here are actually encoded visually in this Transformer block or in this GPT model architecture which I've seen the blue color which I'm highlighting right now with the orange pen that's the transform for block and it's the heart of the entire GPT architecture so here we are creating an instance of this model so that when we pass an input to this model we get the logic sensor as the output one note is that we reduce the context length of only 256 tokens here although we know that the gpt2 model uses 1024 tokens the reason is because you all will be able to follow and execute the code on your laptop computer and we reduce the computational resource requirements that way okay there are two more functions which I want to define the first is text to token IDs what this will do is that whenever we get any text which is the input we'll just convert it into token IDs and this uses this tick token tokenizer to convert the text into token IDs and there is also token IDs to text which is the reverse function so basically when we get any token ID we'll convert it back into text so uh if every effort moves you is the text and when you pass it through the tokenizer you'll get the token ID is corresponding to this and then you can even decode the token IDs and it will get you back the same text just as a proof that our model is working what I've done over here is that uh I have use this generate text simple which is another function we have defined in the previous lecture no need to worry about this right now but what this function does is that it takes in our model and we predict the maximum number of new tokens the number of input tokens is four every effort moves you and then it generates 10 new tokens um so here you can see the we have printed the output text and this shows that the output text consists of four input tokens and 10 output tokens remember one word is not necessarily equal to one token because we are using the bite pair encoder where we even have subwords and characters which can be individual tokens so up till now as we see the model does not produce good text because it has not been trained yet and uh so what we'll be doing is that how to measure what what good text is so that's why we have to define the loss function right now let's start with the main implementation in today's lecture and these are the inputs so the first input as as I told you on the Whiteboard is every effort moves these are the token IDs corresponding to the input text and the second input is I really like and the token IDs which are corresponding to the second input text are 40 1107 and 588 great and the targets are essentially 3626 610 345 these are the True Values for the this for the first batch for the second batch the targets are 1107 588 and 11311 take a note that the targets are the input shifted by one so if you take the inputs and shift it or just take the last two values here and add a new value that's the targets this is because there are three input output pairs okay now now once we get the inputs what we are going to do is that we have the inputs and we have the targets but we have not yet generated the outputs to generate the outputs what we'll do now is that we'll pass in the input through this GPT model we'll pass the input through this GPT model block and generate the output token IDs in this in this kind of a format which I've shown you okay so the first step is that remember that when you pass the input through the GPT block it returns this logits and which are not converted into normalized 0 to one format we have not applied soft Max so the first St step is that to get these Logics and pass them through a soft Max uh always keep an eye out for Dimensions why is it 2A 3A 5257 the reason is because everything can be explained through this diagram look when the vocabulary is so let me first rub many things here just so that Things become a bit easier to understand okay so look when the vocabulary size was equal to 7 over here take a look at the form of this logic sensor so if the vocabulary size was equal to seven for one batch the logic sensor size was three rows and seven columns right now if there were two batches the size would have been 2 comma 3 comma 7 that's the general size of this logic sensor so it's batch size batch size the second is the number of tokens and the third is the vocabulary Dimension so in this case the vocabulary Dimension was seven so there are seven columns in our case there are 50257 columns so this third dimension will be 5257 so here we see that the probab the logic tensor is converted into a tensor of probabilities whose shape is batch size number of tokens and vocabulary size so that will be 2 comma 3A 50257 great and now what we'll do as I mentioned to you the next step is that we extract from each row the index which has the maximum value and that will be done using the AR Max function and these indexes with the maximum value that's our output from each given batch so what we are now going to do is that we are going to do torch. AR Max you can even go to the P Tor documentation and see what argmax does Arc Max basically Returns the indexes of the maximum value so we are doing tor. AR Max Dimension equal to minus one which is along the column so what this will do is that it will look at all the values in the columns and give the index of the value which is the give the index which has the maximum value and so here we see for the first batch there are this is the first tokens output second tokens output third tokens output index remember these are token IDs for the second batch this is the first tokens token ID output this is the second output token ID this is the third output token ID and these output token IDs are the ones which we want as close as possible to these Target token IDs that's the main goal and for this we are going to construct a loss function so now actually let's decode these tokens remember we have already written a function which decodes uh um here we have written a function which converts token IDs into text right let's decode these tokens in the output and let's see what they mean so we have passed our input into a GPT model and we have got these outputs and let's decode these now so if you decode for the first batch the output the input is effort moves you and uh the output is armed H Netflix completely random output which does not make sense we want this output to be every effort moves you uh every effort moves you yeah so input so the target is every effort moves you sorry effort moves you correct this is the true value so we want the target to be effort moves you but the actual output which we have got is armed H Netflix so if you look at the first batch you can see that there's a huge difference between the targets and the outputs right which makes sense because well uh we have not yet optimized we have not even defined the loss function we have not try to minimize the loss function so ideally now what we'll do is that we'll Define a loss function between the targets and the outputs from both the batches so that the target output and the so that the target text and the output text will be as close as possible to each other great now that actually motivates us to go to our next section in today's lecture which is actually defining the loss between the targets and the output so now let let's see what's going on here so we have the inputs and this is the probability tensor which indicates what's the probability of the next token for every input token what I've marked in blue over here is the target indexes so if we want the actual answer to be that for every it should be effort for every effort it should be moves and for every effort moves it should be U the the correct answer of the indices which we want is 1 four and five these are the true values uh so the token IDs here are 3626 610 and 345 but uh in the sample code which I've shown over here we just has seven uh we just has seven vocabulary size the token IDs which are 1 four and five those are the correct ones so now one thing which I would like to mention is that let's say these indexes which are marked with the star right now these are the actual indexes which we want right but if you see the values corresponding to those those values are not the highest because our llm is not trained yet so what I'm going to do now is that I'm going to collect the probabilities at these indexes so I'm going to take this index I'm going to take this index and I'm going to take this index and I'm going to find the probabilities at these these indexes or these indices rather and I'm going to collect these together so for example for the first batch what I collect can be 144 and3 these are the set of probabilities and for the second batch I'll collect another set of three probabilities like this this is exactly what we are doing in the actual problem in the actual problem we know that the target indexes are these so what I'll do is that I look at my probability tensor and I'll get the probability which is corresponding to these indexes I know it will not be maximum because my llm is not optimized but I'll just write down these probabilities for now so uh I I have these Target indices and let me call them i11 i12 I13 for the first batch and i21 i22 and i23 for the second batch so what I'm uh so what I'll be doing now is that I'll be uh looking at batch one and I'll be looking at badge two and I'll find the value corresponding to these indices in the probabilities tensor so in this in in the probability tensor which look which will look something like this for the 50257 vocabulary size also I'll find the probabilities corresponding to i11 i12 and I13 for batch number one and I'll find the probabilities corresponding to i21 i22 and i23 for batch number two so for batch number one I'll aggregate these probabilities together and they look like this P11 p12 p13 for batch two I'll aggregate these probabilities and they look like p21 P22 and p23 remember these are the probabilities which are not maximum right now the goal of training is that to get all of these values as close to one as possible and why do we want all these values close to one as possible because then we'll make sure that the output indexes which have the maximum probabilities will be closer to i11 i12 I13 i21 i22 and i23 which are my targets so the goal of the llm Performing better now is reduced to this problem that I want these probabilities to be as close to one as possible I want all six probabilities to be as close to one why are there six probabilities because there are two batches each batch has three tokens so there are three prediction tasks so that's why there are three probabilities for the first batch three probabilities for the second batch so let me merge these probabilities together and then this will be P11 p12 p13 and this will be p21 P22 and p23 now what I want is I want all of these P11 p12 p13 p21 P22 and p23 I want all of these to be as close to one as possible how do I enforce this mathematically first let's see the workflow so we had the logit tensor we converted it into probabilities through soft Max and then we had the target probabilities tensor what this target probabilities means is that we have the IND indices corresponding to the Target values so we have these indexes i11 i12 I13 i21 i22 i23 these Target probabilities are just the merge merge Target probabilities which is P11 p12 dot dot dot up to p23 these are the six probabilities and we want all of these to be as close to one as possible to all of you students who have studied uh classification and the loss this problem would be familiar since we are dealing with probabilities it's natural that logarithms and cross entropy will come into the picture so instead of directly dealing with this number numbers it's much better mathematically and from an optimization P perspective to just take the logarithm of these values and it comes out to be this and then we take the average of all of these logarithm values and it comes out to be this and then we take the negative of this average so that's 10.77% Target probabilities and we find the mean of so we take the then the log of this so we take log P11 log p12 and the last is Log p23 and then what we do is that we find the mean of this mean and uh so then that will be Sigma which will be the summation of log 1 one summation of log P11 DOT log p23 divided 6 and then I'll just take the negative of this and that's called the negative log likelihood so then that will be negative of summation of log of P11 dot dot dot up till log of p23 divided 6 so now see we want uh we want P11 to be close to uh one we want p all the probabilities to be close to one so we essentially want this negative log likelihood to be as less as possible so this cross entropy loss we want to be as low as possible and as close to zero as possible so uh why are we taking the negative log likelihood the reason we take the negative log likelihood is because it just makes more sense if you if you don't take the negative value then uh the loss function would look reverse of this the loss function would look reverse of this and then we'll have to reframe the problem as trying to maximize the loss instead it just makes more intuitive sense if the negative looks like this and now the whole goal instead it makes more physical and intuitive sense if the log likelihood looks like this and now our whole goal is to bring down this loss as low as possible what will happen if this loss is brought down to let's say zero if the loss is brought down to zero which means that the mean which means this value is Clos to one which means that there is a higher chance that P11 p12 p13 up till p23 are equal to one that is exactly what we want the goal of training is to get these values as close to one as possible so that is the main uh main hope now so initially let's say when we start the training procedure our value uh the x value whose log we are going to take that will not be close to one it will be somewhere maybe here it will be somewhere 0 to one and somewhere here the goal is to go here slowly so that the mean of the probabilities will be equal to 1 okay so this is the negative log likelihood and it's also called as the uh it's also called as the cross entropy loss and uh we want to minimize this cross cross entropy loss as much as possible that will ensure that the indexes at which our tokens uh indexes at which we predict the next token that matches with the target indexes so then the output and the target indexes will match closely to each other and then we know that the large language model is actually doing a very good job so this is exactly what we are going to do right now in the code one thing which I would like to mention about the cross entropy loss is that the cross entropy loss essentially measures the difference between two probability distributions so here what we are actually doing is that we are just adding discrete probabilities but in a sense this can also be called as the categorical or I should call it the cross entropy loss so now let's see the sequence of steps which we are going to implement the logit tensor which is a tensor of probabilities uh has the shape of 2 into 3x 50257 why because it looks something like this so every effort moves and then we have 50257 uh so this is the first batch and this is the second batch what we are going to do is that we are going to flatten this so that these two first batch and second batch are merged together right so these are so this is my output tensor right now every effort moves you I really like it's a merging of these two batches and my target so my target is 2x3 why is it 2 by three because for the first batch this is the target index of the first input every this is the target index of every effort and this is the target IND index of every effort moves 107 is the target index of I so it will be really 588 is the target index of I really and 11311 is the target index of I really like so then chocolate then what we are going to do is we are going to flatten this out so then this will be 3626 610 uh 345 107 588 and 11311 now ideally what we need is that we are going to look at every year and we are going to look at the index corresponding to 3626 so let's say if this is the index corresponding to 3626 this will be P11 for effort we are going to look at the index corresponding to 610 let's say it's this one so that will be p12 similarly for like which is the last we going to look at the index corresponding to 1 311 so let's say this is this so this is p23 so what we'll get is that we we'll get P11 p12 p13 p21 P22 p23 we'll take the log of them we'll add take the mean and then do the negative this whole process is encapsulated in this just one line of code tor. nn. functional cross entropy logits so this is the logits flat I'm calling this logic flat tensor and the second argument here is the target flat what this uh this code will do is that tor. nl. function cross entropy logit flat comma targets flat first it will convert this logits tensor into a tensor of probabilities until now the logits is is not does not represent probabilities so it this function will apply soft Max to the Lo logits and then what it will do is that as I told you before it will find the negative log likelihood so first it will uh find the probabilities P11 p12 p13 p21 P22 and p23 in this logic flat tensor corresponding to the index indices in the Target flat tensor it will take the negative log and then it will find the mean this one line of code is going to do all of the steps for us that's why python is so powerful in one line of code we'll not only convert this Logics into uh a tensor of probabilities through soft Max but we will also get the negative log likelihood and that's the final loss function which we are looking for so this one line of code is going to give us the loss between the output and the Target and that's why it's a very important piece of code uh to do all of these steps in just one line of code I think it's pretty amazing but what I want you all to focus on is what we really are doing over here what we are essentially doing is that we have inputs and those inputs are every effort moves and I really like we are passing them through the GPT architecture and then we get this logits we get this logit tensor what we are going to do with this logit tensor is that we'll first merge them merge this tensor merge the batches and call it logits flat then what we'll do is that corresponding to every token we are going to look at the indexes which corresponds to the Target and then we are going to find the probabilities now in an Ideal World these probabilities will be close to one because that would mean that the output and the in uh Target will match but since the llm is not optimized these probabilities which correspond to the Target tensor indices will be probably be very small and our goal is to bring all of these probabilities close to one as possible so that's why we'll uh take the negative log of these probabilities and take the mean that's the cross entropy loss and through one line of code cross entropy tor. nn. functional doc cross entropy between these two tensors we are going to implement this exact same operation which I just described in words right now so you can even check this and let me type it torch.nn do functional cross entropy and you can check it finds the cross entropy loss between the input Logics and the target this is exactly um what we did right now okay now I'm going to implement this same thing in code we are going to find the cross entropy loss in code okay so for first what we are going to do is that uh we are going to find the P1 P2 P3 so we are going to find P11 p12 p13 and p21 P22 and p23 that's the first step so remember here in the sequence of steps over here first we find these Target probabilities right uh and to find these Target probabilities what we'll do is that we will uh we'll take the target indexes which are uh the indexes corresponding to the True Values and what we are going to do is that we are going to uh take the probas so let me see what the probab probas is so this probas is the output uh this probas is the output probability tensor and what we are going to do is that we are going to find the token probabilities corresponding to the Target indexes and to do that what we are going to do is that we are going to use this line of code and what this will just do is that it will take the target indices and it will index or it will look at this probas which is the tensor of output probabilities and then it will look for those particular indexes so basically what it will be doing is that for zero it that's P11 that will be this for one that will be P1 two and for two that will be p13 similarly when we look at the second batch over here um what we are doing over here is that uh that this is one which means that it's going to look at the second batch so one way to understand what's going on here is to look at the probas and try to see the dimensions so uh let me write it here again for reference just so that your understanding is clear so the probas is actually it will look something like this first let me write it for the first every effort moves right and then uh this is 50257 the number of columns and then second is I really like and the size of this so this whole thing is the probas tensor which is the probability tensor and the size of this is so we have two batches we have three tokens and 50257 so what we are essentially doing here is that uh let me scroll down below yeah what we are doing here is that in this line text idx is equal to zero which means that we are first looking at the first batch so we are looking at this batch and then what we are doing is that we are looking at row 0o row one and row two and from row 0o we'll get the um we'll get the so let's look at the Target answer we'll get the index corresponding we'll get the value corresponding to 3626 index from Row one we'll get the value corresponding to 61 0 and from row two we'll get the value corresponding to 345 now similarly here what we are doing here is that we are looking at so text idx equal to 1 which means we are looking at the second batch we are looking at the second batch and then uh row zero Row one and row two so then we'll look at the targets tensor again and for row zero we'll take the value corresponding to index 1107 for Row one we'll take the value corresponding to index 588 and for row two we'll take the value corresponding to index 11311 so that is essentially uh p21 P22 and p23 so these three values are the P11 p12 P2 p13 and these three values are p21 P22 and p23 the whole goal is to get these values as close to one as possible right then what we'll do in the Second Step as I told you uh over here once we get these P11 p12 p13 p21 P22 p23 we are going to merge these together right so that's what's written here we are going to concatenate these six values together and you can see that they appear like this the next step is that we are going to take the log uh actually after concatenation yeah we take the log over here and then we print these log values then we'll take the mean of these log values and then we are going to do the negative of the log likelihood uh so as I told you we can also not do the log likelihood and just do positive log likelihood but then that would mean maximizing the loss that does not make sense so in deep learning it's conventional to use negative log lik Le so that we minimize the loss right so this is now the negative log likelihood loss value which we ultimately need to minimize now as I told you there's a much simpler way of doing this using the torch.nn do functional cross entropy the first step of this is to flatten the logits and the targets and to use this one line of code so that's what I'm going to do now I'm going to take the logits which was earlier 2A 3A 5257 and I'm going to flatten it I'm going to flatten the these first two Dimensions together so that it's 6A 5257 this is exactly what we saw over here look at this it's 6A 5257 batch one and batch two are merged together then what I'm going to do is that I'm going to look at the Target stenor and I'm going to flatten it so if the target stenor is this two rows and three columns I'm going to merge these two rows into six values and then I'm going to just write one line of code this one line of code will first convert these Logics into a soft Max and then it will find the values corresponding to the Target indexes then it will take the log of these the mean and then the negative log likelihood all of this will be done and I will get my categorical cross entropy loss of 10 remember I want to take this loss as close to zero as possible that's the goal so when we train the large language models later the goal is to bring this categorical cross entropy loss to as low as possible before we end the lecture I want to show you one last thing there is another major of loss and uh that's like cross entropy itself but a minor modification and that's called as perplexity so perplexity actually measures how well the probability distribution predicted by by the model matches the actual distribution of words in the data set and so in that sense it is more interpretable and it's more interpretable way of understanding model uncertainty in predicting the next token and I'll show you why in a minute remember that lower per perplexity score also means better predictions so the formula for perplexity is just to find the exponent of the loss and surprisingly this simple uh modification leads to a lot more intuition so if the exponent of the loss in our case is 48725 it means that the model is roughly as uncertain as if it had to choose the next token randomly from about 48725 tokens in the vocabulary so the number of tokens in the vocabulary is 50 uh 257 I think now what this means is that if the input is uh every effort moves right every effort moves is the input currently our llm is at a stage that to predict the next token that's as as if we have to choose between 48725 tokens that's pretty pretty bad right it means that there is a lot of uncertainty in predicting the next token if the perplexity score was equal to two that's pretty good which means that we just need to predict between two tokens so that means our llm is very accurate but in this case the perplexity score is 48725 48725 which means that 4 48,000 tokens are kind of equally likely to become the next token which means that our model is not good at all uh you can see how it is more interpretable than getting a categorical cross entropy loss of equal to 10 when I get a categorical cross entropy loss of 10 I don't really know how it relates to my vocabulary size also but if you get a uh entropy or a perplexity score of 48725 you can kind of relate it to your vocabulary size and make interpretable predictions like these that the 48,000 next token 48,000 tokens in our vocabulary of 50,000 are equally likely to be the next token and that's pretty bad because our llm is not yet trained so this brings us to the end of today's lecture where basically what we did is we took at we uh we first started with the inputs then we started with the True Values which were known as the targets and then what what we did was we found the output values this was kind of a revision we got the Logics tensor we converted it into a tensor of probabilities and then we got these output tokens so then we Tred to find the loss between the targets and the output using the cross entropy loss and we ultimately saw that using one single line of code torge nn. functional. cross entropy we can find the loss between the loged sensor and the target tensor as the lecture concluded we even looked at another way of measuring loss which was called as perplexity which is much more intuitive and the way perplexity is calculated it just e rais to loss which is exponent of loss and I also mentioned the perplexity concept over here in our case the loss was 10.79 and the perplexity value was 48725 and this is usually more interpretable awesome in the next lecture what we are going to do is that we are going to take an actual data set from a book which is called the verdict and first we'll tokenize this data set we'll divide it into input output input output Pairs and then we are going to get the llm output and we are going to find the loss function or the loss value for this entire uh data set until now in this lecture we just looked at uh in the code we just looked at two sample inputs right we looked at um let me yeah we looked at these two inputs in the next lecture we'll be scaling this up and look at an entire text data set so next lecture will be a lot of fun and uh we'll run the entire architecture on this um Hands-On example you can replace this example with any book which you like Harry Potter book any other book so next lecture is going to be a lot of fun I hope you all are liking these lectures we have now started a new module which is on U the training large language model and we are slowly making a lot of progress on in building the large language models we have already finished stage one now we are on stage two and rapidly moving towards completion thanks a lot everyone and I look forward to seeing you in the next lecture"
}