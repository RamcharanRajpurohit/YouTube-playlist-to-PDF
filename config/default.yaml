# ── LLM Provider Settings ────────────────────────────────────────────
llm:
  # Primary provider used for chapter writing
  primary_provider: gemini

  gemini:
    model: gemini-2.0-flash
    max_output_tokens: 16384
    temperature: 0.3
    max_context_tokens: 1000000

# ── Chunking ─────────────────────────────────────────────────────────
chunking:
  # Target chunk size in tokens (well under model limits to leave room for prompt + output)
  target_chunk_tokens: 12000
  # Number of overlap sentences between consecutive chunks
  overlap_sentences: 3

# ── Transcript ───────────────────────────────────────────────────────
transcript:
  # Preferred languages for transcript (in order of priority)
  languages:
    - en
  # Directory to cache raw transcripts
  cache_dir: output/transcripts
  # Use an exported browser cookies.txt to avoid YouTube IP bans (optional, leave empty)
  cookies_file: ""
  # Number of seconds to delay between transcript API fetch calls
  delay_seconds: 2.0

# ── Processing ───────────────────────────────────────────────────────
processing:
  # Number of chapters to write in parallel per batch.
  # Free tier:  keep at 3-4  (RPM limited)
  # Paid tier:  can go up to 10
  parallel_chapters: 5
  filler_words:
    - "um"
    - "uh"
    - "you know"
    - "i mean"
    - "so yeah"

# ── Book Output ──────────────────────────────────────────────────────
output:
  manuscript_md: output/manuscript.md
  manuscript_pdf: output/manuscript.pdf
  chapters_dir: output/chapters

# ── Verification ─────────────────────────────────────────────────────
verification:
  enabled: false
  provider: gemini
