# ── LLM Provider Settings ────────────────────────────────────────────
llm:
  # Primary provider used for chapter writing
  primary_provider: gemini

  gemini:
    model: gemini-2.5-flash
    max_output_tokens: 65536
    temperature: 0.3
    max_context_tokens: 1000000

# ── Chunking ─────────────────────────────────────────────────────────
chunking:
  # Target chunk size in tokens (well under model limits to leave room for prompt + output)
  target_chunk_tokens: 12000
  # Number of overlap sentences between consecutive chunks
  overlap_sentences: 3

# ── Processing ───────────────────────────────────────────────────────
processing:
  # Number of chapters to write in parallel per batch.
  # Free tier:  keep at 3-4  (RPM limited)
  # Paid tier:  can go up to 10
  parallel_chapters: 5
  filler_words:
    - "um"
    - "uh"
    - "you know"
    - "i mean"
    - "so yeah"

# ── Book Output ──────────────────────────────────────────────────────
output:
  manuscript_md: output/manuscript.md
  manuscript_pdf: output/manuscript.pdf
  chapters_dir: output/chapters

# ── Verification ─────────────────────────────────────────────────────
verification:
  enabled: false
  provider: gemini
